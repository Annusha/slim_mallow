2019-01-08 10:13:48,442 - 10 - update_argpars.py - update - batch_size: 256
2019-01-08 10:13:48,442 - 10 - update_argpars.py - update - bg: False
2019-01-08 10:13:48,442 - 10 - update_argpars.py - update - bg_trh: 85
2019-01-08 10:13:48,442 - 10 - update_argpars.py - update - data: /media/data/kukleva/lab/Breakfast/feat/s1
2019-01-08 10:13:48,442 - 10 - update_argpars.py - update - data_type: 2
2019-01-08 10:13:48,442 - 10 - update_argpars.py - update - dataset: bf
2019-01-08 10:13:48,442 - 10 - update_argpars.py - update - dataset_root: /media/data/kukleva/lab/Breakfast
2019-01-08 10:13:48,442 - 10 - update_argpars.py - update - embed_dim: 30
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - epochs: 12
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - ext: txt
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - feature_dim: 64
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - full: True
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - gmm: 1
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - gmms: one
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - gt: /media/data/kukleva/lab/Breakfast/feat/s1/groundTruth
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - gt_training: False
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - log: DEBUG
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - log_str: slim.mallow._all_!bg_data2_bf_embed30_epochs12_full_gmm1_one_!gt_lr1e-10_!lr_ordering_reg0.1_!viterbi_!zeros_
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - lr: 1e-10
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - lr_adj: False
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - momentum: 0.9
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - num_workers: 4
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - ordering: True
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - prefix: slim.mallow.
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - reg_cov: 0.1
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - resume: False
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - resume_str: 
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - save_embed_feat: False
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - save_likelihood: False
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - save_model: False
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - seed: 0
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - subaction: all
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - vis: False
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - viterbi: False
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - weight_decay: 0.0001
2019-01-08 10:13:48,443 - 10 - update_argpars.py - update - zeros: False
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - batch_size: 256
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - bg: False
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - bg_trh: 85
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - data: /media/data/kukleva/lab/Breakfast/feat/s1
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - data_type: 2
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - dataset: bf
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - dataset_root: /media/data/kukleva/lab/Breakfast
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - embed_dim: 30
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - epochs: 12
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - ext: txt
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - feature_dim: 64
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - full: True
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - gmm: 1
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - gmms: one
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - gt: /media/data/kukleva/lab/Breakfast/feat/s1/groundTruth
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - gt_training: False
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - log: DEBUG
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - log_str: slim.mallow._coffee_!bg_data2_bf_embed30_epochs12_full_gmm1_one_!gt_lr1e-10_!lr_ordering_reg0.1_!viterbi_!zeros_
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - lr: 1e-10
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - lr_adj: False
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - momentum: 0.9
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - num_workers: 4
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - ordering: True
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - prefix: slim.mallow.
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - reg_cov: 0.1
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - resume: False
2019-01-08 10:13:48,444 - 10 - utils.py - update_opt_str - resume_str: 
2019-01-08 10:13:48,445 - 10 - utils.py - update_opt_str - save_embed_feat: False
2019-01-08 10:13:48,445 - 10 - utils.py - update_opt_str - save_likelihood: False
2019-01-08 10:13:48,445 - 10 - utils.py - update_opt_str - save_model: False
2019-01-08 10:13:48,445 - 10 - utils.py - update_opt_str - seed: 0
2019-01-08 10:13:48,445 - 10 - utils.py - update_opt_str - subaction: coffee
2019-01-08 10:13:48,445 - 10 - utils.py - update_opt_str - vis: False
2019-01-08 10:13:48,445 - 10 - utils.py - update_opt_str - viterbi: False
2019-01-08 10:13:48,445 - 10 - utils.py - update_opt_str - weight_decay: 0.0001
2019-01-08 10:13:48,445 - 10 - utils.py - update_opt_str - zeros: False
2019-01-08 10:13:48,481 - 10 - utils.py - wrap - <function GroundTruth.load_gt at 0x7f39322b36a8> took 35.783 ms ~ 0.001 min ~ 0.036 sec
2019-01-08 10:13:48,549 - 10 - corpus.py - __init__ - coffee  subactions: 6
2019-01-08 10:13:48,551 - 10 - corpus.py - _init_videos - .
2019-01-08 10:13:52,079 - 10 - corpus.py - _init_videos - gt statistic: Counter({5: 46874, 2: 27816, 6: 9038, 8: 6455, 7: 4419, 9: 3356})
2019-01-08 10:13:52,079 - 10 - corpus.py - _update_fg_mask - .
2019-01-08 10:13:52,089 - 10 - corpus.py - __init__ - min: -44.877960  max: 33.993587  avg: 0.042994
2019-01-08 10:13:52,089 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:13:52,132 - 10 - corpus.py - rho_sampling - ['64.1837', '100.9221', '929.0535', '90.9104', '17.1809']
2019-01-08 10:13:52,132 - 10 - pipeline.py - baseline - Iteration 0
2019-01-08 10:13:52,172 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:13:52,175 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:13:52,175 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 5', '2: 7', '3: 9', '4: 2', '5: 8']
2019-01-08 10:13:52,178 - 10 - accuracy_class.py - mof_val - frames true: 32694	frames overall : 97958
2019-01-08 10:13:52,178 - 10 - corpus.py - accuracy_corpus - Action: coffee
2019-01-08 10:13:52,178 - 10 - corpus.py - accuracy_corpus - MoF val: 0.33375528287633477
2019-01-08 10:13:52,178 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.0
2019-01-08 10:13:52,178 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:13:52,178 - 10 - accuracy_class.py - mof_classes - label 2: 0.350638  8105 / 23115
2019-01-08 10:13:52,178 - 10 - accuracy_class.py - mof_classes - label 5: 0.341497  13832 / 40504
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - mof_classes - label 6: 0.695081  3971 / 5713
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - mof_classes - label 7: 0.265671  1174 / 4419
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - mof_classes - label 8: 0.743498  4345 / 5844
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - mof_classes - label 9: 0.377533  1267 / 3356
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - mof_classes - average class mof: 0.396274
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - iou_classes - label 2: 0.258971  8105 / 31297
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - iou_classes - label 5: 0.321435  13832 / 43032
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - iou_classes - label 6: 0.218908  3971 / 18140
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - iou_classes - label 7: 0.059938  1174 / 19587
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - iou_classes - label 8: 0.244720  4345 / 17755
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - iou_classes - label 9: 0.068844  1267 / 18404
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - iou_classes - average IoU: 0.195469
2019-01-08 10:13:52,179 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.167545
2019-01-08 10:13:53,359 - 10 - f1_score.py - f1 - f1 score: 0.242657
2019-01-08 10:13:53,362 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1229.946 ms ~ 0.020 min ~ 1.230 sec
2019-01-08 10:13:53,362 - 10 - corpus.py - embedding_training - .
2019-01-08 10:13:53,362 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:13:53,362 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:13:53,362 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:13:53,939 - 10 - training_embed.py - training - create model
2019-01-08 10:13:54,767 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:13:54,768 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:13:55,222 - 10 - training_embed.py - training - Epoch: [0][100/383]	Time 0.002 (0.004)	Data 0.001 (0.001)	Loss 257.7668 (256.6514)	
2019-01-08 10:13:55,403 - 10 - training_embed.py - training - Epoch: [0][200/383]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 254.0910 (256.7857)	
2019-01-08 10:13:55,578 - 10 - training_embed.py - training - Epoch: [0][300/383]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 259.4865 (256.7885)	
2019-01-08 10:13:55,730 - 10 - training_embed.py - training - loss: 256.554489
2019-01-08 10:13:55,730 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:13:56,030 - 10 - training_embed.py - training - Epoch: [1][100/383]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 257.1506 (256.5833)	
2019-01-08 10:13:56,223 - 10 - training_embed.py - training - Epoch: [1][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2673 (256.6075)	
2019-01-08 10:13:56,407 - 10 - training_embed.py - training - Epoch: [1][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.8689 (256.6523)	
2019-01-08 10:13:56,554 - 10 - training_embed.py - training - loss: 256.543150
2019-01-08 10:13:56,554 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:13:56,843 - 10 - training_embed.py - training - Epoch: [2][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5149 (256.6424)	
2019-01-08 10:13:57,035 - 10 - training_embed.py - training - Epoch: [2][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.7109 (256.6652)	
2019-01-08 10:13:57,219 - 10 - training_embed.py - training - Epoch: [2][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.1306 (256.6882)	
2019-01-08 10:13:57,378 - 10 - training_embed.py - training - loss: 256.529009
2019-01-08 10:13:57,379 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:13:57,667 - 10 - training_embed.py - training - Epoch: [3][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.3894 (256.5680)	
2019-01-08 10:13:57,849 - 10 - training_embed.py - training - Epoch: [3][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 259.0860 (256.6146)	
2019-01-08 10:13:58,034 - 10 - training_embed.py - training - Epoch: [3][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.3529 (256.6089)	
2019-01-08 10:13:58,178 - 10 - training_embed.py - training - loss: 256.519457
2019-01-08 10:13:58,178 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:13:58,468 - 10 - training_embed.py - training - Epoch: [4][100/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 255.5330 (256.5166)	
2019-01-08 10:13:58,649 - 10 - training_embed.py - training - Epoch: [4][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.2155 (256.7044)	
2019-01-08 10:13:58,823 - 10 - training_embed.py - training - Epoch: [4][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.1767 (256.7084)	
2019-01-08 10:13:58,970 - 10 - training_embed.py - training - loss: 256.506724
2019-01-08 10:13:58,970 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:13:59,261 - 10 - training_embed.py - training - Epoch: [5][100/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 255.4481 (256.6364)	
2019-01-08 10:13:59,436 - 10 - training_embed.py - training - Epoch: [5][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.5562 (256.6566)	
2019-01-08 10:13:59,624 - 10 - training_embed.py - training - Epoch: [5][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.5092 (256.6333)	
2019-01-08 10:13:59,766 - 10 - training_embed.py - training - loss: 256.493432
2019-01-08 10:13:59,766 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:14:00,069 - 10 - training_embed.py - training - Epoch: [6][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.3463 (256.7031)	
2019-01-08 10:14:00,253 - 10 - training_embed.py - training - Epoch: [6][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.4869 (256.7181)	
2019-01-08 10:14:00,435 - 10 - training_embed.py - training - Epoch: [6][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5363 (256.6934)	
2019-01-08 10:14:00,594 - 10 - training_embed.py - training - loss: 256.483452
2019-01-08 10:14:00,594 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:14:00,886 - 10 - training_embed.py - training - Epoch: [7][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.7028 (256.7042)	
2019-01-08 10:14:01,074 - 10 - training_embed.py - training - Epoch: [7][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.0065 (256.6233)	
2019-01-08 10:14:01,251 - 10 - training_embed.py - training - Epoch: [7][300/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 254.7411 (256.5850)	
2019-01-08 10:14:01,411 - 10 - training_embed.py - training - loss: 256.472721
2019-01-08 10:14:01,411 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:14:01,693 - 10 - training_embed.py - training - Epoch: [8][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.0396 (256.5162)	
2019-01-08 10:14:01,867 - 10 - training_embed.py - training - Epoch: [8][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.8521 (256.5527)	
2019-01-08 10:14:02,061 - 10 - training_embed.py - training - Epoch: [8][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.3915 (256.5909)	
2019-01-08 10:14:02,225 - 10 - training_embed.py - training - loss: 256.460534
2019-01-08 10:14:02,225 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:14:02,526 - 10 - training_embed.py - training - Epoch: [9][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.9100 (256.5182)	
2019-01-08 10:14:02,699 - 10 - training_embed.py - training - Epoch: [9][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.4988 (256.5191)	
2019-01-08 10:14:02,884 - 10 - training_embed.py - training - Epoch: [9][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.3639 (256.6152)	
2019-01-08 10:14:03,037 - 10 - training_embed.py - training - loss: 256.447360
2019-01-08 10:14:03,038 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:14:03,342 - 10 - training_embed.py - training - Epoch: [10][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.7166 (256.5213)	
2019-01-08 10:14:03,519 - 10 - training_embed.py - training - Epoch: [10][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.3950 (256.4548)	
2019-01-08 10:14:03,703 - 10 - training_embed.py - training - Epoch: [10][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.1164 (256.5738)	
2019-01-08 10:14:03,847 - 10 - training_embed.py - training - loss: 256.437155
2019-01-08 10:14:03,847 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:14:04,142 - 10 - training_embed.py - training - Epoch: [11][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.7071 (256.5488)	
2019-01-08 10:14:04,331 - 10 - training_embed.py - training - Epoch: [11][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.9700 (256.4840)	
2019-01-08 10:14:04,515 - 10 - training_embed.py - training - Epoch: [11][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.3718 (256.5294)	
2019-01-08 10:14:04,659 - 10 - training_embed.py - training - loss: 256.424164
2019-01-08 10:14:04,672 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:14:04,816 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 143.559 ms ~ 0.002 min ~ 0.144 sec
2019-01-08 10:14:05,025 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 352.393 ms ~ 0.006 min ~ 0.352 sec
2019-01-08 10:14:05,025 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:14:05,025 - 10 - corpus.py - subactivity_sampler - 0 / 167
2019-01-08 10:14:05,025 - 10 - corpus.py - subactivity_sampler - [16398. 16360. 16342. 16315. 16287. 16256.]
2019-01-08 10:14:08,355 - 10 - corpus.py - subactivity_sampler - 20 / 167
2019-01-08 10:14:08,355 - 10 - corpus.py - subactivity_sampler - [16679. 16076. 16268. 16238. 16440. 16257.]
2019-01-08 10:14:11,738 - 10 - corpus.py - subactivity_sampler - 40 / 167
2019-01-08 10:14:11,738 - 10 - corpus.py - subactivity_sampler - [17014. 15829. 16153. 15985. 16789. 16188.]
2019-01-08 10:14:15,676 - 10 - corpus.py - subactivity_sampler - 60 / 167
2019-01-08 10:14:15,676 - 10 - corpus.py - subactivity_sampler - [17603. 15450. 16121. 15602. 17000. 16182.]
2019-01-08 10:14:18,449 - 10 - corpus.py - subactivity_sampler - 80 / 167
2019-01-08 10:14:18,449 - 10 - corpus.py - subactivity_sampler - [17916. 14929. 16228. 15474. 17533. 15878.]
2019-01-08 10:14:22,141 - 10 - corpus.py - subactivity_sampler - 100 / 167
2019-01-08 10:14:22,141 - 10 - corpus.py - subactivity_sampler - [18211. 14411. 16597. 15106. 17813. 15820.]
2019-01-08 10:14:25,139 - 10 - corpus.py - subactivity_sampler - 120 / 167
2019-01-08 10:14:25,139 - 10 - corpus.py - subactivity_sampler - [18791. 13741. 16491. 14412. 18633. 15890.]
2019-01-08 10:14:28,413 - 10 - corpus.py - subactivity_sampler - 140 / 167
2019-01-08 10:14:28,414 - 10 - corpus.py - subactivity_sampler - [19445. 12786. 16916. 13622. 19657. 15532.]
2019-01-08 10:14:31,953 - 10 - corpus.py - subactivity_sampler - 160 / 167
2019-01-08 10:14:31,953 - 10 - corpus.py - subactivity_sampler - [20079. 11948. 17164. 12796. 20535. 15436.]
2019-01-08 10:14:32,613 - 10 - corpus.py - subactivity_sampler - [20383. 11662. 17288. 12550. 20765. 15310.]
2019-01-08 10:14:32,613 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 27587.999 ms ~ 0.460 min ~ 27.588 sec
2019-01-08 10:14:32,613 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:14:33,138 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:14:33,138 - 10 - corpus.py - ordering_sampler - inv_count_vec: [0. 0. 0. 0. 2.]
2019-01-08 10:14:33,138 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:14:33,152 - 10 - corpus.py - rho_sampling - ['65.3203', '47.0072', '928.1069', '7.5322', '55.2453']
2019-01-08 10:14:33,152 - 10 - pipeline.py - baseline - Iteration 1
2019-01-08 10:14:33,179 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:14:33,181 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:14:33,181 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 9', '2: 5', '3: 7', '4: 2', '5: 8']
2019-01-08 10:14:33,183 - 10 - accuracy_class.py - mof_val - frames true: 34505	frames overall : 97958
2019-01-08 10:14:33,184 - 10 - corpus.py - accuracy_corpus - Action: coffee
2019-01-08 10:14:33,184 - 10 - corpus.py - accuracy_corpus - MoF val: 0.35224279793380836
2019-01-08 10:14:33,184 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3304885767369689
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - mof_classes - label 2: 0.477050  11027 / 23115
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - mof_classes - label 5: 0.328017  13286 / 40504
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - mof_classes - label 6: 0.844915  4827 / 5713
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - mof_classes - label 7: 0.272007  1202 / 4419
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - mof_classes - label 8: 0.712355  4163 / 5844
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - mof_classes - label 9: 0.000000  0 / 3356
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - mof_classes - average class mof: 0.376335
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - iou_classes - label 2: 0.335647  11027 / 32853
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - iou_classes - label 5: 0.298522  13286 / 44506
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - iou_classes - label 6: 0.226950  4827 / 21269
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - iou_classes - label 7: 0.076235  1202 / 15767
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - iou_classes - label 8: 0.245012  4163 / 16991
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - iou_classes - label 9: 0.000000  0 / 15018
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - iou_classes - average IoU: 0.197061
2019-01-08 10:14:33,184 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.168909
2019-01-08 10:14:34,335 - 10 - f1_score.py - f1 - f1 score: 0.250630
2019-01-08 10:14:34,338 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1186.044 ms ~ 0.020 min ~ 1.186 sec
2019-01-08 10:14:34,338 - 10 - corpus.py - embedding_training - .
2019-01-08 10:14:34,338 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:14:34,338 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:14:34,338 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:14:34,915 - 10 - training_embed.py - training - create model
2019-01-08 10:14:34,915 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:14:34,916 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:14:35,219 - 10 - training_embed.py - training - Epoch: [0][100/383]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 258.8199 (256.2470)	
2019-01-08 10:14:35,421 - 10 - training_embed.py - training - Epoch: [0][200/383]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.9568 (256.3104)	
2019-01-08 10:14:35,612 - 10 - training_embed.py - training - Epoch: [0][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.8765 (256.2730)	
2019-01-08 10:14:35,774 - 10 - training_embed.py - training - loss: 256.057391
2019-01-08 10:14:35,775 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:14:36,125 - 10 - training_embed.py - training - Epoch: [1][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.0175 (256.1166)	
2019-01-08 10:14:36,308 - 10 - training_embed.py - training - Epoch: [1][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5395 (256.1484)	
2019-01-08 10:14:36,502 - 10 - training_embed.py - training - Epoch: [1][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.3696 (256.1297)	
2019-01-08 10:14:36,655 - 10 - training_embed.py - training - loss: 256.032902
2019-01-08 10:14:36,655 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:14:36,988 - 10 - training_embed.py - training - Epoch: [2][100/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 255.1688 (256.0588)	
2019-01-08 10:14:37,188 - 10 - training_embed.py - training - Epoch: [2][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.4522 (256.0936)	
2019-01-08 10:14:37,384 - 10 - training_embed.py - training - Epoch: [2][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.7605 (256.1632)	
2019-01-08 10:14:37,532 - 10 - training_embed.py - training - loss: 256.003667
2019-01-08 10:14:37,532 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:14:37,848 - 10 - training_embed.py - training - Epoch: [3][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.9246 (256.0306)	
2019-01-08 10:14:38,043 - 10 - training_embed.py - training - Epoch: [3][200/383]	Time 0.002 (0.002)	Data 0.002 (0.001)	Loss 258.7061 (256.1413)	
2019-01-08 10:14:38,228 - 10 - training_embed.py - training - Epoch: [3][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.3428 (256.1231)	
2019-01-08 10:14:38,389 - 10 - training_embed.py - training - loss: 255.980776
2019-01-08 10:14:38,389 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:14:38,699 - 10 - training_embed.py - training - Epoch: [4][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.2097 (256.1923)	
2019-01-08 10:14:38,900 - 10 - training_embed.py - training - Epoch: [4][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.7916 (256.1738)	
2019-01-08 10:14:39,090 - 10 - training_embed.py - training - Epoch: [4][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.8690 (256.2030)	
2019-01-08 10:14:39,247 - 10 - training_embed.py - training - loss: 255.952604
2019-01-08 10:14:39,247 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:14:39,552 - 10 - training_embed.py - training - Epoch: [5][100/383]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 257.2796 (256.2378)	
2019-01-08 10:14:39,739 - 10 - training_embed.py - training - Epoch: [5][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.1121 (256.1828)	
2019-01-08 10:14:39,952 - 10 - training_embed.py - training - Epoch: [5][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.8457 (256.0708)	
2019-01-08 10:14:40,106 - 10 - training_embed.py - training - loss: 255.924431
2019-01-08 10:14:40,107 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:14:40,432 - 10 - training_embed.py - training - Epoch: [6][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.2114 (256.0715)	
2019-01-08 10:14:40,638 - 10 - training_embed.py - training - Epoch: [6][200/383]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 258.1287 (256.0691)	
2019-01-08 10:14:40,830 - 10 - training_embed.py - training - Epoch: [6][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.2054 (256.1212)	
2019-01-08 10:14:40,993 - 10 - training_embed.py - training - loss: 255.899007
2019-01-08 10:14:40,993 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:14:41,331 - 10 - training_embed.py - training - Epoch: [7][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.1524 (256.1091)	
2019-01-08 10:14:41,541 - 10 - training_embed.py - training - Epoch: [7][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.4135 (256.0288)	
2019-01-08 10:14:41,737 - 10 - training_embed.py - training - Epoch: [7][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.9862 (255.9740)	
2019-01-08 10:14:41,897 - 10 - training_embed.py - training - loss: 255.874356
2019-01-08 10:14:41,897 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:14:42,210 - 10 - training_embed.py - training - Epoch: [8][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.5860 (255.9707)	
2019-01-08 10:14:42,400 - 10 - training_embed.py - training - Epoch: [8][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.5742 (255.9221)	
2019-01-08 10:14:42,604 - 10 - training_embed.py - training - Epoch: [8][300/383]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 255.7916 (255.9700)	
2019-01-08 10:14:42,758 - 10 - training_embed.py - training - loss: 255.846707
2019-01-08 10:14:42,759 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:14:43,088 - 10 - training_embed.py - training - Epoch: [9][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5997 (255.9194)	
2019-01-08 10:14:43,280 - 10 - training_embed.py - training - Epoch: [9][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.1340 (255.9618)	
2019-01-08 10:14:43,474 - 10 - training_embed.py - training - Epoch: [9][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.8698 (255.9621)	
2019-01-08 10:14:43,625 - 10 - training_embed.py - training - loss: 255.819866
2019-01-08 10:14:43,625 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:14:43,957 - 10 - training_embed.py - training - Epoch: [10][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.2855 (255.9193)	
2019-01-08 10:14:44,137 - 10 - training_embed.py - training - Epoch: [10][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.1251 (255.8901)	
2019-01-08 10:14:44,338 - 10 - training_embed.py - training - Epoch: [10][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.1523 (255.9232)	
2019-01-08 10:14:44,493 - 10 - training_embed.py - training - loss: 255.792257
2019-01-08 10:14:44,493 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:14:44,776 - 10 - training_embed.py - training - Epoch: [11][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.5860 (255.7970)	
2019-01-08 10:14:44,969 - 10 - training_embed.py - training - Epoch: [11][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.9744 (255.8538)	
2019-01-08 10:14:45,157 - 10 - training_embed.py - training - Epoch: [11][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.4249 (255.9080)	
2019-01-08 10:14:45,312 - 10 - training_embed.py - training - loss: 255.765344
2019-01-08 10:14:45,329 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:14:45,495 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 165.746 ms ~ 0.003 min ~ 0.166 sec
2019-01-08 10:14:45,704 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 374.831 ms ~ 0.006 min ~ 0.375 sec
2019-01-08 10:14:45,704 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:14:45,704 - 10 - corpus.py - subactivity_sampler - 0 / 167
2019-01-08 10:14:45,705 - 10 - corpus.py - subactivity_sampler - [20383. 11662. 17288. 12550. 20765. 15310.]
2019-01-08 10:14:49,120 - 10 - corpus.py - subactivity_sampler - 20 / 167
2019-01-08 10:14:49,120 - 10 - corpus.py - subactivity_sampler - [20575. 11387. 17261. 12441. 21308. 14986.]
2019-01-08 10:14:52,581 - 10 - corpus.py - subactivity_sampler - 40 / 167
2019-01-08 10:14:52,582 - 10 - corpus.py - subactivity_sampler - [20814. 11012. 17381. 12076. 22119. 14556.]
2019-01-08 10:14:56,619 - 10 - corpus.py - subactivity_sampler - 60 / 167
2019-01-08 10:14:56,619 - 10 - corpus.py - subactivity_sampler - [21011. 10823. 17318. 11788. 22730. 14288.]
2019-01-08 10:14:59,451 - 10 - corpus.py - subactivity_sampler - 80 / 167
2019-01-08 10:14:59,451 - 10 - corpus.py - subactivity_sampler - [21097. 10582. 17612. 11227. 23282. 14158.]
2019-01-08 10:15:03,213 - 10 - corpus.py - subactivity_sampler - 100 / 167
2019-01-08 10:15:03,213 - 10 - corpus.py - subactivity_sampler - [21110. 10341. 17916. 10906. 24070. 13615.]
2019-01-08 10:15:06,263 - 10 - corpus.py - subactivity_sampler - 120 / 167
2019-01-08 10:15:06,263 - 10 - corpus.py - subactivity_sampler - [21254. 10157. 17930. 10484. 24769. 13364.]
2019-01-08 10:15:09,627 - 10 - corpus.py - subactivity_sampler - 140 / 167
2019-01-08 10:15:09,627 - 10 - corpus.py - subactivity_sampler - [21311. 10070. 18267.  9841. 25438. 13031.]
2019-01-08 10:15:13,264 - 10 - corpus.py - subactivity_sampler - 160 / 167
2019-01-08 10:15:13,264 - 10 - corpus.py - subactivity_sampler - [21601.  9899. 18196.  9429. 26041. 12792.]
2019-01-08 10:15:13,948 - 10 - corpus.py - subactivity_sampler - [21596. 10003. 18108.  9365. 26114. 12772.]
2019-01-08 10:15:13,948 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 28244.182 ms ~ 0.471 min ~ 28.244 sec
2019-01-08 10:15:13,948 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:15:14,493 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:15:14,493 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 0.  2.  0. 26.  1.]
2019-01-08 10:15:14,493 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:15:14,510 - 10 - corpus.py - rho_sampling - ['62.9502', '17.2531', '928.5380', '235.2574', '4.6826']
2019-01-08 10:15:14,510 - 10 - pipeline.py - baseline - Iteration 2
2019-01-08 10:15:14,537 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:15:14,539 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:15:14,539 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 9', '2: 5', '3: 7', '4: 2', '5: 8']
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - mof_val - frames true: 36865	frames overall : 97958
2019-01-08 10:15:14,542 - 10 - corpus.py - accuracy_corpus - Action: coffee
2019-01-08 10:15:14,542 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3763347557116315
2019-01-08 10:15:14,542 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3763347557116315
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - mof_classes - label 2: 0.581268  13436 / 23115
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - mof_classes - label 5: 0.342583  13876 / 40504
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - mof_classes - label 6: 0.856818  4895 / 5713
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - mof_classes - label 7: 0.232406  1027 / 4419
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - mof_classes - label 8: 0.621321  3631 / 5844
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - mof_classes - label 9: 0.000000  0 / 3356
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - mof_classes - average class mof: 0.376342
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - iou_classes - label 2: 0.375381  13436 / 35793
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - iou_classes - label 5: 0.310175  13876 / 44736
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - iou_classes - label 6: 0.218390  4895 / 22414
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - iou_classes - label 7: 0.080505  1027 / 12757
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - iou_classes - label 8: 0.242309  3631 / 14985
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - iou_classes - label 9: 0.000000  0 / 13359
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - iou_classes - average IoU: 0.204460
2019-01-08 10:15:14,542 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.175251
2019-01-08 10:15:15,703 - 10 - f1_score.py - f1 - f1 score: 0.262423
2019-01-08 10:15:15,705 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1195.153 ms ~ 0.020 min ~ 1.195 sec
2019-01-08 10:15:15,705 - 10 - corpus.py - embedding_training - .
2019-01-08 10:15:15,705 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:15:15,705 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:15:15,705 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:15:16,271 - 10 - training_embed.py - training - create model
2019-01-08 10:15:16,272 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:15:16,272 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:15:16,578 - 10 - training_embed.py - training - Epoch: [0][100/383]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 257.4086 (255.5403)	
2019-01-08 10:15:16,766 - 10 - training_embed.py - training - Epoch: [0][200/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 254.3621 (255.6299)	
2019-01-08 10:15:16,962 - 10 - training_embed.py - training - Epoch: [0][300/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 257.0050 (255.5880)	
2019-01-08 10:15:17,121 - 10 - training_embed.py - training - loss: 255.359394
2019-01-08 10:15:17,121 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:15:17,432 - 10 - training_embed.py - training - Epoch: [1][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5839 (255.3902)	
2019-01-08 10:15:17,622 - 10 - training_embed.py - training - Epoch: [1][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.4779 (255.4798)	
2019-01-08 10:15:17,807 - 10 - training_embed.py - training - Epoch: [1][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2380 (255.4209)	
2019-01-08 10:15:17,959 - 10 - training_embed.py - training - loss: 255.322775
2019-01-08 10:15:17,959 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:15:18,289 - 10 - training_embed.py - training - Epoch: [2][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.0542 (255.4437)	
2019-01-08 10:15:18,478 - 10 - training_embed.py - training - Epoch: [2][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.5726 (255.4062)	
2019-01-08 10:15:18,675 - 10 - training_embed.py - training - Epoch: [2][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.8384 (255.4610)	
2019-01-08 10:15:18,836 - 10 - training_embed.py - training - loss: 255.283098
2019-01-08 10:15:18,836 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:15:19,140 - 10 - training_embed.py - training - Epoch: [3][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.4168 (255.4037)	
2019-01-08 10:15:19,331 - 10 - training_embed.py - training - Epoch: [3][200/383]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 256.9254 (255.4701)	
2019-01-08 10:15:19,526 - 10 - training_embed.py - training - Epoch: [3][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2546 (255.3810)	
2019-01-08 10:15:19,695 - 10 - training_embed.py - training - loss: 255.249359
2019-01-08 10:15:19,695 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:15:20,008 - 10 - training_embed.py - training - Epoch: [4][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.0895 (255.5359)	
2019-01-08 10:15:20,193 - 10 - training_embed.py - training - Epoch: [4][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.3042 (255.4215)	
2019-01-08 10:15:20,389 - 10 - training_embed.py - training - Epoch: [4][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.0814 (255.4584)	
2019-01-08 10:15:20,549 - 10 - training_embed.py - training - loss: 255.208771
2019-01-08 10:15:20,550 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:15:20,855 - 10 - training_embed.py - training - Epoch: [5][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.4870 (255.5281)	
2019-01-08 10:15:21,040 - 10 - training_embed.py - training - Epoch: [5][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.1716 (255.3472)	
2019-01-08 10:15:21,232 - 10 - training_embed.py - training - Epoch: [5][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.4823 (255.2885)	
2019-01-08 10:15:21,389 - 10 - training_embed.py - training - loss: 255.169964
2019-01-08 10:15:21,389 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:15:21,704 - 10 - training_embed.py - training - Epoch: [6][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.7456 (255.4462)	
2019-01-08 10:15:21,902 - 10 - training_embed.py - training - Epoch: [6][200/383]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 258.3918 (255.3728)	
2019-01-08 10:15:22,095 - 10 - training_embed.py - training - Epoch: [6][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.4093 (255.3298)	
2019-01-08 10:15:22,248 - 10 - training_embed.py - training - loss: 255.132116
2019-01-08 10:15:22,248 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:15:22,549 - 10 - training_embed.py - training - Epoch: [7][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.8105 (255.3421)	
2019-01-08 10:15:22,744 - 10 - training_embed.py - training - Epoch: [7][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.9115 (255.2015)	
2019-01-08 10:15:22,941 - 10 - training_embed.py - training - Epoch: [7][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.2500 (255.2082)	
2019-01-08 10:15:23,098 - 10 - training_embed.py - training - loss: 255.098826
2019-01-08 10:15:23,098 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:15:23,381 - 10 - training_embed.py - training - Epoch: [8][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.1366 (255.1872)	
2019-01-08 10:15:23,563 - 10 - training_embed.py - training - Epoch: [8][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.1623 (255.1805)	
2019-01-08 10:15:23,763 - 10 - training_embed.py - training - Epoch: [8][300/383]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 254.2803 (255.1810)	
2019-01-08 10:15:23,922 - 10 - training_embed.py - training - loss: 255.059070
2019-01-08 10:15:23,923 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:15:24,224 - 10 - training_embed.py - training - Epoch: [9][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5049 (255.1216)	
2019-01-08 10:15:24,417 - 10 - training_embed.py - training - Epoch: [9][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.2598 (255.1274)	
2019-01-08 10:15:24,603 - 10 - training_embed.py - training - Epoch: [9][300/383]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 252.3801 (255.1625)	
2019-01-08 10:15:24,749 - 10 - training_embed.py - training - loss: 255.021104
2019-01-08 10:15:24,749 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:15:25,067 - 10 - training_embed.py - training - Epoch: [10][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.0460 (255.3028)	
2019-01-08 10:15:25,248 - 10 - training_embed.py - training - Epoch: [10][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.3387 (255.2302)	
2019-01-08 10:15:25,447 - 10 - training_embed.py - training - Epoch: [10][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2449 (255.1605)	
2019-01-08 10:15:25,601 - 10 - training_embed.py - training - loss: 254.982855
2019-01-08 10:15:25,601 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:15:25,925 - 10 - training_embed.py - training - Epoch: [11][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.6321 (255.1186)	
2019-01-08 10:15:26,137 - 10 - training_embed.py - training - Epoch: [11][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.7113 (255.0813)	
2019-01-08 10:15:26,323 - 10 - training_embed.py - training - Epoch: [11][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.9643 (255.0746)	
2019-01-08 10:15:26,483 - 10 - training_embed.py - training - loss: 254.944903
2019-01-08 10:15:26,500 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:15:26,655 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 154.919 ms ~ 0.003 min ~ 0.155 sec
2019-01-08 10:15:26,869 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 368.795 ms ~ 0.006 min ~ 0.369 sec
2019-01-08 10:15:26,869 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:15:26,869 - 10 - corpus.py - subactivity_sampler - 0 / 167
2019-01-08 10:15:26,870 - 10 - corpus.py - subactivity_sampler - [21596. 10003. 18108.  9365. 26114. 12772.]
2019-01-08 10:15:30,249 - 10 - corpus.py - subactivity_sampler - 20 / 167
2019-01-08 10:15:30,249 - 10 - corpus.py - subactivity_sampler - [21738.  9781. 18146.  8927. 26937. 12429.]
2019-01-08 10:15:33,659 - 10 - corpus.py - subactivity_sampler - 40 / 167
2019-01-08 10:15:33,659 - 10 - corpus.py - subactivity_sampler - [22109.  9631. 18013.  8347. 27713. 12145.]
2019-01-08 10:15:37,648 - 10 - corpus.py - subactivity_sampler - 60 / 167
2019-01-08 10:15:37,648 - 10 - corpus.py - subactivity_sampler - [22217.  9309. 18020.  7988. 28692. 11732.]
2019-01-08 10:15:40,444 - 10 - corpus.py - subactivity_sampler - 80 / 167
2019-01-08 10:15:40,444 - 10 - corpus.py - subactivity_sampler - [22350.  9132. 18062.  7539. 29261. 11614.]
2019-01-08 10:15:44,155 - 10 - corpus.py - subactivity_sampler - 100 / 167
2019-01-08 10:15:44,156 - 10 - corpus.py - subactivity_sampler - [22373.  8964. 18253.  7318. 29602. 11448.]
2019-01-08 10:15:47,189 - 10 - corpus.py - subactivity_sampler - 120 / 167
2019-01-08 10:15:47,189 - 10 - corpus.py - subactivity_sampler - [22355.  8857. 18135.  6923. 30611. 11077.]
2019-01-08 10:15:50,507 - 10 - corpus.py - subactivity_sampler - 140 / 167
2019-01-08 10:15:50,507 - 10 - corpus.py - subactivity_sampler - [22481.  8680. 17999.  6614. 31401. 10783.]
2019-01-08 10:15:54,100 - 10 - corpus.py - subactivity_sampler - 160 / 167
2019-01-08 10:15:54,100 - 10 - corpus.py - subactivity_sampler - [22535.  8625. 18003.  6325. 32015. 10455.]
2019-01-08 10:15:54,775 - 10 - corpus.py - subactivity_sampler - [22538.  8604. 18023.  6290. 32122. 10381.]
2019-01-08 10:15:54,775 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 27905.662 ms ~ 0.465 min ~ 27.906 sec
2019-01-08 10:15:54,775 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:15:55,302 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:15:55,302 - 10 - corpus.py - ordering_sampler - inv_count_vec: [2. 5. 0. 0. 4.]
2019-01-08 10:15:55,302 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:15:55,317 - 10 - corpus.py - rho_sampling - ['59.2343', '16.3042', '928.4017', '90.0641', '5.4383']
2019-01-08 10:15:55,317 - 10 - pipeline.py - baseline - Iteration 3
2019-01-08 10:15:55,343 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:15:55,345 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:15:55,345 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 9', '2: 5', '3: 7', '4: 2', '5: 8']
2019-01-08 10:15:55,348 - 10 - accuracy_class.py - mof_val - frames true: 38687	frames overall : 97958
2019-01-08 10:15:55,348 - 10 - corpus.py - accuracy_corpus - Action: coffee
2019-01-08 10:15:55,348 - 10 - corpus.py - accuracy_corpus - MoF val: 0.39493456379264585
2019-01-08 10:15:55,348 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.39493456379264585
2019-01-08 10:15:55,348 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:15:55,348 - 10 - accuracy_class.py - mof_classes - label 2: 0.685702  15850 / 23115
2019-01-08 10:15:55,348 - 10 - accuracy_class.py - mof_classes - label 5: 0.348114  14100 / 40504
2019-01-08 10:15:55,348 - 10 - accuracy_class.py - mof_classes - label 6: 0.870296  4972 / 5713
2019-01-08 10:15:55,348 - 10 - accuracy_class.py - mof_classes - label 7: 0.181942  804 / 4419
2019-01-08 10:15:55,348 - 10 - accuracy_class.py - mof_classes - label 8: 0.506674  2961 / 5844
2019-01-08 10:15:55,349 - 10 - accuracy_class.py - mof_classes - label 9: 0.000000  0 / 3356
2019-01-08 10:15:55,349 - 10 - accuracy_class.py - mof_classes - average class mof: 0.370390
2019-01-08 10:15:55,349 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:15:55,349 - 10 - accuracy_class.py - iou_classes - label 2: 0.402417  15850 / 39387
2019-01-08 10:15:55,349 - 10 - accuracy_class.py - iou_classes - label 5: 0.317375  14100 / 44427
2019-01-08 10:15:55,349 - 10 - accuracy_class.py - iou_classes - label 6: 0.213583  4972 / 23279
2019-01-08 10:15:55,349 - 10 - accuracy_class.py - iou_classes - label 7: 0.081171  804 / 9905
2019-01-08 10:15:55,349 - 10 - accuracy_class.py - iou_classes - label 8: 0.223236  2961 / 13264
2019-01-08 10:15:55,349 - 10 - accuracy_class.py - iou_classes - label 9: 0.000000  0 / 11960
2019-01-08 10:15:55,349 - 10 - accuracy_class.py - iou_classes - average IoU: 0.206297
2019-01-08 10:15:55,349 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.176826
2019-01-08 10:15:56,517 - 10 - f1_score.py - f1 - f1 score: 0.272032
2019-01-08 10:15:56,519 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1202.137 ms ~ 0.020 min ~ 1.202 sec
2019-01-08 10:15:56,519 - 10 - corpus.py - embedding_training - .
2019-01-08 10:15:56,519 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:15:56,519 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:15:56,519 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:15:57,077 - 10 - training_embed.py - training - create model
2019-01-08 10:15:57,078 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:15:57,078 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:15:57,391 - 10 - training_embed.py - training - Epoch: [0][100/383]	Time 0.001 (0.003)	Data 0.000 (0.002)	Loss 257.5143 (255.0213)	
2019-01-08 10:15:57,584 - 10 - training_embed.py - training - Epoch: [0][200/383]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.5253 (255.1490)	
2019-01-08 10:15:57,781 - 10 - training_embed.py - training - Epoch: [0][300/383]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 254.2503 (255.0722)	
2019-01-08 10:15:57,950 - 10 - training_embed.py - training - loss: 254.847939
2019-01-08 10:15:57,950 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:15:58,274 - 10 - training_embed.py - training - Epoch: [1][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.3638 (254.8799)	
2019-01-08 10:15:58,460 - 10 - training_embed.py - training - Epoch: [1][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.8597 (254.9884)	
2019-01-08 10:15:58,648 - 10 - training_embed.py - training - Epoch: [1][300/383]	Time 0.002 (0.002)	Data 0.002 (0.001)	Loss 252.9595 (254.9005)	
2019-01-08 10:15:58,796 - 10 - training_embed.py - training - loss: 254.796910
2019-01-08 10:15:58,797 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:15:59,225 - 10 - training_embed.py - training - Epoch: [2][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.9173 (254.9081)	
2019-01-08 10:15:59,421 - 10 - training_embed.py - training - Epoch: [2][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.2685 (254.9559)	
2019-01-08 10:15:59,614 - 10 - training_embed.py - training - Epoch: [2][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.1212 (254.9394)	
2019-01-08 10:15:59,764 - 10 - training_embed.py - training - loss: 254.740141
2019-01-08 10:15:59,765 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:16:00,124 - 10 - training_embed.py - training - Epoch: [3][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.2759 (254.8660)	
2019-01-08 10:16:00,314 - 10 - training_embed.py - training - Epoch: [3][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.3736 (254.8835)	
2019-01-08 10:16:00,511 - 10 - training_embed.py - training - Epoch: [3][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.7186 (254.8471)	
2019-01-08 10:16:00,660 - 10 - training_embed.py - training - loss: 254.690969
2019-01-08 10:16:00,660 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:16:00,985 - 10 - training_embed.py - training - Epoch: [4][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.7830 (254.8965)	
2019-01-08 10:16:01,185 - 10 - training_embed.py - training - Epoch: [4][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.7471 (254.8571)	
2019-01-08 10:16:01,382 - 10 - training_embed.py - training - Epoch: [4][300/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 255.6494 (254.8709)	
2019-01-08 10:16:01,531 - 10 - training_embed.py - training - loss: 254.635246
2019-01-08 10:16:01,531 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:16:01,907 - 10 - training_embed.py - training - Epoch: [5][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.4666 (254.8399)	
2019-01-08 10:16:02,090 - 10 - training_embed.py - training - Epoch: [5][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.0837 (254.7139)	
2019-01-08 10:16:02,288 - 10 - training_embed.py - training - Epoch: [5][300/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 253.1159 (254.6778)	
2019-01-08 10:16:02,443 - 10 - training_embed.py - training - loss: 254.582270
2019-01-08 10:16:02,443 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:16:02,887 - 10 - training_embed.py - training - Epoch: [6][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.2936 (254.9449)	
2019-01-08 10:16:03,084 - 10 - training_embed.py - training - Epoch: [6][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.1478 (254.8760)	
2019-01-08 10:16:03,283 - 10 - training_embed.py - training - Epoch: [6][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.1894 (254.7581)	
2019-01-08 10:16:03,437 - 10 - training_embed.py - training - loss: 254.527736
2019-01-08 10:16:03,437 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:16:03,877 - 10 - training_embed.py - training - Epoch: [7][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.1215 (254.6283)	
2019-01-08 10:16:04,065 - 10 - training_embed.py - training - Epoch: [7][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.6849 (254.4967)	
2019-01-08 10:16:04,262 - 10 - training_embed.py - training - Epoch: [7][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.1184 (254.5608)	
2019-01-08 10:16:04,411 - 10 - training_embed.py - training - loss: 254.476891
2019-01-08 10:16:04,411 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:16:04,840 - 10 - training_embed.py - training - Epoch: [8][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.7242 (254.5062)	
2019-01-08 10:16:05,023 - 10 - training_embed.py - training - Epoch: [8][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.6399 (254.5485)	
2019-01-08 10:16:05,223 - 10 - training_embed.py - training - Epoch: [8][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.9458 (254.5459)	
2019-01-08 10:16:05,377 - 10 - training_embed.py - training - loss: 254.422976
2019-01-08 10:16:05,381 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:16:05,780 - 10 - training_embed.py - training - Epoch: [9][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.6947 (254.4149)	
2019-01-08 10:16:05,961 - 10 - training_embed.py - training - Epoch: [9][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2094 (254.5062)	
2019-01-08 10:16:06,158 - 10 - training_embed.py - training - Epoch: [9][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 250.1252 (254.4853)	
2019-01-08 10:16:06,315 - 10 - training_embed.py - training - loss: 254.369402
2019-01-08 10:16:06,315 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:16:06,721 - 10 - training_embed.py - training - Epoch: [10][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5849 (254.7090)	
2019-01-08 10:16:06,920 - 10 - training_embed.py - training - Epoch: [10][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.3104 (254.5408)	
2019-01-08 10:16:07,108 - 10 - training_embed.py - training - Epoch: [10][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.2982 (254.4751)	
2019-01-08 10:16:07,261 - 10 - training_embed.py - training - loss: 254.315790
2019-01-08 10:16:07,261 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:16:07,716 - 10 - training_embed.py - training - Epoch: [11][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.2960 (254.4525)	
2019-01-08 10:16:07,894 - 10 - training_embed.py - training - Epoch: [11][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 250.5644 (254.4195)	
2019-01-08 10:16:08,098 - 10 - training_embed.py - training - Epoch: [11][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.0362 (254.4041)	
2019-01-08 10:16:08,260 - 10 - training_embed.py - training - loss: 254.262508
2019-01-08 10:16:08,278 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:16:08,444 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 165.954 ms ~ 0.003 min ~ 0.166 sec
2019-01-08 10:16:08,654 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 375.742 ms ~ 0.006 min ~ 0.376 sec
2019-01-08 10:16:08,654 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:16:08,654 - 10 - corpus.py - subactivity_sampler - 0 / 167
2019-01-08 10:16:08,654 - 10 - corpus.py - subactivity_sampler - [22538.  8604. 18023.  6290. 32122. 10381.]
2019-01-08 10:16:12,035 - 10 - corpus.py - subactivity_sampler - 20 / 167
2019-01-08 10:16:12,035 - 10 - corpus.py - subactivity_sampler - [22442.  8512. 18304.  6078. 32492. 10130.]
2019-01-08 10:16:15,462 - 10 - corpus.py - subactivity_sampler - 40 / 167
2019-01-08 10:16:15,462 - 10 - corpus.py - subactivity_sampler - [22500.  8328. 18424.  5733. 33039.  9934.]
2019-01-08 10:16:19,480 - 10 - corpus.py - subactivity_sampler - 60 / 167
2019-01-08 10:16:19,480 - 10 - corpus.py - subactivity_sampler - [22674.  8091. 18484.  5492. 33616.  9601.]
2019-01-08 10:16:22,295 - 10 - corpus.py - subactivity_sampler - 80 / 167
2019-01-08 10:16:22,296 - 10 - corpus.py - subactivity_sampler - [22686.  8046. 18492.  5219. 34030.  9485.]
2019-01-08 10:16:26,029 - 10 - corpus.py - subactivity_sampler - 100 / 167
2019-01-08 10:16:26,029 - 10 - corpus.py - subactivity_sampler - [22755.  7898. 18532.  5072. 34305.  9396.]
2019-01-08 10:16:29,079 - 10 - corpus.py - subactivity_sampler - 120 / 167
2019-01-08 10:16:29,079 - 10 - corpus.py - subactivity_sampler - [22772.  7814. 18456.  4982. 34773.  9161.]
2019-01-08 10:16:32,408 - 10 - corpus.py - subactivity_sampler - 140 / 167
2019-01-08 10:16:32,408 - 10 - corpus.py - subactivity_sampler - [22822.  7744. 18235.  4864. 35401.  8892.]
2019-01-08 10:16:36,016 - 10 - corpus.py - subactivity_sampler - 160 / 167
2019-01-08 10:16:36,017 - 10 - corpus.py - subactivity_sampler - [22884.  7807. 18119.  4675. 35774.  8699.]
2019-01-08 10:16:36,694 - 10 - corpus.py - subactivity_sampler - [22887.  7810. 18115.  4632. 35889.  8625.]
2019-01-08 10:16:36,694 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 28040.563 ms ~ 0.467 min ~ 28.041 sec
2019-01-08 10:16:36,694 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:16:37,262 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:16:37,262 - 10 - corpus.py - ordering_sampler - inv_count_vec: [5. 4. 0. 0. 3.]
2019-01-08 10:16:37,262 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:16:37,281 - 10 - corpus.py - rho_sampling - ['56.4060', '16.7177', '928.2654', '90.6825', '13.2148']
2019-01-08 10:16:37,281 - 10 - pipeline.py - baseline - Iteration 4
2019-01-08 10:16:37,308 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:16:37,310 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:16:37,310 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 9', '2: 5', '3: 7', '4: 2', '5: 8']
2019-01-08 10:16:37,312 - 10 - accuracy_class.py - mof_val - frames true: 39231	frames overall : 97958
2019-01-08 10:16:37,313 - 10 - corpus.py - accuracy_corpus - Action: coffee
2019-01-08 10:16:37,313 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4004879642295678
2019-01-08 10:16:37,313 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4004879642295678
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - mof_classes - label 2: 0.729180  16855 / 23115
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - mof_classes - label 5: 0.352410  14274 / 40504
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - mof_classes - label 6: 0.874672  4997 / 5713
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - mof_classes - label 7: 0.156823  693 / 4419
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - mof_classes - label 8: 0.412731  2412 / 5844
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - mof_classes - label 9: 0.000000  0 / 3356
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - mof_classes - average class mof: 0.360831
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - iou_classes - label 2: 0.399891  16855 / 42149
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - iou_classes - label 5: 0.321885  14274 / 44345
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - iou_classes - label 6: 0.211710  4997 / 23603
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - iou_classes - label 7: 0.082915  693 / 8358
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - iou_classes - label 8: 0.200050  2412 / 12057
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - iou_classes - label 9: 0.000000  0 / 11166
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - iou_classes - average IoU: 0.202742
2019-01-08 10:16:37,313 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.173779
2019-01-08 10:16:38,498 - 10 - f1_score.py - f1 - f1 score: 0.272607
2019-01-08 10:16:38,501 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1219.362 ms ~ 0.020 min ~ 1.219 sec
2019-01-08 10:16:38,501 - 10 - corpus.py - embedding_training - .
2019-01-08 10:16:38,501 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:16:38,501 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:16:38,501 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:16:39,074 - 10 - training_embed.py - training - create model
2019-01-08 10:16:39,075 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:16:39,075 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:16:39,383 - 10 - training_embed.py - training - Epoch: [0][100/383]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 257.5625 (254.7621)	
2019-01-08 10:16:39,597 - 10 - training_embed.py - training - Epoch: [0][200/383]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 257.4071 (254.9343)	
2019-01-08 10:16:39,805 - 10 - training_embed.py - training - Epoch: [0][300/383]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 254.2411 (254.8928)	
2019-01-08 10:16:39,960 - 10 - training_embed.py - training - loss: 254.694865
2019-01-08 10:16:39,961 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:16:40,244 - 10 - training_embed.py - training - Epoch: [1][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.7408 (254.7357)	
2019-01-08 10:16:40,432 - 10 - training_embed.py - training - Epoch: [1][200/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 251.5340 (254.8567)	
2019-01-08 10:16:40,636 - 10 - training_embed.py - training - Epoch: [1][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.1236 (254.7783)	
2019-01-08 10:16:40,795 - 10 - training_embed.py - training - loss: 254.634098
2019-01-08 10:16:40,795 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:16:41,105 - 10 - training_embed.py - training - Epoch: [2][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.8931 (254.7665)	
2019-01-08 10:16:41,293 - 10 - training_embed.py - training - Epoch: [2][200/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 253.8549 (254.7351)	
2019-01-08 10:16:41,496 - 10 - training_embed.py - training - Epoch: [2][300/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 254.1910 (254.7288)	
2019-01-08 10:16:41,663 - 10 - training_embed.py - training - loss: 254.567907
2019-01-08 10:16:41,664 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:16:41,984 - 10 - training_embed.py - training - Epoch: [3][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.9421 (254.7318)	
2019-01-08 10:16:42,171 - 10 - training_embed.py - training - Epoch: [3][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.0761 (254.6686)	
2019-01-08 10:16:42,361 - 10 - training_embed.py - training - Epoch: [3][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.7916 (254.6458)	
2019-01-08 10:16:42,521 - 10 - training_embed.py - training - loss: 254.507337
2019-01-08 10:16:42,521 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:16:42,845 - 10 - training_embed.py - training - Epoch: [4][100/383]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 253.0520 (254.7324)	
2019-01-08 10:16:43,061 - 10 - training_embed.py - training - Epoch: [4][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.1686 (254.7097)	
2019-01-08 10:16:43,251 - 10 - training_embed.py - training - Epoch: [4][300/383]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 254.8379 (254.6818)	
2019-01-08 10:16:43,414 - 10 - training_embed.py - training - loss: 254.439969
2019-01-08 10:16:43,415 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:16:43,728 - 10 - training_embed.py - training - Epoch: [5][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.8637 (254.6443)	
2019-01-08 10:16:43,921 - 10 - training_embed.py - training - Epoch: [5][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.2397 (254.5563)	
2019-01-08 10:16:44,132 - 10 - training_embed.py - training - Epoch: [5][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.3414 (254.5043)	
2019-01-08 10:16:44,278 - 10 - training_embed.py - training - loss: 254.377060
2019-01-08 10:16:44,278 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:16:44,579 - 10 - training_embed.py - training - Epoch: [6][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.0857 (254.7011)	
2019-01-08 10:16:44,773 - 10 - training_embed.py - training - Epoch: [6][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.9201 (254.6578)	
2019-01-08 10:16:44,992 - 10 - training_embed.py - training - Epoch: [6][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.9105 (254.5541)	
2019-01-08 10:16:45,157 - 10 - training_embed.py - training - loss: 254.311589
2019-01-08 10:16:45,157 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:16:45,458 - 10 - training_embed.py - training - Epoch: [7][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.7731 (254.4240)	
2019-01-08 10:16:45,648 - 10 - training_embed.py - training - Epoch: [7][200/383]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 254.3128 (254.2418)	
2019-01-08 10:16:45,841 - 10 - training_embed.py - training - Epoch: [7][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.2114 (254.3161)	
2019-01-08 10:16:45,997 - 10 - training_embed.py - training - loss: 254.250033
2019-01-08 10:16:45,998 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:16:46,330 - 10 - training_embed.py - training - Epoch: [8][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2406 (254.4048)	
2019-01-08 10:16:46,525 - 10 - training_embed.py - training - Epoch: [8][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.0836 (254.3517)	
2019-01-08 10:16:46,719 - 10 - training_embed.py - training - Epoch: [8][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5088 (254.3381)	
2019-01-08 10:16:46,880 - 10 - training_embed.py - training - loss: 254.185930
2019-01-08 10:16:46,881 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:16:47,191 - 10 - training_embed.py - training - Epoch: [9][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.0415 (254.2041)	
2019-01-08 10:16:47,395 - 10 - training_embed.py - training - Epoch: [9][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.5282 (254.3346)	
2019-01-08 10:16:47,585 - 10 - training_embed.py - training - Epoch: [9][300/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 250.9463 (254.2678)	
2019-01-08 10:16:47,747 - 10 - training_embed.py - training - loss: 254.121005
2019-01-08 10:16:47,747 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:16:48,062 - 10 - training_embed.py - training - Epoch: [10][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5233 (254.3671)	
2019-01-08 10:16:48,246 - 10 - training_embed.py - training - Epoch: [10][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.4591 (254.2426)	
2019-01-08 10:16:48,436 - 10 - training_embed.py - training - Epoch: [10][300/383]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 254.0376 (254.2021)	
2019-01-08 10:16:48,596 - 10 - training_embed.py - training - loss: 254.057393
2019-01-08 10:16:48,596 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:16:48,925 - 10 - training_embed.py - training - Epoch: [11][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2764 (254.2069)	
2019-01-08 10:16:49,111 - 10 - training_embed.py - training - Epoch: [11][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.4474 (254.1330)	
2019-01-08 10:16:49,305 - 10 - training_embed.py - training - Epoch: [11][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.1089 (254.1262)	
2019-01-08 10:16:49,465 - 10 - training_embed.py - training - loss: 253.993148
2019-01-08 10:16:49,481 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:16:49,654 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 172.172 ms ~ 0.003 min ~ 0.172 sec
2019-01-08 10:16:49,863 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 381.476 ms ~ 0.006 min ~ 0.381 sec
2019-01-08 10:16:49,863 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:16:49,864 - 10 - corpus.py - subactivity_sampler - 0 / 167
2019-01-08 10:16:49,864 - 10 - corpus.py - subactivity_sampler - [22887.  7810. 18115.  4632. 35889.  8625.]
2019-01-08 10:16:53,242 - 10 - corpus.py - subactivity_sampler - 20 / 167
2019-01-08 10:16:53,242 - 10 - corpus.py - subactivity_sampler - [22894.  7769. 18102.  4582. 36219.  8392.]
2019-01-08 10:16:56,682 - 10 - corpus.py - subactivity_sampler - 40 / 167
2019-01-08 10:16:56,682 - 10 - corpus.py - subactivity_sampler - [22956.  7668. 17895.  4415. 36866.  8158.]
2019-01-08 10:17:00,696 - 10 - corpus.py - subactivity_sampler - 60 / 167
2019-01-08 10:17:00,697 - 10 - corpus.py - subactivity_sampler - [23026.  7571. 17906.  4280. 37237.  7938.]
2019-01-08 10:17:03,521 - 10 - corpus.py - subactivity_sampler - 80 / 167
2019-01-08 10:17:03,522 - 10 - corpus.py - subactivity_sampler - [22920.  7516. 18033.  4129. 37752.  7608.]
2019-01-08 10:17:07,269 - 10 - corpus.py - subactivity_sampler - 100 / 167
2019-01-08 10:17:07,269 - 10 - corpus.py - subactivity_sampler - [22923.  7464. 18030.  4009. 38014.  7518.]
2019-01-08 10:17:10,313 - 10 - corpus.py - subactivity_sampler - 120 / 167
2019-01-08 10:17:10,313 - 10 - corpus.py - subactivity_sampler - [22897.  7433. 18057.  3915. 38397.  7259.]
2019-01-08 10:17:13,664 - 10 - corpus.py - subactivity_sampler - 140 / 167
2019-01-08 10:17:13,664 - 10 - corpus.py - subactivity_sampler - [22905.  7395. 17968.  3818. 38699.  7173.]
2019-01-08 10:17:17,286 - 10 - corpus.py - subactivity_sampler - 160 / 167
2019-01-08 10:17:17,286 - 10 - corpus.py - subactivity_sampler - [22707.  7343. 18204.  3733. 38901.  7070.]
2019-01-08 10:17:17,975 - 10 - corpus.py - subactivity_sampler - [22711.  7327. 18216.  3703. 38948.  7053.]
2019-01-08 10:17:17,975 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 28111.596 ms ~ 0.469 min ~ 28.112 sec
2019-01-08 10:17:17,975 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:17:18,498 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:17:18,498 - 10 - corpus.py - ordering_sampler - inv_count_vec: [1. 3. 0. 0. 2.]
2019-01-08 10:17:18,498 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:17:18,512 - 10 - corpus.py - rho_sampling - ['62.6279', '25.8895', '928.1292', '91.3009', '4.0553']
2019-01-08 10:17:18,512 - 10 - pipeline.py - baseline - Iteration 5
2019-01-08 10:17:18,539 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:17:18,541 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:17:18,541 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 9', '2: 5', '3: 7', '4: 2', '5: 8']
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - mof_val - frames true: 39600	frames overall : 97958
2019-01-08 10:17:18,544 - 10 - corpus.py - accuracy_corpus - Action: coffee
2019-01-08 10:17:18,544 - 10 - corpus.py - accuracy_corpus - MoF val: 0.40425488474652405
2019-01-08 10:17:18,544 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.40425488474652405
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - mof_classes - label 2: 0.764352  17668 / 23115
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - mof_classes - label 5: 0.357249  14470 / 40504
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - mof_classes - label 6: 0.874322  4995 / 5713
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - mof_classes - label 7: 0.132157  584 / 4419
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - mof_classes - label 8: 0.322211  1883 / 5844
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - mof_classes - label 9: 0.000000  0 / 3356
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - mof_classes - average class mof: 0.350041
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - iou_classes - label 2: 0.397973  17668 / 44395
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - iou_classes - label 5: 0.327006  14470 / 44250
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - iou_classes - label 6: 0.213197  4995 / 23429
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - iou_classes - label 7: 0.077474  584 / 7538
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - iou_classes - label 8: 0.170964  1883 / 11014
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - iou_classes - label 9: 0.000000  0 / 10683
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - iou_classes - average IoU: 0.197769
2019-01-08 10:17:18,544 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.169516
2019-01-08 10:17:19,727 - 10 - f1_score.py - f1 - f1 score: 0.272397
2019-01-08 10:17:19,729 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1217.592 ms ~ 0.020 min ~ 1.218 sec
2019-01-08 10:17:19,729 - 10 - corpus.py - embedding_training - .
2019-01-08 10:17:19,729 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:17:19,729 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:17:19,730 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:17:20,291 - 10 - training_embed.py - training - create model
2019-01-08 10:17:20,292 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:17:20,292 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:17:20,607 - 10 - training_embed.py - training - Epoch: [0][100/383]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 256.4475 (254.8466)	
2019-01-08 10:17:20,808 - 10 - training_embed.py - training - Epoch: [0][200/383]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 257.3192 (254.9212)	
2019-01-08 10:17:21,005 - 10 - training_embed.py - training - Epoch: [0][300/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 255.4977 (254.8822)	
2019-01-08 10:17:21,158 - 10 - training_embed.py - training - loss: 254.687100
2019-01-08 10:17:21,158 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:17:21,472 - 10 - training_embed.py - training - Epoch: [1][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.3501 (254.6953)	
2019-01-08 10:17:21,668 - 10 - training_embed.py - training - Epoch: [1][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.1232 (254.8498)	
2019-01-08 10:17:21,878 - 10 - training_embed.py - training - Epoch: [1][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.8196 (254.7730)	
2019-01-08 10:17:22,027 - 10 - training_embed.py - training - loss: 254.616733
2019-01-08 10:17:22,028 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:17:22,354 - 10 - training_embed.py - training - Epoch: [2][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.5028 (254.7950)	
2019-01-08 10:17:22,549 - 10 - training_embed.py - training - Epoch: [2][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.5871 (254.7380)	
2019-01-08 10:17:22,755 - 10 - training_embed.py - training - Epoch: [2][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5893 (254.7289)	
2019-01-08 10:17:22,924 - 10 - training_embed.py - training - loss: 254.542005
2019-01-08 10:17:22,925 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:17:23,236 - 10 - training_embed.py - training - Epoch: [3][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5347 (254.6877)	
2019-01-08 10:17:23,429 - 10 - training_embed.py - training - Epoch: [3][200/383]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 254.3910 (254.6324)	
2019-01-08 10:17:23,620 - 10 - training_embed.py - training - Epoch: [3][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.4272 (254.6292)	
2019-01-08 10:17:23,770 - 10 - training_embed.py - training - loss: 254.472251
2019-01-08 10:17:23,771 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:17:24,112 - 10 - training_embed.py - training - Epoch: [4][100/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 252.4091 (254.6558)	
2019-01-08 10:17:24,304 - 10 - training_embed.py - training - Epoch: [4][200/383]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 254.8438 (254.6324)	
2019-01-08 10:17:24,493 - 10 - training_embed.py - training - Epoch: [4][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5134 (254.6214)	
2019-01-08 10:17:24,649 - 10 - training_embed.py - training - loss: 254.395864
2019-01-08 10:17:24,649 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:17:24,961 - 10 - training_embed.py - training - Epoch: [5][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.6773 (254.6351)	
2019-01-08 10:17:25,154 - 10 - training_embed.py - training - Epoch: [5][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.7361 (254.5127)	
2019-01-08 10:17:25,343 - 10 - training_embed.py - training - Epoch: [5][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.3845 (254.4860)	
2019-01-08 10:17:25,499 - 10 - training_embed.py - training - loss: 254.323594
2019-01-08 10:17:25,499 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:17:25,829 - 10 - training_embed.py - training - Epoch: [6][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.8793 (254.7385)	
2019-01-08 10:17:26,012 - 10 - training_embed.py - training - Epoch: [6][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.0716 (254.6020)	
2019-01-08 10:17:26,203 - 10 - training_embed.py - training - Epoch: [6][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.7213 (254.4818)	
2019-01-08 10:17:26,352 - 10 - training_embed.py - training - loss: 254.249341
2019-01-08 10:17:26,352 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:17:26,678 - 10 - training_embed.py - training - Epoch: [7][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.9093 (254.3263)	
2019-01-08 10:17:26,857 - 10 - training_embed.py - training - Epoch: [7][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.1607 (254.1894)	
2019-01-08 10:17:27,066 - 10 - training_embed.py - training - Epoch: [7][300/383]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 253.0521 (254.2558)	
2019-01-08 10:17:27,233 - 10 - training_embed.py - training - loss: 254.178971
2019-01-08 10:17:27,233 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:17:27,569 - 10 - training_embed.py - training - Epoch: [8][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2034 (254.3237)	
2019-01-08 10:17:27,784 - 10 - training_embed.py - training - Epoch: [8][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.0333 (254.2897)	
2019-01-08 10:17:27,998 - 10 - training_embed.py - training - Epoch: [8][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.7754 (254.2816)	
2019-01-08 10:17:28,161 - 10 - training_embed.py - training - loss: 254.105959
2019-01-08 10:17:28,161 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:17:28,474 - 10 - training_embed.py - training - Epoch: [9][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.5767 (254.1018)	
2019-01-08 10:17:28,668 - 10 - training_embed.py - training - Epoch: [9][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2211 (254.2443)	
2019-01-08 10:17:28,875 - 10 - training_embed.py - training - Epoch: [9][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.6490 (254.1772)	
2019-01-08 10:17:29,046 - 10 - training_embed.py - training - loss: 254.031194
2019-01-08 10:17:29,047 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:17:29,362 - 10 - training_embed.py - training - Epoch: [10][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.7504 (254.2427)	
2019-01-08 10:17:29,551 - 10 - training_embed.py - training - Epoch: [10][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.7136 (254.1304)	
2019-01-08 10:17:29,739 - 10 - training_embed.py - training - Epoch: [10][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.9871 (254.0904)	
2019-01-08 10:17:29,890 - 10 - training_embed.py - training - loss: 253.959597
2019-01-08 10:17:29,890 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:17:30,214 - 10 - training_embed.py - training - Epoch: [11][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.4983 (254.0827)	
2019-01-08 10:17:30,414 - 10 - training_embed.py - training - Epoch: [11][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.3555 (254.0344)	
2019-01-08 10:17:30,623 - 10 - training_embed.py - training - Epoch: [11][300/383]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 255.7838 (254.0026)	
2019-01-08 10:17:30,791 - 10 - training_embed.py - training - loss: 253.885895
2019-01-08 10:17:30,808 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:17:30,970 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 162.271 ms ~ 0.003 min ~ 0.162 sec
2019-01-08 10:17:31,175 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 367.066 ms ~ 0.006 min ~ 0.367 sec
2019-01-08 10:17:31,175 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:17:31,175 - 10 - corpus.py - subactivity_sampler - 0 / 167
2019-01-08 10:17:31,176 - 10 - corpus.py - subactivity_sampler - [22711.  7327. 18216.  3703. 38948.  7053.]
2019-01-08 10:17:34,689 - 10 - corpus.py - subactivity_sampler - 20 / 167
2019-01-08 10:17:34,689 - 10 - corpus.py - subactivity_sampler - [22718.  7281. 18183.  3646. 39280.  6850.]
2019-01-08 10:17:38,084 - 10 - corpus.py - subactivity_sampler - 40 / 167
2019-01-08 10:17:38,084 - 10 - corpus.py - subactivity_sampler - [22734.  7245. 18012.  3590. 39760.  6617.]
2019-01-08 10:17:42,094 - 10 - corpus.py - subactivity_sampler - 60 / 167
2019-01-08 10:17:42,094 - 10 - corpus.py - subactivity_sampler - [22739.  7250. 17920.  3507. 40016.  6526.]
2019-01-08 10:17:45,061 - 10 - corpus.py - subactivity_sampler - 80 / 167
2019-01-08 10:17:45,061 - 10 - corpus.py - subactivity_sampler - [22741.  7219. 17830.  3442. 40662.  6064.]
2019-01-08 10:17:48,874 - 10 - corpus.py - subactivity_sampler - 100 / 167
2019-01-08 10:17:48,875 - 10 - corpus.py - subactivity_sampler - [22731.  7134. 17877.  3379. 40823.  6014.]
2019-01-08 10:17:51,951 - 10 - corpus.py - subactivity_sampler - 120 / 167
2019-01-08 10:17:51,951 - 10 - corpus.py - subactivity_sampler - [22716.  7115. 17797.  3363. 41058.  5909.]
2019-01-08 10:17:55,345 - 10 - corpus.py - subactivity_sampler - 140 / 167
2019-01-08 10:17:55,345 - 10 - corpus.py - subactivity_sampler - [22672.  7107. 17751.  3344. 41360.  5724.]
2019-01-08 10:17:58,932 - 10 - corpus.py - subactivity_sampler - 160 / 167
2019-01-08 10:17:58,932 - 10 - corpus.py - subactivity_sampler - [22626.  7016. 17773.  3334. 41600.  5609.]
2019-01-08 10:17:59,606 - 10 - corpus.py - subactivity_sampler - [22630.  7013. 17765.  3312. 41631.  5607.]
2019-01-08 10:17:59,606 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 28431.115 ms ~ 0.474 min ~ 28.431 sec
2019-01-08 10:17:59,606 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:18:01,238 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:18:01,239 - 10 - corpus.py - ordering_sampler - inv_count_vec: [1. 3. 0. 0. 3.]
2019-01-08 10:18:01,239 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:18:01,275 - 10 - corpus.py - rho_sampling - ['60.0094', '41.4247', '928.5602', '292.2624', '10.8870']
2019-01-08 10:18:01,275 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 10:18:01,302 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:18:01,304 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:18:01,304 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 9', '2: 5', '3: 7', '4: 2', '5: 8']
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - mof_val - frames true: 39827	frames overall : 97958
2019-01-08 10:18:01,307 - 10 - corpus.py - accuracy_corpus - Action: coffee
2019-01-08 10:18:01,307 - 10 - corpus.py - accuracy_corpus - MoF val: 0.40657220441413666
2019-01-08 10:18:01,307 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.40657220441413666
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - mof_classes - label 2: 0.797664  18438 / 23115
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - mof_classes - label 5: 0.350953  14215 / 40504
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - mof_classes - label 6: 0.874147  4994 / 5713
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - mof_classes - label 7: 0.115184  509 / 4419
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - mof_classes - label 8: 0.285934  1671 / 5844
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - mof_classes - label 9: 0.000000  0 / 3356
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - mof_classes - average class mof: 0.346269
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - iou_classes - label 2: 0.398160  18438 / 46308
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - iou_classes - label 5: 0.322672  14215 / 44054
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - iou_classes - label 6: 0.213885  4994 / 23349
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - iou_classes - label 7: 0.070479  509 / 7222
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - iou_classes - label 8: 0.170859  1671 / 9780
2019-01-08 10:18:01,307 - 10 - accuracy_class.py - iou_classes - label 9: 0.000000  0 / 10369
2019-01-08 10:18:01,308 - 10 - accuracy_class.py - iou_classes - average IoU: 0.196009
2019-01-08 10:18:01,308 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.168008
2019-01-08 10:18:02,481 - 10 - f1_score.py - f1 - f1 score: 0.272673
2019-01-08 10:18:02,484 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1208.450 ms ~ 0.020 min ~ 1.208 sec
2019-01-08 10:18:02,484 - 10 - corpus.py - embedding_training - .
2019-01-08 10:18:02,484 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:18:02,484 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:18:02,484 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:18:03,023 - 10 - training_embed.py - training - create model
2019-01-08 10:18:03,024 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:18:03,024 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:18:03,337 - 10 - training_embed.py - training - Epoch: [0][100/383]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 256.3037 (254.6237)	
2019-01-08 10:18:03,535 - 10 - training_embed.py - training - Epoch: [0][200/383]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.8545 (254.7513)	
2019-01-08 10:18:03,723 - 10 - training_embed.py - training - Epoch: [0][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.6006 (254.7054)	
2019-01-08 10:18:03,892 - 10 - training_embed.py - training - loss: 254.513046
2019-01-08 10:18:03,892 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:18:04,205 - 10 - training_embed.py - training - Epoch: [1][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.7299 (254.5654)	
2019-01-08 10:18:04,408 - 10 - training_embed.py - training - Epoch: [1][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.5226 (254.6914)	
2019-01-08 10:18:04,619 - 10 - training_embed.py - training - Epoch: [1][300/383]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 252.6346 (254.5920)	
2019-01-08 10:18:04,782 - 10 - training_embed.py - training - loss: 254.433864
2019-01-08 10:18:04,782 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:18:05,093 - 10 - training_embed.py - training - Epoch: [2][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.5553 (254.6570)	
2019-01-08 10:18:05,307 - 10 - training_embed.py - training - Epoch: [2][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.8800 (254.5499)	
2019-01-08 10:18:05,504 - 10 - training_embed.py - training - Epoch: [2][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.4455 (254.5459)	
2019-01-08 10:18:05,657 - 10 - training_embed.py - training - loss: 254.351283
2019-01-08 10:18:05,657 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:18:05,987 - 10 - training_embed.py - training - Epoch: [3][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.9204 (254.5204)	
2019-01-08 10:18:06,172 - 10 - training_embed.py - training - Epoch: [3][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.9762 (254.4644)	
2019-01-08 10:18:06,365 - 10 - training_embed.py - training - Epoch: [3][300/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 252.5649 (254.4378)	
2019-01-08 10:18:06,527 - 10 - training_embed.py - training - loss: 254.273748
2019-01-08 10:18:06,527 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:18:06,891 - 10 - training_embed.py - training - Epoch: [4][100/383]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 251.9205 (254.4593)	
2019-01-08 10:18:07,127 - 10 - training_embed.py - training - Epoch: [4][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.2141 (254.4093)	
2019-01-08 10:18:07,323 - 10 - training_embed.py - training - Epoch: [4][300/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 254.5824 (254.4034)	
2019-01-08 10:18:07,488 - 10 - training_embed.py - training - loss: 254.190087
2019-01-08 10:18:07,489 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:18:07,830 - 10 - training_embed.py - training - Epoch: [5][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.8826 (254.4560)	
2019-01-08 10:18:08,026 - 10 - training_embed.py - training - Epoch: [5][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.0546 (254.2902)	
2019-01-08 10:18:08,220 - 10 - training_embed.py - training - Epoch: [5][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.4279 (254.2521)	
2019-01-08 10:18:08,367 - 10 - training_embed.py - training - loss: 254.108838
2019-01-08 10:18:08,367 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:18:08,729 - 10 - training_embed.py - training - Epoch: [6][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.6532 (254.5781)	
2019-01-08 10:18:08,912 - 10 - training_embed.py - training - Epoch: [6][200/383]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 257.6372 (254.4213)	
2019-01-08 10:18:09,123 - 10 - training_embed.py - training - Epoch: [6][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.9402 (254.2791)	
2019-01-08 10:18:09,280 - 10 - training_embed.py - training - loss: 254.027096
2019-01-08 10:18:09,280 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:18:09,605 - 10 - training_embed.py - training - Epoch: [7][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.8740 (254.1269)	
2019-01-08 10:18:09,800 - 10 - training_embed.py - training - Epoch: [7][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.2329 (253.9925)	
2019-01-08 10:18:10,014 - 10 - training_embed.py - training - Epoch: [7][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.0547 (254.0320)	
2019-01-08 10:18:10,169 - 10 - training_embed.py - training - loss: 253.948216
2019-01-08 10:18:10,170 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:18:10,497 - 10 - training_embed.py - training - Epoch: [8][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.1049 (254.1151)	
2019-01-08 10:18:10,681 - 10 - training_embed.py - training - Epoch: [8][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.2163 (254.0462)	
2019-01-08 10:18:10,872 - 10 - training_embed.py - training - Epoch: [8][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.1315 (254.0360)	
2019-01-08 10:18:11,037 - 10 - training_embed.py - training - loss: 253.868027
2019-01-08 10:18:11,037 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:18:11,339 - 10 - training_embed.py - training - Epoch: [9][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.1286 (253.8100)	
2019-01-08 10:18:11,544 - 10 - training_embed.py - training - Epoch: [9][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.1557 (253.9848)	
2019-01-08 10:18:11,740 - 10 - training_embed.py - training - Epoch: [9][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.8551 (253.9126)	
2019-01-08 10:18:11,913 - 10 - training_embed.py - training - loss: 253.784858
2019-01-08 10:18:11,913 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:18:12,244 - 10 - training_embed.py - training - Epoch: [10][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.4830 (253.9573)	
2019-01-08 10:18:12,447 - 10 - training_embed.py - training - Epoch: [10][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.1060 (253.8476)	
2019-01-08 10:18:12,647 - 10 - training_embed.py - training - Epoch: [10][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.6809 (253.8131)	
2019-01-08 10:18:12,801 - 10 - training_embed.py - training - loss: 253.705251
2019-01-08 10:18:12,801 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:18:13,116 - 10 - training_embed.py - training - Epoch: [11][100/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.3316 (253.8057)	
2019-01-08 10:18:13,309 - 10 - training_embed.py - training - Epoch: [11][200/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 250.8903 (253.7842)	
2019-01-08 10:18:13,504 - 10 - training_embed.py - training - Epoch: [11][300/383]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.3560 (253.7382)	
2019-01-08 10:18:13,679 - 10 - training_embed.py - training - loss: 253.623739
2019-01-08 10:18:13,704 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:18:13,871 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 166.559 ms ~ 0.003 min ~ 0.167 sec
2019-01-08 10:18:14,082 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 378.575 ms ~ 0.006 min ~ 0.379 sec
2019-01-08 10:18:14,083 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:18:14,083 - 10 - corpus.py - subactivity_sampler - 0 / 167
2019-01-08 10:18:14,083 - 10 - corpus.py - subactivity_sampler - [22630.  7013. 17765.  3312. 41631.  5607.]
2019-01-08 10:18:17,428 - 10 - corpus.py - subactivity_sampler - 20 / 167
2019-01-08 10:18:17,428 - 10 - corpus.py - subactivity_sampler - [22624.  6984. 17917.  3087. 41957.  5389.]
2019-01-08 10:18:20,805 - 10 - corpus.py - subactivity_sampler - 40 / 167
2019-01-08 10:18:20,806 - 10 - corpus.py - subactivity_sampler - [22627.  6986. 17876.  3056. 42363.  5050.]
2019-01-08 10:18:24,782 - 10 - corpus.py - subactivity_sampler - 60 / 167
2019-01-08 10:18:24,782 - 10 - corpus.py - subactivity_sampler - [22629.  7011. 17809.  2901. 42824.  4784.]
2019-01-08 10:18:27,560 - 10 - corpus.py - subactivity_sampler - 80 / 167
2019-01-08 10:18:27,560 - 10 - corpus.py - subactivity_sampler - [22627.  7018. 17722.  2867. 43049.  4675.]
2019-01-08 10:18:31,276 - 10 - corpus.py - subactivity_sampler - 100 / 167
2019-01-08 10:18:31,276 - 10 - corpus.py - subactivity_sampler - [22621.  6977. 17719.  2754. 43283.  4604.]
2019-01-08 10:18:34,269 - 10 - corpus.py - subactivity_sampler - 120 / 167
2019-01-08 10:18:34,269 - 10 - corpus.py - subactivity_sampler - [22614.  6970. 17683.  2736. 43386.  4569.]
2019-01-08 10:18:37,558 - 10 - corpus.py - subactivity_sampler - 140 / 167
2019-01-08 10:18:37,558 - 10 - corpus.py - subactivity_sampler - [22578.  6972. 17627.  2725. 43742.  4314.]
2019-01-08 10:18:41,141 - 10 - corpus.py - subactivity_sampler - 160 / 167
2019-01-08 10:18:41,141 - 10 - corpus.py - subactivity_sampler - [22572.  6923. 17600.  2694. 43999.  4170.]
2019-01-08 10:18:41,817 - 10 - corpus.py - subactivity_sampler - [22551.  6931. 17559.  2682. 44065.  4170.]
2019-01-08 10:18:41,818 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 27734.919 ms ~ 0.462 min ~ 27.735 sec
2019-01-08 10:18:41,818 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:18:43,270 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:18:43,271 - 10 - corpus.py - ordering_sampler - inv_count_vec: [4. 2. 0. 0. 2.]
2019-01-08 10:18:43,271 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:18:43,287 - 10 - corpus.py - rho_sampling - ['56.7381', '17.5836', '928.4240', '91.6523', '29.9902']
2019-01-08 10:18:43,287 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 10:18:43,314 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:18:43,315 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:18:43,316 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 9', '2: 5', '3: 7', '4: 2', '5: 8']
2019-01-08 10:18:43,318 - 10 - accuracy_class.py - mof_val - frames true: 39956	frames overall : 97958
2019-01-08 10:18:43,318 - 10 - corpus.py - accuracy_corpus - Action: coffee
2019-01-08 10:18:43,319 - 10 - corpus.py - accuracy_corpus - MoF val: 0.40788909532656853
2019-01-08 10:18:43,319 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.40788909532656853
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - mof_classes - label 2: 0.828596  19153 / 23115
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - mof_classes - label 5: 0.344756  13964 / 40504
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - mof_classes - label 6: 0.867495  4956 / 5713
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - mof_classes - label 7: 0.109753  485 / 4419
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - mof_classes - label 8: 0.239220  1398 / 5844
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - mof_classes - label 9: 0.000000  0 / 3356
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - mof_classes - average class mof: 0.341403
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 15007
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - iou_classes - label 2: 0.398797  19153 / 48027
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - iou_classes - label 5: 0.316651  13964 / 44099
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - iou_classes - label 6: 0.212631  4956 / 23308
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - iou_classes - label 7: 0.073307  485 / 6616
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - iou_classes - label 8: 0.162256  1398 / 8616
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - iou_classes - label 9: 0.000000  0 / 10287
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - iou_classes - average IoU: 0.193940
2019-01-08 10:18:43,319 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.166235
2019-01-08 10:18:44,517 - 10 - f1_score.py - f1 - f1 score: 0.271858
2019-01-08 10:18:44,520 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1232.592 ms ~ 0.021 min ~ 1.233 sec
2019-01-08 10:18:44,529 - 10 - utils.py - wrap - <function baseline at 0x7f3991d24e18> took 296084.422 ms ~ 4.935 min ~ 296.084 sec
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - batch_size: 256
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - bg: False
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - bg_trh: 85
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - data: /media/data/kukleva/lab/Breakfast/feat/s1
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - data_type: 2
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - dataset: bf
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - dataset_root: /media/data/kukleva/lab/Breakfast
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - embed_dim: 30
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - epochs: 12
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - ext: txt
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - feature_dim: 64
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - full: True
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - gmm: 1
2019-01-08 10:18:44,530 - 10 - utils.py - update_opt_str - gmms: one
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - gt: /media/data/kukleva/lab/Breakfast/feat/s1/groundTruth
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - gt_training: False
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - log: DEBUG
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - log_str: slim.mallow._cereals_!bg_data2_bf_embed30_epochs12_full_gmm1_one_!gt_lr1e-10_!lr_ordering_reg0.1_!viterbi_!zeros_
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - lr: 1e-10
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - lr_adj: False
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - momentum: 0.9
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - num_workers: 4
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - ordering: True
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - prefix: slim.mallow.
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - reg_cov: 0.1
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - resume: False
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - resume_str: 
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - save_embed_feat: False
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - save_likelihood: False
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - save_model: False
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - seed: 0
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - subaction: cereals
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - vis: False
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - viterbi: False
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - weight_decay: 0.0001
2019-01-08 10:18:44,531 - 10 - utils.py - update_opt_str - zeros: False
2019-01-08 10:18:44,549 - 10 - utils.py - wrap - <function GroundTruth.load_gt at 0x7f39322b36a8> took 17.365 ms ~ 0.000 min ~ 0.017 sec
2019-01-08 10:18:44,612 - 10 - corpus.py - __init__ - cereals  subactions: 4
2019-01-08 10:18:44,613 - 10 - corpus.py - _init_videos - .
2019-01-08 10:18:49,303 - 10 - corpus.py - _init_videos - gt statistic: Counter({1: 56926, 2: 46837, 3: 13025, 4: 12763})
2019-01-08 10:18:49,304 - 10 - corpus.py - _update_fg_mask - .
2019-01-08 10:18:49,314 - 10 - corpus.py - __init__ - min: -47.593246  max: 37.088371  avg: 0.121490
2019-01-08 10:18:49,314 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:18:49,334 - 10 - corpus.py - rho_sampling - ['64.1837', '100.9221', '929.0535']
2019-01-08 10:18:49,334 - 10 - pipeline.py - baseline - Iteration 0
2019-01-08 10:18:49,373 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:18:49,375 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:18:49,375 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 4', '1: 1', '2: 2', '3: 3']
2019-01-08 10:18:49,378 - 10 - accuracy_class.py - mof_val - frames true: 64070	frames overall : 129551
2019-01-08 10:18:49,378 - 10 - corpus.py - accuracy_corpus - Action: cereals
2019-01-08 10:18:49,378 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4945542682032559
2019-01-08 10:18:49,378 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.0
2019-01-08 10:18:49,378 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:18:49,378 - 10 - accuracy_class.py - mof_classes - label 1: 0.525701  27337 / 52001
2019-01-08 10:18:49,378 - 10 - accuracy_class.py - mof_classes - label 2: 0.563126  21258 / 37750
2019-01-08 10:18:49,379 - 10 - accuracy_class.py - mof_classes - label 3: 0.801600  6614 / 8251
2019-01-08 10:18:49,379 - 10 - accuracy_class.py - mof_classes - label 4: 0.942860  8861 / 9398
2019-01-08 10:18:49,379 - 10 - accuracy_class.py - mof_classes - average class mof: 0.566657
2019-01-08 10:18:49,379 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:18:49,379 - 10 - accuracy_class.py - iou_classes - label 1: 0.479075  27337 / 57062
2019-01-08 10:18:49,379 - 10 - accuracy_class.py - iou_classes - label 2: 0.435107  21258 / 48857
2019-01-08 10:18:49,379 - 10 - accuracy_class.py - iou_classes - label 3: 0.194804  6614 / 33952
2019-01-08 10:18:49,379 - 10 - accuracy_class.py - iou_classes - label 4: 0.268434  8861 / 33010
2019-01-08 10:18:49,379 - 10 - accuracy_class.py - iou_classes - average IoU: 0.344355
2019-01-08 10:18:49,379 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.275484
2019-01-08 10:18:50,963 - 10 - f1_score.py - f1 - f1 score: 0.492143
2019-01-08 10:18:50,968 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1634.063 ms ~ 0.027 min ~ 1.634 sec
2019-01-08 10:18:50,968 - 10 - corpus.py - embedding_training - .
2019-01-08 10:18:50,968 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:18:50,968 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:18:50,968 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:18:51,781 - 10 - training_embed.py - training - create model
2019-01-08 10:18:51,782 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:18:51,782 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:18:52,096 - 10 - training_embed.py - training - Epoch: [0][100/507]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 155.1864 (153.8537)	
2019-01-08 10:18:52,298 - 10 - training_embed.py - training - Epoch: [0][200/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.0741 (153.8667)	
2019-01-08 10:18:52,500 - 10 - training_embed.py - training - Epoch: [0][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.4504 (153.8573)	
2019-01-08 10:18:52,713 - 10 - training_embed.py - training - Epoch: [0][400/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.6485 (153.8533)	
2019-01-08 10:18:52,914 - 10 - training_embed.py - training - Epoch: [0][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.8037 (153.8506)	
2019-01-08 10:18:52,927 - 10 - training_embed.py - training - loss: 153.838832
2019-01-08 10:18:52,927 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:18:53,275 - 10 - training_embed.py - training - Epoch: [1][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.2744 (153.8912)	
2019-01-08 10:18:53,466 - 10 - training_embed.py - training - Epoch: [1][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9786 (153.9303)	
2019-01-08 10:18:53,665 - 10 - training_embed.py - training - Epoch: [1][300/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.2576 (153.9029)	
2019-01-08 10:18:53,862 - 10 - training_embed.py - training - Epoch: [1][400/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 153.8864 (153.8694)	
2019-01-08 10:18:54,062 - 10 - training_embed.py - training - Epoch: [1][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.5019 (153.8540)	
2019-01-08 10:18:54,072 - 10 - training_embed.py - training - loss: 153.834412
2019-01-08 10:18:54,072 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:18:54,423 - 10 - training_embed.py - training - Epoch: [2][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.1124 (153.8444)	
2019-01-08 10:18:54,636 - 10 - training_embed.py - training - Epoch: [2][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.9041 (153.8976)	
2019-01-08 10:18:54,841 - 10 - training_embed.py - training - Epoch: [2][300/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.1311 (153.8818)	
2019-01-08 10:18:55,050 - 10 - training_embed.py - training - Epoch: [2][400/507]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 152.1211 (153.8302)	
2019-01-08 10:18:55,256 - 10 - training_embed.py - training - Epoch: [2][500/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 153.3862 (153.8414)	
2019-01-08 10:18:55,267 - 10 - training_embed.py - training - loss: 153.828737
2019-01-08 10:18:55,268 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:18:55,607 - 10 - training_embed.py - training - Epoch: [3][100/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 152.5829 (153.8245)	
2019-01-08 10:18:55,810 - 10 - training_embed.py - training - Epoch: [3][200/507]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 153.6774 (153.8579)	
2019-01-08 10:18:56,025 - 10 - training_embed.py - training - Epoch: [3][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.5865 (153.8834)	
2019-01-08 10:18:56,246 - 10 - training_embed.py - training - Epoch: [3][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.4446 (153.8607)	
2019-01-08 10:18:56,458 - 10 - training_embed.py - training - Epoch: [3][500/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 153.8019 (153.8413)	
2019-01-08 10:18:56,468 - 10 - training_embed.py - training - loss: 153.822715
2019-01-08 10:18:56,469 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:18:56,812 - 10 - training_embed.py - training - Epoch: [4][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.0884 (153.6804)	
2019-01-08 10:18:57,017 - 10 - training_embed.py - training - Epoch: [4][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.3198 (153.7539)	
2019-01-08 10:18:57,227 - 10 - training_embed.py - training - Epoch: [4][300/507]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 152.3701 (153.7583)	
2019-01-08 10:18:57,446 - 10 - training_embed.py - training - Epoch: [4][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.2934 (153.8017)	
2019-01-08 10:18:57,664 - 10 - training_embed.py - training - Epoch: [4][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.4147 (153.8339)	
2019-01-08 10:18:57,675 - 10 - training_embed.py - training - loss: 153.817519
2019-01-08 10:18:57,675 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:18:58,046 - 10 - training_embed.py - training - Epoch: [5][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9585 (153.7967)	
2019-01-08 10:18:58,255 - 10 - training_embed.py - training - Epoch: [5][200/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 153.0685 (153.8279)	
2019-01-08 10:18:58,466 - 10 - training_embed.py - training - Epoch: [5][300/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 151.8841 (153.7830)	
2019-01-08 10:18:58,679 - 10 - training_embed.py - training - Epoch: [5][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.2987 (153.7901)	
2019-01-08 10:18:58,903 - 10 - training_embed.py - training - Epoch: [5][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8781 (153.8239)	
2019-01-08 10:18:58,914 - 10 - training_embed.py - training - loss: 153.812108
2019-01-08 10:18:58,914 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:18:59,252 - 10 - training_embed.py - training - Epoch: [6][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.0590 (153.9330)	
2019-01-08 10:18:59,467 - 10 - training_embed.py - training - Epoch: [6][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.3287 (153.9082)	
2019-01-08 10:18:59,677 - 10 - training_embed.py - training - Epoch: [6][300/507]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 153.7874 (153.8147)	
2019-01-08 10:18:59,899 - 10 - training_embed.py - training - Epoch: [6][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 150.7884 (153.8468)	
2019-01-08 10:19:00,107 - 10 - training_embed.py - training - Epoch: [6][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.2961 (153.8252)	
2019-01-08 10:19:00,118 - 10 - training_embed.py - training - loss: 153.805612
2019-01-08 10:19:00,118 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:19:00,466 - 10 - training_embed.py - training - Epoch: [7][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.7127 (153.9791)	
2019-01-08 10:19:00,680 - 10 - training_embed.py - training - Epoch: [7][200/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.8508 (153.8869)	
2019-01-08 10:19:00,897 - 10 - training_embed.py - training - Epoch: [7][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8002 (153.8179)	
2019-01-08 10:19:01,114 - 10 - training_embed.py - training - Epoch: [7][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.7271 (153.8261)	
2019-01-08 10:19:01,317 - 10 - training_embed.py - training - Epoch: [7][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.1803 (153.8200)	
2019-01-08 10:19:01,329 - 10 - training_embed.py - training - loss: 153.800605
2019-01-08 10:19:01,329 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:19:01,680 - 10 - training_embed.py - training - Epoch: [8][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.3839 (153.5558)	
2019-01-08 10:19:01,910 - 10 - training_embed.py - training - Epoch: [8][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.3349 (153.7337)	
2019-01-08 10:19:02,113 - 10 - training_embed.py - training - Epoch: [8][300/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.3392 (153.7885)	
2019-01-08 10:19:02,329 - 10 - training_embed.py - training - Epoch: [8][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.4204 (153.8037)	
2019-01-08 10:19:02,551 - 10 - training_embed.py - training - Epoch: [8][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.6117 (153.8136)	
2019-01-08 10:19:02,563 - 10 - training_embed.py - training - loss: 153.795747
2019-01-08 10:19:02,563 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:19:02,912 - 10 - training_embed.py - training - Epoch: [9][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.8230 (153.7934)	
2019-01-08 10:19:03,119 - 10 - training_embed.py - training - Epoch: [9][200/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 152.7513 (153.9050)	
2019-01-08 10:19:03,335 - 10 - training_embed.py - training - Epoch: [9][300/507]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 154.3040 (153.8471)	
2019-01-08 10:19:03,545 - 10 - training_embed.py - training - Epoch: [9][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.4869 (153.8417)	
2019-01-08 10:19:03,765 - 10 - training_embed.py - training - Epoch: [9][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.4730 (153.8053)	
2019-01-08 10:19:03,777 - 10 - training_embed.py - training - loss: 153.788376
2019-01-08 10:19:03,777 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:19:04,124 - 10 - training_embed.py - training - Epoch: [10][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.2284 (153.8187)	
2019-01-08 10:19:04,342 - 10 - training_embed.py - training - Epoch: [10][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.0095 (153.8203)	
2019-01-08 10:19:04,558 - 10 - training_embed.py - training - Epoch: [10][300/507]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 155.2354 (153.8110)	
2019-01-08 10:19:04,768 - 10 - training_embed.py - training - Epoch: [10][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.2977 (153.8174)	
2019-01-08 10:19:04,966 - 10 - training_embed.py - training - Epoch: [10][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.0858 (153.8073)	
2019-01-08 10:19:04,977 - 10 - training_embed.py - training - loss: 153.783301
2019-01-08 10:19:04,978 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:19:05,339 - 10 - training_embed.py - training - Epoch: [11][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.4837 (153.8324)	
2019-01-08 10:19:05,547 - 10 - training_embed.py - training - Epoch: [11][200/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 152.2118 (153.7871)	
2019-01-08 10:19:05,766 - 10 - training_embed.py - training - Epoch: [11][300/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.4042 (153.8075)	
2019-01-08 10:19:05,980 - 10 - training_embed.py - training - Epoch: [11][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.4292 (153.8016)	
2019-01-08 10:19:06,184 - 10 - training_embed.py - training - Epoch: [11][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.7634 (153.7991)	
2019-01-08 10:19:06,197 - 10 - training_embed.py - training - loss: 153.778509
2019-01-08 10:19:06,216 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:19:06,399 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 182.039 ms ~ 0.003 min ~ 0.182 sec
2019-01-08 10:19:06,588 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 371.883 ms ~ 0.006 min ~ 0.372 sec
2019-01-08 10:19:06,588 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:19:06,589 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:19:06,589 - 10 - corpus.py - subactivity_sampler - [32473. 32398. 32365. 32315.]
2019-01-08 10:19:10,133 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:19:10,133 - 10 - corpus.py - subactivity_sampler - [32487. 32270. 32519. 32275.]
2019-01-08 10:19:13,752 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:19:13,752 - 10 - corpus.py - subactivity_sampler - [32733. 32079. 32565. 32174.]
2019-01-08 10:19:17,103 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:19:17,103 - 10 - corpus.py - subactivity_sampler - [33083. 32050. 32270. 32148.]
2019-01-08 10:19:20,295 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:19:20,295 - 10 - corpus.py - subactivity_sampler - [33301. 32118. 31978. 32154.]
2019-01-08 10:19:23,436 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:19:23,437 - 10 - corpus.py - subactivity_sampler - [33580. 32604. 31367. 32000.]
2019-01-08 10:19:26,794 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:19:26,794 - 10 - corpus.py - subactivity_sampler - [33984. 32586. 30993. 31988.]
2019-01-08 10:19:30,147 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:19:30,147 - 10 - corpus.py - subactivity_sampler - [34659. 32770. 30023. 32099.]
2019-01-08 10:19:33,896 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:19:33,896 - 10 - corpus.py - subactivity_sampler - [34973. 33025. 29204. 32349.]
2019-01-08 10:19:37,360 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:19:37,360 - 10 - corpus.py - subactivity_sampler - [35329. 34003. 27754. 32465.]
2019-01-08 10:19:37,859 - 10 - corpus.py - subactivity_sampler - [35290. 34252. 27539. 32470.]
2019-01-08 10:19:37,859 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 31271.132 ms ~ 0.521 min ~ 31.271 sec
2019-01-08 10:19:37,860 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:19:38,287 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:19:38,288 - 10 - corpus.py - ordering_sampler - inv_count_vec: [0. 0. 0.]
2019-01-08 10:19:38,288 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:19:38,300 - 10 - corpus.py - rho_sampling - ['65.3203', '49.9638', '928.9172']
2019-01-08 10:19:38,301 - 10 - pipeline.py - baseline - Iteration 1
2019-01-08 10:19:38,333 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:19:38,335 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:19:38,335 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 4', '1: 1', '2: 2', '3: 3']
2019-01-08 10:19:38,338 - 10 - accuracy_class.py - mof_val - frames true: 63559	frames overall : 129551
2019-01-08 10:19:38,338 - 10 - corpus.py - accuracy_corpus - Action: cereals
2019-01-08 10:19:38,338 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4906098756474284
2019-01-08 10:19:38,339 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4906098756474284
2019-01-08 10:19:38,339 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:19:38,339 - 10 - accuracy_class.py - mof_classes - label 1: 0.541355  28151 / 52001
2019-01-08 10:19:38,339 - 10 - accuracy_class.py - mof_classes - label 2: 0.520106  19634 / 37750
2019-01-08 10:19:38,339 - 10 - accuracy_class.py - mof_classes - label 3: 0.800509  6605 / 8251
2019-01-08 10:19:38,339 - 10 - accuracy_class.py - mof_classes - label 4: 0.975633  9169 / 9398
2019-01-08 10:19:38,339 - 10 - accuracy_class.py - mof_classes - average class mof: 0.567521
2019-01-08 10:19:38,339 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:19:38,339 - 10 - accuracy_class.py - iou_classes - label 1: 0.484510  28151 / 58102
2019-01-08 10:19:38,339 - 10 - accuracy_class.py - iou_classes - label 2: 0.430051  19634 / 45655
2019-01-08 10:19:38,339 - 10 - accuracy_class.py - iou_classes - label 3: 0.193604  6605 / 34116
2019-01-08 10:19:38,339 - 10 - accuracy_class.py - iou_classes - label 4: 0.258144  9169 / 35519
2019-01-08 10:19:38,339 - 10 - accuracy_class.py - iou_classes - average IoU: 0.341577
2019-01-08 10:19:38,339 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.273262
2019-01-08 10:19:39,934 - 10 - f1_score.py - f1 - f1 score: 0.484084
2019-01-08 10:19:39,939 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1638.341 ms ~ 0.027 min ~ 1.638 sec
2019-01-08 10:19:39,939 - 10 - corpus.py - embedding_training - .
2019-01-08 10:19:39,939 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:19:39,939 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:19:39,939 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:19:40,761 - 10 - training_embed.py - training - create model
2019-01-08 10:19:40,762 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:19:40,762 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:19:41,101 - 10 - training_embed.py - training - Epoch: [0][100/507]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 155.3053 (154.1829)	
2019-01-08 10:19:41,307 - 10 - training_embed.py - training - Epoch: [0][200/507]	Time 0.005 (0.003)	Data 0.002 (0.002)	Loss 154.9628 (154.1505)	
2019-01-08 10:19:41,537 - 10 - training_embed.py - training - Epoch: [0][300/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.1113 (154.1217)	
2019-01-08 10:19:41,752 - 10 - training_embed.py - training - Epoch: [0][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.5635 (154.1460)	
2019-01-08 10:19:41,974 - 10 - training_embed.py - training - Epoch: [0][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.4954 (154.1453)	
2019-01-08 10:19:41,985 - 10 - training_embed.py - training - loss: 154.132113
2019-01-08 10:19:41,986 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:19:42,337 - 10 - training_embed.py - training - Epoch: [1][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.0765 (154.1841)	
2019-01-08 10:19:42,538 - 10 - training_embed.py - training - Epoch: [1][200/507]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 156.0594 (154.2026)	
2019-01-08 10:19:42,752 - 10 - training_embed.py - training - Epoch: [1][300/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.4145 (154.1947)	
2019-01-08 10:19:42,968 - 10 - training_embed.py - training - Epoch: [1][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8348 (154.1620)	
2019-01-08 10:19:43,211 - 10 - training_embed.py - training - Epoch: [1][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.7778 (154.1402)	
2019-01-08 10:19:43,221 - 10 - training_embed.py - training - loss: 154.121484
2019-01-08 10:19:43,221 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:19:43,596 - 10 - training_embed.py - training - Epoch: [2][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.2354 (154.1287)	
2019-01-08 10:19:43,794 - 10 - training_embed.py - training - Epoch: [2][200/507]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 152.1395 (154.1776)	
2019-01-08 10:19:44,013 - 10 - training_embed.py - training - Epoch: [2][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.6685 (154.1554)	
2019-01-08 10:19:44,228 - 10 - training_embed.py - training - Epoch: [2][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 152.8663 (154.1144)	
2019-01-08 10:19:44,445 - 10 - training_embed.py - training - Epoch: [2][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.9377 (154.1204)	
2019-01-08 10:19:44,456 - 10 - training_embed.py - training - loss: 154.109210
2019-01-08 10:19:44,461 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:19:44,829 - 10 - training_embed.py - training - Epoch: [3][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.7235 (154.1399)	
2019-01-08 10:19:45,050 - 10 - training_embed.py - training - Epoch: [3][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.4240 (154.1542)	
2019-01-08 10:19:45,277 - 10 - training_embed.py - training - Epoch: [3][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8570 (154.1554)	
2019-01-08 10:19:45,492 - 10 - training_embed.py - training - Epoch: [3][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.7069 (154.1377)	
2019-01-08 10:19:45,714 - 10 - training_embed.py - training - Epoch: [3][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.6285 (154.1160)	
2019-01-08 10:19:45,726 - 10 - training_embed.py - training - loss: 154.097788
2019-01-08 10:19:45,727 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:19:46,075 - 10 - training_embed.py - training - Epoch: [4][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.9674 (153.9471)	
2019-01-08 10:19:46,287 - 10 - training_embed.py - training - Epoch: [4][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.5717 (154.0690)	
2019-01-08 10:19:46,503 - 10 - training_embed.py - training - Epoch: [4][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.9129 (154.0607)	
2019-01-08 10:19:46,717 - 10 - training_embed.py - training - Epoch: [4][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.4345 (154.0690)	
2019-01-08 10:19:46,925 - 10 - training_embed.py - training - Epoch: [4][500/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.6358 (154.1024)	
2019-01-08 10:19:46,936 - 10 - training_embed.py - training - loss: 154.086501
2019-01-08 10:19:46,937 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:19:47,251 - 10 - training_embed.py - training - Epoch: [5][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.0462 (154.0822)	
2019-01-08 10:19:47,464 - 10 - training_embed.py - training - Epoch: [5][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.9442 (154.0912)	
2019-01-08 10:19:47,678 - 10 - training_embed.py - training - Epoch: [5][300/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 152.0111 (154.0600)	
2019-01-08 10:19:47,898 - 10 - training_embed.py - training - Epoch: [5][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.4778 (154.0692)	
2019-01-08 10:19:48,117 - 10 - training_embed.py - training - Epoch: [5][500/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 155.3866 (154.0873)	
2019-01-08 10:19:48,129 - 10 - training_embed.py - training - loss: 154.074504
2019-01-08 10:19:48,129 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:19:48,491 - 10 - training_embed.py - training - Epoch: [6][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.0593 (154.2092)	
2019-01-08 10:19:48,691 - 10 - training_embed.py - training - Epoch: [6][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.2216 (154.1325)	
2019-01-08 10:19:48,928 - 10 - training_embed.py - training - Epoch: [6][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.2524 (154.0711)	
2019-01-08 10:19:49,144 - 10 - training_embed.py - training - Epoch: [6][400/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 152.9204 (154.0900)	
2019-01-08 10:19:49,386 - 10 - training_embed.py - training - Epoch: [6][500/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.3783 (154.0791)	
2019-01-08 10:19:49,397 - 10 - training_embed.py - training - loss: 154.061477
2019-01-08 10:19:49,397 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:19:49,737 - 10 - training_embed.py - training - Epoch: [7][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9716 (154.1543)	
2019-01-08 10:19:49,952 - 10 - training_embed.py - training - Epoch: [7][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8676 (154.1377)	
2019-01-08 10:19:50,166 - 10 - training_embed.py - training - Epoch: [7][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.0596 (154.1209)	
2019-01-08 10:19:50,385 - 10 - training_embed.py - training - Epoch: [7][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.6699 (154.1091)	
2019-01-08 10:19:50,597 - 10 - training_embed.py - training - Epoch: [7][500/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 151.8442 (154.0714)	
2019-01-08 10:19:50,613 - 10 - training_embed.py - training - loss: 154.051051
2019-01-08 10:19:50,613 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:19:50,980 - 10 - training_embed.py - training - Epoch: [8][100/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 152.2137 (153.7734)	
2019-01-08 10:19:51,200 - 10 - training_embed.py - training - Epoch: [8][200/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.4551 (153.9827)	
2019-01-08 10:19:51,411 - 10 - training_embed.py - training - Epoch: [8][300/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.7828 (154.0451)	
2019-01-08 10:19:51,635 - 10 - training_embed.py - training - Epoch: [8][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.4771 (154.0421)	
2019-01-08 10:19:51,845 - 10 - training_embed.py - training - Epoch: [8][500/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.3797 (154.0593)	
2019-01-08 10:19:51,857 - 10 - training_embed.py - training - loss: 154.039863
2019-01-08 10:19:51,861 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:19:52,183 - 10 - training_embed.py - training - Epoch: [9][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.4874 (154.0562)	
2019-01-08 10:19:52,387 - 10 - training_embed.py - training - Epoch: [9][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.1966 (154.1045)	
2019-01-08 10:19:52,601 - 10 - training_embed.py - training - Epoch: [9][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.3738 (154.0752)	
2019-01-08 10:19:52,829 - 10 - training_embed.py - training - Epoch: [9][400/507]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 154.8496 (154.0726)	
2019-01-08 10:19:53,070 - 10 - training_embed.py - training - Epoch: [9][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.2535 (154.0427)	
2019-01-08 10:19:53,081 - 10 - training_embed.py - training - loss: 154.026348
2019-01-08 10:19:53,081 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:19:53,450 - 10 - training_embed.py - training - Epoch: [10][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.7887 (153.9364)	
2019-01-08 10:19:53,667 - 10 - training_embed.py - training - Epoch: [10][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9769 (154.0172)	
2019-01-08 10:19:53,906 - 10 - training_embed.py - training - Epoch: [10][300/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 156.5721 (154.0122)	
2019-01-08 10:19:54,137 - 10 - training_embed.py - training - Epoch: [10][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.9509 (154.0303)	
2019-01-08 10:19:54,342 - 10 - training_embed.py - training - Epoch: [10][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.0738 (154.0338)	
2019-01-08 10:19:54,357 - 10 - training_embed.py - training - loss: 154.015462
2019-01-08 10:19:54,357 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:19:54,721 - 10 - training_embed.py - training - Epoch: [11][100/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 151.7459 (153.9496)	
2019-01-08 10:19:54,935 - 10 - training_embed.py - training - Epoch: [11][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.6442 (153.9720)	
2019-01-08 10:19:55,155 - 10 - training_embed.py - training - Epoch: [11][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.2043 (154.0202)	
2019-01-08 10:19:55,367 - 10 - training_embed.py - training - Epoch: [11][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.4743 (154.0359)	
2019-01-08 10:19:55,585 - 10 - training_embed.py - training - Epoch: [11][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.6184 (154.0237)	
2019-01-08 10:19:55,597 - 10 - training_embed.py - training - loss: 154.003116
2019-01-08 10:19:55,618 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:19:55,836 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 217.768 ms ~ 0.004 min ~ 0.218 sec
2019-01-08 10:19:56,024 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 405.834 ms ~ 0.007 min ~ 0.406 sec
2019-01-08 10:19:56,024 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:19:56,025 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:19:56,025 - 10 - corpus.py - subactivity_sampler - [35290. 34252. 27539. 32470.]
2019-01-08 10:19:59,635 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:19:59,636 - 10 - corpus.py - subactivity_sampler - [35436. 34423. 27345. 32347.]
2019-01-08 10:20:03,348 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:20:03,349 - 10 - corpus.py - subactivity_sampler - [35427. 35087. 26599. 32438.]
2019-01-08 10:20:06,820 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:20:06,820 - 10 - corpus.py - subactivity_sampler - [35639. 35627. 25620. 32665.]
2019-01-08 10:20:10,126 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:20:10,126 - 10 - corpus.py - subactivity_sampler - [35691. 36186. 24696. 32978.]
2019-01-08 10:20:13,385 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:20:13,385 - 10 - corpus.py - subactivity_sampler - [35845. 36477. 24000. 33229.]
2019-01-08 10:20:16,854 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:20:16,854 - 10 - corpus.py - subactivity_sampler - [35847. 37088. 23113. 33503.]
2019-01-08 10:20:20,320 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:20:20,320 - 10 - corpus.py - subactivity_sampler - [35822. 37597. 22385. 33747.]
2019-01-08 10:20:24,239 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:20:24,240 - 10 - corpus.py - subactivity_sampler - [35753. 38238. 21353. 34207.]
2019-01-08 10:20:27,879 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:20:27,879 - 10 - corpus.py - subactivity_sampler - [35606. 39502. 20554. 33889.]
2019-01-08 10:20:28,402 - 10 - corpus.py - subactivity_sampler - [35606. 39680. 20485. 33780.]
2019-01-08 10:20:28,402 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 32377.638 ms ~ 0.540 min ~ 32.378 sec
2019-01-08 10:20:28,402 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:20:28,836 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:20:28,837 - 10 - corpus.py - ordering_sampler - inv_count_vec: [2. 5. 0.]
2019-01-08 10:20:28,837 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:20:28,848 - 10 - corpus.py - rho_sampling - ['57.8402', '11.6227', '928.7809']
2019-01-08 10:20:28,848 - 10 - pipeline.py - baseline - Iteration 2
2019-01-08 10:20:28,881 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:20:28,883 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:20:28,883 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 4', '1: 1', '2: 2', '3: 3']
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - mof_val - frames true: 60384	frames overall : 129551
2019-01-08 10:20:28,887 - 10 - corpus.py - accuracy_corpus - Action: cereals
2019-01-08 10:20:28,887 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4661021528201249
2019-01-08 10:20:28,887 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4661021528201249
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - mof_classes - label 1: 0.569912  29636 / 52001
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - mof_classes - label 2: 0.390702  14749 / 37750
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - mof_classes - label 3: 0.830445  6852 / 8251
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - mof_classes - label 4: 0.973292  9147 / 9398
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - mof_classes - average class mof: 0.552870
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - iou_classes - label 1: 0.477653  29636 / 62045
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - iou_classes - label 2: 0.339167  14749 / 43486
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - iou_classes - label 3: 0.194775  6852 / 35179
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - iou_classes - label 4: 0.255097  9147 / 35857
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - iou_classes - average IoU: 0.316673
2019-01-08 10:20:28,887 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.253338
2019-01-08 10:20:30,516 - 10 - f1_score.py - f1 - f1 score: 0.458401
2019-01-08 10:20:30,523 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1674.533 ms ~ 0.028 min ~ 1.675 sec
2019-01-08 10:20:30,523 - 10 - corpus.py - embedding_training - .
2019-01-08 10:20:30,523 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:20:30,523 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:20:30,523 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:20:31,345 - 10 - training_embed.py - training - create model
2019-01-08 10:20:31,346 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:20:31,346 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:20:31,693 - 10 - training_embed.py - training - Epoch: [0][100/507]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 155.6187 (154.1802)	
2019-01-08 10:20:31,899 - 10 - training_embed.py - training - Epoch: [0][200/507]	Time 0.004 (0.003)	Data 0.002 (0.002)	Loss 155.0720 (154.1832)	
2019-01-08 10:20:32,099 - 10 - training_embed.py - training - Epoch: [0][300/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.2939 (154.1725)	
2019-01-08 10:20:32,311 - 10 - training_embed.py - training - Epoch: [0][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.1107 (154.1697)	
2019-01-08 10:20:32,542 - 10 - training_embed.py - training - Epoch: [0][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.9320 (154.1909)	
2019-01-08 10:20:32,552 - 10 - training_embed.py - training - loss: 154.179980
2019-01-08 10:20:32,552 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:20:32,903 - 10 - training_embed.py - training - Epoch: [1][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.8892 (154.2602)	
2019-01-08 10:20:33,111 - 10 - training_embed.py - training - Epoch: [1][200/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.8993 (154.2501)	
2019-01-08 10:20:33,341 - 10 - training_embed.py - training - Epoch: [1][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.7335 (154.2286)	
2019-01-08 10:20:33,558 - 10 - training_embed.py - training - Epoch: [1][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.4940 (154.1939)	
2019-01-08 10:20:33,765 - 10 - training_embed.py - training - Epoch: [1][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.2297 (154.1803)	
2019-01-08 10:20:33,778 - 10 - training_embed.py - training - loss: 154.163714
2019-01-08 10:20:33,778 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:20:34,154 - 10 - training_embed.py - training - Epoch: [2][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.8418 (154.1047)	
2019-01-08 10:20:34,356 - 10 - training_embed.py - training - Epoch: [2][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.2210 (154.1935)	
2019-01-08 10:20:34,572 - 10 - training_embed.py - training - Epoch: [2][300/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.6632 (154.1733)	
2019-01-08 10:20:34,791 - 10 - training_embed.py - training - Epoch: [2][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.5677 (154.1459)	
2019-01-08 10:20:34,997 - 10 - training_embed.py - training - Epoch: [2][500/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 153.4367 (154.1548)	
2019-01-08 10:20:35,009 - 10 - training_embed.py - training - loss: 154.145879
2019-01-08 10:20:35,009 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:20:35,372 - 10 - training_embed.py - training - Epoch: [3][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.6772 (154.2309)	
2019-01-08 10:20:35,585 - 10 - training_embed.py - training - Epoch: [3][200/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.6452 (154.1788)	
2019-01-08 10:20:35,805 - 10 - training_embed.py - training - Epoch: [3][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.5471 (154.1821)	
2019-01-08 10:20:36,016 - 10 - training_embed.py - training - Epoch: [3][400/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 153.5665 (154.1512)	
2019-01-08 10:20:36,230 - 10 - training_embed.py - training - Epoch: [3][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.9452 (154.1472)	
2019-01-08 10:20:36,241 - 10 - training_embed.py - training - loss: 154.128403
2019-01-08 10:20:36,242 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:20:36,605 - 10 - training_embed.py - training - Epoch: [4][100/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 155.8708 (153.9321)	
2019-01-08 10:20:36,830 - 10 - training_embed.py - training - Epoch: [4][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.7683 (154.0746)	
2019-01-08 10:20:37,033 - 10 - training_embed.py - training - Epoch: [4][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.8112 (154.0643)	
2019-01-08 10:20:37,251 - 10 - training_embed.py - training - Epoch: [4][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.3699 (154.0941)	
2019-01-08 10:20:37,477 - 10 - training_embed.py - training - Epoch: [4][500/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.0728 (154.1287)	
2019-01-08 10:20:37,488 - 10 - training_embed.py - training - loss: 154.112531
2019-01-08 10:20:37,489 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:20:37,838 - 10 - training_embed.py - training - Epoch: [5][100/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.1834 (154.0624)	
2019-01-08 10:20:38,036 - 10 - training_embed.py - training - Epoch: [5][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.2769 (154.0911)	
2019-01-08 10:20:38,274 - 10 - training_embed.py - training - Epoch: [5][300/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 151.6690 (154.0901)	
2019-01-08 10:20:38,476 - 10 - training_embed.py - training - Epoch: [5][400/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 155.4716 (154.1002)	
2019-01-08 10:20:38,697 - 10 - training_embed.py - training - Epoch: [5][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9164 (154.1050)	
2019-01-08 10:20:38,709 - 10 - training_embed.py - training - loss: 154.094497
2019-01-08 10:20:38,709 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:20:39,077 - 10 - training_embed.py - training - Epoch: [6][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8549 (154.1918)	
2019-01-08 10:20:39,293 - 10 - training_embed.py - training - Epoch: [6][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.9296 (154.1675)	
2019-01-08 10:20:39,533 - 10 - training_embed.py - training - Epoch: [6][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.7309 (154.1170)	
2019-01-08 10:20:39,761 - 10 - training_embed.py - training - Epoch: [6][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.5329 (154.1093)	
2019-01-08 10:20:39,998 - 10 - training_embed.py - training - Epoch: [6][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.3111 (154.0926)	
2019-01-08 10:20:40,009 - 10 - training_embed.py - training - loss: 154.076023
2019-01-08 10:20:40,010 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:20:40,357 - 10 - training_embed.py - training - Epoch: [7][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.7015 (154.1917)	
2019-01-08 10:20:40,554 - 10 - training_embed.py - training - Epoch: [7][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.4161 (154.1358)	
2019-01-08 10:20:40,771 - 10 - training_embed.py - training - Epoch: [7][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.2451 (154.1435)	
2019-01-08 10:20:40,982 - 10 - training_embed.py - training - Epoch: [7][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.1711 (154.1324)	
2019-01-08 10:20:41,197 - 10 - training_embed.py - training - Epoch: [7][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.8025 (154.0817)	
2019-01-08 10:20:41,209 - 10 - training_embed.py - training - loss: 154.060004
2019-01-08 10:20:41,209 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:20:41,534 - 10 - training_embed.py - training - Epoch: [8][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.6224 (153.7982)	
2019-01-08 10:20:41,745 - 10 - training_embed.py - training - Epoch: [8][200/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.7569 (154.0222)	
2019-01-08 10:20:41,962 - 10 - training_embed.py - training - Epoch: [8][300/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.8235 (154.0614)	
2019-01-08 10:20:42,172 - 10 - training_embed.py - training - Epoch: [8][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.5871 (154.0507)	
2019-01-08 10:20:42,412 - 10 - training_embed.py - training - Epoch: [8][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.9677 (154.0639)	
2019-01-08 10:20:42,422 - 10 - training_embed.py - training - loss: 154.043082
2019-01-08 10:20:42,422 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:20:42,783 - 10 - training_embed.py - training - Epoch: [9][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.0517 (154.1163)	
2019-01-08 10:20:42,986 - 10 - training_embed.py - training - Epoch: [9][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.9593 (154.1204)	
2019-01-08 10:20:43,191 - 10 - training_embed.py - training - Epoch: [9][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.4317 (154.1049)	
2019-01-08 10:20:43,417 - 10 - training_embed.py - training - Epoch: [9][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9504 (154.0754)	
2019-01-08 10:20:43,625 - 10 - training_embed.py - training - Epoch: [9][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.1748 (154.0407)	
2019-01-08 10:20:43,637 - 10 - training_embed.py - training - loss: 154.024210
2019-01-08 10:20:43,638 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:20:43,988 - 10 - training_embed.py - training - Epoch: [10][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.0750 (153.8957)	
2019-01-08 10:20:44,197 - 10 - training_embed.py - training - Epoch: [10][200/507]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 154.3850 (154.0062)	
2019-01-08 10:20:44,426 - 10 - training_embed.py - training - Epoch: [10][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.1594 (154.0154)	
2019-01-08 10:20:44,656 - 10 - training_embed.py - training - Epoch: [10][400/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 153.1690 (154.0254)	
2019-01-08 10:20:44,867 - 10 - training_embed.py - training - Epoch: [10][500/507]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 153.1078 (154.0224)	
2019-01-08 10:20:44,879 - 10 - training_embed.py - training - loss: 154.007504
2019-01-08 10:20:44,879 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:20:45,233 - 10 - training_embed.py - training - Epoch: [11][100/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 152.0516 (153.8781)	
2019-01-08 10:20:45,454 - 10 - training_embed.py - training - Epoch: [11][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.7917 (153.9628)	
2019-01-08 10:20:45,677 - 10 - training_embed.py - training - Epoch: [11][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.0461 (154.0055)	
2019-01-08 10:20:45,892 - 10 - training_embed.py - training - Epoch: [11][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 153.8343 (154.0067)	
2019-01-08 10:20:46,106 - 10 - training_embed.py - training - Epoch: [11][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.3999 (154.0097)	
2019-01-08 10:20:46,117 - 10 - training_embed.py - training - loss: 153.989649
2019-01-08 10:20:46,137 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:20:46,379 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 241.672 ms ~ 0.004 min ~ 0.242 sec
2019-01-08 10:20:46,605 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 467.227 ms ~ 0.008 min ~ 0.467 sec
2019-01-08 10:20:46,605 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:20:46,605 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:20:46,606 - 10 - corpus.py - subactivity_sampler - [35606. 39680. 20485. 33780.]
2019-01-08 10:20:50,227 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:20:50,227 - 10 - corpus.py - subactivity_sampler - [35371. 40483. 19603. 34094.]
2019-01-08 10:20:53,930 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:20:53,930 - 10 - corpus.py - subactivity_sampler - [35168. 41885. 18245. 34253.]
2019-01-08 10:20:57,366 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:20:57,366 - 10 - corpus.py - subactivity_sampler - [35069. 43038. 17243. 34201.]
2019-01-08 10:21:00,658 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:21:00,658 - 10 - corpus.py - subactivity_sampler - [34793. 44588. 16184. 33986.]
2019-01-08 10:21:03,885 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:21:03,885 - 10 - corpus.py - subactivity_sampler - [34714. 45274. 15404. 34159.]
2019-01-08 10:21:07,333 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:21:07,333 - 10 - corpus.py - subactivity_sampler - [34522. 46407. 14484. 34138.]
2019-01-08 10:21:10,796 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:21:10,797 - 10 - corpus.py - subactivity_sampler - [34416. 48193. 12984. 33958.]
2019-01-08 10:21:14,687 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:21:14,688 - 10 - corpus.py - subactivity_sampler - [34268. 49362. 11991. 33930.]
2019-01-08 10:21:18,268 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:21:18,268 - 10 - corpus.py - subactivity_sampler - [34011. 50675. 11326. 33539.]
2019-01-08 10:21:18,782 - 10 - corpus.py - subactivity_sampler - [34001. 50837. 11230. 33483.]
2019-01-08 10:21:18,782 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 32176.981 ms ~ 0.536 min ~ 32.177 sec
2019-01-08 10:21:18,782 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:21:19,196 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:21:19,196 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 1. 15.  0.]
2019-01-08 10:21:19,196 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:21:19,207 - 10 - corpus.py - rho_sampling - ['58.7651', '21.4523', '928.6447']
2019-01-08 10:21:19,207 - 10 - pipeline.py - baseline - Iteration 3
2019-01-08 10:21:19,239 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:21:19,242 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:21:19,242 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 4', '1: 1', '2: 3', '3: 2']
2019-01-08 10:21:19,245 - 10 - accuracy_class.py - mof_val - frames true: 55018	frames overall : 129551
2019-01-08 10:21:19,245 - 10 - corpus.py - accuracy_corpus - Action: cereals
2019-01-08 10:21:19,245 - 10 - corpus.py - accuracy_corpus - MoF val: 0.424682171500027
2019-01-08 10:21:19,245 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4187154093754583
2019-01-08 10:21:19,245 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:21:19,245 - 10 - accuracy_class.py - mof_classes - label 1: 0.609296  31684 / 52001
2019-01-08 10:21:19,245 - 10 - accuracy_class.py - mof_classes - label 2: 0.363444  13720 / 37750
2019-01-08 10:21:19,245 - 10 - accuracy_class.py - mof_classes - label 3: 0.080596  665 / 8251
2019-01-08 10:21:19,245 - 10 - accuracy_class.py - mof_classes - label 4: 0.952224  8949 / 9398
2019-01-08 10:21:19,245 - 10 - accuracy_class.py - mof_classes - average class mof: 0.401112
2019-01-08 10:21:19,245 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:21:19,245 - 10 - accuracy_class.py - iou_classes - label 1: 0.445288  31684 / 71154
2019-01-08 10:21:19,245 - 10 - accuracy_class.py - iou_classes - label 2: 0.238555  13720 / 57513
2019-01-08 10:21:19,245 - 10 - accuracy_class.py - iou_classes - label 3: 0.035342  665 / 18816
2019-01-08 10:21:19,246 - 10 - accuracy_class.py - iou_classes - label 4: 0.259768  8949 / 34450
2019-01-08 10:21:19,246 - 10 - accuracy_class.py - iou_classes - average IoU: 0.244738
2019-01-08 10:21:19,246 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.195790
2019-01-08 10:21:20,857 - 10 - f1_score.py - f1 - f1 score: 0.382476
2019-01-08 10:21:20,860 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1653.168 ms ~ 0.028 min ~ 1.653 sec
2019-01-08 10:21:20,860 - 10 - corpus.py - embedding_training - .
2019-01-08 10:21:20,860 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:21:20,860 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:21:20,860 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:21:21,659 - 10 - training_embed.py - training - create model
2019-01-08 10:21:21,660 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:21:21,660 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:21:21,988 - 10 - training_embed.py - training - Epoch: [0][100/507]	Time 0.003 (0.003)	Data 0.002 (0.002)	Loss 156.5502 (154.1205)	
2019-01-08 10:21:22,205 - 10 - training_embed.py - training - Epoch: [0][200/507]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 154.3663 (154.1444)	
2019-01-08 10:21:22,423 - 10 - training_embed.py - training - Epoch: [0][300/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.7904 (154.1581)	
2019-01-08 10:21:22,635 - 10 - training_embed.py - training - Epoch: [0][400/507]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 154.7741 (154.1759)	
2019-01-08 10:21:22,856 - 10 - training_embed.py - training - Epoch: [0][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9512 (154.2047)	
2019-01-08 10:21:22,868 - 10 - training_embed.py - training - loss: 154.188988
2019-01-08 10:21:22,868 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:21:23,211 - 10 - training_embed.py - training - Epoch: [1][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.5620 (154.2603)	
2019-01-08 10:21:23,425 - 10 - training_embed.py - training - Epoch: [1][200/507]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 156.1819 (154.2798)	
2019-01-08 10:21:23,629 - 10 - training_embed.py - training - Epoch: [1][300/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.6173 (154.2500)	
2019-01-08 10:21:23,865 - 10 - training_embed.py - training - Epoch: [1][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.2870 (154.1909)	
2019-01-08 10:21:24,076 - 10 - training_embed.py - training - Epoch: [1][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9302 (154.1759)	
2019-01-08 10:21:24,087 - 10 - training_embed.py - training - loss: 154.159886
2019-01-08 10:21:24,087 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:21:24,459 - 10 - training_embed.py - training - Epoch: [2][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.2056 (154.1552)	
2019-01-08 10:21:24,662 - 10 - training_embed.py - training - Epoch: [2][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.1605 (154.2529)	
2019-01-08 10:21:24,888 - 10 - training_embed.py - training - Epoch: [2][300/507]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 154.6911 (154.2171)	
2019-01-08 10:21:25,117 - 10 - training_embed.py - training - Epoch: [2][400/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 152.6319 (154.1768)	
2019-01-08 10:21:25,336 - 10 - training_embed.py - training - Epoch: [2][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.5478 (154.1425)	
2019-01-08 10:21:25,347 - 10 - training_embed.py - training - loss: 154.129407
2019-01-08 10:21:25,348 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:21:25,722 - 10 - training_embed.py - training - Epoch: [3][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.3958 (154.1649)	
2019-01-08 10:21:25,945 - 10 - training_embed.py - training - Epoch: [3][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9165 (154.1516)	
2019-01-08 10:21:26,152 - 10 - training_embed.py - training - Epoch: [3][300/507]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 154.7092 (154.1706)	
2019-01-08 10:21:26,365 - 10 - training_embed.py - training - Epoch: [3][400/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 152.8960 (154.1339)	
2019-01-08 10:21:26,587 - 10 - training_embed.py - training - Epoch: [3][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.2087 (154.1171)	
2019-01-08 10:21:26,599 - 10 - training_embed.py - training - loss: 154.099329
2019-01-08 10:21:26,600 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:21:26,968 - 10 - training_embed.py - training - Epoch: [4][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.6214 (153.9753)	
2019-01-08 10:21:27,173 - 10 - training_embed.py - training - Epoch: [4][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.9951 (154.0924)	
2019-01-08 10:21:27,386 - 10 - training_embed.py - training - Epoch: [4][300/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 153.5752 (154.0610)	
2019-01-08 10:21:27,604 - 10 - training_embed.py - training - Epoch: [4][400/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 153.3292 (154.0684)	
2019-01-08 10:21:27,836 - 10 - training_embed.py - training - Epoch: [4][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.5987 (154.0874)	
2019-01-08 10:21:27,846 - 10 - training_embed.py - training - loss: 154.070262
2019-01-08 10:21:27,847 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:21:28,208 - 10 - training_embed.py - training - Epoch: [5][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.5047 (153.9783)	
2019-01-08 10:21:28,410 - 10 - training_embed.py - training - Epoch: [5][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.4800 (154.0676)	
2019-01-08 10:21:28,629 - 10 - training_embed.py - training - Epoch: [5][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.9394 (154.0504)	
2019-01-08 10:21:28,844 - 10 - training_embed.py - training - Epoch: [5][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.0268 (154.0345)	
2019-01-08 10:21:29,079 - 10 - training_embed.py - training - Epoch: [5][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9606 (154.0479)	
2019-01-08 10:21:29,090 - 10 - training_embed.py - training - loss: 154.039751
2019-01-08 10:21:29,091 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:21:29,475 - 10 - training_embed.py - training - Epoch: [6][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.7705 (154.1078)	
2019-01-08 10:21:29,691 - 10 - training_embed.py - training - Epoch: [6][200/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.0399 (154.0539)	
2019-01-08 10:21:29,906 - 10 - training_embed.py - training - Epoch: [6][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.3837 (154.0545)	
2019-01-08 10:21:30,109 - 10 - training_embed.py - training - Epoch: [6][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.7659 (154.0270)	
2019-01-08 10:21:30,319 - 10 - training_embed.py - training - Epoch: [6][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.5902 (154.0243)	
2019-01-08 10:21:30,331 - 10 - training_embed.py - training - loss: 154.008578
2019-01-08 10:21:30,331 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:21:30,682 - 10 - training_embed.py - training - Epoch: [7][100/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.4369 (154.0383)	
2019-01-08 10:21:30,902 - 10 - training_embed.py - training - Epoch: [7][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.0200 (154.0846)	
2019-01-08 10:21:31,099 - 10 - training_embed.py - training - Epoch: [7][300/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.5538 (154.0794)	
2019-01-08 10:21:31,306 - 10 - training_embed.py - training - Epoch: [7][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.5845 (154.0810)	
2019-01-08 10:21:31,508 - 10 - training_embed.py - training - Epoch: [7][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.8107 (153.9971)	
2019-01-08 10:21:31,520 - 10 - training_embed.py - training - loss: 153.979895
2019-01-08 10:21:31,520 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:21:31,887 - 10 - training_embed.py - training - Epoch: [8][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.1825 (153.7406)	
2019-01-08 10:21:32,095 - 10 - training_embed.py - training - Epoch: [8][200/507]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 155.8991 (153.9447)	
2019-01-08 10:21:32,307 - 10 - training_embed.py - training - Epoch: [8][300/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.3339 (153.9749)	
2019-01-08 10:21:32,533 - 10 - training_embed.py - training - Epoch: [8][400/507]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 153.5497 (153.9339)	
2019-01-08 10:21:32,736 - 10 - training_embed.py - training - Epoch: [8][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.6423 (153.9681)	
2019-01-08 10:21:32,749 - 10 - training_embed.py - training - loss: 153.950009
2019-01-08 10:21:32,750 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:21:33,137 - 10 - training_embed.py - training - Epoch: [9][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.1673 (154.0432)	
2019-01-08 10:21:33,339 - 10 - training_embed.py - training - Epoch: [9][200/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 153.4180 (154.0062)	
2019-01-08 10:21:33,558 - 10 - training_embed.py - training - Epoch: [9][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.0942 (153.9842)	
2019-01-08 10:21:33,782 - 10 - training_embed.py - training - Epoch: [9][400/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.5107 (153.9757)	
2019-01-08 10:21:33,993 - 10 - training_embed.py - training - Epoch: [9][500/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 152.8095 (153.9311)	
2019-01-08 10:21:34,003 - 10 - training_embed.py - training - loss: 153.918812
2019-01-08 10:21:34,004 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:21:34,357 - 10 - training_embed.py - training - Epoch: [10][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.3491 (153.8376)	
2019-01-08 10:21:34,567 - 10 - training_embed.py - training - Epoch: [10][200/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.2917 (153.8860)	
2019-01-08 10:21:34,786 - 10 - training_embed.py - training - Epoch: [10][300/507]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 155.5903 (153.8679)	
2019-01-08 10:21:35,013 - 10 - training_embed.py - training - Epoch: [10][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 152.8677 (153.9041)	
2019-01-08 10:21:35,231 - 10 - training_embed.py - training - Epoch: [10][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.8421 (153.9038)	
2019-01-08 10:21:35,243 - 10 - training_embed.py - training - loss: 153.889781
2019-01-08 10:21:35,243 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:21:35,603 - 10 - training_embed.py - training - Epoch: [11][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.3236 (153.8156)	
2019-01-08 10:21:35,811 - 10 - training_embed.py - training - Epoch: [11][200/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 151.0279 (153.9072)	
2019-01-08 10:21:36,038 - 10 - training_embed.py - training - Epoch: [11][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.1039 (153.9004)	
2019-01-08 10:21:36,252 - 10 - training_embed.py - training - Epoch: [11][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 153.4123 (153.8817)	
2019-01-08 10:21:36,482 - 10 - training_embed.py - training - Epoch: [11][500/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 156.7455 (153.8797)	
2019-01-08 10:21:36,493 - 10 - training_embed.py - training - loss: 153.858742
2019-01-08 10:21:36,515 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:21:36,717 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 202.755 ms ~ 0.003 min ~ 0.203 sec
2019-01-08 10:21:36,903 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 388.946 ms ~ 0.006 min ~ 0.389 sec
2019-01-08 10:21:36,904 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:21:36,904 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:21:36,904 - 10 - corpus.py - subactivity_sampler - [34001. 50837. 11230. 33483.]
2019-01-08 10:21:40,564 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:21:40,564 - 10 - corpus.py - subactivity_sampler - [33817. 51471. 10472. 33791.]
2019-01-08 10:21:44,303 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:21:44,303 - 10 - corpus.py - subactivity_sampler - [33719. 52110.  9953. 33769.]
2019-01-08 10:21:47,787 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:21:47,787 - 10 - corpus.py - subactivity_sampler - [33633. 53168.  9187. 33563.]
2019-01-08 10:21:51,091 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:21:51,091 - 10 - corpus.py - subactivity_sampler - [33500. 53929.  8868. 33254.]
2019-01-08 10:21:54,341 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:21:54,341 - 10 - corpus.py - subactivity_sampler - [33447. 54908.  8490. 32706.]
2019-01-08 10:21:57,808 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:21:57,809 - 10 - corpus.py - subactivity_sampler - [33301. 56001.  7790. 32459.]
2019-01-08 10:22:01,286 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:22:01,286 - 10 - corpus.py - subactivity_sampler - [33156. 56409.  7588. 32398.]
2019-01-08 10:22:05,176 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:22:05,176 - 10 - corpus.py - subactivity_sampler - [32992. 56883.  7283. 32393.]
2019-01-08 10:22:08,801 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:22:08,801 - 10 - corpus.py - subactivity_sampler - [32807. 57589.  6941. 32214.]
2019-01-08 10:22:09,330 - 10 - corpus.py - subactivity_sampler - [32755. 57651.  6929. 32216.]
2019-01-08 10:22:09,330 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 32426.783 ms ~ 0.540 min ~ 32.427 sec
2019-01-08 10:22:09,330 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:22:09,746 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:22:09,747 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 1. 18.  0.]
2019-01-08 10:22:09,747 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:22:09,757 - 10 - corpus.py - rho_sampling - ['61.5217', '13.6376', '928.5084']
2019-01-08 10:22:09,757 - 10 - pipeline.py - baseline - Iteration 4
2019-01-08 10:22:09,791 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:22:09,793 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:22:09,794 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 4', '1: 1', '2: 3', '3: 2']
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - mof_val - frames true: 55665	frames overall : 129551
2019-01-08 10:22:09,797 - 10 - corpus.py - accuracy_corpus - Action: cereals
2019-01-08 10:22:09,797 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4296763436793232
2019-01-08 10:22:09,797 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4296763436793232
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - mof_classes - label 1: 0.647334  33662 / 52001
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - mof_classes - label 2: 0.341166  12879 / 37750
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - mof_classes - label 3: 0.038298  316 / 8251
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - mof_classes - label 4: 0.937221  8808 / 9398
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - mof_classes - average class mof: 0.392804
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - iou_classes - label 1: 0.442979  33662 / 75990
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - iou_classes - label 2: 0.225603  12879 / 57087
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - iou_classes - label 3: 0.021259  316 / 14864
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - iou_classes - label 4: 0.264148  8808 / 33345
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - iou_classes - average IoU: 0.238497
2019-01-08 10:22:09,797 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.190798
2019-01-08 10:22:11,458 - 10 - f1_score.py - f1 - f1 score: 0.382645
2019-01-08 10:22:11,462 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1704.773 ms ~ 0.028 min ~ 1.705 sec
2019-01-08 10:22:11,462 - 10 - corpus.py - embedding_training - .
2019-01-08 10:22:11,462 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:22:11,462 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:22:11,462 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:22:12,293 - 10 - training_embed.py - training - create model
2019-01-08 10:22:12,294 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:22:12,294 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:22:12,614 - 10 - training_embed.py - training - Epoch: [0][100/507]	Time 0.001 (0.003)	Data 0.000 (0.002)	Loss 156.9023 (153.9554)	
2019-01-08 10:22:12,819 - 10 - training_embed.py - training - Epoch: [0][200/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.2457 (153.9635)	
2019-01-08 10:22:13,035 - 10 - training_embed.py - training - Epoch: [0][300/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.6806 (153.9616)	
2019-01-08 10:22:13,244 - 10 - training_embed.py - training - Epoch: [0][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.8353 (153.9665)	
2019-01-08 10:22:13,463 - 10 - training_embed.py - training - Epoch: [0][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.9440 (153.9918)	
2019-01-08 10:22:13,473 - 10 - training_embed.py - training - loss: 153.973926
2019-01-08 10:22:13,473 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:22:13,860 - 10 - training_embed.py - training - Epoch: [1][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.1081 (154.0553)	
2019-01-08 10:22:14,090 - 10 - training_embed.py - training - Epoch: [1][200/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.6924 (154.0819)	
2019-01-08 10:22:14,299 - 10 - training_embed.py - training - Epoch: [1][300/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 153.8872 (154.0271)	
2019-01-08 10:22:14,515 - 10 - training_embed.py - training - Epoch: [1][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.6909 (153.9772)	
2019-01-08 10:22:14,728 - 10 - training_embed.py - training - Epoch: [1][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9027 (153.9529)	
2019-01-08 10:22:14,739 - 10 - training_embed.py - training - loss: 153.933693
2019-01-08 10:22:14,739 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:22:15,090 - 10 - training_embed.py - training - Epoch: [2][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.8449 (153.9267)	
2019-01-08 10:22:15,294 - 10 - training_embed.py - training - Epoch: [2][200/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 153.3781 (154.0227)	
2019-01-08 10:22:15,512 - 10 - training_embed.py - training - Epoch: [2][300/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.3102 (153.9630)	
2019-01-08 10:22:15,746 - 10 - training_embed.py - training - Epoch: [2][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.2422 (153.9365)	
2019-01-08 10:22:15,972 - 10 - training_embed.py - training - Epoch: [2][500/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.3276 (153.9062)	
2019-01-08 10:22:15,985 - 10 - training_embed.py - training - loss: 153.892804
2019-01-08 10:22:15,985 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:22:16,306 - 10 - training_embed.py - training - Epoch: [3][100/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 151.4818 (153.8609)	
2019-01-08 10:22:16,513 - 10 - training_embed.py - training - Epoch: [3][200/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.4016 (153.9044)	
2019-01-08 10:22:16,730 - 10 - training_embed.py - training - Epoch: [3][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.2744 (153.9171)	
2019-01-08 10:22:16,941 - 10 - training_embed.py - training - Epoch: [3][400/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 152.6248 (153.8746)	
2019-01-08 10:22:17,142 - 10 - training_embed.py - training - Epoch: [3][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.8067 (153.8689)	
2019-01-08 10:22:17,155 - 10 - training_embed.py - training - loss: 153.851674
2019-01-08 10:22:17,155 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:22:17,521 - 10 - training_embed.py - training - Epoch: [4][100/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 155.0162 (153.7519)	
2019-01-08 10:22:17,726 - 10 - training_embed.py - training - Epoch: [4][200/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 151.9464 (153.8222)	
2019-01-08 10:22:17,931 - 10 - training_embed.py - training - Epoch: [4][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.4797 (153.8011)	
2019-01-08 10:22:18,163 - 10 - training_embed.py - training - Epoch: [4][400/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 152.7948 (153.8128)	
2019-01-08 10:22:18,385 - 10 - training_embed.py - training - Epoch: [4][500/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.0618 (153.8296)	
2019-01-08 10:22:18,394 - 10 - training_embed.py - training - loss: 153.812366
2019-01-08 10:22:18,395 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:22:18,756 - 10 - training_embed.py - training - Epoch: [5][100/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.1477 (153.7428)	
2019-01-08 10:22:18,982 - 10 - training_embed.py - training - Epoch: [5][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.6887 (153.7482)	
2019-01-08 10:22:19,199 - 10 - training_embed.py - training - Epoch: [5][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.9312 (153.7683)	
2019-01-08 10:22:19,411 - 10 - training_embed.py - training - Epoch: [5][400/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.2995 (153.7642)	
2019-01-08 10:22:19,633 - 10 - training_embed.py - training - Epoch: [5][500/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.5849 (153.7813)	
2019-01-08 10:22:19,645 - 10 - training_embed.py - training - loss: 153.771175
2019-01-08 10:22:19,645 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:22:19,974 - 10 - training_embed.py - training - Epoch: [6][100/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.5861 (153.8707)	
2019-01-08 10:22:20,200 - 10 - training_embed.py - training - Epoch: [6][200/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.1290 (153.7969)	
2019-01-08 10:22:20,404 - 10 - training_embed.py - training - Epoch: [6][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.4589 (153.7905)	
2019-01-08 10:22:20,656 - 10 - training_embed.py - training - Epoch: [6][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.1443 (153.7602)	
2019-01-08 10:22:20,862 - 10 - training_embed.py - training - Epoch: [6][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.0772 (153.7415)	
2019-01-08 10:22:20,872 - 10 - training_embed.py - training - loss: 153.728735
2019-01-08 10:22:20,872 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:22:21,229 - 10 - training_embed.py - training - Epoch: [7][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.3757 (153.7829)	
2019-01-08 10:22:21,443 - 10 - training_embed.py - training - Epoch: [7][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.1869 (153.7946)	
2019-01-08 10:22:21,667 - 10 - training_embed.py - training - Epoch: [7][300/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.5740 (153.7954)	
2019-01-08 10:22:21,995 - 10 - training_embed.py - training - Epoch: [7][400/507]	Time 0.007 (0.002)	Data 0.006 (0.001)	Loss 154.8685 (153.7962)	
2019-01-08 10:22:22,299 - 10 - training_embed.py - training - Epoch: [7][500/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 150.9662 (153.7082)	
2019-01-08 10:22:22,310 - 10 - training_embed.py - training - loss: 153.689230
2019-01-08 10:22:22,310 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:22:22,690 - 10 - training_embed.py - training - Epoch: [8][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.5866 (153.4324)	
2019-01-08 10:22:22,926 - 10 - training_embed.py - training - Epoch: [8][200/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.1591 (153.6555)	
2019-01-08 10:22:23,153 - 10 - training_embed.py - training - Epoch: [8][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.7794 (153.6744)	
2019-01-08 10:22:23,416 - 10 - training_embed.py - training - Epoch: [8][400/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 153.5076 (153.6535)	
2019-01-08 10:22:23,656 - 10 - training_embed.py - training - Epoch: [8][500/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.7488 (153.6707)	
2019-01-08 10:22:23,667 - 10 - training_embed.py - training - loss: 153.648267
2019-01-08 10:22:23,668 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:22:24,082 - 10 - training_embed.py - training - Epoch: [9][100/507]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 156.4247 (153.7967)	
2019-01-08 10:22:24,329 - 10 - training_embed.py - training - Epoch: [9][200/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.7782 (153.7019)	
2019-01-08 10:22:24,552 - 10 - training_embed.py - training - Epoch: [9][300/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.1536 (153.7050)	
2019-01-08 10:22:24,766 - 10 - training_embed.py - training - Epoch: [9][400/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.5916 (153.6751)	
2019-01-08 10:22:24,989 - 10 - training_embed.py - training - Epoch: [9][500/507]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 152.8139 (153.6211)	
2019-01-08 10:22:25,000 - 10 - training_embed.py - training - loss: 153.606816
2019-01-08 10:22:25,001 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:22:25,386 - 10 - training_embed.py - training - Epoch: [10][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.7818 (153.5880)	
2019-01-08 10:22:25,620 - 10 - training_embed.py - training - Epoch: [10][200/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.8298 (153.6114)	
2019-01-08 10:22:25,856 - 10 - training_embed.py - training - Epoch: [10][300/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.1893 (153.5806)	
2019-01-08 10:22:26,099 - 10 - training_embed.py - training - Epoch: [10][400/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.2182 (153.5904)	
2019-01-08 10:22:26,392 - 10 - training_embed.py - training - Epoch: [10][500/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.6052 (153.5781)	
2019-01-08 10:22:26,403 - 10 - training_embed.py - training - loss: 153.567067
2019-01-08 10:22:26,404 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:22:26,784 - 10 - training_embed.py - training - Epoch: [11][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.2761 (153.4864)	
2019-01-08 10:22:27,026 - 10 - training_embed.py - training - Epoch: [11][200/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 151.7833 (153.5922)	
2019-01-08 10:22:27,259 - 10 - training_embed.py - training - Epoch: [11][300/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.4551 (153.5777)	
2019-01-08 10:22:27,484 - 10 - training_embed.py - training - Epoch: [11][400/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.7314 (153.5606)	
2019-01-08 10:22:27,697 - 10 - training_embed.py - training - Epoch: [11][500/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.9200 (153.5504)	
2019-01-08 10:22:27,709 - 10 - training_embed.py - training - loss: 153.525879
2019-01-08 10:22:27,730 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:22:27,939 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 208.802 ms ~ 0.003 min ~ 0.209 sec
2019-01-08 10:22:28,131 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 401.014 ms ~ 0.007 min ~ 0.401 sec
2019-01-08 10:22:28,131 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:22:28,132 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:22:28,132 - 10 - corpus.py - subactivity_sampler - [32755. 57651.  6929. 32216.]
2019-01-08 10:22:31,708 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:22:31,708 - 10 - corpus.py - subactivity_sampler - [32616. 58270.  6568. 32097.]
2019-01-08 10:22:35,327 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:22:35,327 - 10 - corpus.py - subactivity_sampler - [32605. 58722.  6429. 31795.]
2019-01-08 10:22:38,756 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:22:38,757 - 10 - corpus.py - subactivity_sampler - [32391. 59121.  6342. 31697.]
2019-01-08 10:22:42,005 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:22:42,005 - 10 - corpus.py - subactivity_sampler - [32284. 59648.  6294. 31325.]
2019-01-08 10:22:45,186 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:22:45,186 - 10 - corpus.py - subactivity_sampler - [32268. 59780.  6269. 31234.]
2019-01-08 10:22:48,610 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:22:48,610 - 10 - corpus.py - subactivity_sampler - [32177. 60312.  6168. 30894.]
2019-01-08 10:22:52,031 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:22:52,031 - 10 - corpus.py - subactivity_sampler - [32131. 60681.  6106. 30633.]
2019-01-08 10:22:55,859 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:22:55,860 - 10 - corpus.py - subactivity_sampler - [32044. 60997.  6055. 30455.]
2019-01-08 10:22:59,398 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:22:59,399 - 10 - corpus.py - subactivity_sampler - [31941. 61641.  5987. 29982.]
2019-01-08 10:22:59,911 - 10 - corpus.py - subactivity_sampler - [31933. 61660.  5982. 29976.]
2019-01-08 10:22:59,911 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 31779.537 ms ~ 0.530 min ~ 31.780 sec
2019-01-08 10:22:59,911 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:23:00,321 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:23:00,322 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 2. 17.  0.]
2019-01-08 10:23:00,322 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:23:00,332 - 10 - corpus.py - rho_sampling - ['58.7348', '15.7976', '928.9395']
2019-01-08 10:23:00,333 - 10 - pipeline.py - baseline - Iteration 5
2019-01-08 10:23:00,365 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:23:00,367 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:23:00,367 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 4', '1: 1', '2: 3', '3: 2']
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - mof_val - frames true: 55421	frames overall : 129551
2019-01-08 10:23:00,371 - 10 - corpus.py - accuracy_corpus - Action: cereals
2019-01-08 10:23:00,371 - 10 - corpus.py - accuracy_corpus - MoF val: 0.42779291553133514
2019-01-08 10:23:00,371 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.42779291553133514
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - mof_classes - label 1: 0.667237  34697 / 52001
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - mof_classes - label 2: 0.312689  11804 / 37750
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - mof_classes - label 3: 0.035874  296 / 8251
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - mof_classes - label 4: 0.917642  8624 / 9398
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - mof_classes - average class mof: 0.386688
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - iou_classes - label 1: 0.439403  34697 / 78964
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - iou_classes - label 2: 0.211080  11804 / 55922
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - iou_classes - label 3: 0.021238  296 / 13937
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - iou_classes - label 4: 0.263674  8624 / 32707
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - iou_classes - average IoU: 0.233849
2019-01-08 10:23:00,371 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.187079
2019-01-08 10:23:01,997 - 10 - f1_score.py - f1 - f1 score: 0.376631
2019-01-08 10:23:02,000 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1667.545 ms ~ 0.028 min ~ 1.668 sec
2019-01-08 10:23:02,000 - 10 - corpus.py - embedding_training - .
2019-01-08 10:23:02,000 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:23:02,000 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:23:02,000 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:23:02,803 - 10 - training_embed.py - training - create model
2019-01-08 10:23:02,803 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:23:02,804 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:23:03,147 - 10 - training_embed.py - training - Epoch: [0][100/507]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 156.8439 (153.8505)	
2019-01-08 10:23:03,374 - 10 - training_embed.py - training - Epoch: [0][200/507]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 154.0551 (153.8448)	
2019-01-08 10:23:03,586 - 10 - training_embed.py - training - Epoch: [0][300/507]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 153.6721 (153.8412)	
2019-01-08 10:23:03,803 - 10 - training_embed.py - training - Epoch: [0][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.7157 (153.8536)	
2019-01-08 10:23:04,024 - 10 - training_embed.py - training - Epoch: [0][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.8801 (153.8692)	
2019-01-08 10:23:04,034 - 10 - training_embed.py - training - loss: 153.849758
2019-01-08 10:23:04,034 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:23:04,419 - 10 - training_embed.py - training - Epoch: [1][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.9020 (153.9298)	
2019-01-08 10:23:04,631 - 10 - training_embed.py - training - Epoch: [1][200/507]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 155.6702 (153.9286)	
2019-01-08 10:23:04,834 - 10 - training_embed.py - training - Epoch: [1][300/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.3651 (153.8852)	
2019-01-08 10:23:05,059 - 10 - training_embed.py - training - Epoch: [1][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.8663 (153.8442)	
2019-01-08 10:23:05,273 - 10 - training_embed.py - training - Epoch: [1][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.3248 (153.8216)	
2019-01-08 10:23:05,284 - 10 - training_embed.py - training - loss: 153.802578
2019-01-08 10:23:05,284 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:23:05,677 - 10 - training_embed.py - training - Epoch: [2][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.7103 (153.7577)	
2019-01-08 10:23:05,902 - 10 - training_embed.py - training - Epoch: [2][200/507]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 153.8921 (153.9174)	
2019-01-08 10:23:06,115 - 10 - training_embed.py - training - Epoch: [2][300/507]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 153.6714 (153.8449)	
2019-01-08 10:23:06,334 - 10 - training_embed.py - training - Epoch: [2][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 152.6467 (153.8109)	
2019-01-08 10:23:06,552 - 10 - training_embed.py - training - Epoch: [2][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.7613 (153.7711)	
2019-01-08 10:23:06,562 - 10 - training_embed.py - training - loss: 153.754675
2019-01-08 10:23:06,563 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:23:06,926 - 10 - training_embed.py - training - Epoch: [3][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 150.9611 (153.7534)	
2019-01-08 10:23:07,130 - 10 - training_embed.py - training - Epoch: [3][200/507]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 154.7708 (153.7911)	
2019-01-08 10:23:07,351 - 10 - training_embed.py - training - Epoch: [3][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9102 (153.7947)	
2019-01-08 10:23:07,568 - 10 - training_embed.py - training - Epoch: [3][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.3341 (153.7400)	
2019-01-08 10:23:07,792 - 10 - training_embed.py - training - Epoch: [3][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.2866 (153.7249)	
2019-01-08 10:23:07,804 - 10 - training_embed.py - training - loss: 153.706566
2019-01-08 10:23:07,804 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:23:08,233 - 10 - training_embed.py - training - Epoch: [4][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.3472 (153.6435)	
2019-01-08 10:23:08,447 - 10 - training_embed.py - training - Epoch: [4][200/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.3333 (153.6749)	
2019-01-08 10:23:08,657 - 10 - training_embed.py - training - Epoch: [4][300/507]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 153.1768 (153.6615)	
2019-01-08 10:23:08,871 - 10 - training_embed.py - training - Epoch: [4][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.8399 (153.6618)	
2019-01-08 10:23:09,082 - 10 - training_embed.py - training - Epoch: [4][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.7342 (153.6781)	
2019-01-08 10:23:09,093 - 10 - training_embed.py - training - loss: 153.660278
2019-01-08 10:23:09,093 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:23:09,463 - 10 - training_embed.py - training - Epoch: [5][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.9879 (153.5845)	
2019-01-08 10:23:09,682 - 10 - training_embed.py - training - Epoch: [5][200/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.4545 (153.6003)	
2019-01-08 10:23:09,907 - 10 - training_embed.py - training - Epoch: [5][300/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 151.4524 (153.6072)	
2019-01-08 10:23:10,105 - 10 - training_embed.py - training - Epoch: [5][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.4812 (153.6221)	
2019-01-08 10:23:10,318 - 10 - training_embed.py - training - Epoch: [5][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.1365 (153.6221)	
2019-01-08 10:23:10,328 - 10 - training_embed.py - training - loss: 153.611776
2019-01-08 10:23:10,333 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:23:10,728 - 10 - training_embed.py - training - Epoch: [6][100/507]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 154.4073 (153.7093)	
2019-01-08 10:23:10,940 - 10 - training_embed.py - training - Epoch: [6][200/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.7904 (153.6449)	
2019-01-08 10:23:11,143 - 10 - training_embed.py - training - Epoch: [6][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.8973 (153.6225)	
2019-01-08 10:23:11,362 - 10 - training_embed.py - training - Epoch: [6][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.7958 (153.5955)	
2019-01-08 10:23:11,573 - 10 - training_embed.py - training - Epoch: [6][500/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.5596 (153.5759)	
2019-01-08 10:23:11,585 - 10 - training_embed.py - training - loss: 153.562674
2019-01-08 10:23:11,585 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:23:11,917 - 10 - training_embed.py - training - Epoch: [7][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.4975 (153.6525)	
2019-01-08 10:23:12,127 - 10 - training_embed.py - training - Epoch: [7][200/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.2304 (153.6414)	
2019-01-08 10:23:12,338 - 10 - training_embed.py - training - Epoch: [7][300/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.1714 (153.6365)	
2019-01-08 10:23:12,558 - 10 - training_embed.py - training - Epoch: [7][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.2916 (153.6368)	
2019-01-08 10:23:12,761 - 10 - training_embed.py - training - Epoch: [7][500/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 150.3657 (153.5351)	
2019-01-08 10:23:12,773 - 10 - training_embed.py - training - loss: 153.516204
2019-01-08 10:23:12,773 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:23:13,124 - 10 - training_embed.py - training - Epoch: [8][100/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.0804 (153.3007)	
2019-01-08 10:23:13,339 - 10 - training_embed.py - training - Epoch: [8][200/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.1385 (153.5022)	
2019-01-08 10:23:13,565 - 10 - training_embed.py - training - Epoch: [8][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.1090 (153.4938)	
2019-01-08 10:23:13,781 - 10 - training_embed.py - training - Epoch: [8][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.5710 (153.4700)	
2019-01-08 10:23:14,009 - 10 - training_embed.py - training - Epoch: [8][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.1335 (153.4897)	
2019-01-08 10:23:14,019 - 10 - training_embed.py - training - loss: 153.468187
2019-01-08 10:23:14,020 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:23:14,384 - 10 - training_embed.py - training - Epoch: [9][100/507]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 155.5284 (153.6020)	
2019-01-08 10:23:14,596 - 10 - training_embed.py - training - Epoch: [9][200/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 152.2815 (153.5079)	
2019-01-08 10:23:14,801 - 10 - training_embed.py - training - Epoch: [9][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.1267 (153.5139)	
2019-01-08 10:23:15,018 - 10 - training_embed.py - training - Epoch: [9][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.4471 (153.4844)	
2019-01-08 10:23:15,245 - 10 - training_embed.py - training - Epoch: [9][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.6596 (153.4369)	
2019-01-08 10:23:15,255 - 10 - training_embed.py - training - loss: 153.419875
2019-01-08 10:23:15,256 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:23:15,624 - 10 - training_embed.py - training - Epoch: [10][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.4237 (153.4441)	
2019-01-08 10:23:15,834 - 10 - training_embed.py - training - Epoch: [10][200/507]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 153.7880 (153.4257)	
2019-01-08 10:23:16,044 - 10 - training_embed.py - training - Epoch: [10][300/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.9140 (153.4010)	
2019-01-08 10:23:16,254 - 10 - training_embed.py - training - Epoch: [10][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 152.3047 (153.4042)	
2019-01-08 10:23:16,494 - 10 - training_embed.py - training - Epoch: [10][500/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 152.7413 (153.3853)	
2019-01-08 10:23:16,506 - 10 - training_embed.py - training - loss: 153.373043
2019-01-08 10:23:16,506 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:23:16,865 - 10 - training_embed.py - training - Epoch: [11][100/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 151.8006 (153.3377)	
2019-01-08 10:23:17,086 - 10 - training_embed.py - training - Epoch: [11][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.1607 (153.4105)	
2019-01-08 10:23:17,299 - 10 - training_embed.py - training - Epoch: [11][300/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 155.2569 (153.3836)	
2019-01-08 10:23:17,512 - 10 - training_embed.py - training - Epoch: [11][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 153.3146 (153.3652)	
2019-01-08 10:23:17,725 - 10 - training_embed.py - training - Epoch: [11][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.3854 (153.3492)	
2019-01-08 10:23:17,737 - 10 - training_embed.py - training - loss: 153.324863
2019-01-08 10:23:17,755 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:23:17,960 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 204.773 ms ~ 0.003 min ~ 0.205 sec
2019-01-08 10:23:18,148 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 392.263 ms ~ 0.007 min ~ 0.392 sec
2019-01-08 10:23:18,148 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:23:18,148 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:23:18,149 - 10 - corpus.py - subactivity_sampler - [31933. 61660.  5982. 29976.]
2019-01-08 10:23:21,701 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:23:21,701 - 10 - corpus.py - subactivity_sampler - [31873. 61905.  5851. 29922.]
2019-01-08 10:23:25,336 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:23:25,337 - 10 - corpus.py - subactivity_sampler - [31834. 62148.  5830. 29739.]
2019-01-08 10:23:28,740 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:23:28,741 - 10 - corpus.py - subactivity_sampler - [31811. 62440.  5819. 29481.]
2019-01-08 10:23:31,963 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:23:31,964 - 10 - corpus.py - subactivity_sampler - [31738. 63019.  5746. 29048.]
2019-01-08 10:23:35,144 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:23:35,145 - 10 - corpus.py - subactivity_sampler - [31704. 63217.  5674. 28956.]
2019-01-08 10:23:38,539 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:23:38,539 - 10 - corpus.py - subactivity_sampler - [31638. 63387.  5652. 28874.]
2019-01-08 10:23:41,939 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:23:41,939 - 10 - corpus.py - subactivity_sampler - [31618. 63666.  5606. 28661.]
2019-01-08 10:23:45,790 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:23:45,790 - 10 - corpus.py - subactivity_sampler - [31484. 64145.  5594. 28328.]
2019-01-08 10:23:49,314 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:23:49,314 - 10 - corpus.py - subactivity_sampler - [31459. 64458.  5590. 28044.]
2019-01-08 10:23:49,826 - 10 - corpus.py - subactivity_sampler - [31454. 64466.  5588. 28043.]
2019-01-08 10:23:49,826 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 31678.182 ms ~ 0.528 min ~ 31.678 sec
2019-01-08 10:23:49,826 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:23:51,714 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:23:51,715 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 2. 17.  0.]
2019-01-08 10:23:51,715 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:23:51,733 - 10 - corpus.py - rho_sampling - ['59.3112', '1.3821', '59.0492']
2019-01-08 10:23:51,734 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 10:23:51,772 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:23:51,774 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:23:51,774 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 4', '1: 1', '2: 3', '3: 2']
2019-01-08 10:23:51,777 - 10 - accuracy_class.py - mof_val - frames true: 54821	frames overall : 129551
2019-01-08 10:23:51,777 - 10 - corpus.py - accuracy_corpus - Action: cereals
2019-01-08 10:23:51,777 - 10 - corpus.py - accuracy_corpus - MoF val: 0.42316153483956126
2019-01-08 10:23:51,777 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.42316153483956126
2019-01-08 10:23:51,777 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:23:51,777 - 10 - accuracy_class.py - mof_classes - label 1: 0.678198  35267 / 52001
2019-01-08 10:23:51,777 - 10 - accuracy_class.py - mof_classes - label 2: 0.282755  10674 / 37750
2019-01-08 10:23:51,777 - 10 - accuracy_class.py - mof_classes - label 3: 0.035632  294 / 8251
2019-01-08 10:23:51,777 - 10 - accuracy_class.py - mof_classes - label 4: 0.913599  8586 / 9398
2019-01-08 10:23:51,778 - 10 - accuracy_class.py - mof_classes - average class mof: 0.382037
2019-01-08 10:23:51,778 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:23:51,778 - 10 - accuracy_class.py - iou_classes - label 1: 0.434323  35267 / 81200
2019-01-08 10:23:51,778 - 10 - accuracy_class.py - iou_classes - label 2: 0.193654  10674 / 55119
2019-01-08 10:23:51,778 - 10 - accuracy_class.py - iou_classes - label 3: 0.021705  294 / 13545
2019-01-08 10:23:51,778 - 10 - accuracy_class.py - iou_classes - label 4: 0.266101  8586 / 32266
2019-01-08 10:23:51,778 - 10 - accuracy_class.py - iou_classes - average IoU: 0.228946
2019-01-08 10:23:51,778 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.183156
2019-01-08 10:23:53,441 - 10 - f1_score.py - f1 - f1 score: 0.369796
2019-01-08 10:23:53,444 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1710.696 ms ~ 0.029 min ~ 1.711 sec
2019-01-08 10:23:53,444 - 10 - corpus.py - embedding_training - .
2019-01-08 10:23:53,445 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:23:53,445 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:23:53,445 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:23:54,251 - 10 - training_embed.py - training - create model
2019-01-08 10:23:54,252 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:23:54,252 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:23:54,585 - 10 - training_embed.py - training - Epoch: [0][100/507]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 156.0903 (153.8108)	
2019-01-08 10:23:54,843 - 10 - training_embed.py - training - Epoch: [0][200/507]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 153.8642 (153.8178)	
2019-01-08 10:23:55,055 - 10 - training_embed.py - training - Epoch: [0][300/507]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 153.7966 (153.7920)	
2019-01-08 10:23:55,268 - 10 - training_embed.py - training - Epoch: [0][400/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.6654 (153.8171)	
2019-01-08 10:23:55,500 - 10 - training_embed.py - training - Epoch: [0][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.8407 (153.8277)	
2019-01-08 10:23:55,511 - 10 - training_embed.py - training - loss: 153.808770
2019-01-08 10:23:55,511 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:23:55,900 - 10 - training_embed.py - training - Epoch: [1][100/507]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 152.7601 (153.8718)	
2019-01-08 10:23:56,150 - 10 - training_embed.py - training - Epoch: [1][200/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.6288 (153.8732)	
2019-01-08 10:23:56,379 - 10 - training_embed.py - training - Epoch: [1][300/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.8944 (153.8307)	
2019-01-08 10:23:56,590 - 10 - training_embed.py - training - Epoch: [1][400/507]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 152.9022 (153.7957)	
2019-01-08 10:23:56,799 - 10 - training_embed.py - training - Epoch: [1][500/507]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 154.3716 (153.7769)	
2019-01-08 10:23:56,809 - 10 - training_embed.py - training - loss: 153.757283
2019-01-08 10:23:56,810 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:23:57,191 - 10 - training_embed.py - training - Epoch: [2][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.6167 (153.7139)	
2019-01-08 10:23:57,451 - 10 - training_embed.py - training - Epoch: [2][200/507]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 154.0994 (153.8572)	
2019-01-08 10:23:57,677 - 10 - training_embed.py - training - Epoch: [2][300/507]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 153.4362 (153.7954)	
2019-01-08 10:23:57,891 - 10 - training_embed.py - training - Epoch: [2][400/507]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 152.5834 (153.7670)	
2019-01-08 10:23:58,122 - 10 - training_embed.py - training - Epoch: [2][500/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.7204 (153.7200)	
2019-01-08 10:23:58,133 - 10 - training_embed.py - training - loss: 153.705135
2019-01-08 10:23:58,133 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:23:58,495 - 10 - training_embed.py - training - Epoch: [3][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 150.6509 (153.6902)	
2019-01-08 10:23:58,699 - 10 - training_embed.py - training - Epoch: [3][200/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.0625 (153.7325)	
2019-01-08 10:23:58,922 - 10 - training_embed.py - training - Epoch: [3][300/507]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 155.0195 (153.7426)	
2019-01-08 10:23:59,133 - 10 - training_embed.py - training - Epoch: [3][400/507]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 153.6820 (153.6921)	
2019-01-08 10:23:59,349 - 10 - training_embed.py - training - Epoch: [3][500/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.4390 (153.6733)	
2019-01-08 10:23:59,360 - 10 - training_embed.py - training - loss: 153.653901
2019-01-08 10:23:59,360 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:23:59,713 - 10 - training_embed.py - training - Epoch: [4][100/507]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 153.9261 (153.5903)	
2019-01-08 10:23:59,918 - 10 - training_embed.py - training - Epoch: [4][200/507]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 152.1945 (153.6381)	
2019-01-08 10:24:00,138 - 10 - training_embed.py - training - Epoch: [4][300/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.8858 (153.6041)	
2019-01-08 10:24:00,357 - 10 - training_embed.py - training - Epoch: [4][400/507]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 152.6015 (153.5959)	
2019-01-08 10:24:00,571 - 10 - training_embed.py - training - Epoch: [4][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.2095 (153.6215)	
2019-01-08 10:24:00,583 - 10 - training_embed.py - training - loss: 153.603277
2019-01-08 10:24:00,583 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:24:00,946 - 10 - training_embed.py - training - Epoch: [5][100/507]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 153.9389 (153.5340)	
2019-01-08 10:24:01,161 - 10 - training_embed.py - training - Epoch: [5][200/507]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 152.4829 (153.5511)	
2019-01-08 10:24:01,378 - 10 - training_embed.py - training - Epoch: [5][300/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 151.2649 (153.5483)	
2019-01-08 10:24:01,606 - 10 - training_embed.py - training - Epoch: [5][400/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.3813 (153.5650)	
2019-01-08 10:24:01,829 - 10 - training_embed.py - training - Epoch: [5][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.9112 (153.5596)	
2019-01-08 10:24:01,840 - 10 - training_embed.py - training - loss: 153.550873
2019-01-08 10:24:01,840 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:24:02,192 - 10 - training_embed.py - training - Epoch: [6][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.6990 (153.6511)	
2019-01-08 10:24:02,409 - 10 - training_embed.py - training - Epoch: [6][200/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.4157 (153.5732)	
2019-01-08 10:24:02,623 - 10 - training_embed.py - training - Epoch: [6][300/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.7808 (153.5665)	
2019-01-08 10:24:02,837 - 10 - training_embed.py - training - Epoch: [6][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.7351 (153.5412)	
2019-01-08 10:24:03,054 - 10 - training_embed.py - training - Epoch: [6][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.3132 (153.5134)	
2019-01-08 10:24:03,066 - 10 - training_embed.py - training - loss: 153.497866
2019-01-08 10:24:03,067 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:24:03,440 - 10 - training_embed.py - training - Epoch: [7][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.8536 (153.5568)	
2019-01-08 10:24:03,643 - 10 - training_embed.py - training - Epoch: [7][200/507]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 153.9807 (153.5592)	
2019-01-08 10:24:03,857 - 10 - training_embed.py - training - Epoch: [7][300/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.1782 (153.5680)	
2019-01-08 10:24:04,083 - 10 - training_embed.py - training - Epoch: [7][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.0259 (153.5763)	
2019-01-08 10:24:04,291 - 10 - training_embed.py - training - Epoch: [7][500/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 150.2012 (153.4685)	
2019-01-08 10:24:04,303 - 10 - training_embed.py - training - loss: 153.447509
2019-01-08 10:24:04,303 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:24:04,671 - 10 - training_embed.py - training - Epoch: [8][100/507]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 154.0661 (153.2133)	
2019-01-08 10:24:04,890 - 10 - training_embed.py - training - Epoch: [8][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8543 (153.4230)	
2019-01-08 10:24:05,108 - 10 - training_embed.py - training - Epoch: [8][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.0452 (153.4182)	
2019-01-08 10:24:05,317 - 10 - training_embed.py - training - Epoch: [8][400/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.4331 (153.3919)	
2019-01-08 10:24:05,529 - 10 - training_embed.py - training - Epoch: [8][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.3055 (153.4130)	
2019-01-08 10:24:05,540 - 10 - training_embed.py - training - loss: 153.395251
2019-01-08 10:24:05,541 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:24:05,935 - 10 - training_embed.py - training - Epoch: [9][100/507]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.3716 (153.5109)	
2019-01-08 10:24:06,146 - 10 - training_embed.py - training - Epoch: [9][200/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.0731 (153.4278)	
2019-01-08 10:24:06,366 - 10 - training_embed.py - training - Epoch: [9][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.5965 (153.4300)	
2019-01-08 10:24:06,575 - 10 - training_embed.py - training - Epoch: [9][400/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.3546 (153.4098)	
2019-01-08 10:24:06,784 - 10 - training_embed.py - training - Epoch: [9][500/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 152.8821 (153.3592)	
2019-01-08 10:24:06,795 - 10 - training_embed.py - training - loss: 153.343351
2019-01-08 10:24:06,795 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:24:07,166 - 10 - training_embed.py - training - Epoch: [10][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.3270 (153.3671)	
2019-01-08 10:24:07,384 - 10 - training_embed.py - training - Epoch: [10][200/507]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 153.8137 (153.3199)	
2019-01-08 10:24:07,612 - 10 - training_embed.py - training - Epoch: [10][300/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.0665 (153.3148)	
2019-01-08 10:24:07,818 - 10 - training_embed.py - training - Epoch: [10][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 152.2482 (153.3273)	
2019-01-08 10:24:08,047 - 10 - training_embed.py - training - Epoch: [10][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.6609 (153.3072)	
2019-01-08 10:24:08,062 - 10 - training_embed.py - training - loss: 153.292613
2019-01-08 10:24:08,063 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:24:08,443 - 10 - training_embed.py - training - Epoch: [11][100/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 151.2017 (153.2303)	
2019-01-08 10:24:08,656 - 10 - training_embed.py - training - Epoch: [11][200/507]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 150.8973 (153.3255)	
2019-01-08 10:24:08,881 - 10 - training_embed.py - training - Epoch: [11][300/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.0593 (153.2995)	
2019-01-08 10:24:09,089 - 10 - training_embed.py - training - Epoch: [11][400/507]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 153.2209 (153.2810)	
2019-01-08 10:24:09,291 - 10 - training_embed.py - training - Epoch: [11][500/507]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.2764 (153.2678)	
2019-01-08 10:24:09,304 - 10 - training_embed.py - training - loss: 153.240798
2019-01-08 10:24:09,334 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:24:09,537 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 203.754 ms ~ 0.003 min ~ 0.204 sec
2019-01-08 10:24:09,722 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 388.496 ms ~ 0.006 min ~ 0.388 sec
2019-01-08 10:24:09,722 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:24:09,723 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:24:09,723 - 10 - corpus.py - subactivity_sampler - [31454. 64466.  5588. 28043.]
2019-01-08 10:24:13,287 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:24:13,287 - 10 - corpus.py - subactivity_sampler - [31405. 64786.  5577. 27783.]
2019-01-08 10:24:16,933 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:24:16,934 - 10 - corpus.py - subactivity_sampler - [31400. 64946.  5572. 27633.]
2019-01-08 10:24:20,378 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:24:20,378 - 10 - corpus.py - subactivity_sampler - [31380. 65219.  5504. 27448.]
2019-01-08 10:24:23,626 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:24:23,626 - 10 - corpus.py - subactivity_sampler - [31297. 65395.  5504. 27355.]
2019-01-08 10:24:26,814 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:24:26,814 - 10 - corpus.py - subactivity_sampler - [31260. 65451.  5492. 27348.]
2019-01-08 10:24:30,217 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:24:30,218 - 10 - corpus.py - subactivity_sampler - [31227. 65591.  5485. 27248.]
2019-01-08 10:24:33,612 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:24:33,613 - 10 - corpus.py - subactivity_sampler - [31200. 65698.  5484. 27169.]
2019-01-08 10:24:37,412 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:24:37,412 - 10 - corpus.py - subactivity_sampler - [31146. 65767.  5476. 27162.]
2019-01-08 10:24:40,911 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:24:40,911 - 10 - corpus.py - subactivity_sampler - [31133. 65794.  5476. 27148.]
2019-01-08 10:24:41,421 - 10 - corpus.py - subactivity_sampler - [31073. 65850.  5490. 27138.]
2019-01-08 10:24:41,421 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 31698.915 ms ~ 0.528 min ~ 31.699 sec
2019-01-08 10:24:41,421 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:24:43,464 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:24:43,464 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 2. 17.  1.]
2019-01-08 10:24:43,464 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:24:43,481 - 10 - corpus.py - rho_sampling - ['60.0150', '25.6046', '42.9875']
2019-01-08 10:24:43,481 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 10:24:43,516 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:24:43,518 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:24:43,518 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 4', '1: 1', '2: 3', '3: 2']
2019-01-08 10:24:43,521 - 10 - accuracy_class.py - mof_val - frames true: 54355	frames overall : 129551
2019-01-08 10:24:43,521 - 10 - corpus.py - accuracy_corpus - Action: cereals
2019-01-08 10:24:43,521 - 10 - corpus.py - accuracy_corpus - MoF val: 0.41956449583561684
2019-01-08 10:24:43,521 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.41956449583561684
2019-01-08 10:24:43,521 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:24:43,522 - 10 - accuracy_class.py - mof_classes - label 1: 0.684333  35586 / 52001
2019-01-08 10:24:43,522 - 10 - accuracy_class.py - mof_classes - label 2: 0.263338  9941 / 37750
2019-01-08 10:24:43,522 - 10 - accuracy_class.py - mof_classes - label 3: 0.035874  296 / 8251
2019-01-08 10:24:43,522 - 10 - accuracy_class.py - mof_classes - label 4: 0.907853  8532 / 9398
2019-01-08 10:24:43,522 - 10 - accuracy_class.py - mof_classes - average class mof: 0.378280
2019-01-08 10:24:43,522 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22151
2019-01-08 10:24:43,522 - 10 - accuracy_class.py - iou_classes - label 1: 0.432578  35586 / 82265
2019-01-08 10:24:43,522 - 10 - accuracy_class.py - iou_classes - label 2: 0.180920  9941 / 54947
2019-01-08 10:24:43,522 - 10 - accuracy_class.py - iou_classes - label 3: 0.022016  296 / 13445
2019-01-08 10:24:43,522 - 10 - accuracy_class.py - iou_classes - label 4: 0.267134  8532 / 31939
2019-01-08 10:24:43,522 - 10 - accuracy_class.py - iou_classes - average IoU: 0.225662
2019-01-08 10:24:43,522 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.180529
2019-01-08 10:24:45,126 - 10 - f1_score.py - f1 - f1 score: 0.365829
2019-01-08 10:24:45,131 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1649.798 ms ~ 0.027 min ~ 1.650 sec
2019-01-08 10:24:45,138 - 10 - utils.py - wrap - <function baseline at 0x7f3991d24e18> took 360606.498 ms ~ 6.010 min ~ 360.606 sec
2019-01-08 10:24:45,138 - 10 - utils.py - update_opt_str - batch_size: 256
2019-01-08 10:24:45,138 - 10 - utils.py - update_opt_str - bg: False
2019-01-08 10:24:45,138 - 10 - utils.py - update_opt_str - bg_trh: 85
2019-01-08 10:24:45,138 - 10 - utils.py - update_opt_str - data: /media/data/kukleva/lab/Breakfast/feat/s1
2019-01-08 10:24:45,138 - 10 - utils.py - update_opt_str - data_type: 2
2019-01-08 10:24:45,138 - 10 - utils.py - update_opt_str - dataset: bf
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - dataset_root: /media/data/kukleva/lab/Breakfast
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - embed_dim: 30
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - epochs: 12
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - ext: txt
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - feature_dim: 64
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - full: True
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - gmm: 1
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - gmms: one
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - gt: /media/data/kukleva/lab/Breakfast/feat/s1/groundTruth
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - gt_training: False
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - log: DEBUG
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - log_str: slim.mallow._tea_!bg_data2_bf_embed30_epochs12_full_gmm1_one_!gt_lr1e-10_!lr_ordering_reg0.1_!viterbi_!zeros_
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - lr: 1e-10
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - lr_adj: False
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - momentum: 0.9
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - num_workers: 4
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - ordering: True
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - prefix: slim.mallow.
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - reg_cov: 0.1
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - resume: False
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - resume_str: 
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - save_embed_feat: False
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - save_likelihood: False
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - save_model: False
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - seed: 0
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - subaction: tea
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - vis: False
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - viterbi: False
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - weight_decay: 0.0001
2019-01-08 10:24:45,139 - 10 - utils.py - update_opt_str - zeros: False
2019-01-08 10:24:45,155 - 10 - utils.py - wrap - <function GroundTruth.load_gt at 0x7f39322b36a8> took 15.871 ms ~ 0.000 min ~ 0.016 sec
2019-01-08 10:24:45,248 - 10 - corpus.py - __init__ - tea  subactions: 6
2019-01-08 10:24:45,249 - 10 - corpus.py - _init_videos - .
2019-01-08 10:24:50,058 - 10 - corpus.py - _init_videos - gt statistic: Counter({46: 59621, 45: 49387, 6: 12979, 7: 6514, 47: 2380, 9: 901})
2019-01-08 10:24:50,058 - 10 - corpus.py - _update_fg_mask - .
2019-01-08 10:24:50,069 - 10 - corpus.py - __init__ - min: -45.940422  max: 32.644932  avg: 0.020191
2019-01-08 10:24:50,069 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:24:50,101 - 10 - corpus.py - rho_sampling - ['64.1837', '100.9221', '929.0535', '90.9104', '17.1809']
2019-01-08 10:24:50,101 - 10 - pipeline.py - baseline - Iteration 0
2019-01-08 10:24:50,141 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:24:50,143 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:24:50,143 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 45', '2: 9', '3: 46', '4: 7', '5: 47']
2019-01-08 10:24:50,147 - 10 - accuracy_class.py - mof_val - frames true: 33394	frames overall : 131782
2019-01-08 10:24:50,147 - 10 - corpus.py - accuracy_corpus - Action: tea
2019-01-08 10:24:50,147 - 10 - corpus.py - accuracy_corpus - MoF val: 0.2534033479534383
2019-01-08 10:24:50,147 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.0
2019-01-08 10:24:50,147 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:24:50,147 - 10 - accuracy_class.py - mof_classes - label 6: 0.754149  6862 / 9099
2019-01-08 10:24:50,147 - 10 - accuracy_class.py - mof_classes - label 7: 0.341890  2171 / 6350
2019-01-08 10:24:50,147 - 10 - accuracy_class.py - mof_classes - label 9: 0.000000  0 / 560
2019-01-08 10:24:50,147 - 10 - accuracy_class.py - mof_classes - label 45: 0.313384  11808 / 37679
2019-01-08 10:24:50,147 - 10 - accuracy_class.py - mof_classes - label 46: 0.283260  11838 / 41792
2019-01-08 10:24:50,147 - 10 - accuracy_class.py - mof_classes - label 47: 0.684211  715 / 1045
2019-01-08 10:24:50,147 - 10 - accuracy_class.py - mof_classes - average class mof: 0.339556
2019-01-08 10:24:50,147 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:24:50,147 - 10 - accuracy_class.py - iou_classes - label 6: 0.282596  6862 / 24282
2019-01-08 10:24:50,148 - 10 - accuracy_class.py - iou_classes - label 7: 0.083196  2171 / 26095
2019-01-08 10:24:50,148 - 10 - accuracy_class.py - iou_classes - label 9: 0.000000  0 / 22539
2019-01-08 10:24:50,148 - 10 - accuracy_class.py - iou_classes - label 45: 0.246591  11808 / 47885
2019-01-08 10:24:50,148 - 10 - accuracy_class.py - iou_classes - label 46: 0.228128  11838 / 51892
2019-01-08 10:24:50,148 - 10 - accuracy_class.py - iou_classes - label 47: 0.032178  715 / 22220
2019-01-08 10:24:50,148 - 10 - accuracy_class.py - iou_classes - average IoU: 0.145448
2019-01-08 10:24:50,148 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.124670
2019-01-08 10:24:51,661 - 10 - f1_score.py - f1 - f1 score: 0.213091
2019-01-08 10:24:51,665 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1564.564 ms ~ 0.026 min ~ 1.565 sec
2019-01-08 10:24:51,666 - 10 - corpus.py - embedding_training - .
2019-01-08 10:24:51,666 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:24:51,666 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:24:51,666 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:24:52,489 - 10 - training_embed.py - training - create model
2019-01-08 10:24:52,490 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:24:52,490 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:24:52,812 - 10 - training_embed.py - training - Epoch: [0][100/515]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 256.8608 (256.5665)	
2019-01-08 10:24:53,024 - 10 - training_embed.py - training - Epoch: [0][200/515]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 254.6714 (256.5920)	
2019-01-08 10:24:53,236 - 10 - training_embed.py - training - Epoch: [0][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.0356 (256.5819)	
2019-01-08 10:24:53,454 - 10 - training_embed.py - training - Epoch: [0][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.7363 (256.5322)	
2019-01-08 10:24:53,650 - 10 - training_embed.py - training - Epoch: [0][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.0848 (256.5202)	
2019-01-08 10:24:53,681 - 10 - training_embed.py - training - loss: 256.449400
2019-01-08 10:24:53,682 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:24:54,019 - 10 - training_embed.py - training - Epoch: [1][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.5915 (256.4318)	
2019-01-08 10:24:54,222 - 10 - training_embed.py - training - Epoch: [1][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.7853 (256.5233)	
2019-01-08 10:24:54,429 - 10 - training_embed.py - training - Epoch: [1][300/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 257.7568 (256.5074)	
2019-01-08 10:24:54,645 - 10 - training_embed.py - training - Epoch: [1][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.9122 (256.5331)	
2019-01-08 10:24:54,864 - 10 - training_embed.py - training - Epoch: [1][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.0435 (256.5144)	
2019-01-08 10:24:54,895 - 10 - training_embed.py - training - loss: 256.438173
2019-01-08 10:24:54,896 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:24:55,238 - 10 - training_embed.py - training - Epoch: [2][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.7671 (256.8096)	
2019-01-08 10:24:55,447 - 10 - training_embed.py - training - Epoch: [2][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.7618 (256.5665)	
2019-01-08 10:24:55,670 - 10 - training_embed.py - training - Epoch: [2][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.4791 (256.5336)	
2019-01-08 10:24:55,868 - 10 - training_embed.py - training - Epoch: [2][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.3651 (256.4819)	
2019-01-08 10:24:56,071 - 10 - training_embed.py - training - Epoch: [2][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.6958 (256.5158)	
2019-01-08 10:24:56,096 - 10 - training_embed.py - training - loss: 256.429822
2019-01-08 10:24:56,097 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:24:56,499 - 10 - training_embed.py - training - Epoch: [3][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.7371 (256.3833)	
2019-01-08 10:24:56,707 - 10 - training_embed.py - training - Epoch: [3][200/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 255.3370 (256.4419)	
2019-01-08 10:24:56,902 - 10 - training_embed.py - training - Epoch: [3][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.3489 (256.4935)	
2019-01-08 10:24:57,102 - 10 - training_embed.py - training - Epoch: [3][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.1656 (256.5122)	
2019-01-08 10:24:57,305 - 10 - training_embed.py - training - Epoch: [3][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.2331 (256.5135)	
2019-01-08 10:24:57,335 - 10 - training_embed.py - training - loss: 256.420360
2019-01-08 10:24:57,338 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:24:57,717 - 10 - training_embed.py - training - Epoch: [4][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.3685 (256.2974)	
2019-01-08 10:24:57,933 - 10 - training_embed.py - training - Epoch: [4][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.4583 (256.4683)	
2019-01-08 10:24:58,133 - 10 - training_embed.py - training - Epoch: [4][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.7784 (256.4827)	
2019-01-08 10:24:58,345 - 10 - training_embed.py - training - Epoch: [4][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.2801 (256.5411)	
2019-01-08 10:24:58,555 - 10 - training_embed.py - training - Epoch: [4][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 258.8054 (256.5009)	
2019-01-08 10:24:58,584 - 10 - training_embed.py - training - loss: 256.412251
2019-01-08 10:24:58,584 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:24:58,942 - 10 - training_embed.py - training - Epoch: [5][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.9483 (256.3751)	
2019-01-08 10:24:59,156 - 10 - training_embed.py - training - Epoch: [5][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.2903 (256.4317)	
2019-01-08 10:24:59,361 - 10 - training_embed.py - training - Epoch: [5][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.1835 (256.4882)	
2019-01-08 10:24:59,594 - 10 - training_embed.py - training - Epoch: [5][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.2656 (256.4875)	
2019-01-08 10:24:59,802 - 10 - training_embed.py - training - Epoch: [5][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.9748 (256.4916)	
2019-01-08 10:24:59,831 - 10 - training_embed.py - training - loss: 256.402174
2019-01-08 10:24:59,831 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:25:00,174 - 10 - training_embed.py - training - Epoch: [6][100/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 257.9260 (256.5125)	
2019-01-08 10:25:00,370 - 10 - training_embed.py - training - Epoch: [6][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.7019 (256.4609)	
2019-01-08 10:25:00,591 - 10 - training_embed.py - training - Epoch: [6][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.8081 (256.4665)	
2019-01-08 10:25:00,811 - 10 - training_embed.py - training - Epoch: [6][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 257.0328 (256.4254)	
2019-01-08 10:25:01,027 - 10 - training_embed.py - training - Epoch: [6][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.8336 (256.4785)	
2019-01-08 10:25:01,055 - 10 - training_embed.py - training - loss: 256.392013
2019-01-08 10:25:01,056 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:25:01,423 - 10 - training_embed.py - training - Epoch: [7][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.8039 (256.3242)	
2019-01-08 10:25:01,626 - 10 - training_embed.py - training - Epoch: [7][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 260.4418 (256.4093)	
2019-01-08 10:25:01,822 - 10 - training_embed.py - training - Epoch: [7][300/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 256.7136 (256.4233)	
2019-01-08 10:25:02,024 - 10 - training_embed.py - training - Epoch: [7][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.0141 (256.5158)	
2019-01-08 10:25:02,221 - 10 - training_embed.py - training - Epoch: [7][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.7818 (256.4788)	
2019-01-08 10:25:02,250 - 10 - training_embed.py - training - loss: 256.384104
2019-01-08 10:25:02,250 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:25:02,618 - 10 - training_embed.py - training - Epoch: [8][100/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 256.5915 (256.4650)	
2019-01-08 10:25:02,836 - 10 - training_embed.py - training - Epoch: [8][200/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 257.4861 (256.5244)	
2019-01-08 10:25:03,054 - 10 - training_embed.py - training - Epoch: [8][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.9044 (256.4771)	
2019-01-08 10:25:03,274 - 10 - training_embed.py - training - Epoch: [8][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.1461 (256.4916)	
2019-01-08 10:25:03,485 - 10 - training_embed.py - training - Epoch: [8][500/515]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 255.7909 (256.4783)	
2019-01-08 10:25:03,512 - 10 - training_embed.py - training - loss: 256.373755
2019-01-08 10:25:03,512 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:25:03,875 - 10 - training_embed.py - training - Epoch: [9][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.9815 (256.5415)	
2019-01-08 10:25:04,094 - 10 - training_embed.py - training - Epoch: [9][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.2009 (256.2936)	
2019-01-08 10:25:04,319 - 10 - training_embed.py - training - Epoch: [9][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.4124 (256.3906)	
2019-01-08 10:25:04,527 - 10 - training_embed.py - training - Epoch: [9][400/515]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 256.5312 (256.4180)	
2019-01-08 10:25:04,724 - 10 - training_embed.py - training - Epoch: [9][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.3628 (256.4503)	
2019-01-08 10:25:04,754 - 10 - training_embed.py - training - loss: 256.364888
2019-01-08 10:25:04,754 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:25:05,119 - 10 - training_embed.py - training - Epoch: [10][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.7896 (256.4228)	
2019-01-08 10:25:05,337 - 10 - training_embed.py - training - Epoch: [10][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.5423 (256.3936)	
2019-01-08 10:25:05,555 - 10 - training_embed.py - training - Epoch: [10][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.1483 (256.3873)	
2019-01-08 10:25:05,759 - 10 - training_embed.py - training - Epoch: [10][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.4034 (256.4326)	
2019-01-08 10:25:05,974 - 10 - training_embed.py - training - Epoch: [10][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 255.8413 (256.4497)	
2019-01-08 10:25:06,002 - 10 - training_embed.py - training - loss: 256.355038
2019-01-08 10:25:06,003 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:25:06,365 - 10 - training_embed.py - training - Epoch: [11][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.2078 (256.4424)	
2019-01-08 10:25:06,574 - 10 - training_embed.py - training - Epoch: [11][200/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 255.0966 (256.4561)	
2019-01-08 10:25:06,791 - 10 - training_embed.py - training - Epoch: [11][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.5193 (256.4212)	
2019-01-08 10:25:06,996 - 10 - training_embed.py - training - Epoch: [11][400/515]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 255.5733 (256.4173)	
2019-01-08 10:25:07,218 - 10 - training_embed.py - training - Epoch: [11][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.8593 (256.4381)	
2019-01-08 10:25:07,246 - 10 - training_embed.py - training - loss: 256.346289
2019-01-08 10:25:07,267 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:25:07,469 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 202.669 ms ~ 0.003 min ~ 0.203 sec
2019-01-08 10:25:07,735 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 467.961 ms ~ 0.008 min ~ 0.468 sec
2019-01-08 10:25:07,735 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:25:07,735 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:25:07,735 - 10 - corpus.py - subactivity_sampler - [22045. 22014. 21979. 21938. 21916. 21890.]
2019-01-08 10:25:11,525 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:25:11,525 - 10 - corpus.py - subactivity_sampler - [22780. 21575. 21646. 22046. 21604. 22131.]
2019-01-08 10:25:16,108 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:25:16,108 - 10 - corpus.py - subactivity_sampler - [23897. 20398. 21406. 22291. 21403. 22387.]
2019-01-08 10:25:20,501 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:25:20,501 - 10 - corpus.py - subactivity_sampler - [24333. 20155. 21243. 22127. 21472. 22452.]
2019-01-08 10:25:26,764 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:25:26,765 - 10 - corpus.py - subactivity_sampler - [25133. 19521. 20870. 22424. 20945. 22889.]
2019-01-08 10:25:30,624 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:25:30,624 - 10 - corpus.py - subactivity_sampler - [26874. 17959. 20487. 22740. 20484. 23238.]
2019-01-08 10:25:35,136 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:25:35,137 - 10 - corpus.py - subactivity_sampler - [28928. 16313. 19845. 23206. 19720. 23770.]
2019-01-08 10:25:39,712 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:25:39,712 - 10 - corpus.py - subactivity_sampler - [30590. 14621. 19274. 24505. 18538. 24254.]
2019-01-08 10:25:44,703 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:25:44,704 - 10 - corpus.py - subactivity_sampler - [33405. 12735. 18477. 25406. 17131. 24628.]
2019-01-08 10:25:48,553 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:25:48,554 - 10 - corpus.py - subactivity_sampler - [35581. 11041. 17488. 26576. 15897. 25199.]
2019-01-08 10:25:49,577 - 10 - corpus.py - subactivity_sampler - [35979. 10607. 17191. 27098. 15628. 25279.]
2019-01-08 10:25:49,577 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 41841.964 ms ~ 0.697 min ~ 41.842 sec
2019-01-08 10:25:49,577 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:25:50,239 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:25:50,239 - 10 - corpus.py - ordering_sampler - inv_count_vec: [0. 0. 0. 0. 6.]
2019-01-08 10:25:50,239 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:25:50,262 - 10 - corpus.py - rho_sampling - ['65.3203', '47.0072', '928.6742', '91.5288', '12.0798']
2019-01-08 10:25:50,262 - 10 - pipeline.py - baseline - Iteration 1
2019-01-08 10:25:50,298 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:25:50,300 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:25:50,301 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 9', '2: 45', '3: 46', '4: 7', '5: 47']
2019-01-08 10:25:50,304 - 10 - accuracy_class.py - mof_val - frames true: 35746	frames overall : 131782
2019-01-08 10:25:50,304 - 10 - corpus.py - accuracy_corpus - Action: tea
2019-01-08 10:25:50,304 - 10 - corpus.py - accuracy_corpus - MoF val: 0.27125100544839204
2019-01-08 10:25:50,304 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.24732512786268232
2019-01-08 10:25:50,304 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:25:50,304 - 10 - accuracy_class.py - mof_classes - label 6: 0.937685  8532 / 9099
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - mof_classes - label 7: 0.229291  1456 / 6350
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - mof_classes - label 9: 0.000000  0 / 560
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - mof_classes - label 45: 0.248600  9367 / 37679
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - mof_classes - label 46: 0.371937  15544 / 41792
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - mof_classes - label 47: 0.810526  847 / 1045
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - mof_classes - average class mof: 0.371149
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - iou_classes - label 6: 0.233459  8532 / 36546
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - iou_classes - label 7: 0.070948  1456 / 20522
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - iou_classes - label 9: 0.000000  0 / 11167
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - iou_classes - label 45: 0.205855  9367 / 45503
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - iou_classes - label 46: 0.291381  15544 / 53346
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - iou_classes - label 47: 0.033246  847 / 25477
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - iou_classes - average IoU: 0.139148
2019-01-08 10:25:50,305 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.119270
2019-01-08 10:25:51,802 - 10 - f1_score.py - f1 - f1 score: 0.221431
2019-01-08 10:25:51,807 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1544.862 ms ~ 0.026 min ~ 1.545 sec
2019-01-08 10:25:51,807 - 10 - corpus.py - embedding_training - .
2019-01-08 10:25:51,807 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:25:51,807 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:25:51,807 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:25:52,663 - 10 - training_embed.py - training - create model
2019-01-08 10:25:52,664 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:25:52,664 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:25:52,994 - 10 - training_embed.py - training - Epoch: [0][100/515]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 256.6476 (255.2064)	
2019-01-08 10:25:53,198 - 10 - training_embed.py - training - Epoch: [0][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 252.8148 (255.1849)	
2019-01-08 10:25:53,406 - 10 - training_embed.py - training - Epoch: [0][300/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 256.4846 (255.1311)	
2019-01-08 10:25:53,618 - 10 - training_embed.py - training - Epoch: [0][400/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 254.9212 (255.0716)	
2019-01-08 10:25:53,830 - 10 - training_embed.py - training - Epoch: [0][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 254.8319 (255.0618)	
2019-01-08 10:25:53,863 - 10 - training_embed.py - training - loss: 255.001225
2019-01-08 10:25:53,863 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:25:54,244 - 10 - training_embed.py - training - Epoch: [1][100/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 255.5487 (255.0419)	
2019-01-08 10:25:54,464 - 10 - training_embed.py - training - Epoch: [1][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 254.3426 (255.0832)	
2019-01-08 10:25:54,679 - 10 - training_embed.py - training - Epoch: [1][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2783 (255.0626)	
2019-01-08 10:25:54,890 - 10 - training_embed.py - training - Epoch: [1][400/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 253.4284 (255.0758)	
2019-01-08 10:25:55,101 - 10 - training_embed.py - training - Epoch: [1][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.4863 (255.0598)	
2019-01-08 10:25:55,131 - 10 - training_embed.py - training - loss: 254.974399
2019-01-08 10:25:55,131 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:25:55,493 - 10 - training_embed.py - training - Epoch: [2][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.3807 (255.4310)	
2019-01-08 10:25:55,702 - 10 - training_embed.py - training - Epoch: [2][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.8508 (255.1103)	
2019-01-08 10:25:55,910 - 10 - training_embed.py - training - Epoch: [2][300/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 258.2412 (255.0923)	
2019-01-08 10:25:56,132 - 10 - training_embed.py - training - Epoch: [2][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.1644 (255.0307)	
2019-01-08 10:25:56,343 - 10 - training_embed.py - training - Epoch: [2][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.3217 (255.0344)	
2019-01-08 10:25:56,377 - 10 - training_embed.py - training - loss: 254.949855
2019-01-08 10:25:56,378 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:25:56,733 - 10 - training_embed.py - training - Epoch: [3][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.9731 (254.9075)	
2019-01-08 10:25:56,956 - 10 - training_embed.py - training - Epoch: [3][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.0528 (254.9895)	
2019-01-08 10:25:57,190 - 10 - training_embed.py - training - Epoch: [3][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.7606 (254.9746)	
2019-01-08 10:25:57,417 - 10 - training_embed.py - training - Epoch: [3][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.6193 (254.9835)	
2019-01-08 10:25:57,632 - 10 - training_embed.py - training - Epoch: [3][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.2348 (255.0064)	
2019-01-08 10:25:57,664 - 10 - training_embed.py - training - loss: 254.923925
2019-01-08 10:25:57,665 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:25:58,012 - 10 - training_embed.py - training - Epoch: [4][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.4725 (255.0158)	
2019-01-08 10:25:58,238 - 10 - training_embed.py - training - Epoch: [4][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.5267 (255.1352)	
2019-01-08 10:25:58,458 - 10 - training_embed.py - training - Epoch: [4][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.3578 (255.0210)	
2019-01-08 10:25:58,661 - 10 - training_embed.py - training - Epoch: [4][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.2941 (255.0392)	
2019-01-08 10:25:58,877 - 10 - training_embed.py - training - Epoch: [4][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.4902 (255.0010)	
2019-01-08 10:25:58,907 - 10 - training_embed.py - training - loss: 254.899879
2019-01-08 10:25:58,907 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:25:59,261 - 10 - training_embed.py - training - Epoch: [5][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.4086 (255.0310)	
2019-01-08 10:25:59,476 - 10 - training_embed.py - training - Epoch: [5][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.6120 (254.9678)	
2019-01-08 10:25:59,693 - 10 - training_embed.py - training - Epoch: [5][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.3609 (254.9411)	
2019-01-08 10:25:59,911 - 10 - training_embed.py - training - Epoch: [5][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.0345 (254.9363)	
2019-01-08 10:26:00,143 - 10 - training_embed.py - training - Epoch: [5][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 258.3591 (254.9476)	
2019-01-08 10:26:00,174 - 10 - training_embed.py - training - loss: 254.873412
2019-01-08 10:26:00,175 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:26:00,518 - 10 - training_embed.py - training - Epoch: [6][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.8305 (254.9722)	
2019-01-08 10:26:00,719 - 10 - training_embed.py - training - Epoch: [6][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.4949 (254.9256)	
2019-01-08 10:26:00,930 - 10 - training_embed.py - training - Epoch: [6][300/515]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 254.8482 (254.9171)	
2019-01-08 10:26:01,146 - 10 - training_embed.py - training - Epoch: [6][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 253.6190 (254.8899)	
2019-01-08 10:26:01,355 - 10 - training_embed.py - training - Epoch: [6][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 255.5228 (254.9287)	
2019-01-08 10:26:01,388 - 10 - training_embed.py - training - loss: 254.847961
2019-01-08 10:26:01,389 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:26:01,768 - 10 - training_embed.py - training - Epoch: [7][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.1600 (254.7277)	
2019-01-08 10:26:01,984 - 10 - training_embed.py - training - Epoch: [7][200/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 257.8627 (254.8303)	
2019-01-08 10:26:02,198 - 10 - training_embed.py - training - Epoch: [7][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.8122 (254.8674)	
2019-01-08 10:26:02,401 - 10 - training_embed.py - training - Epoch: [7][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.3532 (254.9066)	
2019-01-08 10:26:02,605 - 10 - training_embed.py - training - Epoch: [7][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.0538 (254.8981)	
2019-01-08 10:26:02,636 - 10 - training_embed.py - training - loss: 254.823477
2019-01-08 10:26:02,637 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:26:03,006 - 10 - training_embed.py - training - Epoch: [8][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.5782 (254.8323)	
2019-01-08 10:26:03,226 - 10 - training_embed.py - training - Epoch: [8][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.7487 (254.8973)	
2019-01-08 10:26:03,450 - 10 - training_embed.py - training - Epoch: [8][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.0033 (254.8776)	
2019-01-08 10:26:03,677 - 10 - training_embed.py - training - Epoch: [8][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.9819 (254.8951)	
2019-01-08 10:26:03,888 - 10 - training_embed.py - training - Epoch: [8][500/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 254.4744 (254.8956)	
2019-01-08 10:26:03,916 - 10 - training_embed.py - training - loss: 254.797532
2019-01-08 10:26:03,916 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:26:04,276 - 10 - training_embed.py - training - Epoch: [9][100/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 258.8439 (254.7378)	
2019-01-08 10:26:04,493 - 10 - training_embed.py - training - Epoch: [9][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.9070 (254.7346)	
2019-01-08 10:26:04,701 - 10 - training_embed.py - training - Epoch: [9][300/515]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 255.8518 (254.7777)	
2019-01-08 10:26:04,915 - 10 - training_embed.py - training - Epoch: [9][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5856 (254.8399)	
2019-01-08 10:26:05,132 - 10 - training_embed.py - training - Epoch: [9][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 252.7572 (254.8663)	
2019-01-08 10:26:05,162 - 10 - training_embed.py - training - loss: 254.772310
2019-01-08 10:26:05,162 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:26:05,525 - 10 - training_embed.py - training - Epoch: [10][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.1978 (254.6964)	
2019-01-08 10:26:05,729 - 10 - training_embed.py - training - Epoch: [10][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.3256 (254.7597)	
2019-01-08 10:26:05,954 - 10 - training_embed.py - training - Epoch: [10][300/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 255.9506 (254.7662)	
2019-01-08 10:26:06,167 - 10 - training_embed.py - training - Epoch: [10][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.9892 (254.8161)	
2019-01-08 10:26:06,368 - 10 - training_embed.py - training - Epoch: [10][500/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 255.4001 (254.8328)	
2019-01-08 10:26:06,400 - 10 - training_embed.py - training - loss: 254.746994
2019-01-08 10:26:06,401 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:26:06,782 - 10 - training_embed.py - training - Epoch: [11][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.8890 (255.0111)	
2019-01-08 10:26:06,995 - 10 - training_embed.py - training - Epoch: [11][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2792 (254.9567)	
2019-01-08 10:26:07,220 - 10 - training_embed.py - training - Epoch: [11][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.5520 (254.8584)	
2019-01-08 10:26:07,436 - 10 - training_embed.py - training - Epoch: [11][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 255.8351 (254.8309)	
2019-01-08 10:26:07,653 - 10 - training_embed.py - training - Epoch: [11][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.4024 (254.8180)	
2019-01-08 10:26:07,681 - 10 - training_embed.py - training - loss: 254.722066
2019-01-08 10:26:07,702 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:26:07,923 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 220.983 ms ~ 0.004 min ~ 0.221 sec
2019-01-08 10:26:08,181 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 479.370 ms ~ 0.008 min ~ 0.479 sec
2019-01-08 10:26:08,181 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:26:08,182 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:26:08,182 - 10 - corpus.py - subactivity_sampler - [35979. 10607. 17191. 27098. 15628. 25279.]
2019-01-08 10:26:12,041 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:26:12,041 - 10 - corpus.py - subactivity_sampler - [37355.  9409. 16778. 27938. 15080. 25222.]
2019-01-08 10:26:16,701 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:26:16,701 - 10 - corpus.py - subactivity_sampler - [38565.  8457. 16220. 28917. 14005. 25618.]
2019-01-08 10:26:21,169 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:26:21,169 - 10 - corpus.py - subactivity_sampler - [39312.  7426. 16140. 29850. 13140. 25914.]
2019-01-08 10:26:27,548 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:26:27,548 - 10 - corpus.py - subactivity_sampler - [40578.  6274. 15722. 30904. 12117. 26187.]
2019-01-08 10:26:31,447 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:26:31,447 - 10 - corpus.py - subactivity_sampler - [41418.  5922. 15249. 31580. 11287. 26326.]
2019-01-08 10:26:35,996 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:26:35,996 - 10 - corpus.py - subactivity_sampler - [42002.  5672. 14613. 33164.  9956. 26375.]
2019-01-08 10:26:40,576 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:26:40,577 - 10 - corpus.py - subactivity_sampler - [43066.  5165. 13777. 33945.  9278. 26551.]
2019-01-08 10:26:45,642 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:26:45,642 - 10 - corpus.py - subactivity_sampler - [43565.  4885. 13113. 35231.  8589. 26399.]
2019-01-08 10:26:49,494 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:26:49,495 - 10 - corpus.py - subactivity_sampler - [43758.  4601. 12932. 35825.  8135. 26531.]
2019-01-08 10:26:50,515 - 10 - corpus.py - subactivity_sampler - [43777.  4541. 12810. 36188.  7909. 26557.]
2019-01-08 10:26:50,515 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 42333.492 ms ~ 0.706 min ~ 42.333 sec
2019-01-08 10:26:50,515 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:26:51,169 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:26:51,170 - 10 - corpus.py - ordering_sampler - inv_count_vec: [1. 0. 0. 0. 9.]
2019-01-08 10:26:51,170 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:26:51,184 - 10 - corpus.py - rho_sampling - ['60.8546', '32.4985', '928.5380', '91.1825', '6.0394']
2019-01-08 10:26:51,184 - 10 - pipeline.py - baseline - Iteration 2
2019-01-08 10:26:51,221 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:26:51,224 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:26:51,224 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 47', '2: 45', '3: 46', '4: 9', '5: 7']
2019-01-08 10:26:51,227 - 10 - accuracy_class.py - mof_val - frames true: 39399	frames overall : 131782
2019-01-08 10:26:51,227 - 10 - corpus.py - accuracy_corpus - Action: tea
2019-01-08 10:26:51,227 - 10 - corpus.py - accuracy_corpus - MoF val: 0.2989710279097297
2019-01-08 10:26:51,227 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.2967173058536067
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - mof_classes - label 6: 0.986702  8978 / 9099
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - mof_classes - label 7: 0.247087  1569 / 6350
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - mof_classes - label 9: 0.446429  250 / 560
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - mof_classes - label 45: 0.212081  7991 / 37679
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - mof_classes - label 46: 0.493181  20611 / 41792
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - mof_classes - label 47: 0.000000  0 / 1045
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - mof_classes - average class mof: 0.340783
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - iou_classes - label 6: 0.204520  8978 / 43898
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - iou_classes - label 7: 0.050067  1569 / 31338
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - iou_classes - label 9: 0.030417  250 / 8219
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - iou_classes - label 45: 0.188032  7991 / 42498
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - iou_classes - label 46: 0.359271  20611 / 57369
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - iou_classes - label 47: 0.000000  0 / 5586
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - iou_classes - average IoU: 0.138718
2019-01-08 10:26:51,228 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.118901
2019-01-08 10:26:52,726 - 10 - f1_score.py - f1 - f1 score: 0.236494
2019-01-08 10:26:52,733 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1548.686 ms ~ 0.026 min ~ 1.549 sec
2019-01-08 10:26:52,733 - 10 - corpus.py - embedding_training - .
2019-01-08 10:26:52,733 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:26:52,733 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:26:52,733 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:26:53,570 - 10 - training_embed.py - training - create model
2019-01-08 10:26:53,571 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:26:53,571 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:26:53,892 - 10 - training_embed.py - training - Epoch: [0][100/515]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 254.8768 (254.3599)	
2019-01-08 10:26:54,100 - 10 - training_embed.py - training - Epoch: [0][200/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 252.6034 (254.4048)	
2019-01-08 10:26:54,326 - 10 - training_embed.py - training - Epoch: [0][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.2528 (254.3969)	
2019-01-08 10:26:54,541 - 10 - training_embed.py - training - Epoch: [0][400/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 253.1632 (254.3920)	
2019-01-08 10:26:54,743 - 10 - training_embed.py - training - Epoch: [0][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2687 (254.3899)	
2019-01-08 10:26:54,774 - 10 - training_embed.py - training - loss: 254.315553
2019-01-08 10:26:54,775 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:26:55,127 - 10 - training_embed.py - training - Epoch: [1][100/515]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 255.4213 (254.4121)	
2019-01-08 10:26:55,335 - 10 - training_embed.py - training - Epoch: [1][200/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 255.2130 (254.3819)	
2019-01-08 10:26:55,565 - 10 - training_embed.py - training - Epoch: [1][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.7033 (254.4183)	
2019-01-08 10:26:55,785 - 10 - training_embed.py - training - Epoch: [1][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 254.2656 (254.3947)	
2019-01-08 10:26:56,004 - 10 - training_embed.py - training - Epoch: [1][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2868 (254.3528)	
2019-01-08 10:26:56,033 - 10 - training_embed.py - training - loss: 254.264765
2019-01-08 10:26:56,033 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:26:56,377 - 10 - training_embed.py - training - Epoch: [2][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.0422 (254.6668)	
2019-01-08 10:26:56,601 - 10 - training_embed.py - training - Epoch: [2][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2960 (254.3618)	
2019-01-08 10:26:56,819 - 10 - training_embed.py - training - Epoch: [2][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.1160 (254.3185)	
2019-01-08 10:26:57,031 - 10 - training_embed.py - training - Epoch: [2][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 253.7019 (254.3237)	
2019-01-08 10:26:57,247 - 10 - training_embed.py - training - Epoch: [2][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 255.1481 (254.3137)	
2019-01-08 10:26:57,279 - 10 - training_embed.py - training - loss: 254.215812
2019-01-08 10:26:57,279 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:26:57,650 - 10 - training_embed.py - training - Epoch: [3][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.7359 (254.2837)	
2019-01-08 10:26:57,854 - 10 - training_embed.py - training - Epoch: [3][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.8005 (254.2820)	
2019-01-08 10:26:58,079 - 10 - training_embed.py - training - Epoch: [3][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.1199 (254.2549)	
2019-01-08 10:26:58,293 - 10 - training_embed.py - training - Epoch: [3][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 256.0791 (254.2618)	
2019-01-08 10:26:58,504 - 10 - training_embed.py - training - Epoch: [3][500/515]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 251.4776 (254.2612)	
2019-01-08 10:26:58,533 - 10 - training_embed.py - training - loss: 254.165385
2019-01-08 10:26:58,534 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:26:58,902 - 10 - training_embed.py - training - Epoch: [4][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5874 (254.3579)	
2019-01-08 10:26:59,112 - 10 - training_embed.py - training - Epoch: [4][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.4407 (254.2795)	
2019-01-08 10:26:59,318 - 10 - training_embed.py - training - Epoch: [4][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.0918 (254.2210)	
2019-01-08 10:26:59,531 - 10 - training_embed.py - training - Epoch: [4][400/515]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 254.8376 (254.2161)	
2019-01-08 10:26:59,744 - 10 - training_embed.py - training - Epoch: [4][500/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 255.8451 (254.2113)	
2019-01-08 10:26:59,772 - 10 - training_embed.py - training - loss: 254.116376
2019-01-08 10:26:59,772 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:27:00,133 - 10 - training_embed.py - training - Epoch: [5][100/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 258.3973 (254.1529)	
2019-01-08 10:27:00,343 - 10 - training_embed.py - training - Epoch: [5][200/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 255.5694 (254.1876)	
2019-01-08 10:27:00,555 - 10 - training_embed.py - training - Epoch: [5][300/515]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 253.3785 (254.1402)	
2019-01-08 10:27:00,758 - 10 - training_embed.py - training - Epoch: [5][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.9318 (254.0984)	
2019-01-08 10:27:00,980 - 10 - training_embed.py - training - Epoch: [5][500/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 256.6671 (254.1358)	
2019-01-08 10:27:01,008 - 10 - training_embed.py - training - loss: 254.065304
2019-01-08 10:27:01,013 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:27:01,383 - 10 - training_embed.py - training - Epoch: [6][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.0624 (254.1882)	
2019-01-08 10:27:01,613 - 10 - training_embed.py - training - Epoch: [6][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.3610 (254.0636)	
2019-01-08 10:27:01,812 - 10 - training_embed.py - training - Epoch: [6][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.6673 (254.1025)	
2019-01-08 10:27:02,026 - 10 - training_embed.py - training - Epoch: [6][400/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 253.5213 (254.0827)	
2019-01-08 10:27:02,229 - 10 - training_embed.py - training - Epoch: [6][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2734 (254.1009)	
2019-01-08 10:27:02,261 - 10 - training_embed.py - training - loss: 254.015294
2019-01-08 10:27:02,262 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:27:02,620 - 10 - training_embed.py - training - Epoch: [7][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.3178 (253.9276)	
2019-01-08 10:27:02,842 - 10 - training_embed.py - training - Epoch: [7][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.9352 (254.0810)	
2019-01-08 10:27:03,043 - 10 - training_embed.py - training - Epoch: [7][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.9507 (254.0578)	
2019-01-08 10:27:03,260 - 10 - training_embed.py - training - Epoch: [7][400/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 253.6824 (254.0433)	
2019-01-08 10:27:03,485 - 10 - training_embed.py - training - Epoch: [7][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.7334 (254.0418)	
2019-01-08 10:27:03,512 - 10 - training_embed.py - training - loss: 253.966330
2019-01-08 10:27:03,512 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:27:03,864 - 10 - training_embed.py - training - Epoch: [8][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.1661 (253.8524)	
2019-01-08 10:27:04,082 - 10 - training_embed.py - training - Epoch: [8][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.7357 (253.9063)	
2019-01-08 10:27:04,286 - 10 - training_embed.py - training - Epoch: [8][300/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 253.8646 (253.9248)	
2019-01-08 10:27:04,490 - 10 - training_embed.py - training - Epoch: [8][400/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 253.5051 (253.9668)	
2019-01-08 10:27:04,707 - 10 - training_embed.py - training - Epoch: [8][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.4066 (254.0011)	
2019-01-08 10:27:04,736 - 10 - training_embed.py - training - loss: 253.915444
2019-01-08 10:27:04,736 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:27:05,096 - 10 - training_embed.py - training - Epoch: [9][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.4883 (253.8741)	
2019-01-08 10:27:05,296 - 10 - training_embed.py - training - Epoch: [9][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.3848 (253.8355)	
2019-01-08 10:27:05,510 - 10 - training_embed.py - training - Epoch: [9][300/515]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 256.2098 (253.8929)	
2019-01-08 10:27:05,716 - 10 - training_embed.py - training - Epoch: [9][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.8484 (253.9430)	
2019-01-08 10:27:05,936 - 10 - training_embed.py - training - Epoch: [9][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.3465 (253.9628)	
2019-01-08 10:27:05,966 - 10 - training_embed.py - training - loss: 253.865823
2019-01-08 10:27:05,966 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:27:06,326 - 10 - training_embed.py - training - Epoch: [10][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.5926 (253.8750)	
2019-01-08 10:27:06,543 - 10 - training_embed.py - training - Epoch: [10][200/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 256.0183 (253.8463)	
2019-01-08 10:27:06,759 - 10 - training_embed.py - training - Epoch: [10][300/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 255.1810 (253.8591)	
2019-01-08 10:27:06,963 - 10 - training_embed.py - training - Epoch: [10][400/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 253.3654 (253.8679)	
2019-01-08 10:27:07,180 - 10 - training_embed.py - training - Epoch: [10][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.6091 (253.8990)	
2019-01-08 10:27:07,207 - 10 - training_embed.py - training - loss: 253.815757
2019-01-08 10:27:07,207 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:27:07,583 - 10 - training_embed.py - training - Epoch: [11][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.8546 (254.0423)	
2019-01-08 10:27:07,802 - 10 - training_embed.py - training - Epoch: [11][200/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 252.4670 (253.9473)	
2019-01-08 10:27:08,023 - 10 - training_embed.py - training - Epoch: [11][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.4804 (253.8656)	
2019-01-08 10:27:08,250 - 10 - training_embed.py - training - Epoch: [11][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 255.2394 (253.8637)	
2019-01-08 10:27:08,480 - 10 - training_embed.py - training - Epoch: [11][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 253.0430 (253.8512)	
2019-01-08 10:27:08,508 - 10 - training_embed.py - training - loss: 253.766565
2019-01-08 10:27:08,525 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:27:08,764 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 238.259 ms ~ 0.004 min ~ 0.238 sec
2019-01-08 10:27:09,030 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 504.144 ms ~ 0.008 min ~ 0.504 sec
2019-01-08 10:27:09,030 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:27:09,030 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:27:09,031 - 10 - corpus.py - subactivity_sampler - [43777.  4541. 12810. 36188.  7909. 26557.]
2019-01-08 10:27:12,924 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:27:12,924 - 10 - corpus.py - subactivity_sampler - [44059.  4439. 12446. 36624.  7553. 26661.]
2019-01-08 10:27:17,615 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:27:17,615 - 10 - corpus.py - subactivity_sampler - [44245.  4250. 11839. 37591.  7107. 26750.]
2019-01-08 10:27:22,097 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:27:22,097 - 10 - corpus.py - subactivity_sampler - [44673.  3740. 11838. 38047.  6541. 26943.]
2019-01-08 10:27:28,473 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:27:28,474 - 10 - corpus.py - subactivity_sampler - [45422.  3494. 11596. 38259.  5881. 27130.]
2019-01-08 10:27:32,368 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:27:32,368 - 10 - corpus.py - subactivity_sampler - [45603.  3399. 11409. 38959.  5361. 27051.]
2019-01-08 10:27:36,930 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:27:36,930 - 10 - corpus.py - subactivity_sampler - [45693.  3237. 10943. 39682.  5158. 27069.]
2019-01-08 10:27:41,518 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:27:41,519 - 10 - corpus.py - subactivity_sampler - [45989.  3173. 10634. 39909.  4974. 27103.]
2019-01-08 10:27:46,550 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:27:46,551 - 10 - corpus.py - subactivity_sampler - [46008.  3116. 10479. 40209.  4814. 27156.]
2019-01-08 10:27:50,420 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:27:50,420 - 10 - corpus.py - subactivity_sampler - [46029.  3081. 10299. 40616.  4722. 27035.]
2019-01-08 10:27:51,441 - 10 - corpus.py - subactivity_sampler - [46035.  3064. 10268. 40672.  4699. 27044.]
2019-01-08 10:27:51,441 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 42411.236 ms ~ 0.707 min ~ 42.411 sec
2019-01-08 10:27:51,441 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:27:52,100 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:27:52,100 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 1.  4.  0.  2. 10.]
2019-01-08 10:27:52,100 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:27:52,119 - 10 - corpus.py - rho_sampling - ['61.5503', '13.6696', '928.4017', '86.3411', '11.9934']
2019-01-08 10:27:52,119 - 10 - pipeline.py - baseline - Iteration 3
2019-01-08 10:27:52,155 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:27:52,158 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:27:52,158 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 47', '2: 45', '3: 46', '4: 9', '5: 7']
2019-01-08 10:27:52,162 - 10 - accuracy_class.py - mof_val - frames true: 40763	frames overall : 131782
2019-01-08 10:27:52,162 - 10 - corpus.py - accuracy_corpus - Action: tea
2019-01-08 10:27:52,162 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3093214551304427
2019-01-08 10:27:52,162 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3093214551304427
2019-01-08 10:27:52,162 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:27:52,162 - 10 - accuracy_class.py - mof_classes - label 6: 0.988900  8998 / 9099
2019-01-08 10:27:52,162 - 10 - accuracy_class.py - mof_classes - label 7: 0.259528  1648 / 6350
2019-01-08 10:27:52,162 - 10 - accuracy_class.py - mof_classes - label 9: 0.196429  110 / 560
2019-01-08 10:27:52,162 - 10 - accuracy_class.py - mof_classes - label 45: 0.185355  6984 / 37679
2019-01-08 10:27:52,162 - 10 - accuracy_class.py - mof_classes - label 46: 0.550895  23023 / 41792
2019-01-08 10:27:52,162 - 10 - accuracy_class.py - mof_classes - label 47: 0.000000  0 / 1045
2019-01-08 10:27:52,162 - 10 - accuracy_class.py - mof_classes - average class mof: 0.311587
2019-01-08 10:27:52,162 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:27:52,163 - 10 - accuracy_class.py - iou_classes - label 6: 0.195032  8998 / 46136
2019-01-08 10:27:52,163 - 10 - accuracy_class.py - iou_classes - label 7: 0.051912  1648 / 31746
2019-01-08 10:27:52,163 - 10 - accuracy_class.py - iou_classes - label 9: 0.021363  110 / 5149
2019-01-08 10:27:52,163 - 10 - accuracy_class.py - iou_classes - label 45: 0.170495  6984 / 40963
2019-01-08 10:27:52,163 - 10 - accuracy_class.py - iou_classes - label 46: 0.387325  23023 / 59441
2019-01-08 10:27:52,163 - 10 - accuracy_class.py - iou_classes - label 47: 0.000000  0 / 4109
2019-01-08 10:27:52,163 - 10 - accuracy_class.py - iou_classes - average IoU: 0.137688
2019-01-08 10:27:52,163 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.118018
2019-01-08 10:27:53,690 - 10 - f1_score.py - f1 - f1 score: 0.242582
2019-01-08 10:27:53,695 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1575.701 ms ~ 0.026 min ~ 1.576 sec
2019-01-08 10:27:53,695 - 10 - corpus.py - embedding_training - .
2019-01-08 10:27:53,695 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:27:53,695 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:27:53,695 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:27:54,549 - 10 - training_embed.py - training - create model
2019-01-08 10:27:54,550 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:27:54,550 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:27:54,886 - 10 - training_embed.py - training - Epoch: [0][100/515]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 256.3754 (254.6917)	
2019-01-08 10:27:55,101 - 10 - training_embed.py - training - Epoch: [0][200/515]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 254.5020 (254.7154)	
2019-01-08 10:27:55,314 - 10 - training_embed.py - training - Epoch: [0][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.9767 (254.6367)	
2019-01-08 10:27:55,538 - 10 - training_embed.py - training - Epoch: [0][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.0300 (254.6485)	
2019-01-08 10:27:55,761 - 10 - training_embed.py - training - Epoch: [0][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.4710 (254.6703)	
2019-01-08 10:27:55,791 - 10 - training_embed.py - training - loss: 254.593719
2019-01-08 10:27:55,791 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:27:56,147 - 10 - training_embed.py - training - Epoch: [1][100/515]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 255.9387 (254.7275)	
2019-01-08 10:27:56,367 - 10 - training_embed.py - training - Epoch: [1][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.1701 (254.6978)	
2019-01-08 10:27:56,589 - 10 - training_embed.py - training - Epoch: [1][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2229 (254.6870)	
2019-01-08 10:27:56,808 - 10 - training_embed.py - training - Epoch: [1][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 254.1047 (254.6798)	
2019-01-08 10:27:57,027 - 10 - training_embed.py - training - Epoch: [1][500/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 254.8951 (254.6136)	
2019-01-08 10:27:57,055 - 10 - training_embed.py - training - loss: 254.526672
2019-01-08 10:27:57,055 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:27:57,417 - 10 - training_embed.py - training - Epoch: [2][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.6277 (254.8793)	
2019-01-08 10:27:57,621 - 10 - training_embed.py - training - Epoch: [2][200/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 252.6077 (254.6208)	
2019-01-08 10:27:57,840 - 10 - training_embed.py - training - Epoch: [2][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.6429 (254.5692)	
2019-01-08 10:27:58,065 - 10 - training_embed.py - training - Epoch: [2][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.8537 (254.5843)	
2019-01-08 10:27:58,285 - 10 - training_embed.py - training - Epoch: [2][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.7049 (254.5606)	
2019-01-08 10:27:58,315 - 10 - training_embed.py - training - loss: 254.461863
2019-01-08 10:27:58,316 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:27:58,674 - 10 - training_embed.py - training - Epoch: [3][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 252.6666 (254.4649)	
2019-01-08 10:27:58,882 - 10 - training_embed.py - training - Epoch: [3][200/515]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 253.9346 (254.4306)	
2019-01-08 10:27:59,095 - 10 - training_embed.py - training - Epoch: [3][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.6398 (254.4914)	
2019-01-08 10:27:59,305 - 10 - training_embed.py - training - Epoch: [3][400/515]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 257.8842 (254.5246)	
2019-01-08 10:27:59,514 - 10 - training_embed.py - training - Epoch: [3][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 250.7023 (254.4900)	
2019-01-08 10:27:59,546 - 10 - training_embed.py - training - loss: 254.395717
2019-01-08 10:27:59,546 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:27:59,910 - 10 - training_embed.py - training - Epoch: [4][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5737 (254.5519)	
2019-01-08 10:28:00,121 - 10 - training_embed.py - training - Epoch: [4][200/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 253.1986 (254.5256)	
2019-01-08 10:28:00,334 - 10 - training_embed.py - training - Epoch: [4][300/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 253.6331 (254.4543)	
2019-01-08 10:28:00,543 - 10 - training_embed.py - training - Epoch: [4][400/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 255.2616 (254.4505)	
2019-01-08 10:28:00,754 - 10 - training_embed.py - training - Epoch: [4][500/515]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 256.4817 (254.4222)	
2019-01-08 10:28:00,785 - 10 - training_embed.py - training - loss: 254.330163
2019-01-08 10:28:00,785 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:28:01,112 - 10 - training_embed.py - training - Epoch: [5][100/515]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 258.7955 (254.4169)	
2019-01-08 10:28:01,328 - 10 - training_embed.py - training - Epoch: [5][200/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 256.1110 (254.4266)	
2019-01-08 10:28:01,557 - 10 - training_embed.py - training - Epoch: [5][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2873 (254.3470)	
2019-01-08 10:28:01,784 - 10 - training_embed.py - training - Epoch: [5][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.7400 (254.3107)	
2019-01-08 10:28:01,998 - 10 - training_embed.py - training - Epoch: [5][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 256.5360 (254.3358)	
2019-01-08 10:28:02,026 - 10 - training_embed.py - training - loss: 254.263162
2019-01-08 10:28:02,027 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:28:02,374 - 10 - training_embed.py - training - Epoch: [6][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.3451 (254.3685)	
2019-01-08 10:28:02,599 - 10 - training_embed.py - training - Epoch: [6][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.9897 (254.2716)	
2019-01-08 10:28:02,800 - 10 - training_embed.py - training - Epoch: [6][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.1428 (254.3230)	
2019-01-08 10:28:03,010 - 10 - training_embed.py - training - Epoch: [6][400/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 253.7192 (254.2881)	
2019-01-08 10:28:03,209 - 10 - training_embed.py - training - Epoch: [6][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.3862 (254.2839)	
2019-01-08 10:28:03,237 - 10 - training_embed.py - training - loss: 254.197706
2019-01-08 10:28:03,237 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:28:03,595 - 10 - training_embed.py - training - Epoch: [7][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.6069 (254.1032)	
2019-01-08 10:28:03,804 - 10 - training_embed.py - training - Epoch: [7][200/515]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 254.8399 (254.2740)	
2019-01-08 10:28:04,031 - 10 - training_embed.py - training - Epoch: [7][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.5636 (254.2594)	
2019-01-08 10:28:04,231 - 10 - training_embed.py - training - Epoch: [7][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.8269 (254.2204)	
2019-01-08 10:28:04,446 - 10 - training_embed.py - training - Epoch: [7][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.4555 (254.2169)	
2019-01-08 10:28:04,472 - 10 - training_embed.py - training - loss: 254.132789
2019-01-08 10:28:04,472 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:28:04,820 - 10 - training_embed.py - training - Epoch: [8][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.4467 (254.1135)	
2019-01-08 10:28:05,033 - 10 - training_embed.py - training - Epoch: [8][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.6168 (254.0606)	
2019-01-08 10:28:05,249 - 10 - training_embed.py - training - Epoch: [8][300/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 252.8818 (254.0787)	
2019-01-08 10:28:05,475 - 10 - training_embed.py - training - Epoch: [8][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.1516 (254.1610)	
2019-01-08 10:28:05,685 - 10 - training_embed.py - training - Epoch: [8][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 252.9550 (254.1578)	
2019-01-08 10:28:05,717 - 10 - training_embed.py - training - loss: 254.065727
2019-01-08 10:28:05,717 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:28:06,087 - 10 - training_embed.py - training - Epoch: [9][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.4750 (253.9644)	
2019-01-08 10:28:06,284 - 10 - training_embed.py - training - Epoch: [9][200/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 254.0089 (253.9364)	
2019-01-08 10:28:06,501 - 10 - training_embed.py - training - Epoch: [9][300/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 254.9355 (254.0087)	
2019-01-08 10:28:06,706 - 10 - training_embed.py - training - Epoch: [9][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 252.4940 (254.0568)	
2019-01-08 10:28:06,921 - 10 - training_embed.py - training - Epoch: [9][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 254.5140 (254.0893)	
2019-01-08 10:28:06,949 - 10 - training_embed.py - training - loss: 253.999931
2019-01-08 10:28:06,949 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:28:07,323 - 10 - training_embed.py - training - Epoch: [10][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.7447 (254.0818)	
2019-01-08 10:28:07,529 - 10 - training_embed.py - training - Epoch: [10][200/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 256.3910 (253.9930)	
2019-01-08 10:28:07,733 - 10 - training_embed.py - training - Epoch: [10][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.6280 (254.0331)	
2019-01-08 10:28:07,952 - 10 - training_embed.py - training - Epoch: [10][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.5206 (254.0134)	
2019-01-08 10:28:08,166 - 10 - training_embed.py - training - Epoch: [10][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.0974 (254.0151)	
2019-01-08 10:28:08,194 - 10 - training_embed.py - training - loss: 253.934309
2019-01-08 10:28:08,195 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:28:08,581 - 10 - training_embed.py - training - Epoch: [11][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.4287 (254.0218)	
2019-01-08 10:28:08,793 - 10 - training_embed.py - training - Epoch: [11][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.2262 (254.0063)	
2019-01-08 10:28:09,015 - 10 - training_embed.py - training - Epoch: [11][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.4072 (253.9355)	
2019-01-08 10:28:09,233 - 10 - training_embed.py - training - Epoch: [11][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.4678 (253.9437)	
2019-01-08 10:28:09,450 - 10 - training_embed.py - training - Epoch: [11][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.8840 (253.9608)	
2019-01-08 10:28:09,481 - 10 - training_embed.py - training - loss: 253.868350
2019-01-08 10:28:09,505 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:28:09,739 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 233.836 ms ~ 0.004 min ~ 0.234 sec
2019-01-08 10:28:10,014 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 509.260 ms ~ 0.008 min ~ 0.509 sec
2019-01-08 10:28:10,014 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:28:10,014 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:28:10,015 - 10 - corpus.py - subactivity_sampler - [46035.  3064. 10268. 40672.  4699. 27044.]
2019-01-08 10:28:13,863 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:28:13,863 - 10 - corpus.py - subactivity_sampler - [46099.  2996. 10033. 41054.  4529. 27071.]
2019-01-08 10:28:18,542 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:28:18,543 - 10 - corpus.py - subactivity_sampler - [46234.  2945.  9669. 41630.  4079. 27225.]
2019-01-08 10:28:23,094 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:28:23,095 - 10 - corpus.py - subactivity_sampler - [46348.  2842.  9597. 41802.  3946. 27247.]
2019-01-08 10:28:29,516 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:28:29,516 - 10 - corpus.py - subactivity_sampler - [46743.  2707.  9230. 41902.  3630. 27570.]
2019-01-08 10:28:33,487 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:28:33,487 - 10 - corpus.py - subactivity_sampler - [46779.  2673.  9054. 42204.  3565. 27507.]
2019-01-08 10:28:37,989 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:28:37,990 - 10 - corpus.py - subactivity_sampler - [46749.  2592.  8893. 42642.  3524. 27382.]
2019-01-08 10:28:42,511 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:28:42,511 - 10 - corpus.py - subactivity_sampler - [46796.  2585.  8783. 42902.  3377. 27339.]
2019-01-08 10:28:47,474 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:28:47,474 - 10 - corpus.py - subactivity_sampler - [46789.  2583.  8672. 43049.  3294. 27395.]
2019-01-08 10:28:51,251 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:28:51,251 - 10 - corpus.py - subactivity_sampler - [46780.  2588.  8497. 43220.  3299. 27398.]
2019-01-08 10:28:52,276 - 10 - corpus.py - subactivity_sampler - [46796.  2580.  8485. 43228.  3295. 27398.]
2019-01-08 10:28:52,276 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 42262.221 ms ~ 0.704 min ~ 42.262 sec
2019-01-08 10:28:52,276 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:28:52,930 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:28:52,931 - 10 - corpus.py - ordering_sampler - inv_count_vec: [0. 4. 0. 2. 7.]
2019-01-08 10:28:52,931 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:28:52,951 - 10 - corpus.py - rho_sampling - ['63.5242', '36.2396', '51.3890', '282.7728', '26.8397']
2019-01-08 10:28:52,951 - 10 - pipeline.py - baseline - Iteration 4
2019-01-08 10:28:52,987 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:28:52,990 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:28:52,990 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 47', '2: 45', '3: 46', '4: 9', '5: 7']
2019-01-08 10:28:52,994 - 10 - accuracy_class.py - mof_val - frames true: 41756	frames overall : 131782
2019-01-08 10:28:52,994 - 10 - corpus.py - accuracy_corpus - Action: tea
2019-01-08 10:28:52,994 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3168566268534398
2019-01-08 10:28:52,994 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3168566268534398
2019-01-08 10:28:52,994 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:28:52,994 - 10 - accuracy_class.py - mof_classes - label 6: 0.988900  8998 / 9099
2019-01-08 10:28:52,994 - 10 - accuracy_class.py - mof_classes - label 7: 0.298110  1893 / 6350
2019-01-08 10:28:52,994 - 10 - accuracy_class.py - mof_classes - label 9: 0.130357  73 / 560
2019-01-08 10:28:52,994 - 10 - accuracy_class.py - mof_classes - label 45: 0.163805  6172 / 37679
2019-01-08 10:28:52,994 - 10 - accuracy_class.py - mof_classes - label 46: 0.589108  24620 / 41792
2019-01-08 10:28:52,994 - 10 - accuracy_class.py - mof_classes - label 47: 0.000000  0 / 1045
2019-01-08 10:28:52,994 - 10 - accuracy_class.py - mof_classes - average class mof: 0.310040
2019-01-08 10:28:52,994 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:28:52,994 - 10 - accuracy_class.py - iou_classes - label 6: 0.191867  8998 / 46897
2019-01-08 10:28:52,994 - 10 - accuracy_class.py - iou_classes - label 7: 0.059426  1893 / 31855
2019-01-08 10:28:52,995 - 10 - accuracy_class.py - iou_classes - label 9: 0.019302  73 / 3782
2019-01-08 10:28:52,995 - 10 - accuracy_class.py - iou_classes - label 45: 0.154331  6172 / 39992
2019-01-08 10:28:52,995 - 10 - accuracy_class.py - iou_classes - label 46: 0.407616  24620 / 60400
2019-01-08 10:28:52,995 - 10 - accuracy_class.py - iou_classes - label 47: 0.000000  0 / 3625
2019-01-08 10:28:52,995 - 10 - accuracy_class.py - iou_classes - average IoU: 0.138757
2019-01-08 10:28:52,995 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.118935
2019-01-08 10:28:54,502 - 10 - f1_score.py - f1 - f1 score: 0.246575
2019-01-08 10:28:54,506 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1554.972 ms ~ 0.026 min ~ 1.555 sec
2019-01-08 10:28:54,507 - 10 - corpus.py - embedding_training - .
2019-01-08 10:28:54,507 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:28:54,507 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:28:54,507 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:28:55,402 - 10 - training_embed.py - training - create model
2019-01-08 10:28:55,403 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:28:55,403 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:28:55,745 - 10 - training_embed.py - training - Epoch: [0][100/515]	Time 0.003 (0.003)	Data 0.002 (0.002)	Loss 255.2661 (254.8098)	
2019-01-08 10:28:55,968 - 10 - training_embed.py - training - Epoch: [0][200/515]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 254.8190 (254.8993)	
2019-01-08 10:28:56,188 - 10 - training_embed.py - training - Epoch: [0][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 257.0017 (254.8074)	
2019-01-08 10:28:56,413 - 10 - training_embed.py - training - Epoch: [0][400/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 254.4639 (254.8313)	
2019-01-08 10:28:56,639 - 10 - training_embed.py - training - Epoch: [0][500/515]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 254.7524 (254.8394)	
2019-01-08 10:28:56,690 - 10 - training_embed.py - training - loss: 254.764654
2019-01-08 10:28:56,693 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:28:57,071 - 10 - training_embed.py - training - Epoch: [1][100/515]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 255.7559 (254.8723)	
2019-01-08 10:28:57,291 - 10 - training_embed.py - training - Epoch: [1][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.0442 (254.8719)	
2019-01-08 10:28:57,503 - 10 - training_embed.py - training - Epoch: [1][300/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 254.6986 (254.8407)	
2019-01-08 10:28:57,727 - 10 - training_embed.py - training - Epoch: [1][400/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.2470 (254.8278)	
2019-01-08 10:28:57,946 - 10 - training_embed.py - training - Epoch: [1][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.7114 (254.7805)	
2019-01-08 10:28:57,976 - 10 - training_embed.py - training - loss: 254.689397
2019-01-08 10:28:57,976 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:28:58,340 - 10 - training_embed.py - training - Epoch: [2][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.9102 (254.9710)	
2019-01-08 10:28:58,560 - 10 - training_embed.py - training - Epoch: [2][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.0669 (254.7638)	
2019-01-08 10:28:58,826 - 10 - training_embed.py - training - Epoch: [2][300/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 255.1318 (254.7349)	
2019-01-08 10:28:59,087 - 10 - training_embed.py - training - Epoch: [2][400/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.9140 (254.7446)	
2019-01-08 10:28:59,358 - 10 - training_embed.py - training - Epoch: [2][500/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.4831 (254.7175)	
2019-01-08 10:28:59,387 - 10 - training_embed.py - training - loss: 254.616239
2019-01-08 10:28:59,388 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:28:59,768 - 10 - training_embed.py - training - Epoch: [3][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.4504 (254.5858)	
2019-01-08 10:29:00,002 - 10 - training_embed.py - training - Epoch: [3][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 254.5821 (254.5543)	
2019-01-08 10:29:00,222 - 10 - training_embed.py - training - Epoch: [3][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.0829 (254.6174)	
2019-01-08 10:29:00,446 - 10 - training_embed.py - training - Epoch: [3][400/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 258.3533 (254.6486)	
2019-01-08 10:29:00,653 - 10 - training_embed.py - training - Epoch: [3][500/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 250.0647 (254.6295)	
2019-01-08 10:29:00,683 - 10 - training_embed.py - training - loss: 254.541610
2019-01-08 10:29:00,683 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:29:01,041 - 10 - training_embed.py - training - Epoch: [4][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.0735 (254.6467)	
2019-01-08 10:29:01,268 - 10 - training_embed.py - training - Epoch: [4][200/515]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 253.6238 (254.6693)	
2019-01-08 10:29:01,494 - 10 - training_embed.py - training - Epoch: [4][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 254.6775 (254.6104)	
2019-01-08 10:29:01,738 - 10 - training_embed.py - training - Epoch: [4][400/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 255.7262 (254.6006)	
2019-01-08 10:29:01,997 - 10 - training_embed.py - training - Epoch: [4][500/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.7239 (254.5604)	
2019-01-08 10:29:02,025 - 10 - training_embed.py - training - loss: 254.467558
2019-01-08 10:29:02,025 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:29:02,398 - 10 - training_embed.py - training - Epoch: [5][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 259.1489 (254.5308)	
2019-01-08 10:29:02,663 - 10 - training_embed.py - training - Epoch: [5][200/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 256.0919 (254.5376)	
2019-01-08 10:29:02,902 - 10 - training_embed.py - training - Epoch: [5][300/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 254.4599 (254.4817)	
2019-01-08 10:29:03,148 - 10 - training_embed.py - training - Epoch: [5][400/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 254.4117 (254.4627)	
2019-01-08 10:29:03,406 - 10 - training_embed.py - training - Epoch: [5][500/515]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 257.1478 (254.4641)	
2019-01-08 10:29:03,448 - 10 - training_embed.py - training - loss: 254.391599
2019-01-08 10:29:03,448 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:29:03,846 - 10 - training_embed.py - training - Epoch: [6][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 257.6725 (254.5177)	
2019-01-08 10:29:04,110 - 10 - training_embed.py - training - Epoch: [6][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 252.0603 (254.4084)	
2019-01-08 10:29:04,332 - 10 - training_embed.py - training - Epoch: [6][300/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 255.4136 (254.4396)	
2019-01-08 10:29:04,561 - 10 - training_embed.py - training - Epoch: [6][400/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 254.5033 (254.4227)	
2019-01-08 10:29:04,792 - 10 - training_embed.py - training - Epoch: [6][500/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 252.8086 (254.4028)	
2019-01-08 10:29:04,821 - 10 - training_embed.py - training - loss: 254.318406
2019-01-08 10:29:04,821 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:29:05,241 - 10 - training_embed.py - training - Epoch: [7][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.2251 (254.3121)	
2019-01-08 10:29:05,473 - 10 - training_embed.py - training - Epoch: [7][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.3229 (254.4283)	
2019-01-08 10:29:05,707 - 10 - training_embed.py - training - Epoch: [7][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 254.8950 (254.3779)	
2019-01-08 10:29:05,921 - 10 - training_embed.py - training - Epoch: [7][400/515]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 254.5386 (254.3320)	
2019-01-08 10:29:06,138 - 10 - training_embed.py - training - Epoch: [7][500/515]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 255.9929 (254.3253)	
2019-01-08 10:29:06,163 - 10 - training_embed.py - training - loss: 254.244890
2019-01-08 10:29:06,163 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:29:06,511 - 10 - training_embed.py - training - Epoch: [8][100/515]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 252.8623 (254.2354)	
2019-01-08 10:29:06,720 - 10 - training_embed.py - training - Epoch: [8][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.0196 (254.1590)	
2019-01-08 10:29:06,947 - 10 - training_embed.py - training - Epoch: [8][300/515]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 253.6821 (254.1986)	
2019-01-08 10:29:07,183 - 10 - training_embed.py - training - Epoch: [8][400/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.2927 (254.2780)	
2019-01-08 10:29:07,429 - 10 - training_embed.py - training - Epoch: [8][500/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.0806 (254.2694)	
2019-01-08 10:29:07,460 - 10 - training_embed.py - training - loss: 254.169232
2019-01-08 10:29:07,460 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:29:07,817 - 10 - training_embed.py - training - Epoch: [9][100/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 256.5409 (254.0975)	
2019-01-08 10:29:08,038 - 10 - training_embed.py - training - Epoch: [9][200/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 254.3906 (254.0978)	
2019-01-08 10:29:08,271 - 10 - training_embed.py - training - Epoch: [9][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.1518 (254.1456)	
2019-01-08 10:29:08,485 - 10 - training_embed.py - training - Epoch: [9][400/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 252.4630 (254.1818)	
2019-01-08 10:29:08,721 - 10 - training_embed.py - training - Epoch: [9][500/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 254.7924 (254.1926)	
2019-01-08 10:29:08,753 - 10 - training_embed.py - training - loss: 254.094739
2019-01-08 10:29:08,753 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:29:09,144 - 10 - training_embed.py - training - Epoch: [10][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.5778 (254.1441)	
2019-01-08 10:29:09,346 - 10 - training_embed.py - training - Epoch: [10][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.8620 (254.0714)	
2019-01-08 10:29:09,573 - 10 - training_embed.py - training - Epoch: [10][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.5342 (254.1142)	
2019-01-08 10:29:09,783 - 10 - training_embed.py - training - Epoch: [10][400/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 252.9434 (254.0929)	
2019-01-08 10:29:10,017 - 10 - training_embed.py - training - Epoch: [10][500/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.2939 (254.1077)	
2019-01-08 10:29:10,045 - 10 - training_embed.py - training - loss: 254.020565
2019-01-08 10:29:10,046 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:29:10,421 - 10 - training_embed.py - training - Epoch: [11][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.5494 (254.0830)	
2019-01-08 10:29:10,629 - 10 - training_embed.py - training - Epoch: [11][200/515]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 251.6146 (254.0780)	
2019-01-08 10:29:10,854 - 10 - training_embed.py - training - Epoch: [11][300/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 255.0188 (254.0077)	
2019-01-08 10:29:11,071 - 10 - training_embed.py - training - Epoch: [11][400/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.1314 (254.0087)	
2019-01-08 10:29:11,286 - 10 - training_embed.py - training - Epoch: [11][500/515]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 255.2014 (254.0302)	
2019-01-08 10:29:11,314 - 10 - training_embed.py - training - loss: 253.946419
2019-01-08 10:29:11,337 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:29:11,571 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 234.380 ms ~ 0.004 min ~ 0.234 sec
2019-01-08 10:29:11,889 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 551.903 ms ~ 0.009 min ~ 0.552 sec
2019-01-08 10:29:11,889 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:29:11,889 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:29:11,890 - 10 - corpus.py - subactivity_sampler - [46796.  2580.  8485. 43228.  3295. 27398.]
2019-01-08 10:29:15,874 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:29:15,874 - 10 - corpus.py - subactivity_sampler - [46862.  2545.  8334. 43409.  3242. 27390.]
2019-01-08 10:29:20,478 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:29:20,479 - 10 - corpus.py - subactivity_sampler - [47137.  2530.  8032. 43531.  3153. 27399.]
2019-01-08 10:29:24,872 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:29:24,872 - 10 - corpus.py - subactivity_sampler - [47106.  2508.  7987. 43745.  3111. 27325.]
2019-01-08 10:29:31,143 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:29:31,144 - 10 - corpus.py - subactivity_sampler - [47190.  2464.  7904. 44090.  2852. 27282.]
2019-01-08 10:29:35,138 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:29:35,139 - 10 - corpus.py - subactivity_sampler - [47176.  2452.  7803. 44298.  2828. 27225.]
2019-01-08 10:29:39,720 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:29:39,720 - 10 - corpus.py - subactivity_sampler - [47245.  2412.  7712. 44432.  2892. 27089.]
2019-01-08 10:29:44,328 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:29:44,328 - 10 - corpus.py - subactivity_sampler - [47367.  2406.  7545. 44556.  2853. 27055.]
2019-01-08 10:29:49,463 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:29:49,463 - 10 - corpus.py - subactivity_sampler - [47568.  2393.  7412. 44522.  2819. 27068.]
2019-01-08 10:29:53,353 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:29:53,353 - 10 - corpus.py - subactivity_sampler - [47602.  2330.  7339. 44640.  2815. 27056.]
2019-01-08 10:29:54,357 - 10 - corpus.py - subactivity_sampler - [47602.  2327.  7341. 44641.  2814. 27057.]
2019-01-08 10:29:54,357 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 42468.064 ms ~ 0.708 min ~ 42.468 sec
2019-01-08 10:29:54,357 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:29:55,037 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:29:55,038 - 10 - corpus.py - ordering_sampler - inv_count_vec: [0. 4. 2. 0. 6.]
2019-01-08 10:29:55,038 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:29:55,053 - 10 - corpus.py - rho_sampling - ['63.0026', '18.2957', '923.0387', '89.6201', '36.3499']
2019-01-08 10:29:55,053 - 10 - pipeline.py - baseline - Iteration 5
2019-01-08 10:29:55,089 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:29:55,092 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:29:55,092 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 45', '1: 47', '2: 6', '3: 46', '4: 9', '5: 7']
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - mof_val - frames true: 42895	frames overall : 131782
2019-01-08 10:29:55,096 - 10 - corpus.py - accuracy_corpus - Action: tea
2019-01-08 10:29:55,096 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3254996888801202
2019-01-08 10:29:55,096 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.31750921977204777
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - mof_classes - label 6: 0.007034  64 / 9099
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - mof_classes - label 7: 0.308504  1959 / 6350
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - mof_classes - label 9: 0.035714  20 / 560
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - mof_classes - label 45: 0.409698  15437 / 37679
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - mof_classes - label 46: 0.608131  25415 / 41792
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - mof_classes - label 47: 0.000000  0 / 1045
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - mof_classes - average class mof: 0.195583
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - iou_classes - label 6: 0.003908  64 / 16376
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - iou_classes - label 7: 0.062293  1959 / 31448
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - iou_classes - label 9: 0.005963  20 / 3354
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - iou_classes - label 45: 0.221021  15437 / 69844
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - iou_classes - label 46: 0.416516  25415 / 61018
2019-01-08 10:29:55,096 - 10 - accuracy_class.py - iou_classes - label 47: 0.000000  0 / 3372
2019-01-08 10:29:55,097 - 10 - accuracy_class.py - iou_classes - average IoU: 0.118284
2019-01-08 10:29:55,097 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.101386
2019-01-08 10:29:56,635 - 10 - f1_score.py - f1 - f1 score: 0.241824
2019-01-08 10:29:56,639 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1585.690 ms ~ 0.026 min ~ 1.586 sec
2019-01-08 10:29:56,639 - 10 - corpus.py - embedding_training - .
2019-01-08 10:29:56,639 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:29:56,639 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:29:56,639 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:29:57,485 - 10 - training_embed.py - training - create model
2019-01-08 10:29:57,486 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:29:57,486 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:29:57,807 - 10 - training_embed.py - training - Epoch: [0][100/515]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 255.2222 (255.0488)	
2019-01-08 10:29:58,018 - 10 - training_embed.py - training - Epoch: [0][200/515]	Time 0.004 (0.003)	Data 0.003 (0.002)	Loss 254.3568 (255.1083)	
2019-01-08 10:29:58,233 - 10 - training_embed.py - training - Epoch: [0][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.0857 (254.9848)	
2019-01-08 10:29:58,451 - 10 - training_embed.py - training - Epoch: [0][400/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 254.2242 (255.0289)	
2019-01-08 10:29:58,654 - 10 - training_embed.py - training - Epoch: [0][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.2977 (255.0390)	
2019-01-08 10:29:58,687 - 10 - training_embed.py - training - loss: 254.965306
2019-01-08 10:29:58,687 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:29:59,070 - 10 - training_embed.py - training - Epoch: [1][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.3138 (255.1151)	
2019-01-08 10:29:59,306 - 10 - training_embed.py - training - Epoch: [1][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.1762 (255.0680)	
2019-01-08 10:29:59,527 - 10 - training_embed.py - training - Epoch: [1][300/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 255.0546 (255.0370)	
2019-01-08 10:29:59,745 - 10 - training_embed.py - training - Epoch: [1][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 255.7021 (255.0297)	
2019-01-08 10:29:59,977 - 10 - training_embed.py - training - Epoch: [1][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 255.2664 (254.9738)	
2019-01-08 10:30:00,008 - 10 - training_embed.py - training - loss: 254.883489
2019-01-08 10:30:00,008 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:30:00,381 - 10 - training_embed.py - training - Epoch: [2][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.0156 (255.1513)	
2019-01-08 10:30:00,598 - 10 - training_embed.py - training - Epoch: [2][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.4569 (254.9628)	
2019-01-08 10:30:00,810 - 10 - training_embed.py - training - Epoch: [2][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.9881 (254.9222)	
2019-01-08 10:30:01,033 - 10 - training_embed.py - training - Epoch: [2][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.8562 (254.9326)	
2019-01-08 10:30:01,259 - 10 - training_embed.py - training - Epoch: [2][500/515]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 257.4854 (254.9048)	
2019-01-08 10:30:01,291 - 10 - training_embed.py - training - loss: 254.803736
2019-01-08 10:30:01,291 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:30:01,670 - 10 - training_embed.py - training - Epoch: [3][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.0676 (254.7612)	
2019-01-08 10:30:01,908 - 10 - training_embed.py - training - Epoch: [3][200/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 254.6833 (254.7525)	
2019-01-08 10:30:02,142 - 10 - training_embed.py - training - Epoch: [3][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.2627 (254.7844)	
2019-01-08 10:30:02,350 - 10 - training_embed.py - training - Epoch: [3][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.1349 (254.8248)	
2019-01-08 10:30:02,571 - 10 - training_embed.py - training - Epoch: [3][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 251.5098 (254.8101)	
2019-01-08 10:30:02,600 - 10 - training_embed.py - training - loss: 254.721743
2019-01-08 10:30:02,600 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:30:02,951 - 10 - training_embed.py - training - Epoch: [4][100/515]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 255.3623 (254.8678)	
2019-01-08 10:30:03,168 - 10 - training_embed.py - training - Epoch: [4][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.2126 (254.8289)	
2019-01-08 10:30:03,391 - 10 - training_embed.py - training - Epoch: [4][300/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 255.2005 (254.7881)	
2019-01-08 10:30:03,621 - 10 - training_embed.py - training - Epoch: [4][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.8474 (254.7819)	
2019-01-08 10:30:03,906 - 10 - training_embed.py - training - Epoch: [4][500/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 256.5897 (254.7314)	
2019-01-08 10:30:03,943 - 10 - training_embed.py - training - loss: 254.641639
2019-01-08 10:30:03,944 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:30:04,446 - 10 - training_embed.py - training - Epoch: [5][100/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 258.1627 (254.6814)	
2019-01-08 10:30:04,688 - 10 - training_embed.py - training - Epoch: [5][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.1296 (254.7133)	
2019-01-08 10:30:04,906 - 10 - training_embed.py - training - Epoch: [5][300/515]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 255.0188 (254.6681)	
2019-01-08 10:30:05,123 - 10 - training_embed.py - training - Epoch: [5][400/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 254.7940 (254.6367)	
2019-01-08 10:30:05,354 - 10 - training_embed.py - training - Epoch: [5][500/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.9129 (254.6333)	
2019-01-08 10:30:05,383 - 10 - training_embed.py - training - loss: 254.558783
2019-01-08 10:30:05,383 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:30:05,740 - 10 - training_embed.py - training - Epoch: [6][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 257.7611 (254.6111)	
2019-01-08 10:30:05,940 - 10 - training_embed.py - training - Epoch: [6][200/515]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 252.1965 (254.5808)	
2019-01-08 10:30:06,147 - 10 - training_embed.py - training - Epoch: [6][300/515]	Time 0.004 (0.003)	Data 0.001 (0.001)	Loss 255.5700 (254.5927)	
2019-01-08 10:30:06,384 - 10 - training_embed.py - training - Epoch: [6][400/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 254.4300 (254.5718)	
2019-01-08 10:30:06,618 - 10 - training_embed.py - training - Epoch: [6][500/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 253.0624 (254.5620)	
2019-01-08 10:30:06,652 - 10 - training_embed.py - training - loss: 254.479498
2019-01-08 10:30:06,652 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:30:07,134 - 10 - training_embed.py - training - Epoch: [7][100/515]	Time 0.006 (0.003)	Data 0.002 (0.001)	Loss 254.8505 (254.5130)	
2019-01-08 10:30:07,394 - 10 - training_embed.py - training - Epoch: [7][200/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 255.0264 (254.6083)	
2019-01-08 10:30:07,666 - 10 - training_embed.py - training - Epoch: [7][300/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 255.0136 (254.5896)	
2019-01-08 10:30:07,905 - 10 - training_embed.py - training - Epoch: [7][400/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 254.9757 (254.5224)	
2019-01-08 10:30:08,143 - 10 - training_embed.py - training - Epoch: [7][500/515]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 255.3885 (254.4837)	
2019-01-08 10:30:08,170 - 10 - training_embed.py - training - loss: 254.399676
2019-01-08 10:30:08,170 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:30:08,571 - 10 - training_embed.py - training - Epoch: [8][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.5002 (254.4195)	
2019-01-08 10:30:08,790 - 10 - training_embed.py - training - Epoch: [8][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.0137 (254.3665)	
2019-01-08 10:30:09,003 - 10 - training_embed.py - training - Epoch: [8][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.8254 (254.3885)	
2019-01-08 10:30:09,216 - 10 - training_embed.py - training - Epoch: [8][400/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 254.1330 (254.4297)	
2019-01-08 10:30:09,441 - 10 - training_embed.py - training - Epoch: [8][500/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.2753 (254.4146)	
2019-01-08 10:30:09,473 - 10 - training_embed.py - training - loss: 254.317326
2019-01-08 10:30:09,473 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:30:09,835 - 10 - training_embed.py - training - Epoch: [9][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.1901 (254.2571)	
2019-01-08 10:30:10,057 - 10 - training_embed.py - training - Epoch: [9][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.0700 (254.2522)	
2019-01-08 10:30:10,262 - 10 - training_embed.py - training - Epoch: [9][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.2782 (254.2982)	
2019-01-08 10:30:10,476 - 10 - training_embed.py - training - Epoch: [9][400/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 252.3147 (254.3297)	
2019-01-08 10:30:10,713 - 10 - training_embed.py - training - Epoch: [9][500/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 254.4747 (254.3310)	
2019-01-08 10:30:10,740 - 10 - training_embed.py - training - loss: 254.235511
2019-01-08 10:30:10,740 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:30:11,109 - 10 - training_embed.py - training - Epoch: [10][100/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 253.2747 (254.2557)	
2019-01-08 10:30:11,333 - 10 - training_embed.py - training - Epoch: [10][200/515]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 255.9433 (254.1840)	
2019-01-08 10:30:11,553 - 10 - training_embed.py - training - Epoch: [10][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 257.0234 (254.2466)	
2019-01-08 10:30:11,769 - 10 - training_embed.py - training - Epoch: [10][400/515]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 254.2345 (254.2203)	
2019-01-08 10:30:11,986 - 10 - training_embed.py - training - Epoch: [10][500/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.1417 (254.2399)	
2019-01-08 10:30:12,015 - 10 - training_embed.py - training - loss: 254.154930
2019-01-08 10:30:12,015 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:30:12,379 - 10 - training_embed.py - training - Epoch: [11][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.6050 (254.1197)	
2019-01-08 10:30:12,591 - 10 - training_embed.py - training - Epoch: [11][200/515]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 251.6257 (254.1775)	
2019-01-08 10:30:12,803 - 10 - training_embed.py - training - Epoch: [11][300/515]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 253.9431 (254.1254)	
2019-01-08 10:30:13,027 - 10 - training_embed.py - training - Epoch: [11][400/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.1867 (254.1461)	
2019-01-08 10:30:13,244 - 10 - training_embed.py - training - Epoch: [11][500/515]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 254.7276 (254.1566)	
2019-01-08 10:30:13,273 - 10 - training_embed.py - training - loss: 254.074406
2019-01-08 10:30:13,296 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:30:13,534 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 237.478 ms ~ 0.004 min ~ 0.237 sec
2019-01-08 10:30:13,800 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 503.839 ms ~ 0.008 min ~ 0.504 sec
2019-01-08 10:30:13,800 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:30:13,801 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:30:13,801 - 10 - corpus.py - subactivity_sampler - [47602.  2327.  7341. 44641.  2814. 27057.]
2019-01-08 10:30:17,627 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:30:17,628 - 10 - corpus.py - subactivity_sampler - [47569.  2327.  7124. 44927.  2799. 27036.]
2019-01-08 10:30:22,193 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:30:22,193 - 10 - corpus.py - subactivity_sampler - [47532.  2321.  7042. 45192.  2774. 26921.]
2019-01-08 10:30:26,621 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:30:26,622 - 10 - corpus.py - subactivity_sampler - [47542.  2315.  6770. 45566.  2747. 26842.]
2019-01-08 10:30:33,019 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:30:33,019 - 10 - corpus.py - subactivity_sampler - [47479.  2269.  6494. 45952.  2723. 26865.]
2019-01-08 10:30:37,050 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:30:37,050 - 10 - corpus.py - subactivity_sampler - [47504.  2262.  6432. 46005.  2719. 26860.]
2019-01-08 10:30:41,735 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:30:41,735 - 10 - corpus.py - subactivity_sampler - [47474.  2253.  6296. 46353.  2732. 26674.]
2019-01-08 10:30:46,315 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:30:46,315 - 10 - corpus.py - subactivity_sampler - [47463.  2250.  6271. 46434.  2704. 26660.]
2019-01-08 10:30:51,264 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:30:51,265 - 10 - corpus.py - subactivity_sampler - [47615.  2247.  6105. 46418.  2690. 26707.]
2019-01-08 10:30:55,051 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:30:55,051 - 10 - corpus.py - subactivity_sampler - [47725.  2247.  5994. 46448.  2692. 26676.]
2019-01-08 10:30:56,047 - 10 - corpus.py - subactivity_sampler - [47725.  2245.  5996. 46445.  2692. 26679.]
2019-01-08 10:30:56,048 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 42247.610 ms ~ 0.704 min ~ 42.248 sec
2019-01-08 10:30:56,048 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:30:58,001 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:30:58,001 - 10 - corpus.py - ordering_sampler - inv_count_vec: [0. 1. 0. 2. 6.]
2019-01-08 10:30:58,001 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:30:58,026 - 10 - corpus.py - rho_sampling - ['62.6627', '39.4200', '51.9862', '283.8570', '3.0863']
2019-01-08 10:30:58,026 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 10:30:58,062 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:30:58,065 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:30:58,065 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 45', '1: 47', '2: 6', '3: 46', '4: 9', '5: 7']
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - mof_val - frames true: 43881	frames overall : 131782
2019-01-08 10:30:58,069 - 10 - corpus.py - accuracy_corpus - Action: tea
2019-01-08 10:30:58,069 - 10 - corpus.py - accuracy_corpus - MoF val: 0.33298174257485846
2019-01-08 10:30:58,069 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.33298174257485846
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - mof_classes - label 6: 0.007254  66 / 9099
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - mof_classes - label 7: 0.313701  1992 / 6350
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - mof_classes - label 9: 0.044643  25 / 560
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - mof_classes - label 45: 0.416917  15709 / 37679
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - mof_classes - label 46: 0.624258  26089 / 41792
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - mof_classes - label 47: 0.000000  0 / 1045
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - mof_classes - average class mof: 0.200967
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - iou_classes - label 6: 0.004392  66 / 15029
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - iou_classes - label 7: 0.064181  1992 / 31037
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - iou_classes - label 9: 0.007747  25 / 3227
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - iou_classes - label 45: 0.225396  15709 / 69695
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - iou_classes - label 46: 0.419788  26089 / 62148
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - iou_classes - label 47: 0.000000  0 / 3290
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - iou_classes - average IoU: 0.120251
2019-01-08 10:30:58,069 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.103072
2019-01-08 10:30:59,608 - 10 - f1_score.py - f1 - f1 score: 0.247067
2019-01-08 10:30:59,613 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1586.701 ms ~ 0.026 min ~ 1.587 sec
2019-01-08 10:30:59,613 - 10 - corpus.py - embedding_training - .
2019-01-08 10:30:59,613 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:30:59,613 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:30:59,613 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:31:00,441 - 10 - training_embed.py - training - create model
2019-01-08 10:31:00,442 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:31:00,442 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:31:00,784 - 10 - training_embed.py - training - Epoch: [0][100/515]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 255.0863 (255.1749)	
2019-01-08 10:31:00,995 - 10 - training_embed.py - training - Epoch: [0][200/515]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 254.7084 (255.2362)	
2019-01-08 10:31:01,212 - 10 - training_embed.py - training - Epoch: [0][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 257.8203 (255.1001)	
2019-01-08 10:31:01,434 - 10 - training_embed.py - training - Epoch: [0][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.4394 (255.1537)	
2019-01-08 10:31:01,660 - 10 - training_embed.py - training - Epoch: [0][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.5522 (255.1639)	
2019-01-08 10:31:01,689 - 10 - training_embed.py - training - loss: 255.087985
2019-01-08 10:31:01,689 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:31:02,059 - 10 - training_embed.py - training - Epoch: [1][100/515]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 255.7714 (255.2418)	
2019-01-08 10:31:02,272 - 10 - training_embed.py - training - Epoch: [1][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.7068 (255.2195)	
2019-01-08 10:31:02,486 - 10 - training_embed.py - training - Epoch: [1][300/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.6553 (255.1719)	
2019-01-08 10:31:02,702 - 10 - training_embed.py - training - Epoch: [1][400/515]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 255.6255 (255.1477)	
2019-01-08 10:31:02,912 - 10 - training_embed.py - training - Epoch: [1][500/515]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 256.0120 (255.0890)	
2019-01-08 10:31:02,940 - 10 - training_embed.py - training - loss: 254.998588
2019-01-08 10:31:02,940 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:31:03,319 - 10 - training_embed.py - training - Epoch: [2][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.2064 (255.2461)	
2019-01-08 10:31:03,543 - 10 - training_embed.py - training - Epoch: [2][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.1825 (255.0713)	
2019-01-08 10:31:03,750 - 10 - training_embed.py - training - Epoch: [2][300/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 256.0185 (255.0219)	
2019-01-08 10:31:03,958 - 10 - training_embed.py - training - Epoch: [2][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 253.9496 (255.0384)	
2019-01-08 10:31:04,171 - 10 - training_embed.py - training - Epoch: [2][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 257.2122 (255.0082)	
2019-01-08 10:31:04,200 - 10 - training_embed.py - training - loss: 254.911545
2019-01-08 10:31:04,200 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:31:04,589 - 10 - training_embed.py - training - Epoch: [3][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 253.1521 (254.7738)	
2019-01-08 10:31:04,792 - 10 - training_embed.py - training - Epoch: [3][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.1762 (254.8082)	
2019-01-08 10:31:05,022 - 10 - training_embed.py - training - Epoch: [3][300/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 257.6349 (254.8836)	
2019-01-08 10:31:05,241 - 10 - training_embed.py - training - Epoch: [3][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 257.9919 (254.9333)	
2019-01-08 10:31:05,464 - 10 - training_embed.py - training - Epoch: [3][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 251.2710 (254.9079)	
2019-01-08 10:31:05,493 - 10 - training_embed.py - training - loss: 254.822359
2019-01-08 10:31:05,494 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:31:05,879 - 10 - training_embed.py - training - Epoch: [4][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.6396 (254.9303)	
2019-01-08 10:31:06,095 - 10 - training_embed.py - training - Epoch: [4][200/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 253.6363 (254.9266)	
2019-01-08 10:31:06,326 - 10 - training_embed.py - training - Epoch: [4][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.2275 (254.9003)	
2019-01-08 10:31:06,550 - 10 - training_embed.py - training - Epoch: [4][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 255.9563 (254.8823)	
2019-01-08 10:31:06,792 - 10 - training_embed.py - training - Epoch: [4][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.3882 (254.8248)	
2019-01-08 10:31:06,827 - 10 - training_embed.py - training - loss: 254.734766
2019-01-08 10:31:06,827 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:31:07,177 - 10 - training_embed.py - training - Epoch: [5][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 258.5828 (254.7252)	
2019-01-08 10:31:07,394 - 10 - training_embed.py - training - Epoch: [5][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 256.0305 (254.7914)	
2019-01-08 10:31:07,596 - 10 - training_embed.py - training - Epoch: [5][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.6153 (254.7519)	
2019-01-08 10:31:07,815 - 10 - training_embed.py - training - Epoch: [5][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.2020 (254.7102)	
2019-01-08 10:31:08,045 - 10 - training_embed.py - training - Epoch: [5][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 256.5023 (254.7113)	
2019-01-08 10:31:08,078 - 10 - training_embed.py - training - loss: 254.644556
2019-01-08 10:31:08,078 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:31:08,448 - 10 - training_embed.py - training - Epoch: [6][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 258.0225 (254.7021)	
2019-01-08 10:31:08,672 - 10 - training_embed.py - training - Epoch: [6][200/515]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 252.3599 (254.6752)	
2019-01-08 10:31:08,876 - 10 - training_embed.py - training - Epoch: [6][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.3242 (254.6515)	
2019-01-08 10:31:09,104 - 10 - training_embed.py - training - Epoch: [6][400/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 253.5523 (254.6499)	
2019-01-08 10:31:09,319 - 10 - training_embed.py - training - Epoch: [6][500/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 253.0133 (254.6389)	
2019-01-08 10:31:09,350 - 10 - training_embed.py - training - loss: 254.557758
2019-01-08 10:31:09,350 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:31:09,751 - 10 - training_embed.py - training - Epoch: [7][100/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 255.3719 (254.6157)	
2019-01-08 10:31:09,975 - 10 - training_embed.py - training - Epoch: [7][200/515]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 254.9316 (254.6671)	
2019-01-08 10:31:10,203 - 10 - training_embed.py - training - Epoch: [7][300/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.9491 (254.6521)	
2019-01-08 10:31:10,417 - 10 - training_embed.py - training - Epoch: [7][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.8886 (254.5917)	
2019-01-08 10:31:10,624 - 10 - training_embed.py - training - Epoch: [7][500/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 256.0898 (254.5559)	
2019-01-08 10:31:10,653 - 10 - training_embed.py - training - loss: 254.470542
2019-01-08 10:31:10,654 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:31:11,009 - 10 - training_embed.py - training - Epoch: [8][100/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 253.6946 (254.4566)	
2019-01-08 10:31:11,224 - 10 - training_embed.py - training - Epoch: [8][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.3582 (254.4347)	
2019-01-08 10:31:11,457 - 10 - training_embed.py - training - Epoch: [8][300/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 254.3329 (254.4379)	
2019-01-08 10:31:11,672 - 10 - training_embed.py - training - Epoch: [8][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.9671 (254.4928)	
2019-01-08 10:31:11,892 - 10 - training_embed.py - training - Epoch: [8][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.9421 (254.4799)	
2019-01-08 10:31:11,920 - 10 - training_embed.py - training - loss: 254.381123
2019-01-08 10:31:11,920 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:31:12,283 - 10 - training_embed.py - training - Epoch: [9][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.3705 (254.3014)	
2019-01-08 10:31:12,486 - 10 - training_embed.py - training - Epoch: [9][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.1611 (254.2881)	
2019-01-08 10:31:12,705 - 10 - training_embed.py - training - Epoch: [9][300/515]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 254.9049 (254.3435)	
2019-01-08 10:31:12,924 - 10 - training_embed.py - training - Epoch: [9][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.7125 (254.3590)	
2019-01-08 10:31:13,167 - 10 - training_embed.py - training - Epoch: [9][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.6228 (254.3858)	
2019-01-08 10:31:13,197 - 10 - training_embed.py - training - loss: 254.292152
2019-01-08 10:31:13,198 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:31:13,563 - 10 - training_embed.py - training - Epoch: [10][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.9271 (254.3713)	
2019-01-08 10:31:13,771 - 10 - training_embed.py - training - Epoch: [10][200/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 256.1398 (254.2878)	
2019-01-08 10:31:13,981 - 10 - training_embed.py - training - Epoch: [10][300/515]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 256.9984 (254.3430)	
2019-01-08 10:31:14,212 - 10 - training_embed.py - training - Epoch: [10][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 254.0028 (254.3022)	
2019-01-08 10:31:14,461 - 10 - training_embed.py - training - Epoch: [10][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 253.6966 (254.2901)	
2019-01-08 10:31:14,493 - 10 - training_embed.py - training - loss: 254.204049
2019-01-08 10:31:14,493 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:31:14,840 - 10 - training_embed.py - training - Epoch: [11][100/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.1015 (254.2078)	
2019-01-08 10:31:15,057 - 10 - training_embed.py - training - Epoch: [11][200/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 252.2027 (254.2478)	
2019-01-08 10:31:15,281 - 10 - training_embed.py - training - Epoch: [11][300/515]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 253.7391 (254.1743)	
2019-01-08 10:31:15,498 - 10 - training_embed.py - training - Epoch: [11][400/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 256.0865 (254.1970)	
2019-01-08 10:31:15,715 - 10 - training_embed.py - training - Epoch: [11][500/515]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 255.0230 (254.2037)	
2019-01-08 10:31:15,743 - 10 - training_embed.py - training - loss: 254.116243
2019-01-08 10:31:15,765 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:31:15,994 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 228.684 ms ~ 0.004 min ~ 0.229 sec
2019-01-08 10:31:16,261 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 496.236 ms ~ 0.008 min ~ 0.496 sec
2019-01-08 10:31:16,261 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:31:16,262 - 10 - corpus.py - subactivity_sampler - 0 / 184
2019-01-08 10:31:16,262 - 10 - corpus.py - subactivity_sampler - [47725.  2245.  5996. 46445.  2692. 26679.]
2019-01-08 10:31:20,116 - 10 - corpus.py - subactivity_sampler - 20 / 184
2019-01-08 10:31:20,116 - 10 - corpus.py - subactivity_sampler - [47725.  2257.  5968. 46494.  2690. 26648.]
2019-01-08 10:31:24,960 - 10 - corpus.py - subactivity_sampler - 40 / 184
2019-01-08 10:31:24,960 - 10 - corpus.py - subactivity_sampler - [47678.  2255.  5933. 46641.  2678. 26597.]
2019-01-08 10:31:29,738 - 10 - corpus.py - subactivity_sampler - 60 / 184
2019-01-08 10:31:29,738 - 10 - corpus.py - subactivity_sampler - [47643.  2253.  5805. 46703.  2672. 26706.]
2019-01-08 10:31:36,112 - 10 - corpus.py - subactivity_sampler - 80 / 184
2019-01-08 10:31:36,112 - 10 - corpus.py - subactivity_sampler - [47668.  2187.  5645. 46994.  2662. 26626.]
2019-01-08 10:31:39,955 - 10 - corpus.py - subactivity_sampler - 100 / 184
2019-01-08 10:31:39,955 - 10 - corpus.py - subactivity_sampler - [47558.  2186.  5624. 47125.  2661. 26628.]
2019-01-08 10:31:44,483 - 10 - corpus.py - subactivity_sampler - 120 / 184
2019-01-08 10:31:44,483 - 10 - corpus.py - subactivity_sampler - [47545.  2149.  5561. 47270.  2681. 26576.]
2019-01-08 10:31:48,979 - 10 - corpus.py - subactivity_sampler - 140 / 184
2019-01-08 10:31:48,979 - 10 - corpus.py - subactivity_sampler - [47544.  2149.  5543. 47299.  2665. 26582.]
2019-01-08 10:31:53,939 - 10 - corpus.py - subactivity_sampler - 160 / 184
2019-01-08 10:31:53,939 - 10 - corpus.py - subactivity_sampler - [47570.  2154.  5531. 47259.  2659. 26609.]
2019-01-08 10:31:57,717 - 10 - corpus.py - subactivity_sampler - 180 / 184
2019-01-08 10:31:57,717 - 10 - corpus.py - subactivity_sampler - [47528.  2151.  5480. 47277.  2660. 26686.]
2019-01-08 10:31:58,750 - 10 - corpus.py - subactivity_sampler - [47543.  2150.  5466. 47280.  2660. 26683.]
2019-01-08 10:31:58,750 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 42488.676 ms ~ 0.708 min ~ 42.489 sec
2019-01-08 10:31:58,750 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:32:00,675 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:32:00,675 - 10 - corpus.py - ordering_sampler - inv_count_vec: [0. 0. 1. 0. 6.]
2019-01-08 10:32:00,675 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:32:00,693 - 10 - corpus.py - rho_sampling - ['63.3880', '31.4757', '923.3660', '91.5571', '6.9304']
2019-01-08 10:32:00,693 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 10:32:00,729 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:32:00,731 - 10 - accuracy_class.py - mof - # gt_labels: 7   # pr_labels: 6
2019-01-08 10:32:00,732 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 45', '1: 47', '2: 6', '3: 46', '4: 9', '5: 7']
2019-01-08 10:32:00,736 - 10 - accuracy_class.py - mof_val - frames true: 44273	frames overall : 131782
2019-01-08 10:32:00,736 - 10 - corpus.py - accuracy_corpus - Action: tea
2019-01-08 10:32:00,736 - 10 - corpus.py - accuracy_corpus - MoF val: 0.33595635215735076
2019-01-08 10:32:00,736 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.33595635215735076
2019-01-08 10:32:00,736 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:32:00,736 - 10 - accuracy_class.py - mof_classes - label 6: 0.005715  52 / 9099
2019-01-08 10:32:00,736 - 10 - accuracy_class.py - mof_classes - label 7: 0.318740  2024 / 6350
2019-01-08 10:32:00,736 - 10 - accuracy_class.py - mof_classes - label 9: 0.046429  26 / 560
2019-01-08 10:32:00,736 - 10 - accuracy_class.py - mof_classes - label 45: 0.414342  15612 / 37679
2019-01-08 10:32:00,736 - 10 - accuracy_class.py - mof_classes - label 46: 0.635504  26559 / 41792
2019-01-08 10:32:00,736 - 10 - accuracy_class.py - mof_classes - label 47: 0.000000  0 / 1045
2019-01-08 10:32:00,736 - 10 - accuracy_class.py - mof_classes - average class mof: 0.202961
2019-01-08 10:32:00,736 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35257
2019-01-08 10:32:00,736 - 10 - accuracy_class.py - iou_classes - label 6: 0.003583  52 / 14513
2019-01-08 10:32:00,737 - 10 - accuracy_class.py - iou_classes - label 7: 0.065271  2024 / 31009
2019-01-08 10:32:00,737 - 10 - accuracy_class.py - iou_classes - label 9: 0.008140  26 / 3194
2019-01-08 10:32:00,737 - 10 - accuracy_class.py - iou_classes - label 45: 0.224278  15612 / 69610
2019-01-08 10:32:00,737 - 10 - accuracy_class.py - iou_classes - label 46: 0.424856  26559 / 62513
2019-01-08 10:32:00,737 - 10 - accuracy_class.py - iou_classes - label 47: 0.000000  0 / 3195
2019-01-08 10:32:00,737 - 10 - accuracy_class.py - iou_classes - average IoU: 0.121021
2019-01-08 10:32:00,737 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.103733
2019-01-08 10:32:02,328 - 10 - f1_score.py - f1 - f1 score: 0.249514
2019-01-08 10:32:02,332 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 1638.875 ms ~ 0.027 min ~ 1.639 sec
2019-01-08 10:32:02,339 - 10 - utils.py - wrap - <function baseline at 0x7f3991d24e18> took 437199.939 ms ~ 7.287 min ~ 437.200 sec
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - batch_size: 256
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - bg: False
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - bg_trh: 85
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - data: /media/data/kukleva/lab/Breakfast/feat/s1
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - data_type: 2
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - dataset: bf
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - dataset_root: /media/data/kukleva/lab/Breakfast
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - embed_dim: 30
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - epochs: 12
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - ext: txt
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - feature_dim: 64
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - full: True
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - gmm: 1
2019-01-08 10:32:02,340 - 10 - utils.py - update_opt_str - gmms: one
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - gt: /media/data/kukleva/lab/Breakfast/feat/s1/groundTruth
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - gt_training: False
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - log: DEBUG
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - log_str: slim.mallow._milk_!bg_data2_bf_embed30_epochs12_full_gmm1_one_!gt_lr1e-10_!lr_ordering_reg0.1_!viterbi_!zeros_
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - lr: 1e-10
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - lr_adj: False
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - momentum: 0.9
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - num_workers: 4
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - ordering: True
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - prefix: slim.mallow.
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - reg_cov: 0.1
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - resume: False
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - resume_str: 
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - save_embed_feat: False
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - save_likelihood: False
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - save_model: False
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - seed: 0
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - subaction: milk
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - vis: False
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - viterbi: False
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - weight_decay: 0.0001
2019-01-08 10:32:02,341 - 10 - utils.py - update_opt_str - zeros: False
2019-01-08 10:32:02,386 - 10 - utils.py - wrap - <function GroundTruth.load_gt at 0x7f39322b36a8> took 45.031 ms ~ 0.001 min ~ 0.045 sec
2019-01-08 10:32:02,473 - 10 - corpus.py - __init__ - milk  subactions: 4
2019-01-08 10:32:02,474 - 10 - corpus.py - _init_videos - .
2019-01-08 10:32:09,326 - 10 - corpus.py - _init_videos - gt statistic: Counter({24: 68258, 25: 50965, 2: 49840, 6: 8324})
2019-01-08 10:32:09,326 - 10 - corpus.py - _update_fg_mask - .
2019-01-08 10:32:09,340 - 10 - corpus.py - __init__ - min: -46.482628  max: 34.366859  avg: 0.037216
2019-01-08 10:32:09,340 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:32:09,361 - 10 - corpus.py - rho_sampling - ['64.1837', '100.9221', '929.0535']
2019-01-08 10:32:09,361 - 10 - pipeline.py - baseline - Iteration 0
2019-01-08 10:32:09,412 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:32:09,415 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:32:09,415 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 24', '2: 2', '3: 25']
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - mof_val - frames true: 77647	frames overall : 177387
2019-01-08 10:32:09,419 - 10 - corpus.py - accuracy_corpus - Action: milk
2019-01-08 10:32:09,419 - 10 - corpus.py - accuracy_corpus - MoF val: 0.43772655267860666
2019-01-08 10:32:09,419 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.0
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - mof_classes - label 2: 0.396537  17590 / 44359
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - mof_classes - label 6: 0.997537  6076 / 6091
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - mof_classes - label 24: 0.489589  28544 / 58302
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - mof_classes - label 25: 0.623517  25437 / 40796
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - mof_classes - average class mof: 0.501436
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - iou_classes - label 2: 0.247391  17590 / 71102
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - iou_classes - label 6: 0.136758  6076 / 44429
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - iou_classes - label 24: 0.385126  28544 / 74116
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - iou_classes - label 25: 0.426502  25437 / 59641
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - iou_classes - average IoU: 0.298944
2019-01-08 10:32:09,419 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.239155
2019-01-08 10:32:11,990 - 10 - f1_score.py - f1 - f1 score: 0.456307
2019-01-08 10:32:11,998 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 2637.528 ms ~ 0.044 min ~ 2.638 sec
2019-01-08 10:32:11,999 - 10 - corpus.py - embedding_training - .
2019-01-08 10:32:11,999 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:32:11,999 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:32:11,999 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:32:13,178 - 10 - training_embed.py - training - create model
2019-01-08 10:32:13,178 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:32:13,179 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:32:13,531 - 10 - training_embed.py - training - Epoch: [0][100/693]	Time 0.002 (0.003)	Data 0.000 (0.002)	Loss 154.0361 (154.9469)	
2019-01-08 10:32:13,759 - 10 - training_embed.py - training - Epoch: [0][200/693]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 155.5785 (154.9218)	
2019-01-08 10:32:13,957 - 10 - training_embed.py - training - Epoch: [0][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.0558 (154.8469)	
2019-01-08 10:32:14,165 - 10 - training_embed.py - training - Epoch: [0][400/693]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.8572 (154.8701)	
2019-01-08 10:32:14,371 - 10 - training_embed.py - training - Epoch: [0][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.4025 (154.8428)	
2019-01-08 10:32:14,593 - 10 - training_embed.py - training - Epoch: [0][600/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.0539 (154.8369)	
2019-01-08 10:32:14,811 - 10 - training_embed.py - training - loss: 154.816432
2019-01-08 10:32:14,811 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:32:15,176 - 10 - training_embed.py - training - Epoch: [1][100/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.8917 (154.6923)	
2019-01-08 10:32:15,397 - 10 - training_embed.py - training - Epoch: [1][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.4433 (154.6972)	
2019-01-08 10:32:15,631 - 10 - training_embed.py - training - Epoch: [1][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.6192 (154.7523)	
2019-01-08 10:32:15,850 - 10 - training_embed.py - training - Epoch: [1][400/693]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 157.1646 (154.7620)	
2019-01-08 10:32:16,060 - 10 - training_embed.py - training - Epoch: [1][500/693]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 154.6660 (154.8051)	
2019-01-08 10:32:16,293 - 10 - training_embed.py - training - Epoch: [1][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.1458 (154.8096)	
2019-01-08 10:32:16,484 - 10 - training_embed.py - training - loss: 154.804716
2019-01-08 10:32:16,485 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:32:16,859 - 10 - training_embed.py - training - Epoch: [2][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.6694 (154.8824)	
2019-01-08 10:32:17,063 - 10 - training_embed.py - training - Epoch: [2][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.8736 (154.8285)	
2019-01-08 10:32:17,297 - 10 - training_embed.py - training - Epoch: [2][300/693]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 153.1650 (154.8826)	
2019-01-08 10:32:17,503 - 10 - training_embed.py - training - Epoch: [2][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.2247 (154.8266)	
2019-01-08 10:32:17,720 - 10 - training_embed.py - training - Epoch: [2][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.2402 (154.8358)	
2019-01-08 10:32:17,945 - 10 - training_embed.py - training - Epoch: [2][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.1779 (154.8206)	
2019-01-08 10:32:18,153 - 10 - training_embed.py - training - loss: 154.793199
2019-01-08 10:32:18,153 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:32:18,526 - 10 - training_embed.py - training - Epoch: [3][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.1158 (154.7562)	
2019-01-08 10:32:18,728 - 10 - training_embed.py - training - Epoch: [3][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.2203 (154.7803)	
2019-01-08 10:32:18,947 - 10 - training_embed.py - training - Epoch: [3][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.0268 (154.7946)	
2019-01-08 10:32:19,178 - 10 - training_embed.py - training - Epoch: [3][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.5014 (154.8092)	
2019-01-08 10:32:19,380 - 10 - training_embed.py - training - Epoch: [3][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.3647 (154.8074)	
2019-01-08 10:32:19,594 - 10 - training_embed.py - training - Epoch: [3][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.6947 (154.7496)	
2019-01-08 10:32:19,780 - 10 - training_embed.py - training - loss: 154.781621
2019-01-08 10:32:19,781 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:32:20,142 - 10 - training_embed.py - training - Epoch: [4][100/693]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 157.0240 (154.7270)	
2019-01-08 10:32:20,348 - 10 - training_embed.py - training - Epoch: [4][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9345 (154.7874)	
2019-01-08 10:32:20,561 - 10 - training_embed.py - training - Epoch: [4][300/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.4920 (154.7652)	
2019-01-08 10:32:20,772 - 10 - training_embed.py - training - Epoch: [4][400/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.3555 (154.7396)	
2019-01-08 10:32:21,001 - 10 - training_embed.py - training - Epoch: [4][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.4614 (154.7404)	
2019-01-08 10:32:21,223 - 10 - training_embed.py - training - Epoch: [4][600/693]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.3313 (154.7513)	
2019-01-08 10:32:21,410 - 10 - training_embed.py - training - loss: 154.769863
2019-01-08 10:32:21,410 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:32:21,754 - 10 - training_embed.py - training - Epoch: [5][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.7182 (154.8290)	
2019-01-08 10:32:21,957 - 10 - training_embed.py - training - Epoch: [5][200/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.6624 (154.7473)	
2019-01-08 10:32:22,164 - 10 - training_embed.py - training - Epoch: [5][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.6872 (154.7477)	
2019-01-08 10:32:22,389 - 10 - training_embed.py - training - Epoch: [5][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.1981 (154.7574)	
2019-01-08 10:32:22,612 - 10 - training_embed.py - training - Epoch: [5][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.1685 (154.7982)	
2019-01-08 10:32:22,815 - 10 - training_embed.py - training - Epoch: [5][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.5683 (154.7943)	
2019-01-08 10:32:23,003 - 10 - training_embed.py - training - loss: 154.758368
2019-01-08 10:32:23,003 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:32:23,344 - 10 - training_embed.py - training - Epoch: [6][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8893 (154.7987)	
2019-01-08 10:32:23,548 - 10 - training_embed.py - training - Epoch: [6][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.5838 (154.7456)	
2019-01-08 10:32:23,764 - 10 - training_embed.py - training - Epoch: [6][300/693]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 153.5621 (154.7436)	
2019-01-08 10:32:23,974 - 10 - training_embed.py - training - Epoch: [6][400/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.0608 (154.7657)	
2019-01-08 10:32:24,180 - 10 - training_embed.py - training - Epoch: [6][500/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 156.0534 (154.7525)	
2019-01-08 10:32:24,395 - 10 - training_embed.py - training - Epoch: [6][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.5067 (154.7627)	
2019-01-08 10:32:24,592 - 10 - training_embed.py - training - loss: 154.746786
2019-01-08 10:32:24,592 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:32:24,968 - 10 - training_embed.py - training - Epoch: [7][100/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.9044 (154.7546)	
2019-01-08 10:32:25,167 - 10 - training_embed.py - training - Epoch: [7][200/693]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 155.0601 (154.6744)	
2019-01-08 10:32:25,369 - 10 - training_embed.py - training - Epoch: [7][300/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.5383 (154.7008)	
2019-01-08 10:32:25,586 - 10 - training_embed.py - training - Epoch: [7][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.3540 (154.6912)	
2019-01-08 10:32:25,791 - 10 - training_embed.py - training - Epoch: [7][500/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.4267 (154.7271)	
2019-01-08 10:32:26,023 - 10 - training_embed.py - training - Epoch: [7][600/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 156.7824 (154.7740)	
2019-01-08 10:32:26,207 - 10 - training_embed.py - training - loss: 154.735255
2019-01-08 10:32:26,207 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:32:26,577 - 10 - training_embed.py - training - Epoch: [8][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.3557 (154.7647)	
2019-01-08 10:32:26,798 - 10 - training_embed.py - training - Epoch: [8][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9803 (154.7259)	
2019-01-08 10:32:27,001 - 10 - training_embed.py - training - Epoch: [8][300/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 153.9381 (154.6988)	
2019-01-08 10:32:27,215 - 10 - training_embed.py - training - Epoch: [8][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.7367 (154.6711)	
2019-01-08 10:32:27,416 - 10 - training_embed.py - training - Epoch: [8][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.5994 (154.7195)	
2019-01-08 10:32:27,620 - 10 - training_embed.py - training - Epoch: [8][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.8014 (154.7350)	
2019-01-08 10:32:27,817 - 10 - training_embed.py - training - loss: 154.723767
2019-01-08 10:32:27,818 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:32:28,160 - 10 - training_embed.py - training - Epoch: [9][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.3833 (154.7414)	
2019-01-08 10:32:28,364 - 10 - training_embed.py - training - Epoch: [9][200/693]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 155.5391 (154.6864)	
2019-01-08 10:32:28,591 - 10 - training_embed.py - training - Epoch: [9][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.9908 (154.7675)	
2019-01-08 10:32:28,800 - 10 - training_embed.py - training - Epoch: [9][400/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.9980 (154.7471)	
2019-01-08 10:32:29,026 - 10 - training_embed.py - training - Epoch: [9][500/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 152.5238 (154.7217)	
2019-01-08 10:32:29,238 - 10 - training_embed.py - training - Epoch: [9][600/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.4985 (154.7250)	
2019-01-08 10:32:29,425 - 10 - training_embed.py - training - loss: 154.712201
2019-01-08 10:32:29,426 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:32:29,786 - 10 - training_embed.py - training - Epoch: [10][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9643 (154.8135)	
2019-01-08 10:32:29,996 - 10 - training_embed.py - training - Epoch: [10][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.2189 (154.7194)	
2019-01-08 10:32:30,204 - 10 - training_embed.py - training - Epoch: [10][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.5408 (154.7986)	
2019-01-08 10:32:30,415 - 10 - training_embed.py - training - Epoch: [10][400/693]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 154.9774 (154.7329)	
2019-01-08 10:32:30,632 - 10 - training_embed.py - training - Epoch: [10][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.6697 (154.7039)	
2019-01-08 10:32:30,859 - 10 - training_embed.py - training - Epoch: [10][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.7190 (154.7216)	
2019-01-08 10:32:31,062 - 10 - training_embed.py - training - loss: 154.700492
2019-01-08 10:32:31,062 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:32:31,461 - 10 - training_embed.py - training - Epoch: [11][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.6705 (154.7235)	
2019-01-08 10:32:31,676 - 10 - training_embed.py - training - Epoch: [11][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9946 (154.7274)	
2019-01-08 10:32:31,911 - 10 - training_embed.py - training - Epoch: [11][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.8633 (154.7186)	
2019-01-08 10:32:32,115 - 10 - training_embed.py - training - Epoch: [11][400/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 152.5122 (154.7281)	
2019-01-08 10:32:32,352 - 10 - training_embed.py - training - Epoch: [11][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.0677 (154.7028)	
2019-01-08 10:32:32,585 - 10 - training_embed.py - training - Epoch: [11][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.3513 (154.7183)	
2019-01-08 10:32:32,785 - 10 - training_embed.py - training - loss: 154.688909
2019-01-08 10:32:32,810 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:32:33,062 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 251.785 ms ~ 0.004 min ~ 0.252 sec
2019-01-08 10:32:33,290 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 480.171 ms ~ 0.008 min ~ 0.480 sec
2019-01-08 10:32:33,291 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:32:33,291 - 10 - corpus.py - subactivity_sampler - 0 / 187
2019-01-08 10:32:33,291 - 10 - corpus.py - subactivity_sampler - [44414. 44358. 44333. 44282.]
2019-01-08 10:32:38,065 - 10 - corpus.py - subactivity_sampler - 20 / 187
2019-01-08 10:32:38,065 - 10 - corpus.py - subactivity_sampler - [44514. 44967. 43894. 44012.]
2019-01-08 10:32:45,143 - 10 - corpus.py - subactivity_sampler - 40 / 187
2019-01-08 10:32:45,143 - 10 - corpus.py - subactivity_sampler - [44588. 45232. 43819. 43748.]
2019-01-08 10:32:50,024 - 10 - corpus.py - subactivity_sampler - 60 / 187
2019-01-08 10:32:50,024 - 10 - corpus.py - subactivity_sampler - [44521. 45932. 43276. 43658.]
2019-01-08 10:32:54,657 - 10 - corpus.py - subactivity_sampler - 80 / 187
2019-01-08 10:32:54,658 - 10 - corpus.py - subactivity_sampler - [44834. 46435. 42600. 43518.]
2019-01-08 10:33:00,409 - 10 - corpus.py - subactivity_sampler - 100 / 187
2019-01-08 10:33:00,409 - 10 - corpus.py - subactivity_sampler - [45778. 46399. 42202. 43008.]
2019-01-08 10:33:07,362 - 10 - corpus.py - subactivity_sampler - 120 / 187
2019-01-08 10:33:07,362 - 10 - corpus.py - subactivity_sampler - [46586. 46570. 41617. 42614.]
2019-01-08 10:33:13,984 - 10 - corpus.py - subactivity_sampler - 140 / 187
2019-01-08 10:33:13,985 - 10 - corpus.py - subactivity_sampler - [47415. 46424. 40972. 42576.]
2019-01-08 10:33:19,394 - 10 - corpus.py - subactivity_sampler - 160 / 187
2019-01-08 10:33:19,394 - 10 - corpus.py - subactivity_sampler - [47211. 47478. 40754. 41944.]
2019-01-08 10:33:24,172 - 10 - corpus.py - subactivity_sampler - 180 / 187
2019-01-08 10:33:24,173 - 10 - corpus.py - subactivity_sampler - [47349. 48752. 40265. 41021.]
2019-01-08 10:33:25,916 - 10 - corpus.py - subactivity_sampler - [47442. 48812. 40358. 40775.]
2019-01-08 10:33:25,916 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 52625.498 ms ~ 0.877 min ~ 52.625 sec
2019-01-08 10:33:25,916 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:33:26,511 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:33:26,511 - 10 - corpus.py - ordering_sampler - inv_count_vec: [2. 0. 0.]
2019-01-08 10:33:26,511 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:33:26,525 - 10 - corpus.py - rho_sampling - ['59.5468', '49.9638', '928.9172']
2019-01-08 10:33:26,525 - 10 - pipeline.py - baseline - Iteration 1
2019-01-08 10:33:26,574 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:33:26,577 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:33:26,577 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 24', '2: 2', '3: 25']
2019-01-08 10:33:26,582 - 10 - accuracy_class.py - mof_val - frames true: 78842	frames overall : 177387
2019-01-08 10:33:26,583 - 10 - corpus.py - accuracy_corpus - Action: milk
2019-01-08 10:33:26,583 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4444632357500832
2019-01-08 10:33:26,583 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4444632357500832
2019-01-08 10:33:26,583 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:33:26,583 - 10 - accuracy_class.py - mof_classes - label 2: 0.373836  16583 / 44359
2019-01-08 10:33:26,583 - 10 - accuracy_class.py - mof_classes - label 6: 1.000000  6091 / 6091
2019-01-08 10:33:26,583 - 10 - accuracy_class.py - mof_classes - label 24: 0.548300  31967 / 58302
2019-01-08 10:33:26,583 - 10 - accuracy_class.py - mof_classes - label 25: 0.593220  24201 / 40796
2019-01-08 10:33:26,583 - 10 - accuracy_class.py - mof_classes - average class mof: 0.503071
2019-01-08 10:33:26,583 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:33:26,583 - 10 - accuracy_class.py - iou_classes - label 2: 0.243388  16583 / 68134
2019-01-08 10:33:26,583 - 10 - accuracy_class.py - iou_classes - label 6: 0.128388  6091 / 47442
2019-01-08 10:33:26,583 - 10 - accuracy_class.py - iou_classes - label 24: 0.425393  31967 / 75147
2019-01-08 10:33:26,583 - 10 - accuracy_class.py - iou_classes - label 25: 0.421841  24201 / 57370
2019-01-08 10:33:26,583 - 10 - accuracy_class.py - iou_classes - average IoU: 0.304752
2019-01-08 10:33:26,583 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.243802
2019-01-08 10:33:28,988 - 10 - f1_score.py - f1 - f1 score: 0.450694
2019-01-08 10:33:28,999 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 2474.355 ms ~ 0.041 min ~ 2.474 sec
2019-01-08 10:33:28,999 - 10 - corpus.py - embedding_training - .
2019-01-08 10:33:28,999 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:33:29,000 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:33:29,000 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:33:30,435 - 10 - training_embed.py - training - create model
2019-01-08 10:33:30,436 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:33:30,436 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:33:30,760 - 10 - training_embed.py - training - Epoch: [0][100/693]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 154.6540 (155.3027)	
2019-01-08 10:33:30,974 - 10 - training_embed.py - training - Epoch: [0][200/693]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 156.0125 (155.3195)	
2019-01-08 10:33:31,208 - 10 - training_embed.py - training - Epoch: [0][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.2546 (155.2688)	
2019-01-08 10:33:31,413 - 10 - training_embed.py - training - Epoch: [0][400/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 156.1365 (155.3085)	
2019-01-08 10:33:31,633 - 10 - training_embed.py - training - Epoch: [0][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8067 (155.2834)	
2019-01-08 10:33:31,837 - 10 - training_embed.py - training - Epoch: [0][600/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.3122 (155.2793)	
2019-01-08 10:33:32,033 - 10 - training_embed.py - training - loss: 155.236724
2019-01-08 10:33:32,033 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:33:32,399 - 10 - training_embed.py - training - Epoch: [1][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8273 (155.1440)	
2019-01-08 10:33:32,600 - 10 - training_embed.py - training - Epoch: [1][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.1017 (155.1614)	
2019-01-08 10:33:32,828 - 10 - training_embed.py - training - Epoch: [1][300/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.9206 (155.1990)	
2019-01-08 10:33:33,067 - 10 - training_embed.py - training - Epoch: [1][400/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 156.6861 (155.2111)	
2019-01-08 10:33:33,267 - 10 - training_embed.py - training - Epoch: [1][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.6546 (155.2162)	
2019-01-08 10:33:33,489 - 10 - training_embed.py - training - Epoch: [1][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.3252 (155.2240)	
2019-01-08 10:33:33,678 - 10 - training_embed.py - training - loss: 155.214204
2019-01-08 10:33:33,678 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:33:34,029 - 10 - training_embed.py - training - Epoch: [2][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 152.8276 (155.1270)	
2019-01-08 10:33:34,254 - 10 - training_embed.py - training - Epoch: [2][200/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 156.2078 (155.1414)	
2019-01-08 10:33:34,469 - 10 - training_embed.py - training - Epoch: [2][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8380 (155.2460)	
2019-01-08 10:33:34,674 - 10 - training_embed.py - training - Epoch: [2][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 153.1333 (155.2106)	
2019-01-08 10:33:34,892 - 10 - training_embed.py - training - Epoch: [2][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.4014 (155.2130)	
2019-01-08 10:33:35,119 - 10 - training_embed.py - training - Epoch: [2][600/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.3701 (155.1795)	
2019-01-08 10:33:35,420 - 10 - training_embed.py - training - loss: 155.191893
2019-01-08 10:33:35,421 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:33:35,952 - 10 - training_embed.py - training - Epoch: [3][100/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 154.3849 (155.1160)	
2019-01-08 10:33:36,291 - 10 - training_embed.py - training - Epoch: [3][200/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 156.6466 (155.1748)	
2019-01-08 10:33:36,551 - 10 - training_embed.py - training - Epoch: [3][300/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 156.4559 (155.1813)	
2019-01-08 10:33:36,763 - 10 - training_embed.py - training - Epoch: [3][400/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 154.2364 (155.1777)	
2019-01-08 10:33:36,973 - 10 - training_embed.py - training - Epoch: [3][500/693]	Time 0.006 (0.003)	Data 0.004 (0.001)	Loss 154.1321 (155.1981)	
2019-01-08 10:33:37,190 - 10 - training_embed.py - training - Epoch: [3][600/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 153.7334 (155.1352)	
2019-01-08 10:33:37,506 - 10 - training_embed.py - training - loss: 155.169379
2019-01-08 10:33:37,506 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:33:38,032 - 10 - training_embed.py - training - Epoch: [4][100/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 157.1952 (155.2564)	
2019-01-08 10:33:38,401 - 10 - training_embed.py - training - Epoch: [4][200/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 155.1366 (155.2154)	
2019-01-08 10:33:38,705 - 10 - training_embed.py - training - Epoch: [4][300/693]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 155.6256 (155.1888)	
2019-01-08 10:33:39,037 - 10 - training_embed.py - training - Epoch: [4][400/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.2813 (155.1396)	
2019-01-08 10:33:39,358 - 10 - training_embed.py - training - Epoch: [4][500/693]	Time 0.005 (0.003)	Data 0.003 (0.001)	Loss 155.5213 (155.1318)	
2019-01-08 10:33:39,634 - 10 - training_embed.py - training - Epoch: [4][600/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.7821 (155.1386)	
2019-01-08 10:33:39,819 - 10 - training_embed.py - training - loss: 155.146872
2019-01-08 10:33:39,820 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:33:40,170 - 10 - training_embed.py - training - Epoch: [5][100/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 157.8644 (155.1030)	
2019-01-08 10:33:40,393 - 10 - training_embed.py - training - Epoch: [5][200/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.9331 (155.0912)	
2019-01-08 10:33:40,643 - 10 - training_embed.py - training - Epoch: [5][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.5782 (155.0878)	
2019-01-08 10:33:40,858 - 10 - training_embed.py - training - Epoch: [5][400/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 154.7843 (155.1158)	
2019-01-08 10:33:41,084 - 10 - training_embed.py - training - Epoch: [5][500/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 155.6614 (155.1625)	
2019-01-08 10:33:41,326 - 10 - training_embed.py - training - Epoch: [5][600/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.9561 (155.1680)	
2019-01-08 10:33:41,529 - 10 - training_embed.py - training - loss: 155.124546
2019-01-08 10:33:41,530 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:33:41,939 - 10 - training_embed.py - training - Epoch: [6][100/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.7273 (155.1397)	
2019-01-08 10:33:42,163 - 10 - training_embed.py - training - Epoch: [6][200/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 155.7495 (155.0949)	
2019-01-08 10:33:42,390 - 10 - training_embed.py - training - Epoch: [6][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.1375 (155.0951)	
2019-01-08 10:33:42,628 - 10 - training_embed.py - training - Epoch: [6][400/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.5557 (155.1142)	
2019-01-08 10:33:42,848 - 10 - training_embed.py - training - Epoch: [6][500/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 156.2797 (155.1307)	
2019-01-08 10:33:43,067 - 10 - training_embed.py - training - Epoch: [6][600/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.7696 (155.1213)	
2019-01-08 10:33:43,264 - 10 - training_embed.py - training - loss: 155.102013
2019-01-08 10:33:43,265 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:33:43,619 - 10 - training_embed.py - training - Epoch: [7][100/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 155.4217 (155.0159)	
2019-01-08 10:33:43,838 - 10 - training_embed.py - training - Epoch: [7][200/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.7342 (155.0044)	
2019-01-08 10:33:44,062 - 10 - training_embed.py - training - Epoch: [7][300/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 156.7750 (155.0596)	
2019-01-08 10:33:44,285 - 10 - training_embed.py - training - Epoch: [7][400/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 155.3813 (155.0599)	
2019-01-08 10:33:44,506 - 10 - training_embed.py - training - Epoch: [7][500/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 154.5594 (155.0741)	
2019-01-08 10:33:44,741 - 10 - training_embed.py - training - Epoch: [7][600/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 156.5340 (155.1049)	
2019-01-08 10:33:44,938 - 10 - training_embed.py - training - loss: 155.079715
2019-01-08 10:33:44,938 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:33:45,344 - 10 - training_embed.py - training - Epoch: [8][100/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 154.9997 (155.1006)	
2019-01-08 10:33:45,571 - 10 - training_embed.py - training - Epoch: [8][200/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.6118 (155.0569)	
2019-01-08 10:33:45,814 - 10 - training_embed.py - training - Epoch: [8][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.5315 (155.0231)	
2019-01-08 10:33:46,053 - 10 - training_embed.py - training - Epoch: [8][400/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 154.1174 (155.0241)	
2019-01-08 10:33:46,289 - 10 - training_embed.py - training - Epoch: [8][500/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.3686 (155.0559)	
2019-01-08 10:33:46,518 - 10 - training_embed.py - training - Epoch: [8][600/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 156.6021 (155.0653)	
2019-01-08 10:33:46,718 - 10 - training_embed.py - training - loss: 155.057450
2019-01-08 10:33:46,718 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:33:47,104 - 10 - training_embed.py - training - Epoch: [9][100/693]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 154.4272 (155.0189)	
2019-01-08 10:33:47,319 - 10 - training_embed.py - training - Epoch: [9][200/693]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 155.1000 (154.9688)	
2019-01-08 10:33:47,554 - 10 - training_embed.py - training - Epoch: [9][300/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 155.5015 (155.0510)	
2019-01-08 10:33:47,774 - 10 - training_embed.py - training - Epoch: [9][400/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.8359 (155.0376)	
2019-01-08 10:33:47,985 - 10 - training_embed.py - training - Epoch: [9][500/693]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 152.2636 (155.0296)	
2019-01-08 10:33:48,203 - 10 - training_embed.py - training - Epoch: [9][600/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.6727 (155.0477)	
2019-01-08 10:33:48,399 - 10 - training_embed.py - training - loss: 155.035008
2019-01-08 10:33:48,400 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:33:48,763 - 10 - training_embed.py - training - Epoch: [10][100/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 154.6335 (155.1054)	
2019-01-08 10:33:49,008 - 10 - training_embed.py - training - Epoch: [10][200/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.3812 (155.0777)	
2019-01-08 10:33:49,277 - 10 - training_embed.py - training - Epoch: [10][300/693]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 155.1351 (155.1356)	
2019-01-08 10:33:49,600 - 10 - training_embed.py - training - Epoch: [10][400/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 155.1860 (155.0722)	
2019-01-08 10:33:49,904 - 10 - training_embed.py - training - Epoch: [10][500/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 155.2662 (155.0284)	
2019-01-08 10:33:50,149 - 10 - training_embed.py - training - Epoch: [10][600/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.8974 (155.0415)	
2019-01-08 10:33:50,441 - 10 - training_embed.py - training - loss: 155.012572
2019-01-08 10:33:50,442 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:33:50,821 - 10 - training_embed.py - training - Epoch: [11][100/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 153.2402 (154.9353)	
2019-01-08 10:33:51,133 - 10 - training_embed.py - training - Epoch: [11][200/693]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 155.9838 (155.0298)	
2019-01-08 10:33:51,369 - 10 - training_embed.py - training - Epoch: [11][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 156.5397 (155.0121)	
2019-01-08 10:33:51,608 - 10 - training_embed.py - training - Epoch: [11][400/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 154.2125 (155.0466)	
2019-01-08 10:33:51,844 - 10 - training_embed.py - training - Epoch: [11][500/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.7666 (155.0219)	
2019-01-08 10:33:52,080 - 10 - training_embed.py - training - Epoch: [11][600/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.2040 (155.0181)	
2019-01-08 10:33:52,299 - 10 - training_embed.py - training - loss: 154.990266
2019-01-08 10:33:52,325 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:33:52,626 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 300.485 ms ~ 0.005 min ~ 0.300 sec
2019-01-08 10:33:52,864 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 539.196 ms ~ 0.009 min ~ 0.539 sec
2019-01-08 10:33:52,864 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:33:52,865 - 10 - corpus.py - subactivity_sampler - 0 / 187
2019-01-08 10:33:52,865 - 10 - corpus.py - subactivity_sampler - [47442. 48812. 40358. 40775.]
2019-01-08 10:33:57,733 - 10 - corpus.py - subactivity_sampler - 20 / 187
2019-01-08 10:33:57,733 - 10 - corpus.py - subactivity_sampler - [47291. 49951. 39681. 40464.]
2019-01-08 10:34:04,900 - 10 - corpus.py - subactivity_sampler - 40 / 187
2019-01-08 10:34:04,900 - 10 - corpus.py - subactivity_sampler - [47159. 50470. 39351. 40407.]
2019-01-08 10:34:10,168 - 10 - corpus.py - subactivity_sampler - 60 / 187
2019-01-08 10:34:10,169 - 10 - corpus.py - subactivity_sampler - [47099. 50711. 39132. 40445.]
2019-01-08 10:34:15,109 - 10 - corpus.py - subactivity_sampler - 80 / 187
2019-01-08 10:34:15,109 - 10 - corpus.py - subactivity_sampler - [47026. 51172. 38784. 40405.]
2019-01-08 10:34:21,220 - 10 - corpus.py - subactivity_sampler - 100 / 187
2019-01-08 10:34:21,220 - 10 - corpus.py - subactivity_sampler - [47260. 51554. 38110. 40463.]
2019-01-08 10:34:28,164 - 10 - corpus.py - subactivity_sampler - 120 / 187
2019-01-08 10:34:28,164 - 10 - corpus.py - subactivity_sampler - [47249. 52271. 37469. 40398.]
2019-01-08 10:34:35,203 - 10 - corpus.py - subactivity_sampler - 140 / 187
2019-01-08 10:34:35,203 - 10 - corpus.py - subactivity_sampler - [47337. 52647. 37178. 40225.]
2019-01-08 10:34:40,775 - 10 - corpus.py - subactivity_sampler - 160 / 187
2019-01-08 10:34:40,775 - 10 - corpus.py - subactivity_sampler - [47416. 53443. 36332. 40196.]
2019-01-08 10:34:45,732 - 10 - corpus.py - subactivity_sampler - 180 / 187
2019-01-08 10:34:45,732 - 10 - corpus.py - subactivity_sampler - [47465. 53452. 36337. 40133.]
2019-01-08 10:34:47,619 - 10 - corpus.py - subactivity_sampler - [47471. 53475. 36261. 40180.]
2019-01-08 10:34:47,619 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 54754.669 ms ~ 0.913 min ~ 54.755 sec
2019-01-08 10:34:47,619 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:34:48,141 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:34:48,141 - 10 - corpus.py - ordering_sampler - inv_count_vec: [4. 6. 0.]
2019-01-08 10:34:48,142 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:34:48,154 - 10 - corpus.py - rho_sampling - ['52.3901', '23.5350', '928.7809']
2019-01-08 10:34:48,155 - 10 - pipeline.py - baseline - Iteration 2
2019-01-08 10:34:48,200 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:34:48,203 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:34:48,203 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 24', '2: 2', '3: 25']
2019-01-08 10:34:48,207 - 10 - accuracy_class.py - mof_val - frames true: 79594	frames overall : 177387
2019-01-08 10:34:48,207 - 10 - corpus.py - accuracy_corpus - Action: milk
2019-01-08 10:34:48,207 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4487025543021755
2019-01-08 10:34:48,207 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4487025543021755
2019-01-08 10:34:48,207 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:34:48,207 - 10 - accuracy_class.py - mof_classes - label 2: 0.342681  15201 / 44359
2019-01-08 10:34:48,207 - 10 - accuracy_class.py - mof_classes - label 6: 1.000000  6091 / 6091
2019-01-08 10:34:48,208 - 10 - accuracy_class.py - mof_classes - label 24: 0.584766  34093 / 58302
2019-01-08 10:34:48,208 - 10 - accuracy_class.py - mof_classes - label 25: 0.593416  24209 / 40796
2019-01-08 10:34:48,208 - 10 - accuracy_class.py - mof_classes - average class mof: 0.504173
2019-01-08 10:34:48,208 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:34:48,208 - 10 - accuracy_class.py - iou_classes - label 2: 0.232364  15201 / 65419
2019-01-08 10:34:48,208 - 10 - accuracy_class.py - iou_classes - label 6: 0.128310  6091 / 47471
2019-01-08 10:34:48,208 - 10 - accuracy_class.py - iou_classes - label 24: 0.438868  34093 / 77684
2019-01-08 10:34:48,208 - 10 - accuracy_class.py - iou_classes - label 25: 0.426463  24209 / 56767
2019-01-08 10:34:48,208 - 10 - accuracy_class.py - iou_classes - average IoU: 0.306501
2019-01-08 10:34:48,208 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.245201
2019-01-08 10:34:50,618 - 10 - f1_score.py - f1 - f1 score: 0.448349
2019-01-08 10:34:50,629 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 2474.698 ms ~ 0.041 min ~ 2.475 sec
2019-01-08 10:34:50,629 - 10 - corpus.py - embedding_training - .
2019-01-08 10:34:50,629 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:34:50,630 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:34:50,630 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:34:51,778 - 10 - training_embed.py - training - create model
2019-01-08 10:34:51,778 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:34:51,779 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:34:52,123 - 10 - training_embed.py - training - Epoch: [0][100/693]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 155.7278 (155.7135)	
2019-01-08 10:34:52,401 - 10 - training_embed.py - training - Epoch: [0][200/693]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 155.7553 (155.7356)	
2019-01-08 10:34:52,683 - 10 - training_embed.py - training - Epoch: [0][300/693]	Time 0.001 (0.003)	Data 0.000 (0.002)	Loss 155.4504 (155.6788)	
2019-01-08 10:34:52,946 - 10 - training_embed.py - training - Epoch: [0][400/693]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 157.0287 (155.7278)	
2019-01-08 10:34:53,242 - 10 - training_embed.py - training - Epoch: [0][500/693]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 154.9753 (155.7120)	
2019-01-08 10:34:53,473 - 10 - training_embed.py - training - Epoch: [0][600/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.7126 (155.6918)	
2019-01-08 10:34:53,699 - 10 - training_embed.py - training - loss: 155.648117
2019-01-08 10:34:53,699 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:34:54,062 - 10 - training_embed.py - training - Epoch: [1][100/693]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 155.3814 (155.5351)	
2019-01-08 10:34:54,290 - 10 - training_embed.py - training - Epoch: [1][200/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 155.4582 (155.5990)	
2019-01-08 10:34:54,512 - 10 - training_embed.py - training - Epoch: [1][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 156.5162 (155.6129)	
2019-01-08 10:34:54,750 - 10 - training_embed.py - training - Epoch: [1][400/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 157.5074 (155.6280)	
2019-01-08 10:34:54,975 - 10 - training_embed.py - training - Epoch: [1][500/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 155.0040 (155.6282)	
2019-01-08 10:34:55,214 - 10 - training_embed.py - training - Epoch: [1][600/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 155.5332 (155.6368)	
2019-01-08 10:34:55,443 - 10 - training_embed.py - training - loss: 155.618743
2019-01-08 10:34:55,443 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:34:55,829 - 10 - training_embed.py - training - Epoch: [2][100/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 153.6748 (155.5537)	
2019-01-08 10:34:56,134 - 10 - training_embed.py - training - Epoch: [2][200/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 156.0423 (155.5390)	
2019-01-08 10:34:56,461 - 10 - training_embed.py - training - Epoch: [2][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.4839 (155.6360)	
2019-01-08 10:34:56,727 - 10 - training_embed.py - training - Epoch: [2][400/693]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 153.7739 (155.5949)	
2019-01-08 10:34:57,073 - 10 - training_embed.py - training - Epoch: [2][500/693]	Time 0.009 (0.003)	Data 0.000 (0.001)	Loss 157.4168 (155.6077)	
2019-01-08 10:34:57,420 - 10 - training_embed.py - training - Epoch: [2][600/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.6608 (155.5749)	
2019-01-08 10:34:57,621 - 10 - training_embed.py - training - loss: 155.589641
2019-01-08 10:34:57,622 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:34:58,048 - 10 - training_embed.py - training - Epoch: [3][100/693]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 154.4161 (155.4966)	
2019-01-08 10:34:58,294 - 10 - training_embed.py - training - Epoch: [3][200/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 156.8207 (155.5764)	
2019-01-08 10:34:58,515 - 10 - training_embed.py - training - Epoch: [3][300/693]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 157.3373 (155.5662)	
2019-01-08 10:34:58,746 - 10 - training_embed.py - training - Epoch: [3][400/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 154.3089 (155.5653)	
2019-01-08 10:34:58,970 - 10 - training_embed.py - training - Epoch: [3][500/693]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 154.0723 (155.5801)	
2019-01-08 10:34:59,194 - 10 - training_embed.py - training - Epoch: [3][600/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 153.7913 (155.5284)	
2019-01-08 10:34:59,399 - 10 - training_embed.py - training - loss: 155.560346
2019-01-08 10:34:59,400 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:34:59,775 - 10 - training_embed.py - training - Epoch: [4][100/693]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 157.1028 (155.6306)	
2019-01-08 10:34:59,989 - 10 - training_embed.py - training - Epoch: [4][200/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 156.0289 (155.5858)	
2019-01-08 10:35:00,213 - 10 - training_embed.py - training - Epoch: [4][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 156.8898 (155.5855)	
2019-01-08 10:35:00,446 - 10 - training_embed.py - training - Epoch: [4][400/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 156.1343 (155.5408)	
2019-01-08 10:35:00,660 - 10 - training_embed.py - training - Epoch: [4][500/693]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 155.5791 (155.5222)	
2019-01-08 10:35:00,874 - 10 - training_embed.py - training - Epoch: [4][600/693]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 155.7676 (155.5258)	
2019-01-08 10:35:01,084 - 10 - training_embed.py - training - loss: 155.531115
2019-01-08 10:35:01,085 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:35:01,457 - 10 - training_embed.py - training - Epoch: [5][100/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 157.8991 (155.4469)	
2019-01-08 10:35:01,672 - 10 - training_embed.py - training - Epoch: [5][200/693]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 155.7822 (155.4601)	
2019-01-08 10:35:01,894 - 10 - training_embed.py - training - Epoch: [5][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.2513 (155.4848)	
2019-01-08 10:35:02,110 - 10 - training_embed.py - training - Epoch: [5][400/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.2966 (155.5202)	
2019-01-08 10:35:02,319 - 10 - training_embed.py - training - Epoch: [5][500/693]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 155.9740 (155.5631)	
2019-01-08 10:35:02,545 - 10 - training_embed.py - training - Epoch: [5][600/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.8491 (155.5513)	
2019-01-08 10:35:02,736 - 10 - training_embed.py - training - loss: 155.501903
2019-01-08 10:35:02,736 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:35:03,260 - 10 - training_embed.py - training - Epoch: [6][100/693]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 156.3065 (155.6246)	
2019-01-08 10:35:03,563 - 10 - training_embed.py - training - Epoch: [6][200/693]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 155.7517 (155.4880)	
2019-01-08 10:35:03,792 - 10 - training_embed.py - training - Epoch: [6][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.8352 (155.4804)	
2019-01-08 10:35:04,016 - 10 - training_embed.py - training - Epoch: [6][400/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.7541 (155.4984)	
2019-01-08 10:35:04,236 - 10 - training_embed.py - training - Epoch: [6][500/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 156.2103 (155.4966)	
2019-01-08 10:35:04,442 - 10 - training_embed.py - training - Epoch: [6][600/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 156.4446 (155.4925)	
2019-01-08 10:35:04,641 - 10 - training_embed.py - training - loss: 155.472772
2019-01-08 10:35:04,641 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:35:04,960 - 10 - training_embed.py - training - Epoch: [7][100/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.3857 (155.3601)	
2019-01-08 10:35:05,177 - 10 - training_embed.py - training - Epoch: [7][200/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 154.8095 (155.3484)	
2019-01-08 10:35:05,401 - 10 - training_embed.py - training - Epoch: [7][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 157.4238 (155.4139)	
2019-01-08 10:35:05,615 - 10 - training_embed.py - training - Epoch: [7][400/693]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 154.7579 (155.4217)	
2019-01-08 10:35:05,850 - 10 - training_embed.py - training - Epoch: [7][500/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 155.7971 (155.4355)	
2019-01-08 10:35:06,078 - 10 - training_embed.py - training - Epoch: [7][600/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 156.8068 (155.4591)	
2019-01-08 10:35:06,272 - 10 - training_embed.py - training - loss: 155.443583
2019-01-08 10:35:06,272 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:35:06,635 - 10 - training_embed.py - training - Epoch: [8][100/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.5237 (155.4860)	
2019-01-08 10:35:06,851 - 10 - training_embed.py - training - Epoch: [8][200/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.6321 (155.4435)	
2019-01-08 10:35:07,067 - 10 - training_embed.py - training - Epoch: [8][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.6418 (155.3980)	
2019-01-08 10:35:07,333 - 10 - training_embed.py - training - Epoch: [8][400/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.0174 (155.3912)	
2019-01-08 10:35:07,591 - 10 - training_embed.py - training - Epoch: [8][500/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.6906 (155.4178)	
2019-01-08 10:35:07,816 - 10 - training_embed.py - training - Epoch: [8][600/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 157.1025 (155.4230)	
2019-01-08 10:35:08,041 - 10 - training_embed.py - training - loss: 155.414610
2019-01-08 10:35:08,042 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:35:08,418 - 10 - training_embed.py - training - Epoch: [9][100/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.6892 (155.3584)	
2019-01-08 10:35:08,629 - 10 - training_embed.py - training - Epoch: [9][200/693]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 154.9640 (155.3204)	
2019-01-08 10:35:08,849 - 10 - training_embed.py - training - Epoch: [9][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.8645 (155.3689)	
2019-01-08 10:35:09,075 - 10 - training_embed.py - training - Epoch: [9][400/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 156.1731 (155.3481)	
2019-01-08 10:35:09,280 - 10 - training_embed.py - training - Epoch: [9][500/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 152.5247 (155.3621)	
2019-01-08 10:35:09,495 - 10 - training_embed.py - training - Epoch: [9][600/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 155.5153 (155.3952)	
2019-01-08 10:35:09,687 - 10 - training_embed.py - training - loss: 155.385339
2019-01-08 10:35:09,687 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:35:10,042 - 10 - training_embed.py - training - Epoch: [10][100/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.1714 (155.4098)	
2019-01-08 10:35:10,264 - 10 - training_embed.py - training - Epoch: [10][200/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.1175 (155.4384)	
2019-01-08 10:35:10,466 - 10 - training_embed.py - training - Epoch: [10][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.6453 (155.4607)	
2019-01-08 10:35:10,684 - 10 - training_embed.py - training - Epoch: [10][400/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 155.5462 (155.4154)	
2019-01-08 10:35:10,903 - 10 - training_embed.py - training - Epoch: [10][500/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.4303 (155.3819)	
2019-01-08 10:35:11,121 - 10 - training_embed.py - training - Epoch: [10][600/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 156.3700 (155.3845)	
2019-01-08 10:35:11,314 - 10 - training_embed.py - training - loss: 155.356020
2019-01-08 10:35:11,315 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:35:11,674 - 10 - training_embed.py - training - Epoch: [11][100/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 153.8705 (155.3341)	
2019-01-08 10:35:11,884 - 10 - training_embed.py - training - Epoch: [11][200/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 156.1279 (155.3974)	
2019-01-08 10:35:12,108 - 10 - training_embed.py - training - Epoch: [11][300/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 156.5457 (155.3708)	
2019-01-08 10:35:12,357 - 10 - training_embed.py - training - Epoch: [11][400/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 154.0179 (155.3923)	
2019-01-08 10:35:12,644 - 10 - training_embed.py - training - Epoch: [11][500/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 155.0942 (155.3602)	
2019-01-08 10:35:12,935 - 10 - training_embed.py - training - Epoch: [11][600/693]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 156.0989 (155.3586)	
2019-01-08 10:35:13,191 - 10 - training_embed.py - training - loss: 155.326983
2019-01-08 10:35:13,223 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:35:13,564 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 340.376 ms ~ 0.006 min ~ 0.340 sec
2019-01-08 10:35:13,834 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 610.263 ms ~ 0.010 min ~ 0.610 sec
2019-01-08 10:35:13,834 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:35:13,834 - 10 - corpus.py - subactivity_sampler - 0 / 187
2019-01-08 10:35:13,835 - 10 - corpus.py - subactivity_sampler - [47471. 53475. 36261. 40180.]
2019-01-08 10:35:18,623 - 10 - corpus.py - subactivity_sampler - 20 / 187
2019-01-08 10:35:18,623 - 10 - corpus.py - subactivity_sampler - [47427. 53686. 36058. 40216.]
2019-01-08 10:35:25,577 - 10 - corpus.py - subactivity_sampler - 40 / 187
2019-01-08 10:35:25,577 - 10 - corpus.py - subactivity_sampler - [47211. 54123. 35682. 40371.]
2019-01-08 10:35:30,503 - 10 - corpus.py - subactivity_sampler - 60 / 187
2019-01-08 10:35:30,503 - 10 - corpus.py - subactivity_sampler - [47183. 54474. 35255. 40475.]
2019-01-08 10:35:35,268 - 10 - corpus.py - subactivity_sampler - 80 / 187
2019-01-08 10:35:35,268 - 10 - corpus.py - subactivity_sampler - [47003. 54783. 35040. 40561.]
2019-01-08 10:35:41,111 - 10 - corpus.py - subactivity_sampler - 100 / 187
2019-01-08 10:35:41,111 - 10 - corpus.py - subactivity_sampler - [47070. 55123. 34578. 40616.]
2019-01-08 10:35:47,746 - 10 - corpus.py - subactivity_sampler - 120 / 187
2019-01-08 10:35:47,746 - 10 - corpus.py - subactivity_sampler - [47033. 55192. 34364. 40798.]
2019-01-08 10:35:54,351 - 10 - corpus.py - subactivity_sampler - 140 / 187
2019-01-08 10:35:54,351 - 10 - corpus.py - subactivity_sampler - [46910. 55519. 33978. 40980.]
2019-01-08 10:35:59,879 - 10 - corpus.py - subactivity_sampler - 160 / 187
2019-01-08 10:35:59,879 - 10 - corpus.py - subactivity_sampler - [46989. 55278. 33968. 41152.]
2019-01-08 10:36:05,363 - 10 - corpus.py - subactivity_sampler - 180 / 187
2019-01-08 10:36:05,363 - 10 - corpus.py - subactivity_sampler - [46928. 55433. 33892. 41134.]
2019-01-08 10:36:07,457 - 10 - corpus.py - subactivity_sampler - [46923. 55222. 34017. 41225.]
2019-01-08 10:36:07,457 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 53623.590 ms ~ 0.894 min ~ 53.624 sec
2019-01-08 10:36:07,457 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:36:08,268 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:36:08,269 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 5. 19.  0.]
2019-01-08 10:36:08,269 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:36:08,298 - 10 - corpus.py - rho_sampling - ['51.0235', '8.3783', '928.6447']
2019-01-08 10:36:08,299 - 10 - pipeline.py - baseline - Iteration 3
2019-01-08 10:36:08,374 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:36:08,378 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:36:08,378 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 24', '2: 2', '3: 25']
2019-01-08 10:36:08,385 - 10 - accuracy_class.py - mof_val - frames true: 80436	frames overall : 177387
2019-01-08 10:36:08,385 - 10 - corpus.py - accuracy_corpus - Action: milk
2019-01-08 10:36:08,385 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4534492381065129
2019-01-08 10:36:08,385 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4534492381065129
2019-01-08 10:36:08,385 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:36:08,385 - 10 - accuracy_class.py - mof_classes - label 2: 0.329967  14637 / 44359
2019-01-08 10:36:08,385 - 10 - accuracy_class.py - mof_classes - label 6: 0.996388  6069 / 6091
2019-01-08 10:36:08,385 - 10 - accuracy_class.py - mof_classes - label 24: 0.603444  35182 / 58302
2019-01-08 10:36:08,385 - 10 - accuracy_class.py - mof_classes - label 25: 0.601726  24548 / 40796
2019-01-08 10:36:08,385 - 10 - accuracy_class.py - mof_classes - average class mof: 0.506305
2019-01-08 10:36:08,385 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:36:08,385 - 10 - accuracy_class.py - iou_classes - label 2: 0.229640  14637 / 63739
2019-01-08 10:36:08,385 - 10 - accuracy_class.py - iou_classes - label 6: 0.129279  6069 / 46945
2019-01-08 10:36:08,385 - 10 - accuracy_class.py - iou_classes - label 24: 0.449082  35182 / 78342
2019-01-08 10:36:08,385 - 10 - accuracy_class.py - iou_classes - label 25: 0.427122  24548 / 57473
2019-01-08 10:36:08,385 - 10 - accuracy_class.py - iou_classes - average IoU: 0.308781
2019-01-08 10:36:08,386 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.247025
2019-01-08 10:36:11,132 - 10 - f1_score.py - f1 - f1 score: 0.448224
2019-01-08 10:36:11,139 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 2840.694 ms ~ 0.047 min ~ 2.841 sec
2019-01-08 10:36:11,139 - 10 - corpus.py - embedding_training - .
2019-01-08 10:36:11,139 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:36:11,140 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:36:11,140 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:36:12,867 - 10 - training_embed.py - training - create model
2019-01-08 10:36:12,868 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:36:12,868 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:36:13,227 - 10 - training_embed.py - training - Epoch: [0][100/693]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 155.6914 (155.8790)	
2019-01-08 10:36:13,468 - 10 - training_embed.py - training - Epoch: [0][200/693]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 156.1111 (155.8842)	
2019-01-08 10:36:13,767 - 10 - training_embed.py - training - Epoch: [0][300/693]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 154.9539 (155.8304)	
2019-01-08 10:36:14,028 - 10 - training_embed.py - training - Epoch: [0][400/693]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 157.8250 (155.8797)	
2019-01-08 10:36:14,266 - 10 - training_embed.py - training - Epoch: [0][500/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 154.7781 (155.8595)	
2019-01-08 10:36:14,520 - 10 - training_embed.py - training - Epoch: [0][600/693]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 155.5830 (155.8436)	
2019-01-08 10:36:14,803 - 10 - training_embed.py - training - loss: 155.792877
2019-01-08 10:36:14,804 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:36:15,340 - 10 - training_embed.py - training - Epoch: [1][100/693]	Time 0.003 (0.003)	Data 0.002 (0.002)	Loss 155.7070 (155.6702)	
2019-01-08 10:36:15,772 - 10 - training_embed.py - training - Epoch: [1][200/693]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 156.8977 (155.7612)	
2019-01-08 10:36:16,252 - 10 - training_embed.py - training - Epoch: [1][300/693]	Time 0.005 (0.003)	Data 0.002 (0.002)	Loss 155.9212 (155.7664)	
2019-01-08 10:36:16,671 - 10 - training_embed.py - training - Epoch: [1][400/693]	Time 0.007 (0.003)	Data 0.005 (0.002)	Loss 157.0799 (155.7741)	
2019-01-08 10:36:17,134 - 10 - training_embed.py - training - Epoch: [1][500/693]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 155.3524 (155.7672)	
2019-01-08 10:36:17,546 - 10 - training_embed.py - training - Epoch: [1][600/693]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 156.0135 (155.7691)	
2019-01-08 10:36:17,903 - 10 - training_embed.py - training - loss: 155.758502
2019-01-08 10:36:17,904 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:36:18,616 - 10 - training_embed.py - training - Epoch: [2][100/693]	Time 0.007 (0.004)	Data 0.005 (0.002)	Loss 153.8686 (155.7531)	
2019-01-08 10:36:19,042 - 10 - training_embed.py - training - Epoch: [2][200/693]	Time 0.006 (0.004)	Data 0.005 (0.002)	Loss 156.1098 (155.6924)	
2019-01-08 10:36:19,549 - 10 - training_embed.py - training - Epoch: [2][300/693]	Time 0.008 (0.004)	Data 0.002 (0.002)	Loss 155.7525 (155.7798)	
2019-01-08 10:36:20,052 - 10 - training_embed.py - training - Epoch: [2][400/693]	Time 0.009 (0.004)	Data 0.003 (0.002)	Loss 153.8513 (155.7113)	
2019-01-08 10:36:20,617 - 10 - training_embed.py - training - Epoch: [2][500/693]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 157.1188 (155.7372)	
2019-01-08 10:36:21,103 - 10 - training_embed.py - training - Epoch: [2][600/693]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 156.0390 (155.7067)	
2019-01-08 10:36:21,532 - 10 - training_embed.py - training - loss: 155.724463
2019-01-08 10:36:21,533 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:36:22,180 - 10 - training_embed.py - training - Epoch: [3][100/693]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 154.1103 (155.6411)	
2019-01-08 10:36:22,695 - 10 - training_embed.py - training - Epoch: [3][200/693]	Time 0.008 (0.004)	Data 0.002 (0.002)	Loss 156.7280 (155.6860)	
2019-01-08 10:36:23,317 - 10 - training_embed.py - training - Epoch: [3][300/693]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 157.3107 (155.6901)	
2019-01-08 10:36:23,753 - 10 - training_embed.py - training - Epoch: [3][400/693]	Time 0.009 (0.004)	Data 0.001 (0.002)	Loss 154.4685 (155.6863)	
2019-01-08 10:36:24,152 - 10 - training_embed.py - training - Epoch: [3][500/693]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 154.4005 (155.7012)	
2019-01-08 10:36:24,654 - 10 - training_embed.py - training - Epoch: [3][600/693]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 154.3706 (155.6531)	
2019-01-08 10:36:25,030 - 10 - training_embed.py - training - loss: 155.690104
2019-01-08 10:36:25,030 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:36:25,722 - 10 - training_embed.py - training - Epoch: [4][100/693]	Time 0.008 (0.004)	Data 0.000 (0.002)	Loss 156.9425 (155.6925)	
2019-01-08 10:36:26,191 - 10 - training_embed.py - training - Epoch: [4][200/693]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 156.3616 (155.6645)	
2019-01-08 10:36:26,670 - 10 - training_embed.py - training - Epoch: [4][300/693]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 156.5550 (155.6762)	
2019-01-08 10:36:27,136 - 10 - training_embed.py - training - Epoch: [4][400/693]	Time 0.007 (0.004)	Data 0.002 (0.002)	Loss 155.8494 (155.6363)	
2019-01-08 10:36:27,688 - 10 - training_embed.py - training - Epoch: [4][500/693]	Time 0.005 (0.005)	Data 0.004 (0.002)	Loss 155.4420 (155.6352)	
2019-01-08 10:36:28,178 - 10 - training_embed.py - training - Epoch: [4][600/693]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 155.6946 (155.6444)	
2019-01-08 10:36:28,527 - 10 - training_embed.py - training - loss: 155.655806
2019-01-08 10:36:28,528 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:36:29,264 - 10 - training_embed.py - training - Epoch: [5][100/693]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 157.7984 (155.5998)	
2019-01-08 10:36:29,730 - 10 - training_embed.py - training - Epoch: [5][200/693]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 155.9485 (155.6188)	
2019-01-08 10:36:30,245 - 10 - training_embed.py - training - Epoch: [5][300/693]	Time 0.006 (0.005)	Data 0.005 (0.002)	Loss 155.6978 (155.6249)	
2019-01-08 10:36:30,675 - 10 - training_embed.py - training - Epoch: [5][400/693]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 155.3968 (155.6505)	
2019-01-08 10:36:31,192 - 10 - training_embed.py - training - Epoch: [5][500/693]	Time 0.005 (0.005)	Data 0.002 (0.002)	Loss 156.4929 (155.6806)	
2019-01-08 10:36:31,715 - 10 - training_embed.py - training - Epoch: [5][600/693]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 155.2351 (155.6663)	
2019-01-08 10:36:32,203 - 10 - training_embed.py - training - loss: 155.621618
2019-01-08 10:36:32,203 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:36:32,895 - 10 - training_embed.py - training - Epoch: [6][100/693]	Time 0.002 (0.005)	Data 0.000 (0.002)	Loss 156.2873 (155.7419)	
2019-01-08 10:36:33,310 - 10 - training_embed.py - training - Epoch: [6][200/693]	Time 0.002 (0.005)	Data 0.000 (0.002)	Loss 155.8220 (155.5848)	
2019-01-08 10:36:33,820 - 10 - training_embed.py - training - Epoch: [6][300/693]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 154.6180 (155.5864)	
2019-01-08 10:36:34,242 - 10 - training_embed.py - training - Epoch: [6][400/693]	Time 0.008 (0.005)	Data 0.001 (0.002)	Loss 155.2514 (155.6172)	
2019-01-08 10:36:34,689 - 10 - training_embed.py - training - Epoch: [6][500/693]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 156.2139 (155.5976)	
2019-01-08 10:36:35,103 - 10 - training_embed.py - training - Epoch: [6][600/693]	Time 0.005 (0.005)	Data 0.003 (0.002)	Loss 156.4606 (155.5937)	
2019-01-08 10:36:35,449 - 10 - training_embed.py - training - loss: 155.587480
2019-01-08 10:36:35,449 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:36:36,119 - 10 - training_embed.py - training - Epoch: [7][100/693]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 155.1971 (155.5853)	
2019-01-08 10:36:36,565 - 10 - training_embed.py - training - Epoch: [7][200/693]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 155.1866 (155.5099)	
2019-01-08 10:36:37,097 - 10 - training_embed.py - training - Epoch: [7][300/693]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 157.7279 (155.5592)	
2019-01-08 10:36:37,502 - 10 - training_embed.py - training - Epoch: [7][400/693]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 154.4725 (155.5610)	
2019-01-08 10:36:38,071 - 10 - training_embed.py - training - Epoch: [7][500/693]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 155.3902 (155.5576)	
2019-01-08 10:36:38,491 - 10 - training_embed.py - training - Epoch: [7][600/693]	Time 0.005 (0.005)	Data 0.002 (0.002)	Loss 157.0910 (155.5816)	
2019-01-08 10:36:38,981 - 10 - training_embed.py - training - loss: 155.553381
2019-01-08 10:36:38,982 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:36:39,630 - 10 - training_embed.py - training - Epoch: [8][100/693]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 155.1986 (155.5426)	
2019-01-08 10:36:40,138 - 10 - training_embed.py - training - Epoch: [8][200/693]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 154.6767 (155.4776)	
2019-01-08 10:36:40,602 - 10 - training_embed.py - training - Epoch: [8][300/693]	Time 0.009 (0.005)	Data 0.002 (0.002)	Loss 155.9704 (155.4716)	
2019-01-08 10:36:41,075 - 10 - training_embed.py - training - Epoch: [8][400/693]	Time 0.012 (0.005)	Data 0.011 (0.002)	Loss 154.7496 (155.4847)	
2019-01-08 10:36:41,559 - 10 - training_embed.py - training - Epoch: [8][500/693]	Time 0.006 (0.005)	Data 0.002 (0.002)	Loss 154.1570 (155.5141)	
2019-01-08 10:36:42,170 - 10 - training_embed.py - training - Epoch: [8][600/693]	Time 0.008 (0.005)	Data 0.003 (0.002)	Loss 157.0773 (155.5165)	
2019-01-08 10:36:42,771 - 10 - training_embed.py - training - loss: 155.519352
2019-01-08 10:36:42,772 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:36:43,394 - 10 - training_embed.py - training - Epoch: [9][100/693]	Time 0.006 (0.005)	Data 0.004 (0.002)	Loss 154.1533 (155.5103)	
2019-01-08 10:36:43,864 - 10 - training_embed.py - training - Epoch: [9][200/693]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 154.6368 (155.4532)	
2019-01-08 10:36:44,444 - 10 - training_embed.py - training - Epoch: [9][300/693]	Time 0.013 (0.005)	Data 0.002 (0.002)	Loss 155.5914 (155.4806)	
2019-01-08 10:36:44,924 - 10 - training_embed.py - training - Epoch: [9][400/693]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 155.9554 (155.4420)	
2019-01-08 10:36:45,353 - 10 - training_embed.py - training - Epoch: [9][500/693]	Time 0.004 (0.005)	Data 0.003 (0.002)	Loss 152.8695 (155.4620)	
2019-01-08 10:36:45,859 - 10 - training_embed.py - training - Epoch: [9][600/693]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 155.1565 (155.4926)	
2019-01-08 10:36:46,257 - 10 - training_embed.py - training - loss: 155.485084
2019-01-08 10:36:46,258 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:36:46,951 - 10 - training_embed.py - training - Epoch: [10][100/693]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 155.5273 (155.5702)	
2019-01-08 10:36:47,463 - 10 - training_embed.py - training - Epoch: [10][200/693]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 155.1600 (155.5589)	
2019-01-08 10:36:47,945 - 10 - training_embed.py - training - Epoch: [10][300/693]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 156.0926 (155.5815)	
2019-01-08 10:36:48,443 - 10 - training_embed.py - training - Epoch: [10][400/693]	Time 0.002 (0.005)	Data 0.000 (0.002)	Loss 155.6651 (155.5278)	
2019-01-08 10:36:48,912 - 10 - training_embed.py - training - Epoch: [10][500/693]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 156.6424 (155.4909)	
2019-01-08 10:36:49,426 - 10 - training_embed.py - training - Epoch: [10][600/693]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 155.8700 (155.4753)	
2019-01-08 10:36:49,794 - 10 - training_embed.py - training - loss: 155.450716
2019-01-08 10:36:49,794 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:36:50,486 - 10 - training_embed.py - training - Epoch: [11][100/693]	Time 0.006 (0.005)	Data 0.003 (0.002)	Loss 154.3822 (155.3493)	
2019-01-08 10:36:50,906 - 10 - training_embed.py - training - Epoch: [11][200/693]	Time 0.002 (0.005)	Data 0.000 (0.002)	Loss 155.4239 (155.4698)	
2019-01-08 10:36:51,375 - 10 - training_embed.py - training - Epoch: [11][300/693]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 157.2902 (155.4443)	
2019-01-08 10:36:51,896 - 10 - training_embed.py - training - Epoch: [11][400/693]	Time 0.002 (0.005)	Data 0.000 (0.002)	Loss 155.1953 (155.4770)	
2019-01-08 10:36:52,362 - 10 - training_embed.py - training - Epoch: [11][500/693]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 154.7681 (155.4422)	
2019-01-08 10:36:52,865 - 10 - training_embed.py - training - Epoch: [11][600/693]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 156.0011 (155.4405)	
2019-01-08 10:36:53,258 - 10 - training_embed.py - training - loss: 155.416693
2019-01-08 10:36:53,310 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:36:53,890 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 579.731 ms ~ 0.010 min ~ 0.580 sec
2019-01-08 10:36:54,284 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 974.225 ms ~ 0.016 min ~ 0.974 sec
2019-01-08 10:36:54,284 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:36:54,285 - 10 - corpus.py - subactivity_sampler - 0 / 187
2019-01-08 10:36:54,285 - 10 - corpus.py - subactivity_sampler - [46923. 55222. 34017. 41225.]
2019-01-08 10:37:00,050 - 10 - corpus.py - subactivity_sampler - 20 / 187
2019-01-08 10:37:00,050 - 10 - corpus.py - subactivity_sampler - [46895. 55298. 33875. 41319.]
2019-01-08 10:37:08,845 - 10 - corpus.py - subactivity_sampler - 40 / 187
2019-01-08 10:37:08,845 - 10 - corpus.py - subactivity_sampler - [46753. 55566. 33613. 41455.]
2019-01-08 10:37:14,976 - 10 - corpus.py - subactivity_sampler - 60 / 187
2019-01-08 10:37:14,977 - 10 - corpus.py - subactivity_sampler - [46723. 56130. 33059. 41475.]
2019-01-08 10:37:22,037 - 10 - corpus.py - subactivity_sampler - 80 / 187
2019-01-08 10:37:22,037 - 10 - corpus.py - subactivity_sampler - [46580. 56398. 32726. 41683.]
2019-01-08 10:37:30,429 - 10 - corpus.py - subactivity_sampler - 100 / 187
2019-01-08 10:37:30,429 - 10 - corpus.py - subactivity_sampler - [46538. 56572. 32454. 41823.]
2019-01-08 10:37:40,692 - 10 - corpus.py - subactivity_sampler - 120 / 187
2019-01-08 10:37:40,692 - 10 - corpus.py - subactivity_sampler - [46525. 56914. 32159. 41789.]
2019-01-08 10:37:51,413 - 10 - corpus.py - subactivity_sampler - 140 / 187
2019-01-08 10:37:51,414 - 10 - corpus.py - subactivity_sampler - [46454. 57097. 31990. 41846.]
2019-01-08 10:37:59,319 - 10 - corpus.py - subactivity_sampler - 160 / 187
2019-01-08 10:37:59,320 - 10 - corpus.py - subactivity_sampler - [46473. 57208. 31677. 42029.]
2019-01-08 10:38:06,397 - 10 - corpus.py - subactivity_sampler - 180 / 187
2019-01-08 10:38:06,397 - 10 - corpus.py - subactivity_sampler - [46213. 57476. 31361. 42337.]
2019-01-08 10:38:08,954 - 10 - corpus.py - subactivity_sampler - [46204. 57475. 31275. 42433.]
2019-01-08 10:38:08,954 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 74670.432 ms ~ 1.245 min ~ 74.670 sec
2019-01-08 10:38:08,955 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:38:09,934 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:38:09,934 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 5. 22.  0.]
2019-01-08 10:38:09,934 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:38:09,953 - 10 - corpus.py - rho_sampling - ['53.1438', '17.9389', '50.9195']
2019-01-08 10:38:09,953 - 10 - pipeline.py - baseline - Iteration 4
2019-01-08 10:38:10,022 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:38:10,028 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:38:10,028 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 24', '2: 2', '3: 25']
2019-01-08 10:38:10,037 - 10 - accuracy_class.py - mof_val - frames true: 80071	frames overall : 177387
2019-01-08 10:38:10,037 - 10 - corpus.py - accuracy_corpus - Action: milk
2019-01-08 10:38:10,037 - 10 - corpus.py - accuracy_corpus - MoF val: 0.45139159013907443
2019-01-08 10:38:10,037 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.45139159013907443
2019-01-08 10:38:10,037 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:38:10,037 - 10 - accuracy_class.py - mof_classes - label 2: 0.300503  13330 / 44359
2019-01-08 10:38:10,037 - 10 - accuracy_class.py - mof_classes - label 6: 0.989000  6024 / 6091
2019-01-08 10:38:10,037 - 10 - accuracy_class.py - mof_classes - label 24: 0.614987  35855 / 58302
2019-01-08 10:38:10,037 - 10 - accuracy_class.py - mof_classes - label 25: 0.609422  24862 / 40796
2019-01-08 10:38:10,038 - 10 - accuracy_class.py - mof_classes - average class mof: 0.502783
2019-01-08 10:38:10,038 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:38:10,038 - 10 - accuracy_class.py - iou_classes - label 2: 0.213951  13330 / 62304
2019-01-08 10:38:10,038 - 10 - accuracy_class.py - iou_classes - label 6: 0.130190  6024 / 46271
2019-01-08 10:38:10,038 - 10 - accuracy_class.py - iou_classes - label 24: 0.448625  35855 / 79922
2019-01-08 10:38:10,038 - 10 - accuracy_class.py - iou_classes - label 25: 0.425960  24862 / 58367
2019-01-08 10:38:10,038 - 10 - accuracy_class.py - iou_classes - average IoU: 0.304681
2019-01-08 10:38:10,038 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.243745
2019-01-08 10:38:14,074 - 10 - f1_score.py - f1 - f1 score: 0.444336
2019-01-08 10:38:14,084 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 4130.934 ms ~ 0.069 min ~ 4.131 sec
2019-01-08 10:38:14,085 - 10 - corpus.py - embedding_training - .
2019-01-08 10:38:14,085 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:38:14,085 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:38:14,085 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:38:16,647 - 10 - training_embed.py - training - create model
2019-01-08 10:38:16,650 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:38:16,650 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:38:17,557 - 10 - training_embed.py - training - Epoch: [0][100/693]	Time 0.004 (0.009)	Data 0.002 (0.004)	Loss 155.5674 (155.9747)	
2019-01-08 10:38:18,345 - 10 - training_embed.py - training - Epoch: [0][200/693]	Time 0.006 (0.008)	Data 0.005 (0.003)	Loss 155.5910 (155.9973)	
2019-01-08 10:38:19,166 - 10 - training_embed.py - training - Epoch: [0][300/693]	Time 0.003 (0.008)	Data 0.000 (0.003)	Loss 155.4283 (155.9491)	
2019-01-08 10:38:19,976 - 10 - training_embed.py - training - Epoch: [0][400/693]	Time 0.012 (0.008)	Data 0.002 (0.003)	Loss 158.4385 (155.9820)	
2019-01-08 10:38:20,772 - 10 - training_embed.py - training - Epoch: [0][500/693]	Time 0.005 (0.008)	Data 0.002 (0.003)	Loss 154.5785 (155.9639)	
2019-01-08 10:38:21,345 - 10 - training_embed.py - training - Epoch: [0][600/693]	Time 0.003 (0.008)	Data 0.001 (0.003)	Loss 155.7324 (155.9572)	
2019-01-08 10:38:21,892 - 10 - training_embed.py - training - loss: 155.904536
2019-01-08 10:38:21,892 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:38:22,678 - 10 - training_embed.py - training - Epoch: [1][100/693]	Time 0.004 (0.008)	Data 0.002 (0.003)	Loss 155.8282 (155.7706)	
2019-01-08 10:38:23,470 - 10 - training_embed.py - training - Epoch: [1][200/693]	Time 0.009 (0.008)	Data 0.001 (0.003)	Loss 157.0449 (155.8706)	
2019-01-08 10:38:23,989 - 10 - training_embed.py - training - Epoch: [1][300/693]	Time 0.004 (0.007)	Data 0.002 (0.003)	Loss 156.1637 (155.8877)	
2019-01-08 10:38:24,584 - 10 - training_embed.py - training - Epoch: [1][400/693]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 155.9242 (155.8927)	
2019-01-08 10:38:25,094 - 10 - training_embed.py - training - Epoch: [1][500/693]	Time 0.001 (0.007)	Data 0.000 (0.003)	Loss 155.3999 (155.8857)	
2019-01-08 10:38:25,603 - 10 - training_embed.py - training - Epoch: [1][600/693]	Time 0.003 (0.007)	Data 0.002 (0.003)	Loss 156.4409 (155.8792)	
2019-01-08 10:38:26,088 - 10 - training_embed.py - training - loss: 155.865804
2019-01-08 10:38:26,089 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:38:26,868 - 10 - training_embed.py - training - Epoch: [2][100/693]	Time 0.006 (0.007)	Data 0.002 (0.003)	Loss 154.0727 (155.8766)	
2019-01-08 10:38:27,453 - 10 - training_embed.py - training - Epoch: [2][200/693]	Time 0.008 (0.007)	Data 0.006 (0.003)	Loss 156.0360 (155.7901)	
2019-01-08 10:38:28,014 - 10 - training_embed.py - training - Epoch: [2][300/693]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 155.6853 (155.8734)	
2019-01-08 10:38:28,606 - 10 - training_embed.py - training - Epoch: [2][400/693]	Time 0.009 (0.007)	Data 0.001 (0.003)	Loss 154.3465 (155.8169)	
2019-01-08 10:38:29,146 - 10 - training_embed.py - training - Epoch: [2][500/693]	Time 0.004 (0.007)	Data 0.002 (0.003)	Loss 156.7793 (155.8324)	
2019-01-08 10:38:29,690 - 10 - training_embed.py - training - Epoch: [2][600/693]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 155.9232 (155.8076)	
2019-01-08 10:38:30,153 - 10 - training_embed.py - training - loss: 155.827520
2019-01-08 10:38:30,154 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:38:30,902 - 10 - training_embed.py - training - Epoch: [3][100/693]	Time 0.004 (0.007)	Data 0.002 (0.003)	Loss 154.8003 (155.7307)	
2019-01-08 10:38:31,395 - 10 - training_embed.py - training - Epoch: [3][200/693]	Time 0.008 (0.006)	Data 0.002 (0.003)	Loss 156.5378 (155.8034)	
2019-01-08 10:38:31,940 - 10 - training_embed.py - training - Epoch: [3][300/693]	Time 0.004 (0.006)	Data 0.002 (0.003)	Loss 157.9348 (155.7889)	
2019-01-08 10:38:32,435 - 10 - training_embed.py - training - Epoch: [3][400/693]	Time 0.004 (0.006)	Data 0.002 (0.003)	Loss 154.4973 (155.7888)	
2019-01-08 10:38:32,950 - 10 - training_embed.py - training - Epoch: [3][500/693]	Time 0.003 (0.006)	Data 0.002 (0.002)	Loss 154.8554 (155.8047)	
2019-01-08 10:38:33,490 - 10 - training_embed.py - training - Epoch: [3][600/693]	Time 0.007 (0.006)	Data 0.000 (0.002)	Loss 154.0847 (155.7584)	
2019-01-08 10:38:33,972 - 10 - training_embed.py - training - loss: 155.788911
2019-01-08 10:38:33,975 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:38:34,652 - 10 - training_embed.py - training - Epoch: [4][100/693]	Time 0.004 (0.006)	Data 0.002 (0.003)	Loss 157.0553 (155.7964)	
2019-01-08 10:38:35,164 - 10 - training_embed.py - training - Epoch: [4][200/693]	Time 0.003 (0.006)	Data 0.002 (0.003)	Loss 157.5128 (155.7685)	
2019-01-08 10:38:35,715 - 10 - training_embed.py - training - Epoch: [4][300/693]	Time 0.003 (0.006)	Data 0.002 (0.002)	Loss 156.8764 (155.7667)	
2019-01-08 10:38:36,243 - 10 - training_embed.py - training - Epoch: [4][400/693]	Time 0.005 (0.006)	Data 0.004 (0.002)	Loss 155.8381 (155.7315)	
2019-01-08 10:38:36,730 - 10 - training_embed.py - training - Epoch: [4][500/693]	Time 0.004 (0.006)	Data 0.002 (0.002)	Loss 155.7058 (155.7264)	
2019-01-08 10:38:37,223 - 10 - training_embed.py - training - Epoch: [4][600/693]	Time 0.005 (0.006)	Data 0.003 (0.002)	Loss 155.4368 (155.7392)	
2019-01-08 10:38:37,725 - 10 - training_embed.py - training - loss: 155.750367
2019-01-08 10:38:37,729 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:38:38,412 - 10 - training_embed.py - training - Epoch: [5][100/693]	Time 0.009 (0.006)	Data 0.002 (0.002)	Loss 157.6585 (155.7301)	
2019-01-08 10:38:38,939 - 10 - training_embed.py - training - Epoch: [5][200/693]	Time 0.008 (0.006)	Data 0.003 (0.002)	Loss 156.0603 (155.7297)	
2019-01-08 10:38:39,422 - 10 - training_embed.py - training - Epoch: [5][300/693]	Time 0.007 (0.006)	Data 0.005 (0.002)	Loss 155.8802 (155.7369)	
2019-01-08 10:38:39,947 - 10 - training_embed.py - training - Epoch: [5][400/693]	Time 0.003 (0.006)	Data 0.001 (0.002)	Loss 155.5517 (155.7479)	
2019-01-08 10:38:40,443 - 10 - training_embed.py - training - Epoch: [5][500/693]	Time 0.003 (0.006)	Data 0.001 (0.002)	Loss 156.6392 (155.7723)	
2019-01-08 10:38:40,949 - 10 - training_embed.py - training - Epoch: [5][600/693]	Time 0.003 (0.006)	Data 0.002 (0.002)	Loss 156.1709 (155.7554)	
2019-01-08 10:38:41,402 - 10 - training_embed.py - training - loss: 155.711818
2019-01-08 10:38:41,403 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:38:42,112 - 10 - training_embed.py - training - Epoch: [6][100/693]	Time 0.012 (0.006)	Data 0.008 (0.002)	Loss 156.4185 (155.8280)	
2019-01-08 10:38:42,651 - 10 - training_embed.py - training - Epoch: [6][200/693]	Time 0.007 (0.006)	Data 0.005 (0.002)	Loss 156.2561 (155.6349)	
2019-01-08 10:38:43,157 - 10 - training_embed.py - training - Epoch: [6][300/693]	Time 0.003 (0.006)	Data 0.001 (0.002)	Loss 154.5051 (155.6373)	
2019-01-08 10:38:43,732 - 10 - training_embed.py - training - Epoch: [6][400/693]	Time 0.002 (0.006)	Data 0.000 (0.002)	Loss 154.8483 (155.6886)	
2019-01-08 10:38:44,251 - 10 - training_embed.py - training - Epoch: [6][500/693]	Time 0.003 (0.006)	Data 0.001 (0.002)	Loss 156.6566 (155.6778)	
2019-01-08 10:38:44,794 - 10 - training_embed.py - training - Epoch: [6][600/693]	Time 0.007 (0.006)	Data 0.002 (0.002)	Loss 156.1922 (155.6783)	
2019-01-08 10:38:45,241 - 10 - training_embed.py - training - loss: 155.673403
2019-01-08 10:38:45,241 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:38:45,942 - 10 - training_embed.py - training - Epoch: [7][100/693]	Time 0.005 (0.006)	Data 0.001 (0.002)	Loss 155.7056 (155.6694)	
2019-01-08 10:38:46,462 - 10 - training_embed.py - training - Epoch: [7][200/693]	Time 0.004 (0.006)	Data 0.002 (0.002)	Loss 155.0627 (155.6116)	
2019-01-08 10:38:46,996 - 10 - training_embed.py - training - Epoch: [7][300/693]	Time 0.005 (0.006)	Data 0.003 (0.002)	Loss 157.9318 (155.6459)	
2019-01-08 10:38:47,494 - 10 - training_embed.py - training - Epoch: [7][400/693]	Time 0.003 (0.006)	Data 0.001 (0.002)	Loss 154.3520 (155.6489)	
2019-01-08 10:38:48,026 - 10 - training_embed.py - training - Epoch: [7][500/693]	Time 0.007 (0.006)	Data 0.002 (0.002)	Loss 155.4102 (155.6389)	
2019-01-08 10:38:48,554 - 10 - training_embed.py - training - Epoch: [7][600/693]	Time 0.004 (0.006)	Data 0.000 (0.002)	Loss 157.3933 (155.6607)	
2019-01-08 10:38:49,030 - 10 - training_embed.py - training - loss: 155.634910
2019-01-08 10:38:49,030 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:38:49,804 - 10 - training_embed.py - training - Epoch: [8][100/693]	Time 0.012 (0.006)	Data 0.010 (0.002)	Loss 154.9054 (155.6311)	
2019-01-08 10:38:50,386 - 10 - training_embed.py - training - Epoch: [8][200/693]	Time 0.003 (0.006)	Data 0.002 (0.002)	Loss 154.6238 (155.5535)	
2019-01-08 10:38:50,882 - 10 - training_embed.py - training - Epoch: [8][300/693]	Time 0.001 (0.006)	Data 0.000 (0.002)	Loss 156.1435 (155.5300)	
2019-01-08 10:38:51,393 - 10 - training_embed.py - training - Epoch: [8][400/693]	Time 0.005 (0.006)	Data 0.002 (0.002)	Loss 155.1086 (155.5462)	
2019-01-08 10:38:51,882 - 10 - training_embed.py - training - Epoch: [8][500/693]	Time 0.008 (0.006)	Data 0.006 (0.002)	Loss 153.7846 (155.5768)	
2019-01-08 10:38:52,342 - 10 - training_embed.py - training - Epoch: [8][600/693]	Time 0.012 (0.006)	Data 0.011 (0.002)	Loss 157.5492 (155.5916)	
2019-01-08 10:38:52,801 - 10 - training_embed.py - training - loss: 155.596682
2019-01-08 10:38:52,801 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:38:53,490 - 10 - training_embed.py - training - Epoch: [9][100/693]	Time 0.005 (0.006)	Data 0.000 (0.002)	Loss 155.2015 (155.5506)	
2019-01-08 10:38:54,015 - 10 - training_embed.py - training - Epoch: [9][200/693]	Time 0.002 (0.006)	Data 0.001 (0.002)	Loss 154.2574 (155.5085)	
2019-01-08 10:38:54,529 - 10 - training_embed.py - training - Epoch: [9][300/693]	Time 0.001 (0.006)	Data 0.000 (0.002)	Loss 155.5769 (155.5319)	
2019-01-08 10:38:55,058 - 10 - training_embed.py - training - Epoch: [9][400/693]	Time 0.006 (0.006)	Data 0.000 (0.002)	Loss 156.4547 (155.4960)	
2019-01-08 10:38:55,624 - 10 - training_embed.py - training - Epoch: [9][500/693]	Time 0.009 (0.006)	Data 0.000 (0.002)	Loss 153.1264 (155.5188)	
2019-01-08 10:38:56,142 - 10 - training_embed.py - training - Epoch: [9][600/693]	Time 0.002 (0.006)	Data 0.000 (0.002)	Loss 155.1006 (155.5658)	
2019-01-08 10:38:56,576 - 10 - training_embed.py - training - loss: 155.558180
2019-01-08 10:38:56,577 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:38:57,358 - 10 - training_embed.py - training - Epoch: [10][100/693]	Time 0.004 (0.006)	Data 0.002 (0.002)	Loss 155.6747 (155.6588)	
2019-01-08 10:38:57,891 - 10 - training_embed.py - training - Epoch: [10][200/693]	Time 0.006 (0.006)	Data 0.004 (0.002)	Loss 155.4495 (155.6359)	
2019-01-08 10:38:58,380 - 10 - training_embed.py - training - Epoch: [10][300/693]	Time 0.006 (0.006)	Data 0.005 (0.002)	Loss 156.7132 (155.6518)	
2019-01-08 10:38:58,858 - 10 - training_embed.py - training - Epoch: [10][400/693]	Time 0.002 (0.006)	Data 0.000 (0.002)	Loss 154.9512 (155.5980)	
2019-01-08 10:38:59,347 - 10 - training_embed.py - training - Epoch: [10][500/693]	Time 0.003 (0.006)	Data 0.001 (0.002)	Loss 156.9323 (155.5633)	
2019-01-08 10:38:59,868 - 10 - training_embed.py - training - Epoch: [10][600/693]	Time 0.008 (0.006)	Data 0.004 (0.002)	Loss 156.3327 (155.5407)	
2019-01-08 10:39:00,292 - 10 - training_embed.py - training - loss: 155.519591
2019-01-08 10:39:00,295 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:39:00,980 - 10 - training_embed.py - training - Epoch: [11][100/693]	Time 0.008 (0.006)	Data 0.000 (0.002)	Loss 154.5756 (155.4257)	
2019-01-08 10:39:01,410 - 10 - training_embed.py - training - Epoch: [11][200/693]	Time 0.004 (0.006)	Data 0.002 (0.002)	Loss 155.7327 (155.5255)	
2019-01-08 10:39:01,923 - 10 - training_embed.py - training - Epoch: [11][300/693]	Time 0.006 (0.006)	Data 0.004 (0.002)	Loss 156.8087 (155.5088)	
2019-01-08 10:39:02,399 - 10 - training_embed.py - training - Epoch: [11][400/693]	Time 0.006 (0.006)	Data 0.004 (0.002)	Loss 155.3504 (155.5462)	
2019-01-08 10:39:02,909 - 10 - training_embed.py - training - Epoch: [11][500/693]	Time 0.007 (0.006)	Data 0.002 (0.002)	Loss 154.6664 (155.5169)	
2019-01-08 10:39:03,432 - 10 - training_embed.py - training - Epoch: [11][600/693]	Time 0.005 (0.006)	Data 0.003 (0.002)	Loss 156.3671 (155.5051)	
2019-01-08 10:39:03,851 - 10 - training_embed.py - training - loss: 155.481296
2019-01-08 10:39:03,897 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:39:04,328 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 431.023 ms ~ 0.007 min ~ 0.431 sec
2019-01-08 10:39:04,689 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 792.477 ms ~ 0.013 min ~ 0.792 sec
2019-01-08 10:39:04,689 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:39:04,690 - 10 - corpus.py - subactivity_sampler - 0 / 187
2019-01-08 10:39:04,691 - 10 - corpus.py - subactivity_sampler - [46204. 57475. 31275. 42433.]
2019-01-08 10:39:10,110 - 10 - corpus.py - subactivity_sampler - 20 / 187
2019-01-08 10:39:10,110 - 10 - corpus.py - subactivity_sampler - [46143. 57645. 30744. 42855.]
2019-01-08 10:39:18,202 - 10 - corpus.py - subactivity_sampler - 40 / 187
2019-01-08 10:39:18,203 - 10 - corpus.py - subactivity_sampler - [46073. 57742. 30607. 42965.]
2019-01-08 10:39:24,144 - 10 - corpus.py - subactivity_sampler - 60 / 187
2019-01-08 10:39:24,145 - 10 - corpus.py - subactivity_sampler - [46054. 57820. 30368. 43145.]
2019-01-08 10:39:29,643 - 10 - corpus.py - subactivity_sampler - 80 / 187
2019-01-08 10:39:29,643 - 10 - corpus.py - subactivity_sampler - [46414. 57703. 30103. 43167.]
2019-01-08 10:39:35,703 - 10 - corpus.py - subactivity_sampler - 100 / 187
2019-01-08 10:39:35,704 - 10 - corpus.py - subactivity_sampler - [46389. 57850. 29828. 43320.]
2019-01-08 10:39:42,295 - 10 - corpus.py - subactivity_sampler - 120 / 187
2019-01-08 10:39:42,295 - 10 - corpus.py - subactivity_sampler - [46366. 57968. 29313. 43740.]
2019-01-08 10:39:49,239 - 10 - corpus.py - subactivity_sampler - 140 / 187
2019-01-08 10:39:49,239 - 10 - corpus.py - subactivity_sampler - [46223. 58548. 28807. 43809.]
2019-01-08 10:39:54,754 - 10 - corpus.py - subactivity_sampler - 160 / 187
2019-01-08 10:39:54,754 - 10 - corpus.py - subactivity_sampler - [46212. 58570. 28715. 43890.]
2019-01-08 10:39:59,701 - 10 - corpus.py - subactivity_sampler - 180 / 187
2019-01-08 10:39:59,701 - 10 - corpus.py - subactivity_sampler - [46175. 58953. 28470. 43789.]
2019-01-08 10:40:01,468 - 10 - corpus.py - subactivity_sampler - [46183. 58963. 28417. 43824.]
2019-01-08 10:40:01,468 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 56778.264 ms ~ 0.946 min ~ 56.778 sec
2019-01-08 10:40:01,468 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:40:01,988 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:40:01,988 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 4. 17.  5.]
2019-01-08 10:40:01,988 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:40:01,998 - 10 - corpus.py - rho_sampling - ['56.8093', '8.3047', '918.2213']
2019-01-08 10:40:01,998 - 10 - pipeline.py - baseline - Iteration 5
2019-01-08 10:40:02,042 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:40:02,045 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:40:02,045 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 24', '2: 2', '3: 25']
2019-01-08 10:40:02,049 - 10 - accuracy_class.py - mof_val - frames true: 79080	frames overall : 177387
2019-01-08 10:40:02,049 - 10 - corpus.py - accuracy_corpus - Action: milk
2019-01-08 10:40:02,049 - 10 - corpus.py - accuracy_corpus - MoF val: 0.44580493497268686
2019-01-08 10:40:02,049 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.44580493497268686
2019-01-08 10:40:02,049 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:40:02,050 - 10 - accuracy_class.py - mof_classes - label 2: 0.265132  11761 / 44359
2019-01-08 10:40:02,050 - 10 - accuracy_class.py - mof_classes - label 6: 0.986045  6006 / 6091
2019-01-08 10:40:02,050 - 10 - accuracy_class.py - mof_classes - label 24: 0.621145  36214 / 58302
2019-01-08 10:40:02,050 - 10 - accuracy_class.py - mof_classes - label 25: 0.615232  25099 / 40796
2019-01-08 10:40:02,050 - 10 - accuracy_class.py - mof_classes - average class mof: 0.497511
2019-01-08 10:40:02,050 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:40:02,050 - 10 - accuracy_class.py - iou_classes - label 2: 0.192756  11761 / 61015
2019-01-08 10:40:02,050 - 10 - accuracy_class.py - iou_classes - label 6: 0.129809  6006 / 46268
2019-01-08 10:40:02,050 - 10 - accuracy_class.py - iou_classes - label 24: 0.446805  36214 / 81051
2019-01-08 10:40:02,050 - 10 - accuracy_class.py - iou_classes - label 25: 0.421683  25099 / 59521
2019-01-08 10:40:02,050 - 10 - accuracy_class.py - iou_classes - average IoU: 0.297763
2019-01-08 10:40:02,050 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.238211
2019-01-08 10:40:04,406 - 10 - f1_score.py - f1 - f1 score: 0.441241
2019-01-08 10:40:04,413 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 2415.649 ms ~ 0.040 min ~ 2.416 sec
2019-01-08 10:40:04,413 - 10 - corpus.py - embedding_training - .
2019-01-08 10:40:04,413 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:40:04,414 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:40:04,414 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:40:05,552 - 10 - training_embed.py - training - create model
2019-01-08 10:40:05,553 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:40:05,553 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:40:05,894 - 10 - training_embed.py - training - Epoch: [0][100/693]	Time 0.001 (0.003)	Data 0.000 (0.002)	Loss 156.3325 (156.0720)	
2019-01-08 10:40:06,123 - 10 - training_embed.py - training - Epoch: [0][200/693]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 156.4090 (156.1034)	
2019-01-08 10:40:06,332 - 10 - training_embed.py - training - Epoch: [0][300/693]	Time 0.005 (0.003)	Data 0.001 (0.001)	Loss 155.2585 (156.0508)	
2019-01-08 10:40:06,549 - 10 - training_embed.py - training - Epoch: [0][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 158.4281 (156.0697)	
2019-01-08 10:40:06,774 - 10 - training_embed.py - training - Epoch: [0][500/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.0154 (156.0425)	
2019-01-08 10:40:06,988 - 10 - training_embed.py - training - Epoch: [0][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.9954 (156.0358)	
2019-01-08 10:40:07,193 - 10 - training_embed.py - training - loss: 155.988323
2019-01-08 10:40:07,194 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:40:07,579 - 10 - training_embed.py - training - Epoch: [1][100/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.6796 (155.8958)	
2019-01-08 10:40:07,809 - 10 - training_embed.py - training - Epoch: [1][200/693]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 156.8291 (155.9807)	
2019-01-08 10:40:08,033 - 10 - training_embed.py - training - Epoch: [1][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.4684 (155.9869)	
2019-01-08 10:40:08,251 - 10 - training_embed.py - training - Epoch: [1][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.8781 (155.9965)	
2019-01-08 10:40:08,484 - 10 - training_embed.py - training - Epoch: [1][500/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.4756 (155.9712)	
2019-01-08 10:40:08,698 - 10 - training_embed.py - training - Epoch: [1][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.5524 (155.9727)	
2019-01-08 10:40:08,891 - 10 - training_embed.py - training - loss: 155.946504
2019-01-08 10:40:08,891 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:40:09,235 - 10 - training_embed.py - training - Epoch: [2][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.1657 (155.9069)	
2019-01-08 10:40:09,455 - 10 - training_embed.py - training - Epoch: [2][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.1685 (155.8519)	
2019-01-08 10:40:09,686 - 10 - training_embed.py - training - Epoch: [2][300/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.4646 (155.9463)	
2019-01-08 10:40:09,909 - 10 - training_embed.py - training - Epoch: [2][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.2443 (155.8885)	
2019-01-08 10:40:10,135 - 10 - training_embed.py - training - Epoch: [2][500/693]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 156.8416 (155.8984)	
2019-01-08 10:40:10,359 - 10 - training_embed.py - training - Epoch: [2][600/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 156.1660 (155.8772)	
2019-01-08 10:40:10,571 - 10 - training_embed.py - training - loss: 155.905267
2019-01-08 10:40:10,572 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:40:10,942 - 10 - training_embed.py - training - Epoch: [3][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8583 (155.7581)	
2019-01-08 10:40:11,153 - 10 - training_embed.py - training - Epoch: [3][200/693]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 156.4007 (155.8631)	
2019-01-08 10:40:11,362 - 10 - training_embed.py - training - Epoch: [3][300/693]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 158.0635 (155.8626)	
2019-01-08 10:40:11,578 - 10 - training_embed.py - training - Epoch: [3][400/693]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 154.0620 (155.8676)	
2019-01-08 10:40:11,806 - 10 - training_embed.py - training - Epoch: [3][500/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.3773 (155.8745)	
2019-01-08 10:40:12,038 - 10 - training_embed.py - training - Epoch: [3][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.3095 (155.8368)	
2019-01-08 10:40:12,223 - 10 - training_embed.py - training - loss: 155.863758
2019-01-08 10:40:12,223 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:40:12,589 - 10 - training_embed.py - training - Epoch: [4][100/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 156.4475 (155.8835)	
2019-01-08 10:40:12,804 - 10 - training_embed.py - training - Epoch: [4][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.5369 (155.8161)	
2019-01-08 10:40:13,008 - 10 - training_embed.py - training - Epoch: [4][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.9883 (155.8155)	
2019-01-08 10:40:13,225 - 10 - training_embed.py - training - Epoch: [4][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.4227 (155.7869)	
2019-01-08 10:40:13,444 - 10 - training_embed.py - training - Epoch: [4][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.5777 (155.7914)	
2019-01-08 10:40:13,668 - 10 - training_embed.py - training - Epoch: [4][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.8296 (155.8023)	
2019-01-08 10:40:13,869 - 10 - training_embed.py - training - loss: 155.822154
2019-01-08 10:40:13,870 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:40:14,223 - 10 - training_embed.py - training - Epoch: [5][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.1715 (155.8040)	
2019-01-08 10:40:14,438 - 10 - training_embed.py - training - Epoch: [5][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.3672 (155.7962)	
2019-01-08 10:40:14,659 - 10 - training_embed.py - training - Epoch: [5][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.6914 (155.7896)	
2019-01-08 10:40:14,887 - 10 - training_embed.py - training - Epoch: [5][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.0109 (155.8068)	
2019-01-08 10:40:15,110 - 10 - training_embed.py - training - Epoch: [5][500/693]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 156.5455 (155.8250)	
2019-01-08 10:40:15,329 - 10 - training_embed.py - training - Epoch: [5][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.2523 (155.8239)	
2019-01-08 10:40:15,529 - 10 - training_embed.py - training - loss: 155.780695
2019-01-08 10:40:15,530 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:40:15,876 - 10 - training_embed.py - training - Epoch: [6][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.9166 (155.9224)	
2019-01-08 10:40:16,096 - 10 - training_embed.py - training - Epoch: [6][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.8974 (155.7009)	
2019-01-08 10:40:16,333 - 10 - training_embed.py - training - Epoch: [6][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.0507 (155.7124)	
2019-01-08 10:40:16,563 - 10 - training_embed.py - training - Epoch: [6][400/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.0317 (155.7491)	
2019-01-08 10:40:16,783 - 10 - training_embed.py - training - Epoch: [6][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.1438 (155.7360)	
2019-01-08 10:40:16,990 - 10 - training_embed.py - training - Epoch: [6][600/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.2481 (155.7451)	
2019-01-08 10:40:17,209 - 10 - training_embed.py - training - loss: 155.739272
2019-01-08 10:40:17,210 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:40:17,603 - 10 - training_embed.py - training - Epoch: [7][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.8088 (155.7529)	
2019-01-08 10:40:17,830 - 10 - training_embed.py - training - Epoch: [7][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8726 (155.6789)	
2019-01-08 10:40:18,036 - 10 - training_embed.py - training - Epoch: [7][300/693]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 158.3463 (155.7101)	
2019-01-08 10:40:18,250 - 10 - training_embed.py - training - Epoch: [7][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.2198 (155.6979)	
2019-01-08 10:40:18,480 - 10 - training_embed.py - training - Epoch: [7][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.2761 (155.7079)	
2019-01-08 10:40:18,712 - 10 - training_embed.py - training - Epoch: [7][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.3701 (155.7213)	
2019-01-08 10:40:18,904 - 10 - training_embed.py - training - loss: 155.697748
2019-01-08 10:40:18,904 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:40:19,248 - 10 - training_embed.py - training - Epoch: [8][100/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 155.6667 (155.6418)	
2019-01-08 10:40:19,473 - 10 - training_embed.py - training - Epoch: [8][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.3208 (155.6039)	
2019-01-08 10:40:19,705 - 10 - training_embed.py - training - Epoch: [8][300/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 156.0732 (155.5847)	
2019-01-08 10:40:19,931 - 10 - training_embed.py - training - Epoch: [8][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.2874 (155.6050)	
2019-01-08 10:40:20,150 - 10 - training_embed.py - training - Epoch: [8][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.1073 (155.6342)	
2019-01-08 10:40:20,364 - 10 - training_embed.py - training - Epoch: [8][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 158.1507 (155.6546)	
2019-01-08 10:40:20,561 - 10 - training_embed.py - training - loss: 155.656528
2019-01-08 10:40:20,561 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:40:20,943 - 10 - training_embed.py - training - Epoch: [9][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.7756 (155.6395)	
2019-01-08 10:40:21,147 - 10 - training_embed.py - training - Epoch: [9][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.7788 (155.5823)	
2019-01-08 10:40:21,359 - 10 - training_embed.py - training - Epoch: [9][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.3899 (155.5846)	
2019-01-08 10:40:21,589 - 10 - training_embed.py - training - Epoch: [9][400/693]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 155.9347 (155.5497)	
2019-01-08 10:40:21,795 - 10 - training_embed.py - training - Epoch: [9][500/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 153.2800 (155.5707)	
2019-01-08 10:40:22,014 - 10 - training_embed.py - training - Epoch: [9][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8364 (155.6119)	
2019-01-08 10:40:22,225 - 10 - training_embed.py - training - loss: 155.615108
2019-01-08 10:40:22,226 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:40:22,585 - 10 - training_embed.py - training - Epoch: [10][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.5365 (155.7153)	
2019-01-08 10:40:22,809 - 10 - training_embed.py - training - Epoch: [10][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.6435 (155.6945)	
2019-01-08 10:40:23,030 - 10 - training_embed.py - training - Epoch: [10][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.6309 (155.6958)	
2019-01-08 10:40:23,253 - 10 - training_embed.py - training - Epoch: [10][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.1433 (155.6289)	
2019-01-08 10:40:23,469 - 10 - training_embed.py - training - Epoch: [10][500/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 157.7633 (155.6010)	
2019-01-08 10:40:23,683 - 10 - training_embed.py - training - Epoch: [10][600/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 157.0554 (155.5959)	
2019-01-08 10:40:23,885 - 10 - training_embed.py - training - loss: 155.573502
2019-01-08 10:40:23,886 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:40:24,242 - 10 - training_embed.py - training - Epoch: [11][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.3256 (155.4456)	
2019-01-08 10:40:24,458 - 10 - training_embed.py - training - Epoch: [11][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.9326 (155.5730)	
2019-01-08 10:40:24,681 - 10 - training_embed.py - training - Epoch: [11][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.0412 (155.5606)	
2019-01-08 10:40:24,912 - 10 - training_embed.py - training - Epoch: [11][400/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.5416 (155.5891)	
2019-01-08 10:40:25,133 - 10 - training_embed.py - training - Epoch: [11][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.7952 (155.5586)	
2019-01-08 10:40:25,337 - 10 - training_embed.py - training - Epoch: [11][600/693]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 155.5186 (155.5542)	
2019-01-08 10:40:25,537 - 10 - training_embed.py - training - loss: 155.532364
2019-01-08 10:40:25,563 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:40:25,847 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 283.262 ms ~ 0.005 min ~ 0.283 sec
2019-01-08 10:40:26,070 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 506.240 ms ~ 0.008 min ~ 0.506 sec
2019-01-08 10:40:26,070 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:40:26,070 - 10 - corpus.py - subactivity_sampler - 0 / 187
2019-01-08 10:40:26,070 - 10 - corpus.py - subactivity_sampler - [46183. 58963. 28417. 43824.]
2019-01-08 10:40:30,649 - 10 - corpus.py - subactivity_sampler - 20 / 187
2019-01-08 10:40:30,650 - 10 - corpus.py - subactivity_sampler - [46183. 59002. 28171. 44031.]
2019-01-08 10:40:37,516 - 10 - corpus.py - subactivity_sampler - 40 / 187
2019-01-08 10:40:37,516 - 10 - corpus.py - subactivity_sampler - [46161. 59097. 27781. 44348.]
2019-01-08 10:40:42,419 - 10 - corpus.py - subactivity_sampler - 60 / 187
2019-01-08 10:40:42,420 - 10 - corpus.py - subactivity_sampler - [46046. 59235. 27682. 44424.]
2019-01-08 10:40:47,125 - 10 - corpus.py - subactivity_sampler - 80 / 187
2019-01-08 10:40:47,125 - 10 - corpus.py - subactivity_sampler - [46026. 59260. 27292. 44809.]
2019-01-08 10:40:52,902 - 10 - corpus.py - subactivity_sampler - 100 / 187
2019-01-08 10:40:52,902 - 10 - corpus.py - subactivity_sampler - [46007. 59321. 27155. 44904.]
2019-01-08 10:40:59,473 - 10 - corpus.py - subactivity_sampler - 120 / 187
2019-01-08 10:40:59,474 - 10 - corpus.py - subactivity_sampler - [45983. 59365. 26896. 45143.]
2019-01-08 10:41:06,046 - 10 - corpus.py - subactivity_sampler - 140 / 187
2019-01-08 10:41:06,046 - 10 - corpus.py - subactivity_sampler - [45965. 59390. 26868. 45164.]
2019-01-08 10:41:11,512 - 10 - corpus.py - subactivity_sampler - 160 / 187
2019-01-08 10:41:11,512 - 10 - corpus.py - subactivity_sampler - [45962. 59529. 26654. 45242.]
2019-01-08 10:41:16,344 - 10 - corpus.py - subactivity_sampler - 180 / 187
2019-01-08 10:41:16,344 - 10 - corpus.py - subactivity_sampler - [45931. 59568. 26461. 45427.]
2019-01-08 10:41:18,100 - 10 - corpus.py - subactivity_sampler - [45906. 59600. 26449. 45432.]
2019-01-08 10:41:18,100 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 52030.381 ms ~ 0.867 min ~ 52.030 sec
2019-01-08 10:41:18,100 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:41:19,676 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:41:19,676 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 4. 20.  0.]
2019-01-08 10:41:19,676 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:41:19,689 - 10 - corpus.py - rho_sampling - ['54.4280', '21.6726', '52.6366']
2019-01-08 10:41:19,689 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 10:41:19,739 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:41:19,742 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:41:19,742 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 24', '2: 2', '3: 25']
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - mof_val - frames true: 78790	frames overall : 177387
2019-01-08 10:41:19,747 - 10 - corpus.py - accuracy_corpus - Action: milk
2019-01-08 10:41:19,747 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4441700913821193
2019-01-08 10:41:19,747 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4441700913821193
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - mof_classes - label 2: 0.247684  10987 / 44359
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - mof_classes - label 6: 0.981448  5978 / 6091
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - mof_classes - label 24: 0.621745  36249 / 58302
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - mof_classes - label 25: 0.626924  25576 / 40796
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - mof_classes - average class mof: 0.495560
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - iou_classes - label 2: 0.183665  10987 / 59821
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - iou_classes - label 6: 0.129903  5978 / 46019
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - iou_classes - label 24: 0.443940  36249 / 81653
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - iou_classes - label 25: 0.421684  25576 / 60652
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - iou_classes - average IoU: 0.294798
2019-01-08 10:41:19,747 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.235838
2019-01-08 10:41:22,075 - 10 - f1_score.py - f1 - f1 score: 0.439305
2019-01-08 10:41:22,082 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 2392.969 ms ~ 0.040 min ~ 2.393 sec
2019-01-08 10:41:22,082 - 10 - corpus.py - embedding_training - .
2019-01-08 10:41:22,082 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:41:22,082 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:41:22,082 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:41:23,222 - 10 - training_embed.py - training - create model
2019-01-08 10:41:23,223 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:41:23,224 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:41:23,575 - 10 - training_embed.py - training - Epoch: [0][100/693]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 157.0810 (156.0477)	
2019-01-08 10:41:23,789 - 10 - training_embed.py - training - Epoch: [0][200/693]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 156.7986 (156.1168)	
2019-01-08 10:41:24,012 - 10 - training_embed.py - training - Epoch: [0][300/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.4280 (156.0700)	
2019-01-08 10:41:24,229 - 10 - training_embed.py - training - Epoch: [0][400/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 158.3691 (156.0955)	
2019-01-08 10:41:24,447 - 10 - training_embed.py - training - Epoch: [0][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.0857 (156.0720)	
2019-01-08 10:41:24,674 - 10 - training_embed.py - training - Epoch: [0][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.2755 (156.0624)	
2019-01-08 10:41:24,862 - 10 - training_embed.py - training - loss: 156.018991
2019-01-08 10:41:24,862 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:41:25,215 - 10 - training_embed.py - training - Epoch: [1][100/693]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 155.7142 (155.9042)	
2019-01-08 10:41:25,425 - 10 - training_embed.py - training - Epoch: [1][200/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 156.7561 (156.0046)	
2019-01-08 10:41:25,655 - 10 - training_embed.py - training - Epoch: [1][300/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.8450 (156.0230)	
2019-01-08 10:41:25,881 - 10 - training_embed.py - training - Epoch: [1][400/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.8558 (156.0382)	
2019-01-08 10:41:26,098 - 10 - training_embed.py - training - Epoch: [1][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.6952 (156.0042)	
2019-01-08 10:41:26,318 - 10 - training_embed.py - training - Epoch: [1][600/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 156.5775 (156.0032)	
2019-01-08 10:41:26,521 - 10 - training_embed.py - training - loss: 155.975111
2019-01-08 10:41:26,521 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:41:26,894 - 10 - training_embed.py - training - Epoch: [2][100/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.1826 (155.9498)	
2019-01-08 10:41:27,126 - 10 - training_embed.py - training - Epoch: [2][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.3704 (155.8619)	
2019-01-08 10:41:27,350 - 10 - training_embed.py - training - Epoch: [2][300/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.7869 (155.9484)	
2019-01-08 10:41:27,589 - 10 - training_embed.py - training - Epoch: [2][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.4111 (155.8968)	
2019-01-08 10:41:27,804 - 10 - training_embed.py - training - Epoch: [2][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.1500 (155.9130)	
2019-01-08 10:41:28,028 - 10 - training_embed.py - training - Epoch: [2][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.8133 (155.8881)	
2019-01-08 10:41:28,223 - 10 - training_embed.py - training - loss: 155.931500
2019-01-08 10:41:28,224 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:41:28,579 - 10 - training_embed.py - training - Epoch: [3][100/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.0934 (155.8087)	
2019-01-08 10:41:28,788 - 10 - training_embed.py - training - Epoch: [3][200/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 156.6958 (155.9202)	
2019-01-08 10:41:29,012 - 10 - training_embed.py - training - Epoch: [3][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.9541 (155.8852)	
2019-01-08 10:41:29,229 - 10 - training_embed.py - training - Epoch: [3][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.3168 (155.8989)	
2019-01-08 10:41:29,456 - 10 - training_embed.py - training - Epoch: [3][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.3419 (155.9105)	
2019-01-08 10:41:29,674 - 10 - training_embed.py - training - Epoch: [3][600/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.3048 (155.8604)	
2019-01-08 10:41:29,888 - 10 - training_embed.py - training - loss: 155.887749
2019-01-08 10:41:29,889 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:41:30,254 - 10 - training_embed.py - training - Epoch: [4][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.9996 (155.9632)	
2019-01-08 10:41:30,488 - 10 - training_embed.py - training - Epoch: [4][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.2699 (155.8886)	
2019-01-08 10:41:30,709 - 10 - training_embed.py - training - Epoch: [4][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.4247 (155.8546)	
2019-01-08 10:41:30,939 - 10 - training_embed.py - training - Epoch: [4][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.9546 (155.8276)	
2019-01-08 10:41:31,158 - 10 - training_embed.py - training - Epoch: [4][500/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.7023 (155.8200)	
2019-01-08 10:41:31,377 - 10 - training_embed.py - training - Epoch: [4][600/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 156.0665 (155.8328)	
2019-01-08 10:41:31,574 - 10 - training_embed.py - training - loss: 155.844044
2019-01-08 10:41:31,574 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:41:31,956 - 10 - training_embed.py - training - Epoch: [5][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.7775 (155.8347)	
2019-01-08 10:41:32,176 - 10 - training_embed.py - training - Epoch: [5][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.4729 (155.8507)	
2019-01-08 10:41:32,403 - 10 - training_embed.py - training - Epoch: [5][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.6179 (155.8204)	
2019-01-08 10:41:32,636 - 10 - training_embed.py - training - Epoch: [5][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.8175 (155.8265)	
2019-01-08 10:41:32,851 - 10 - training_embed.py - training - Epoch: [5][500/693]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 156.0127 (155.8501)	
2019-01-08 10:41:33,089 - 10 - training_embed.py - training - Epoch: [5][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.2479 (155.8441)	
2019-01-08 10:41:33,285 - 10 - training_embed.py - training - loss: 155.800307
2019-01-08 10:41:33,285 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:41:33,652 - 10 - training_embed.py - training - Epoch: [6][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.2245 (155.9109)	
2019-01-08 10:41:33,876 - 10 - training_embed.py - training - Epoch: [6][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.6280 (155.7396)	
2019-01-08 10:41:34,111 - 10 - training_embed.py - training - Epoch: [6][300/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.1523 (155.7411)	
2019-01-08 10:41:34,323 - 10 - training_embed.py - training - Epoch: [6][400/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.2794 (155.7781)	
2019-01-08 10:41:34,540 - 10 - training_embed.py - training - Epoch: [6][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.3265 (155.7606)	
2019-01-08 10:41:34,764 - 10 - training_embed.py - training - Epoch: [6][600/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.9242 (155.7647)	
2019-01-08 10:41:34,959 - 10 - training_embed.py - training - loss: 155.756779
2019-01-08 10:41:34,960 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:41:35,317 - 10 - training_embed.py - training - Epoch: [7][100/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 155.4099 (155.7302)	
2019-01-08 10:41:35,540 - 10 - training_embed.py - training - Epoch: [7][200/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 154.5083 (155.6763)	
2019-01-08 10:41:35,753 - 10 - training_embed.py - training - Epoch: [7][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 158.0840 (155.7008)	
2019-01-08 10:41:35,992 - 10 - training_embed.py - training - Epoch: [7][400/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 154.0677 (155.7158)	
2019-01-08 10:41:36,229 - 10 - training_embed.py - training - Epoch: [7][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.0811 (155.7201)	
2019-01-08 10:41:36,448 - 10 - training_embed.py - training - Epoch: [7][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.2941 (155.7405)	
2019-01-08 10:41:36,654 - 10 - training_embed.py - training - loss: 155.713017
2019-01-08 10:41:36,655 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:41:37,017 - 10 - training_embed.py - training - Epoch: [8][100/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 156.4768 (155.7313)	
2019-01-08 10:41:37,239 - 10 - training_embed.py - training - Epoch: [8][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.5127 (155.6520)	
2019-01-08 10:41:37,453 - 10 - training_embed.py - training - Epoch: [8][300/693]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 155.8293 (155.6267)	
2019-01-08 10:41:37,680 - 10 - training_embed.py - training - Epoch: [8][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.3856 (155.6305)	
2019-01-08 10:41:37,910 - 10 - training_embed.py - training - Epoch: [8][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.1630 (155.6543)	
2019-01-08 10:41:38,126 - 10 - training_embed.py - training - Epoch: [8][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.9626 (155.6698)	
2019-01-08 10:41:38,322 - 10 - training_embed.py - training - loss: 155.669583
2019-01-08 10:41:38,323 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:41:38,687 - 10 - training_embed.py - training - Epoch: [9][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.7208 (155.6179)	
2019-01-08 10:41:38,889 - 10 - training_embed.py - training - Epoch: [9][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.4091 (155.5888)	
2019-01-08 10:41:39,120 - 10 - training_embed.py - training - Epoch: [9][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.3176 (155.5829)	
2019-01-08 10:41:39,336 - 10 - training_embed.py - training - Epoch: [9][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.8591 (155.5496)	
2019-01-08 10:41:39,562 - 10 - training_embed.py - training - Epoch: [9][500/693]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 153.7599 (155.5865)	
2019-01-08 10:41:39,787 - 10 - training_embed.py - training - Epoch: [9][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9569 (155.6281)	
2019-01-08 10:41:39,989 - 10 - training_embed.py - training - loss: 155.625975
2019-01-08 10:41:39,989 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:41:40,346 - 10 - training_embed.py - training - Epoch: [10][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.6300 (155.7224)	
2019-01-08 10:41:40,560 - 10 - training_embed.py - training - Epoch: [10][200/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.6449 (155.7107)	
2019-01-08 10:41:40,791 - 10 - training_embed.py - training - Epoch: [10][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 156.4300 (155.7029)	
2019-01-08 10:41:41,029 - 10 - training_embed.py - training - Epoch: [10][400/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.9552 (155.6402)	
2019-01-08 10:41:41,241 - 10 - training_embed.py - training - Epoch: [10][500/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 157.1705 (155.6168)	
2019-01-08 10:41:41,468 - 10 - training_embed.py - training - Epoch: [10][600/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.2981 (155.6141)	
2019-01-08 10:41:41,669 - 10 - training_embed.py - training - loss: 155.582104
2019-01-08 10:41:41,669 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:41:42,054 - 10 - training_embed.py - training - Epoch: [11][100/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 154.8796 (155.4803)	
2019-01-08 10:41:42,255 - 10 - training_embed.py - training - Epoch: [11][200/693]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 155.4210 (155.5832)	
2019-01-08 10:41:42,471 - 10 - training_embed.py - training - Epoch: [11][300/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 157.1403 (155.5676)	
2019-01-08 10:41:42,682 - 10 - training_embed.py - training - Epoch: [11][400/693]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 155.7033 (155.5985)	
2019-01-08 10:41:42,908 - 10 - training_embed.py - training - Epoch: [11][500/693]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 155.0161 (155.5757)	
2019-01-08 10:41:43,139 - 10 - training_embed.py - training - Epoch: [11][600/693]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 156.0586 (155.5735)	
2019-01-08 10:41:43,337 - 10 - training_embed.py - training - loss: 155.538762
2019-01-08 10:41:43,366 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:41:43,698 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 331.506 ms ~ 0.006 min ~ 0.332 sec
2019-01-08 10:41:43,970 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 604.484 ms ~ 0.010 min ~ 0.604 sec
2019-01-08 10:41:43,971 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:41:43,971 - 10 - corpus.py - subactivity_sampler - 0 / 187
2019-01-08 10:41:43,971 - 10 - corpus.py - subactivity_sampler - [45906. 59600. 26449. 45432.]
2019-01-08 10:41:48,664 - 10 - corpus.py - subactivity_sampler - 20 / 187
2019-01-08 10:41:48,664 - 10 - corpus.py - subactivity_sampler - [45892. 59622. 26380. 45493.]
2019-01-08 10:41:55,682 - 10 - corpus.py - subactivity_sampler - 40 / 187
2019-01-08 10:41:55,682 - 10 - corpus.py - subactivity_sampler - [45884. 59649. 26244. 45610.]
2019-01-08 10:42:00,637 - 10 - corpus.py - subactivity_sampler - 60 / 187
2019-01-08 10:42:00,638 - 10 - corpus.py - subactivity_sampler - [45862. 59672. 26206. 45647.]
2019-01-08 10:42:05,345 - 10 - corpus.py - subactivity_sampler - 80 / 187
2019-01-08 10:42:05,346 - 10 - corpus.py - subactivity_sampler - [45849. 59723. 26124. 45691.]
2019-01-08 10:42:11,310 - 10 - corpus.py - subactivity_sampler - 100 / 187
2019-01-08 10:42:11,310 - 10 - corpus.py - subactivity_sampler - [45845. 59726. 26076. 45740.]
2019-01-08 10:42:17,890 - 10 - corpus.py - subactivity_sampler - 120 / 187
2019-01-08 10:42:17,890 - 10 - corpus.py - subactivity_sampler - [45838. 59729. 26025. 45795.]
2019-01-08 10:42:24,480 - 10 - corpus.py - subactivity_sampler - 140 / 187
2019-01-08 10:42:24,480 - 10 - corpus.py - subactivity_sampler - [45837. 59846. 25910. 45794.]
2019-01-08 10:42:30,018 - 10 - corpus.py - subactivity_sampler - 160 / 187
2019-01-08 10:42:30,018 - 10 - corpus.py - subactivity_sampler - [45827. 59880. 25910. 45770.]
2019-01-08 10:42:34,888 - 10 - corpus.py - subactivity_sampler - 180 / 187
2019-01-08 10:42:34,889 - 10 - corpus.py - subactivity_sampler - [45825. 59878. 25849. 45835.]
2019-01-08 10:42:36,664 - 10 - corpus.py - subactivity_sampler - [45822. 59880. 25846. 45839.]
2019-01-08 10:42:36,664 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 52693.762 ms ~ 0.878 min ~ 52.694 sec
2019-01-08 10:42:36,664 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:42:38,282 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:42:38,284 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 4. 20.  4.]
2019-01-08 10:42:38,284 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:42:38,290 - 10 - corpus.py - rho_sampling - ['55.2217', '0.6574', '13.6290']
2019-01-08 10:42:38,290 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 10:42:38,339 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:42:38,342 - 10 - accuracy_class.py - mof - # gt_labels: 5   # pr_labels: 4
2019-01-08 10:42:38,342 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 6', '1: 24', '2: 2', '3: 25']
2019-01-08 10:42:38,346 - 10 - accuracy_class.py - mof_val - frames true: 78299	frames overall : 177387
2019-01-08 10:42:38,346 - 10 - corpus.py - accuracy_corpus - Action: milk
2019-01-08 10:42:38,346 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4414021320615378
2019-01-08 10:42:38,346 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4414021320615378
2019-01-08 10:42:38,346 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:42:38,346 - 10 - accuracy_class.py - mof_classes - label 2: 0.233662  10365 / 44359
2019-01-08 10:42:38,346 - 10 - accuracy_class.py - mof_classes - label 6: 0.977672  5955 / 6091
2019-01-08 10:42:38,346 - 10 - accuracy_class.py - mof_classes - label 24: 0.623461  36349 / 58302
2019-01-08 10:42:38,346 - 10 - accuracy_class.py - mof_classes - label 25: 0.628248  25630 / 40796
2019-01-08 10:42:38,346 - 10 - accuracy_class.py - mof_classes - average class mof: 0.492608
2019-01-08 10:42:38,346 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 27839
2019-01-08 10:42:38,346 - 10 - accuracy_class.py - iou_classes - label 2: 0.173212  10365 / 59840
2019-01-08 10:42:38,346 - 10 - accuracy_class.py - iou_classes - label 6: 0.129575  5955 / 45958
2019-01-08 10:42:38,346 - 10 - accuracy_class.py - iou_classes - label 24: 0.444185  36349 / 81833
2019-01-08 10:42:38,346 - 10 - accuracy_class.py - iou_classes - label 25: 0.420129  25630 / 61005
2019-01-08 10:42:38,347 - 10 - accuracy_class.py - iou_classes - average IoU: 0.291775
2019-01-08 10:42:38,347 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.233420
2019-01-08 10:42:40,674 - 10 - f1_score.py - f1 - f1 score: 0.435115
2019-01-08 10:42:40,681 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 2390.589 ms ~ 0.040 min ~ 2.391 sec
2019-01-08 10:42:40,689 - 10 - utils.py - wrap - <function baseline at 0x7f3991d24e18> took 638347.826 ms ~ 10.639 min ~ 638.348 sec
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - batch_size: 256
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - bg: False
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - bg_trh: 85
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - data: /media/data/kukleva/lab/Breakfast/feat/s1
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - data_type: 2
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - dataset: bf
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - dataset_root: /media/data/kukleva/lab/Breakfast
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - embed_dim: 30
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - epochs: 12
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - ext: txt
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - feature_dim: 64
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - full: True
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - gmm: 1
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - gmms: one
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - gt: /media/data/kukleva/lab/Breakfast/feat/s1/groundTruth
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - gt_training: False
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - log: DEBUG
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - log_str: slim.mallow._juice_!bg_data2_bf_embed30_epochs12_full_gmm1_one_!gt_lr1e-10_!lr_ordering_reg0.1_!viterbi_!zeros_
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - lr: 1e-10
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - lr_adj: False
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - momentum: 0.9
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - num_workers: 4
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - ordering: True
2019-01-08 10:42:40,690 - 10 - utils.py - update_opt_str - prefix: slim.mallow.
2019-01-08 10:42:40,691 - 10 - utils.py - update_opt_str - reg_cov: 0.1
2019-01-08 10:42:40,691 - 10 - utils.py - update_opt_str - resume: False
2019-01-08 10:42:40,691 - 10 - utils.py - update_opt_str - resume_str: 
2019-01-08 10:42:40,691 - 10 - utils.py - update_opt_str - save_embed_feat: False
2019-01-08 10:42:40,691 - 10 - utils.py - update_opt_str - save_likelihood: False
2019-01-08 10:42:40,691 - 10 - utils.py - update_opt_str - save_model: False
2019-01-08 10:42:40,691 - 10 - utils.py - update_opt_str - seed: 0
2019-01-08 10:42:40,691 - 10 - utils.py - update_opt_str - subaction: juice
2019-01-08 10:42:40,691 - 10 - utils.py - update_opt_str - vis: False
2019-01-08 10:42:40,691 - 10 - utils.py - update_opt_str - viterbi: False
2019-01-08 10:42:40,691 - 10 - utils.py - update_opt_str - weight_decay: 0.0001
2019-01-08 10:42:40,691 - 10 - utils.py - update_opt_str - zeros: False
2019-01-08 10:42:40,708 - 10 - utils.py - wrap - <function GroundTruth.load_gt at 0x7f39322b36a8> took 16.635 ms ~ 0.000 min ~ 0.017 sec
2019-01-08 10:42:40,808 - 10 - corpus.py - __init__ - juice  subactions: 7
2019-01-08 10:42:40,809 - 10 - corpus.py - _init_videos - .
2019-01-08 10:42:49,675 - 10 - corpus.py - _init_videos - gt statistic: Counter({20: 137048, 21: 47064, 19: 30798, 22: 16397, 14: 5622, 23: 2972, 18: 1561})
2019-01-08 10:42:49,675 - 10 - corpus.py - _update_fg_mask - .
2019-01-08 10:42:49,698 - 10 - corpus.py - __init__ - min: -43.612568  max: 34.947136  avg: 0.166101
2019-01-08 10:42:49,698 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:42:49,739 - 10 - corpus.py - rho_sampling - ['64.0767', '103.1747', '929.0535', '90.9104', '17.1809', '164.1557']
2019-01-08 10:42:49,739 - 10 - pipeline.py - baseline - Iteration 0
2019-01-08 10:42:49,816 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:42:49,820 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 10:42:49,820 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 19', '1: 23', '2: 14', '3: 20', '4: 18', '5: 22', '6: 21']
2019-01-08 10:42:49,828 - 10 - accuracy_class.py - mof_val - frames true: 77662	frames overall : 241462
2019-01-08 10:42:49,828 - 10 - corpus.py - accuracy_corpus - Action: juice
2019-01-08 10:42:49,828 - 10 - corpus.py - accuracy_corpus - MoF val: 0.32163238936147304
2019-01-08 10:42:49,828 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.0
2019-01-08 10:42:49,828 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:42:49,828 - 10 - accuracy_class.py - mof_classes - label 14: 0.027243  92 / 3377
2019-01-08 10:42:49,828 - 10 - accuracy_class.py - mof_classes - label 18: 0.000000  0 / 1411
2019-01-08 10:42:49,828 - 10 - accuracy_class.py - mof_classes - label 19: 0.610624  14059 / 23024
2019-01-08 10:42:49,828 - 10 - accuracy_class.py - mof_classes - label 20: 0.241338  32717 / 135565
2019-01-08 10:42:49,828 - 10 - accuracy_class.py - mof_classes - label 21: 0.551691  20844 / 37782
2019-01-08 10:42:49,828 - 10 - accuracy_class.py - mof_classes - label 22: 0.588299  8748 / 14870
2019-01-08 10:42:49,828 - 10 - accuracy_class.py - mof_classes - label 23: 0.423986  1202 / 2835
2019-01-08 10:42:49,828 - 10 - accuracy_class.py - mof_classes - average class mof: 0.305398
2019-01-08 10:42:49,828 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:42:49,828 - 10 - accuracy_class.py - iou_classes - label 14: 0.002433  92 / 37809
2019-01-08 10:42:49,828 - 10 - accuracy_class.py - iou_classes - label 18: 0.000000  0 / 35876
2019-01-08 10:42:49,829 - 10 - accuracy_class.py - iou_classes - label 19: 0.322995  14059 / 43527
2019-01-08 10:42:49,829 - 10 - accuracy_class.py - iou_classes - label 20: 0.238207  32717 / 137347
2019-01-08 10:42:49,829 - 10 - accuracy_class.py - iou_classes - label 21: 0.405857  20844 / 51358
2019-01-08 10:42:49,829 - 10 - accuracy_class.py - iou_classes - label 22: 0.215611  8748 / 40573
2019-01-08 10:42:49,829 - 10 - accuracy_class.py - iou_classes - label 23: 0.033228  1202 / 36174
2019-01-08 10:42:49,829 - 10 - accuracy_class.py - iou_classes - average IoU: 0.174047
2019-01-08 10:42:49,829 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.152291
2019-01-08 10:42:52,907 - 10 - f1_score.py - f1 - f1 score: 0.326192
2019-01-08 10:42:52,918 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3179.414 ms ~ 0.053 min ~ 3.179 sec
2019-01-08 10:42:52,918 - 10 - corpus.py - embedding_training - .
2019-01-08 10:42:52,918 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:42:52,919 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:42:52,919 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:42:54,258 - 10 - training_embed.py - training - create model
2019-01-08 10:42:54,259 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:42:54,259 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:42:54,584 - 10 - training_embed.py - training - Epoch: [0][100/944]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 303.1122 (308.0287)	
2019-01-08 10:42:54,796 - 10 - training_embed.py - training - Epoch: [0][200/944]	Time 0.003 (0.003)	Data 0.002 (0.002)	Loss 308.0568 (308.1691)	
2019-01-08 10:42:55,005 - 10 - training_embed.py - training - Epoch: [0][300/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.2763 (308.1337)	
2019-01-08 10:42:55,215 - 10 - training_embed.py - training - Epoch: [0][400/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.5452 (308.1415)	
2019-01-08 10:42:55,430 - 10 - training_embed.py - training - Epoch: [0][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.2265 (308.1055)	
2019-01-08 10:42:55,645 - 10 - training_embed.py - training - Epoch: [0][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.4512 (308.1628)	
2019-01-08 10:42:55,864 - 10 - training_embed.py - training - Epoch: [0][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5062 (308.1817)	
2019-01-08 10:42:56,068 - 10 - training_embed.py - training - Epoch: [0][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3394 (308.2280)	
2019-01-08 10:42:56,288 - 10 - training_embed.py - training - Epoch: [0][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.5135 (308.2275)	
2019-01-08 10:42:56,373 - 10 - training_embed.py - training - loss: 308.162594
2019-01-08 10:42:56,374 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:42:56,752 - 10 - training_embed.py - training - Epoch: [1][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4513 (307.9393)	
2019-01-08 10:42:56,957 - 10 - training_embed.py - training - Epoch: [1][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.1753 (308.2073)	
2019-01-08 10:42:57,171 - 10 - training_embed.py - training - Epoch: [1][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 312.8985 (308.1910)	
2019-01-08 10:42:57,376 - 10 - training_embed.py - training - Epoch: [1][400/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.3899 (308.1626)	
2019-01-08 10:42:57,600 - 10 - training_embed.py - training - Epoch: [1][500/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.1535 (308.1736)	
2019-01-08 10:42:57,812 - 10 - training_embed.py - training - Epoch: [1][600/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 311.1045 (308.1850)	
2019-01-08 10:42:58,028 - 10 - training_embed.py - training - Epoch: [1][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8485 (308.1604)	
2019-01-08 10:42:58,239 - 10 - training_embed.py - training - Epoch: [1][800/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.8766 (308.1787)	
2019-01-08 10:42:58,450 - 10 - training_embed.py - training - Epoch: [1][900/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.7024 (308.1518)	
2019-01-08 10:42:58,535 - 10 - training_embed.py - training - loss: 308.103508
2019-01-08 10:42:58,536 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:42:58,884 - 10 - training_embed.py - training - Epoch: [2][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.9305 (307.8866)	
2019-01-08 10:42:59,086 - 10 - training_embed.py - training - Epoch: [2][200/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.5056 (308.0917)	
2019-01-08 10:42:59,296 - 10 - training_embed.py - training - Epoch: [2][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4176 (308.0729)	
2019-01-08 10:42:59,503 - 10 - training_embed.py - training - Epoch: [2][400/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.2802 (308.0999)	
2019-01-08 10:42:59,715 - 10 - training_embed.py - training - Epoch: [2][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2355 (308.0900)	
2019-01-08 10:42:59,922 - 10 - training_embed.py - training - Epoch: [2][600/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.4681 (308.1157)	
2019-01-08 10:43:00,134 - 10 - training_embed.py - training - Epoch: [2][700/944]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 303.9810 (308.1463)	
2019-01-08 10:43:00,347 - 10 - training_embed.py - training - Epoch: [2][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4479 (308.0989)	
2019-01-08 10:43:00,559 - 10 - training_embed.py - training - Epoch: [2][900/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.4900 (308.0943)	
2019-01-08 10:43:00,649 - 10 - training_embed.py - training - loss: 308.043270
2019-01-08 10:43:00,649 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:43:01,027 - 10 - training_embed.py - training - Epoch: [3][100/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.2964 (308.1153)	
2019-01-08 10:43:01,237 - 10 - training_embed.py - training - Epoch: [3][200/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.8342 (308.3061)	
2019-01-08 10:43:01,439 - 10 - training_embed.py - training - Epoch: [3][300/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 308.6925 (308.1719)	
2019-01-08 10:43:01,650 - 10 - training_embed.py - training - Epoch: [3][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5327 (308.1446)	
2019-01-08 10:43:01,867 - 10 - training_embed.py - training - Epoch: [3][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2189 (308.1079)	
2019-01-08 10:43:02,074 - 10 - training_embed.py - training - Epoch: [3][600/944]	Time 0.006 (0.002)	Data 0.001 (0.001)	Loss 306.3680 (308.0201)	
2019-01-08 10:43:02,274 - 10 - training_embed.py - training - Epoch: [3][700/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.0309 (308.0097)	
2019-01-08 10:43:02,480 - 10 - training_embed.py - training - Epoch: [3][800/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.6279 (308.0277)	
2019-01-08 10:43:02,700 - 10 - training_embed.py - training - Epoch: [3][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6601 (308.0313)	
2019-01-08 10:43:02,789 - 10 - training_embed.py - training - loss: 307.985547
2019-01-08 10:43:02,789 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:43:03,146 - 10 - training_embed.py - training - Epoch: [4][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2416 (308.1197)	
2019-01-08 10:43:03,357 - 10 - training_embed.py - training - Epoch: [4][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4415 (308.1369)	
2019-01-08 10:43:03,560 - 10 - training_embed.py - training - Epoch: [4][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1219 (308.0929)	
2019-01-08 10:43:03,776 - 10 - training_embed.py - training - Epoch: [4][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1558 (308.1655)	
2019-01-08 10:43:03,984 - 10 - training_embed.py - training - Epoch: [4][500/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.8706 (308.0993)	
2019-01-08 10:43:04,205 - 10 - training_embed.py - training - Epoch: [4][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.4559 (308.0400)	
2019-01-08 10:43:04,429 - 10 - training_embed.py - training - Epoch: [4][700/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.3816 (308.0018)	
2019-01-08 10:43:04,641 - 10 - training_embed.py - training - Epoch: [4][800/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.5880 (307.9784)	
2019-01-08 10:43:04,848 - 10 - training_embed.py - training - Epoch: [4][900/944]	Time 0.005 (0.002)	Data 0.002 (0.001)	Loss 308.0234 (307.9797)	
2019-01-08 10:43:04,938 - 10 - training_embed.py - training - loss: 307.928549
2019-01-08 10:43:04,938 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:43:05,289 - 10 - training_embed.py - training - Epoch: [5][100/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.5962 (308.0347)	
2019-01-08 10:43:05,507 - 10 - training_embed.py - training - Epoch: [5][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4576 (307.9185)	
2019-01-08 10:43:05,710 - 10 - training_embed.py - training - Epoch: [5][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8043 (307.9996)	
2019-01-08 10:43:05,923 - 10 - training_embed.py - training - Epoch: [5][400/944]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 309.1866 (307.9808)	
2019-01-08 10:43:06,139 - 10 - training_embed.py - training - Epoch: [5][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.7225 (308.0670)	
2019-01-08 10:43:06,340 - 10 - training_embed.py - training - Epoch: [5][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3842 (307.9970)	
2019-01-08 10:43:06,543 - 10 - training_embed.py - training - Epoch: [5][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7155 (307.9517)	
2019-01-08 10:43:06,754 - 10 - training_embed.py - training - Epoch: [5][800/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 309.8363 (307.9193)	
2019-01-08 10:43:06,976 - 10 - training_embed.py - training - Epoch: [5][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1199 (307.9253)	
2019-01-08 10:43:07,071 - 10 - training_embed.py - training - loss: 307.866976
2019-01-08 10:43:07,072 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:43:07,426 - 10 - training_embed.py - training - Epoch: [6][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4052 (307.8819)	
2019-01-08 10:43:07,631 - 10 - training_embed.py - training - Epoch: [6][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0171 (307.8419)	
2019-01-08 10:43:07,844 - 10 - training_embed.py - training - Epoch: [6][300/944]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 303.2388 (307.8650)	
2019-01-08 10:43:08,050 - 10 - training_embed.py - training - Epoch: [6][400/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.1103 (307.9379)	
2019-01-08 10:43:08,271 - 10 - training_embed.py - training - Epoch: [6][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4229 (307.9786)	
2019-01-08 10:43:08,480 - 10 - training_embed.py - training - Epoch: [6][600/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.6772 (307.9711)	
2019-01-08 10:43:08,689 - 10 - training_embed.py - training - Epoch: [6][700/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.1252 (307.9744)	
2019-01-08 10:43:08,901 - 10 - training_embed.py - training - Epoch: [6][800/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.2895 (307.8954)	
2019-01-08 10:43:09,113 - 10 - training_embed.py - training - Epoch: [6][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.5081 (307.8429)	
2019-01-08 10:43:09,205 - 10 - training_embed.py - training - loss: 307.809752
2019-01-08 10:43:09,205 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:43:09,567 - 10 - training_embed.py - training - Epoch: [7][100/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.9555 (307.9890)	
2019-01-08 10:43:09,779 - 10 - training_embed.py - training - Epoch: [7][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.3740 (307.9974)	
2019-01-08 10:43:09,999 - 10 - training_embed.py - training - Epoch: [7][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8461 (307.9474)	
2019-01-08 10:43:10,215 - 10 - training_embed.py - training - Epoch: [7][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.8162 (307.8925)	
2019-01-08 10:43:10,430 - 10 - training_embed.py - training - Epoch: [7][500/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.1293 (307.8124)	
2019-01-08 10:43:10,659 - 10 - training_embed.py - training - Epoch: [7][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7563 (307.7982)	
2019-01-08 10:43:10,872 - 10 - training_embed.py - training - Epoch: [7][700/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.8619 (307.7933)	
2019-01-08 10:43:11,104 - 10 - training_embed.py - training - Epoch: [7][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3204 (307.8142)	
2019-01-08 10:43:11,316 - 10 - training_embed.py - training - Epoch: [7][900/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.3235 (307.8087)	
2019-01-08 10:43:11,405 - 10 - training_embed.py - training - loss: 307.750033
2019-01-08 10:43:11,405 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:43:11,761 - 10 - training_embed.py - training - Epoch: [8][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2357 (307.5309)	
2019-01-08 10:43:11,983 - 10 - training_embed.py - training - Epoch: [8][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.7488 (307.7299)	
2019-01-08 10:43:12,186 - 10 - training_embed.py - training - Epoch: [8][300/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.3396 (307.7042)	
2019-01-08 10:43:12,391 - 10 - training_embed.py - training - Epoch: [8][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.1729 (307.7340)	
2019-01-08 10:43:12,602 - 10 - training_embed.py - training - Epoch: [8][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4291 (307.6696)	
2019-01-08 10:43:12,834 - 10 - training_embed.py - training - Epoch: [8][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2186 (307.6975)	
2019-01-08 10:43:13,046 - 10 - training_embed.py - training - Epoch: [8][700/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.9642 (307.7238)	
2019-01-08 10:43:13,260 - 10 - training_embed.py - training - Epoch: [8][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2159 (307.7362)	
2019-01-08 10:43:13,481 - 10 - training_embed.py - training - Epoch: [8][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.1169 (307.7602)	
2019-01-08 10:43:13,577 - 10 - training_embed.py - training - loss: 307.689765
2019-01-08 10:43:13,577 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:43:13,943 - 10 - training_embed.py - training - Epoch: [9][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.1022 (307.5670)	
2019-01-08 10:43:14,158 - 10 - training_embed.py - training - Epoch: [9][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.1112 (307.6315)	
2019-01-08 10:43:14,359 - 10 - training_embed.py - training - Epoch: [9][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7906 (307.6763)	
2019-01-08 10:43:14,575 - 10 - training_embed.py - training - Epoch: [9][400/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.5024 (307.6043)	
2019-01-08 10:43:14,789 - 10 - training_embed.py - training - Epoch: [9][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8428 (307.5932)	
2019-01-08 10:43:15,008 - 10 - training_embed.py - training - Epoch: [9][600/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.4032 (307.6421)	
2019-01-08 10:43:15,212 - 10 - training_embed.py - training - Epoch: [9][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.2605 (307.6831)	
2019-01-08 10:43:15,426 - 10 - training_embed.py - training - Epoch: [9][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.9478 (307.6698)	
2019-01-08 10:43:15,645 - 10 - training_embed.py - training - Epoch: [9][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.7409 (307.6811)	
2019-01-08 10:43:15,732 - 10 - training_embed.py - training - loss: 307.630827
2019-01-08 10:43:15,732 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:43:16,089 - 10 - training_embed.py - training - Epoch: [10][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.0237 (307.7744)	
2019-01-08 10:43:16,293 - 10 - training_embed.py - training - Epoch: [10][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.8635 (307.6659)	
2019-01-08 10:43:16,506 - 10 - training_embed.py - training - Epoch: [10][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4429 (307.6952)	
2019-01-08 10:43:16,711 - 10 - training_embed.py - training - Epoch: [10][400/944]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 308.7422 (307.6765)	
2019-01-08 10:43:16,916 - 10 - training_embed.py - training - Epoch: [10][500/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.0230 (307.6525)	
2019-01-08 10:43:17,117 - 10 - training_embed.py - training - Epoch: [10][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2777 (307.6039)	
2019-01-08 10:43:17,320 - 10 - training_embed.py - training - Epoch: [10][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3260 (307.6318)	
2019-01-08 10:43:17,545 - 10 - training_embed.py - training - Epoch: [10][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.9644 (307.6352)	
2019-01-08 10:43:17,758 - 10 - training_embed.py - training - Epoch: [10][900/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.0483 (307.6342)	
2019-01-08 10:43:17,848 - 10 - training_embed.py - training - loss: 307.573127
2019-01-08 10:43:17,848 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:43:18,202 - 10 - training_embed.py - training - Epoch: [11][100/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 308.3887 (307.4306)	
2019-01-08 10:43:18,405 - 10 - training_embed.py - training - Epoch: [11][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4154 (307.5506)	
2019-01-08 10:43:18,621 - 10 - training_embed.py - training - Epoch: [11][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.7328 (307.5503)	
2019-01-08 10:43:18,842 - 10 - training_embed.py - training - Epoch: [11][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.0034 (307.5653)	
2019-01-08 10:43:19,047 - 10 - training_embed.py - training - Epoch: [11][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4948 (307.4977)	
2019-01-08 10:43:19,248 - 10 - training_embed.py - training - Epoch: [11][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5178 (307.5556)	
2019-01-08 10:43:19,469 - 10 - training_embed.py - training - Epoch: [11][700/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.4627 (307.5669)	
2019-01-08 10:43:19,674 - 10 - training_embed.py - training - Epoch: [11][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.5644 (307.5647)	
2019-01-08 10:43:19,889 - 10 - training_embed.py - training - Epoch: [11][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5066 (307.5574)	
2019-01-08 10:43:19,979 - 10 - training_embed.py - training - loss: 307.513058
2019-01-08 10:43:20,010 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:43:20,353 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 343.032 ms ~ 0.006 min ~ 0.343 sec
2019-01-08 10:43:20,747 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 736.846 ms ~ 0.012 min ~ 0.737 sec
2019-01-08 10:43:20,747 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:43:20,747 - 10 - corpus.py - subactivity_sampler - 0 / 162
2019-01-08 10:43:20,748 - 10 - corpus.py - subactivity_sampler - [34562. 34541. 34524. 34499. 34465. 34451. 34420.]
2019-01-08 10:43:38,414 - 10 - corpus.py - subactivity_sampler - 20 / 162
2019-01-08 10:43:38,414 - 10 - corpus.py - subactivity_sampler - [34483. 34060. 35026. 34954. 33919. 34468. 34552.]
2019-01-08 10:43:54,355 - 10 - corpus.py - subactivity_sampler - 40 / 162
2019-01-08 10:43:54,355 - 10 - corpus.py - subactivity_sampler - [34732. 33040. 36245. 34369. 33663. 35165. 34248.]
2019-01-08 10:44:06,086 - 10 - corpus.py - subactivity_sampler - 60 / 162
2019-01-08 10:44:06,086 - 10 - corpus.py - subactivity_sampler - [34997. 32234. 36885. 35759. 32313. 34958. 34316.]
2019-01-08 10:44:21,088 - 10 - corpus.py - subactivity_sampler - 80 / 162
2019-01-08 10:44:21,088 - 10 - corpus.py - subactivity_sampler - [35100. 31100. 39119. 36263. 31113. 34632. 34135.]
2019-01-08 10:44:36,336 - 10 - corpus.py - subactivity_sampler - 100 / 162
2019-01-08 10:44:36,336 - 10 - corpus.py - subactivity_sampler - [35262. 29590. 40864. 37364. 29205. 34725. 34452.]
2019-01-08 10:44:51,460 - 10 - corpus.py - subactivity_sampler - 120 / 162
2019-01-08 10:44:51,461 - 10 - corpus.py - subactivity_sampler - [35786. 27862. 42693. 40161. 26639. 33837. 34484.]
2019-01-08 10:45:09,134 - 10 - corpus.py - subactivity_sampler - 140 / 162
2019-01-08 10:45:09,134 - 10 - corpus.py - subactivity_sampler - [36020. 25820. 44824. 43127. 23629. 33398. 34644.]
2019-01-08 10:45:25,343 - 10 - corpus.py - subactivity_sampler - 160 / 162
2019-01-08 10:45:25,343 - 10 - corpus.py - subactivity_sampler - [36403. 24059. 45938. 47262. 20307. 32945. 34548.]
2019-01-08 10:45:26,155 - 10 - corpus.py - subactivity_sampler - [36425. 23890. 46108. 47515. 20018. 32885. 34621.]
2019-01-08 10:45:26,155 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 125408.058 ms ~ 2.090 min ~ 125.408 sec
2019-01-08 10:45:26,155 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:45:27,404 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:45:27,404 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 1.  0.  0.  2. 18.  0.]
2019-01-08 10:45:27,404 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:45:27,432 - 10 - corpus.py - rho_sampling - ['59.8995', '47.3569', '928.1069', '86.0690', '8.8853', '165.2429']
2019-01-08 10:45:27,432 - 10 - pipeline.py - baseline - Iteration 1
2019-01-08 10:45:27,502 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:45:27,506 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 10:45:27,506 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 19', '1: 23', '2: 14', '3: 20', '4: 18', '5: 22', '6: 21']
2019-01-08 10:45:27,514 - 10 - accuracy_class.py - mof_val - frames true: 91639	frames overall : 241462
2019-01-08 10:45:27,514 - 10 - corpus.py - accuracy_corpus - Action: juice
2019-01-08 10:45:27,514 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3795172739395847
2019-01-08 10:45:27,514 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3795172739395847
2019-01-08 10:45:27,514 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:45:27,514 - 10 - accuracy_class.py - mof_classes - label 14: 0.007995  27 / 3377
2019-01-08 10:45:27,514 - 10 - accuracy_class.py - mof_classes - label 18: 0.000000  0 / 1411
2019-01-08 10:45:27,514 - 10 - accuracy_class.py - mof_classes - label 19: 0.633730  14591 / 23024
2019-01-08 10:45:27,514 - 10 - accuracy_class.py - mof_classes - label 20: 0.335182  45439 / 135565
2019-01-08 10:45:27,514 - 10 - accuracy_class.py - mof_classes - label 21: 0.572177  21618 / 37782
2019-01-08 10:45:27,514 - 10 - accuracy_class.py - mof_classes - label 22: 0.605985  9011 / 14870
2019-01-08 10:45:27,515 - 10 - accuracy_class.py - mof_classes - label 23: 0.336155  953 / 2835
2019-01-08 10:45:27,515 - 10 - accuracy_class.py - mof_classes - average class mof: 0.311403
2019-01-08 10:45:27,515 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:45:27,515 - 10 - accuracy_class.py - iou_classes - label 14: 0.000546  27 / 49458
2019-01-08 10:45:27,515 - 10 - accuracy_class.py - iou_classes - label 18: 0.000000  0 / 21429
2019-01-08 10:45:27,515 - 10 - accuracy_class.py - iou_classes - label 19: 0.325271  14591 / 44858
2019-01-08 10:45:27,515 - 10 - accuracy_class.py - iou_classes - label 20: 0.330127  45439 / 137641
2019-01-08 10:45:27,515 - 10 - accuracy_class.py - iou_classes - label 21: 0.425677  21618 / 50785
2019-01-08 10:45:27,515 - 10 - accuracy_class.py - iou_classes - label 22: 0.232578  9011 / 38744
2019-01-08 10:45:27,515 - 10 - accuracy_class.py - iou_classes - label 23: 0.036978  953 / 25772
2019-01-08 10:45:27,515 - 10 - accuracy_class.py - iou_classes - average IoU: 0.193025
2019-01-08 10:45:27,515 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.168897
2019-01-08 10:45:30,578 - 10 - f1_score.py - f1 - f1 score: 0.352933
2019-01-08 10:45:30,589 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3156.680 ms ~ 0.053 min ~ 3.157 sec
2019-01-08 10:45:30,589 - 10 - corpus.py - embedding_training - .
2019-01-08 10:45:30,589 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:45:30,589 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:45:30,589 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:45:32,186 - 10 - training_embed.py - training - create model
2019-01-08 10:45:32,186 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:45:32,186 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:45:32,528 - 10 - training_embed.py - training - Epoch: [0][100/944]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 304.0214 (307.9238)	
2019-01-08 10:45:32,741 - 10 - training_embed.py - training - Epoch: [0][200/944]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 307.7724 (308.1005)	
2019-01-08 10:45:32,957 - 10 - training_embed.py - training - Epoch: [0][300/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 309.0685 (308.0138)	
2019-01-08 10:45:33,166 - 10 - training_embed.py - training - Epoch: [0][400/944]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 307.9525 (308.0488)	
2019-01-08 10:45:33,374 - 10 - training_embed.py - training - Epoch: [0][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9423 (308.0585)	
2019-01-08 10:45:33,586 - 10 - training_embed.py - training - Epoch: [0][600/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 312.9705 (308.0759)	
2019-01-08 10:45:33,803 - 10 - training_embed.py - training - Epoch: [0][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3055 (308.0745)	
2019-01-08 10:45:34,038 - 10 - training_embed.py - training - Epoch: [0][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2556 (308.1165)	
2019-01-08 10:45:34,249 - 10 - training_embed.py - training - Epoch: [0][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6457 (308.1108)	
2019-01-08 10:45:34,348 - 10 - training_embed.py - training - loss: 308.074508
2019-01-08 10:45:34,349 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:45:34,751 - 10 - training_embed.py - training - Epoch: [1][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3974 (307.9887)	
2019-01-08 10:45:34,971 - 10 - training_embed.py - training - Epoch: [1][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.0363 (308.1164)	
2019-01-08 10:45:35,193 - 10 - training_embed.py - training - Epoch: [1][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.9191 (308.1428)	
2019-01-08 10:45:35,411 - 10 - training_embed.py - training - Epoch: [1][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9502 (308.1007)	
2019-01-08 10:45:35,613 - 10 - training_embed.py - training - Epoch: [1][500/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.3582 (308.0648)	
2019-01-08 10:45:35,839 - 10 - training_embed.py - training - Epoch: [1][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1527 (308.0020)	
2019-01-08 10:45:36,063 - 10 - training_embed.py - training - Epoch: [1][700/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.5374 (307.9885)	
2019-01-08 10:45:36,290 - 10 - training_embed.py - training - Epoch: [1][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.8316 (307.9946)	
2019-01-08 10:45:36,500 - 10 - training_embed.py - training - Epoch: [1][900/944]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 309.3025 (307.9762)	
2019-01-08 10:45:36,590 - 10 - training_embed.py - training - loss: 307.938928
2019-01-08 10:45:36,590 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:45:36,981 - 10 - training_embed.py - training - Epoch: [2][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3870 (307.7172)	
2019-01-08 10:45:37,191 - 10 - training_embed.py - training - Epoch: [2][200/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.4580 (307.8279)	
2019-01-08 10:45:37,401 - 10 - training_embed.py - training - Epoch: [2][300/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.7444 (307.8034)	
2019-01-08 10:45:37,612 - 10 - training_embed.py - training - Epoch: [2][400/944]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 310.3508 (307.8549)	
2019-01-08 10:45:37,822 - 10 - training_embed.py - training - Epoch: [2][500/944]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 306.6048 (307.7897)	
2019-01-08 10:45:38,054 - 10 - training_embed.py - training - Epoch: [2][600/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 312.0995 (307.8350)	
2019-01-08 10:45:38,259 - 10 - training_embed.py - training - Epoch: [2][700/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 302.0023 (307.8771)	
2019-01-08 10:45:38,478 - 10 - training_embed.py - training - Epoch: [2][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5568 (307.8507)	
2019-01-08 10:45:38,695 - 10 - training_embed.py - training - Epoch: [2][900/944]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 306.9651 (307.8710)	
2019-01-08 10:45:38,791 - 10 - training_embed.py - training - loss: 307.802721
2019-01-08 10:45:38,791 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:45:39,184 - 10 - training_embed.py - training - Epoch: [3][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5277 (307.7007)	
2019-01-08 10:45:39,386 - 10 - training_embed.py - training - Epoch: [3][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1041 (307.9779)	
2019-01-08 10:45:39,601 - 10 - training_embed.py - training - Epoch: [3][300/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.3690 (307.8488)	
2019-01-08 10:45:39,832 - 10 - training_embed.py - training - Epoch: [3][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7016 (307.8798)	
2019-01-08 10:45:40,056 - 10 - training_embed.py - training - Epoch: [3][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6733 (307.8647)	
2019-01-08 10:45:40,271 - 10 - training_embed.py - training - Epoch: [3][600/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.4804 (307.7707)	
2019-01-08 10:45:40,483 - 10 - training_embed.py - training - Epoch: [3][700/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.1829 (307.7651)	
2019-01-08 10:45:40,721 - 10 - training_embed.py - training - Epoch: [3][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2956 (307.7665)	
2019-01-08 10:45:40,949 - 10 - training_embed.py - training - Epoch: [3][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5487 (307.7286)	
2019-01-08 10:45:41,049 - 10 - training_embed.py - training - loss: 307.670151
2019-01-08 10:45:41,049 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:45:41,427 - 10 - training_embed.py - training - Epoch: [4][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0002 (307.5820)	
2019-01-08 10:45:41,648 - 10 - training_embed.py - training - Epoch: [4][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6805 (307.6389)	
2019-01-08 10:45:41,859 - 10 - training_embed.py - training - Epoch: [4][300/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.3312 (307.7184)	
2019-01-08 10:45:42,073 - 10 - training_embed.py - training - Epoch: [4][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4170 (307.7175)	
2019-01-08 10:45:42,282 - 10 - training_embed.py - training - Epoch: [4][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9506 (307.6735)	
2019-01-08 10:45:42,499 - 10 - training_embed.py - training - Epoch: [4][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1459 (307.5932)	
2019-01-08 10:45:42,699 - 10 - training_embed.py - training - Epoch: [4][700/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.3731 (307.5575)	
2019-01-08 10:45:42,920 - 10 - training_embed.py - training - Epoch: [4][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6000 (307.5410)	
2019-01-08 10:45:43,148 - 10 - training_embed.py - training - Epoch: [4][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7739 (307.5576)	
2019-01-08 10:45:43,238 - 10 - training_embed.py - training - loss: 307.535797
2019-01-08 10:45:43,238 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:45:43,595 - 10 - training_embed.py - training - Epoch: [5][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8763 (307.7083)	
2019-01-08 10:45:43,799 - 10 - training_embed.py - training - Epoch: [5][200/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.9829 (307.5704)	
2019-01-08 10:45:44,020 - 10 - training_embed.py - training - Epoch: [5][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1549 (307.6391)	
2019-01-08 10:45:44,237 - 10 - training_embed.py - training - Epoch: [5][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.4894 (307.6069)	
2019-01-08 10:45:44,451 - 10 - training_embed.py - training - Epoch: [5][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8387 (307.6198)	
2019-01-08 10:45:44,668 - 10 - training_embed.py - training - Epoch: [5][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5757 (307.5628)	
2019-01-08 10:45:44,882 - 10 - training_embed.py - training - Epoch: [5][700/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.2315 (307.5070)	
2019-01-08 10:45:45,097 - 10 - training_embed.py - training - Epoch: [5][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.7338 (307.4812)	
2019-01-08 10:45:45,327 - 10 - training_embed.py - training - Epoch: [5][900/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.3087 (307.4607)	
2019-01-08 10:45:45,418 - 10 - training_embed.py - training - loss: 307.398652
2019-01-08 10:45:45,419 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:45:45,810 - 10 - training_embed.py - training - Epoch: [6][100/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.5643 (307.0800)	
2019-01-08 10:45:46,025 - 10 - training_embed.py - training - Epoch: [6][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.3806 (307.2314)	
2019-01-08 10:45:46,253 - 10 - training_embed.py - training - Epoch: [6][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.6735 (307.2890)	
2019-01-08 10:45:46,469 - 10 - training_embed.py - training - Epoch: [6][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.4025 (307.3530)	
2019-01-08 10:45:46,685 - 10 - training_embed.py - training - Epoch: [6][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4547 (307.3590)	
2019-01-08 10:45:46,889 - 10 - training_embed.py - training - Epoch: [6][600/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.6145 (307.4042)	
2019-01-08 10:45:47,115 - 10 - training_embed.py - training - Epoch: [6][700/944]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 304.6283 (307.4118)	
2019-01-08 10:45:47,319 - 10 - training_embed.py - training - Epoch: [6][800/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.7861 (307.3228)	
2019-01-08 10:45:47,552 - 10 - training_embed.py - training - Epoch: [6][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.2268 (307.2923)	
2019-01-08 10:45:47,640 - 10 - training_embed.py - training - loss: 307.265728
2019-01-08 10:45:47,640 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:45:48,051 - 10 - training_embed.py - training - Epoch: [7][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.0716 (307.4087)	
2019-01-08 10:45:48,268 - 10 - training_embed.py - training - Epoch: [7][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2385 (307.3384)	
2019-01-08 10:45:48,483 - 10 - training_embed.py - training - Epoch: [7][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.3901 (307.2600)	
2019-01-08 10:45:48,693 - 10 - training_embed.py - training - Epoch: [7][400/944]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 307.2345 (307.2447)	
2019-01-08 10:45:48,923 - 10 - training_embed.py - training - Epoch: [7][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6076 (307.2016)	
2019-01-08 10:45:49,160 - 10 - training_embed.py - training - Epoch: [7][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.2930 (307.2123)	
2019-01-08 10:45:49,380 - 10 - training_embed.py - training - Epoch: [7][700/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.0016 (307.2158)	
2019-01-08 10:45:49,603 - 10 - training_embed.py - training - Epoch: [7][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.3921 (307.2129)	
2019-01-08 10:45:49,820 - 10 - training_embed.py - training - Epoch: [7][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2969 (307.1907)	
2019-01-08 10:45:49,912 - 10 - training_embed.py - training - loss: 307.131055
2019-01-08 10:45:49,912 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:45:50,278 - 10 - training_embed.py - training - Epoch: [8][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4795 (307.1189)	
2019-01-08 10:45:50,505 - 10 - training_embed.py - training - Epoch: [8][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.1560 (307.2467)	
2019-01-08 10:45:50,714 - 10 - training_embed.py - training - Epoch: [8][300/944]	Time 0.007 (0.002)	Data 0.004 (0.001)	Loss 306.3755 (307.1786)	
2019-01-08 10:45:50,946 - 10 - training_embed.py - training - Epoch: [8][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.5823 (307.1154)	
2019-01-08 10:45:51,178 - 10 - training_embed.py - training - Epoch: [8][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.9729 (307.0314)	
2019-01-08 10:45:51,396 - 10 - training_embed.py - training - Epoch: [8][600/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 309.6378 (307.0459)	
2019-01-08 10:45:51,618 - 10 - training_embed.py - training - Epoch: [8][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1378 (307.0597)	
2019-01-08 10:45:51,832 - 10 - training_embed.py - training - Epoch: [8][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.2694 (307.0806)	
2019-01-08 10:45:52,054 - 10 - training_embed.py - training - Epoch: [8][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2628 (307.0741)	
2019-01-08 10:45:52,144 - 10 - training_embed.py - training - loss: 306.995852
2019-01-08 10:45:52,144 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:45:52,527 - 10 - training_embed.py - training - Epoch: [9][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.8015 (306.7182)	
2019-01-08 10:45:52,742 - 10 - training_embed.py - training - Epoch: [9][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.5119 (306.8377)	
2019-01-08 10:45:52,951 - 10 - training_embed.py - training - Epoch: [9][300/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.2241 (306.8738)	
2019-01-08 10:45:53,176 - 10 - training_embed.py - training - Epoch: [9][400/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.8159 (306.8695)	
2019-01-08 10:45:53,409 - 10 - training_embed.py - training - Epoch: [9][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3052 (306.8432)	
2019-01-08 10:45:53,625 - 10 - training_embed.py - training - Epoch: [9][600/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.4518 (306.8710)	
2019-01-08 10:45:53,846 - 10 - training_embed.py - training - Epoch: [9][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.0580 (306.9228)	
2019-01-08 10:45:54,062 - 10 - training_embed.py - training - Epoch: [9][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 311.2850 (306.9308)	
2019-01-08 10:45:54,285 - 10 - training_embed.py - training - Epoch: [9][900/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.6443 (306.9175)	
2019-01-08 10:45:54,379 - 10 - training_embed.py - training - loss: 306.860550
2019-01-08 10:45:54,379 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:45:54,751 - 10 - training_embed.py - training - Epoch: [10][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0335 (307.0077)	
2019-01-08 10:45:54,961 - 10 - training_embed.py - training - Epoch: [10][200/944]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 304.7911 (306.8682)	
2019-01-08 10:45:55,181 - 10 - training_embed.py - training - Epoch: [10][300/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.6705 (306.9483)	
2019-01-08 10:45:55,398 - 10 - training_embed.py - training - Epoch: [10][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1568 (306.8674)	
2019-01-08 10:45:55,622 - 10 - training_embed.py - training - Epoch: [10][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2213 (306.8294)	
2019-01-08 10:45:55,838 - 10 - training_embed.py - training - Epoch: [10][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2845 (306.8201)	
2019-01-08 10:45:56,053 - 10 - training_embed.py - training - Epoch: [10][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.1151 (306.8165)	
2019-01-08 10:45:56,269 - 10 - training_embed.py - training - Epoch: [10][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.3292 (306.8082)	
2019-01-08 10:45:56,504 - 10 - training_embed.py - training - Epoch: [10][900/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.0766 (306.7974)	
2019-01-08 10:45:56,604 - 10 - training_embed.py - training - loss: 306.727476
2019-01-08 10:45:56,604 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:45:56,978 - 10 - training_embed.py - training - Epoch: [11][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5928 (306.4710)	
2019-01-08 10:45:57,202 - 10 - training_embed.py - training - Epoch: [11][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.4661 (306.6348)	
2019-01-08 10:45:57,436 - 10 - training_embed.py - training - Epoch: [11][300/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.9230 (306.7231)	
2019-01-08 10:45:57,654 - 10 - training_embed.py - training - Epoch: [11][400/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.8223 (306.7478)	
2019-01-08 10:45:57,873 - 10 - training_embed.py - training - Epoch: [11][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8966 (306.6524)	
2019-01-08 10:45:58,094 - 10 - training_embed.py - training - Epoch: [11][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.2396 (306.7348)	
2019-01-08 10:45:58,323 - 10 - training_embed.py - training - Epoch: [11][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8640 (306.7111)	
2019-01-08 10:45:58,549 - 10 - training_embed.py - training - Epoch: [11][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7213 (306.6470)	
2019-01-08 10:45:58,772 - 10 - training_embed.py - training - Epoch: [11][900/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.3483 (306.6225)	
2019-01-08 10:45:58,862 - 10 - training_embed.py - training - loss: 306.592181
2019-01-08 10:45:58,900 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:45:59,312 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 410.921 ms ~ 0.007 min ~ 0.411 sec
2019-01-08 10:45:59,701 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 800.523 ms ~ 0.013 min ~ 0.801 sec
2019-01-08 10:45:59,701 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:45:59,702 - 10 - corpus.py - subactivity_sampler - 0 / 162
2019-01-08 10:45:59,702 - 10 - corpus.py - subactivity_sampler - [36425. 23890. 46108. 47515. 20018. 32885. 34621.]
2019-01-08 10:46:17,716 - 10 - corpus.py - subactivity_sampler - 20 / 162
2019-01-08 10:46:17,716 - 10 - corpus.py - subactivity_sampler - [36512. 22471. 47966. 49062. 17654. 33214. 34583.]
2019-01-08 10:46:33,897 - 10 - corpus.py - subactivity_sampler - 40 / 162
2019-01-08 10:46:33,897 - 10 - corpus.py - subactivity_sampler - [36580. 21182. 49972. 50562. 14747. 34165. 34254.]
2019-01-08 10:46:45,907 - 10 - corpus.py - subactivity_sampler - 60 / 162
2019-01-08 10:46:45,908 - 10 - corpus.py - subactivity_sampler - [36722. 20257. 51113. 51937. 13243. 34096. 34094.]
2019-01-08 10:47:01,192 - 10 - corpus.py - subactivity_sampler - 80 / 162
2019-01-08 10:47:01,192 - 10 - corpus.py - subactivity_sampler - [37105. 18878. 52416. 53595. 10996. 34780. 33692.]
2019-01-08 10:47:16,647 - 10 - corpus.py - subactivity_sampler - 100 / 162
2019-01-08 10:47:16,647 - 10 - corpus.py - subactivity_sampler - [37133. 17946. 54088. 54312.  9353. 34895. 33735.]
2019-01-08 10:47:32,032 - 10 - corpus.py - subactivity_sampler - 120 / 162
2019-01-08 10:47:32,032 - 10 - corpus.py - subactivity_sampler - [37271. 16640. 56003. 54257.  8504. 35485. 33302.]
2019-01-08 10:47:50,283 - 10 - corpus.py - subactivity_sampler - 140 / 162
2019-01-08 10:47:50,283 - 10 - corpus.py - subactivity_sampler - [37333. 15433. 57226. 55165.  7852. 35421. 33032.]
2019-01-08 10:48:06,783 - 10 - corpus.py - subactivity_sampler - 160 / 162
2019-01-08 10:48:06,783 - 10 - corpus.py - subactivity_sampler - [37254. 14214. 58816. 54960.  7661. 35800. 32757.]
2019-01-08 10:48:07,599 - 10 - corpus.py - subactivity_sampler - [37231. 14139. 59066. 54809.  7655. 35960. 32602.]
2019-01-08 10:48:07,599 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 127897.863 ms ~ 2.132 min ~ 127.898 sec
2019-01-08 10:48:07,599 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:48:08,820 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:48:08,820 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 4.  5.  0.  5. 25.  0.]
2019-01-08 10:48:08,821 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:48:08,841 - 10 - corpus.py - rho_sampling - ['56.6282', '11.3153', '928.5380', '80.4136', '15.6379', '935.3993']
2019-01-08 10:48:08,841 - 10 - pipeline.py - baseline - Iteration 2
2019-01-08 10:48:08,911 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:48:08,915 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 10:48:08,915 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 19', '1: 23', '2: 20', '3: 14', '4: 18', '5: 22', '6: 21']
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - mof_val - frames true: 100732	frames overall : 241462
2019-01-08 10:48:08,923 - 10 - corpus.py - accuracy_corpus - Action: juice
2019-01-08 10:48:08,923 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4171753733506722
2019-01-08 10:48:08,923 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4063993506224582
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - mof_classes - label 14: 0.000000  0 / 3377
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - mof_classes - label 18: 0.000000  0 / 1411
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - mof_classes - label 19: 0.668954  15402 / 23024
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - mof_classes - label 20: 0.404839  54882 / 135565
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - mof_classes - label 21: 0.537081  20292 / 37782
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - mof_classes - label 22: 0.633154  9415 / 14870
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - mof_classes - label 23: 0.261376  741 / 2835
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - mof_classes - average class mof: 0.313175
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - iou_classes - label 14: 0.000000  0 / 58186
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - iou_classes - label 18: 0.000000  0 / 9066
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - iou_classes - label 19: 0.343388  15402 / 44853
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - iou_classes - label 20: 0.392718  54882 / 139749
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - iou_classes - label 21: 0.405095  20292 / 50092
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - iou_classes - label 22: 0.227333  9415 / 41415
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - iou_classes - label 23: 0.045648  741 / 16233
2019-01-08 10:48:08,923 - 10 - accuracy_class.py - iou_classes - average IoU: 0.202026
2019-01-08 10:48:08,924 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.176773
2019-01-08 10:48:12,263 - 10 - f1_score.py - f1 - f1 score: 0.366585
2019-01-08 10:48:12,273 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3432.245 ms ~ 0.057 min ~ 3.432 sec
2019-01-08 10:48:12,273 - 10 - corpus.py - embedding_training - .
2019-01-08 10:48:12,273 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:48:12,273 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:48:12,273 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:48:14,021 - 10 - training_embed.py - training - create model
2019-01-08 10:48:14,022 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:48:14,022 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:48:14,436 - 10 - training_embed.py - training - Epoch: [0][100/944]	Time 0.002 (0.004)	Data 0.001 (0.003)	Loss 303.9823 (308.2287)	
2019-01-08 10:48:14,650 - 10 - training_embed.py - training - Epoch: [0][200/944]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 305.4731 (308.1748)	
2019-01-08 10:48:14,864 - 10 - training_embed.py - training - Epoch: [0][300/944]	Time 0.001 (0.003)	Data 0.000 (0.002)	Loss 309.3585 (308.1171)	
2019-01-08 10:48:15,085 - 10 - training_embed.py - training - Epoch: [0][400/944]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 308.4655 (308.1697)	
2019-01-08 10:48:15,305 - 10 - training_embed.py - training - Epoch: [0][500/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 307.0641 (308.2184)	
2019-01-08 10:48:15,512 - 10 - training_embed.py - training - Epoch: [0][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.6871 (308.2189)	
2019-01-08 10:48:15,742 - 10 - training_embed.py - training - Epoch: [0][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9542 (308.1862)	
2019-01-08 10:48:15,964 - 10 - training_embed.py - training - Epoch: [0][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6963 (308.2111)	
2019-01-08 10:48:16,181 - 10 - training_embed.py - training - Epoch: [0][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6418 (308.1939)	
2019-01-08 10:48:16,275 - 10 - training_embed.py - training - loss: 308.159851
2019-01-08 10:48:16,275 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:48:16,705 - 10 - training_embed.py - training - Epoch: [1][100/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 307.9498 (308.0971)	
2019-01-08 10:48:16,910 - 10 - training_embed.py - training - Epoch: [1][200/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 303.9634 (308.1706)	
2019-01-08 10:48:17,143 - 10 - training_embed.py - training - Epoch: [1][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 306.3246 (308.1401)	
2019-01-08 10:48:17,372 - 10 - training_embed.py - training - Epoch: [1][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9789 (308.0986)	
2019-01-08 10:48:17,584 - 10 - training_embed.py - training - Epoch: [1][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2874 (308.0666)	
2019-01-08 10:48:17,790 - 10 - training_embed.py - training - Epoch: [1][600/944]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 306.5987 (308.0203)	
2019-01-08 10:48:18,009 - 10 - training_embed.py - training - Epoch: [1][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7870 (307.9853)	
2019-01-08 10:48:18,230 - 10 - training_embed.py - training - Epoch: [1][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4818 (307.9826)	
2019-01-08 10:48:18,450 - 10 - training_embed.py - training - Epoch: [1][900/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 309.5358 (307.9639)	
2019-01-08 10:48:18,536 - 10 - training_embed.py - training - loss: 307.904088
2019-01-08 10:48:18,536 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:48:18,979 - 10 - training_embed.py - training - Epoch: [2][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2547 (307.6907)	
2019-01-08 10:48:19,201 - 10 - training_embed.py - training - Epoch: [2][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.9185 (307.7313)	
2019-01-08 10:48:19,410 - 10 - training_embed.py - training - Epoch: [2][300/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.5172 (307.7775)	
2019-01-08 10:48:19,623 - 10 - training_embed.py - training - Epoch: [2][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.9769 (307.8087)	
2019-01-08 10:48:19,850 - 10 - training_embed.py - training - Epoch: [2][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5898 (307.7351)	
2019-01-08 10:48:20,069 - 10 - training_embed.py - training - Epoch: [2][600/944]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 311.2242 (307.7265)	
2019-01-08 10:48:20,302 - 10 - training_embed.py - training - Epoch: [2][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.2470 (307.7550)	
2019-01-08 10:48:20,523 - 10 - training_embed.py - training - Epoch: [2][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.7364 (307.7109)	
2019-01-08 10:48:20,724 - 10 - training_embed.py - training - Epoch: [2][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9627 (307.7176)	
2019-01-08 10:48:20,816 - 10 - training_embed.py - training - loss: 307.650166
2019-01-08 10:48:20,816 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:48:21,246 - 10 - training_embed.py - training - Epoch: [3][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.7697 (307.5744)	
2019-01-08 10:48:21,453 - 10 - training_embed.py - training - Epoch: [3][200/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.3236 (307.6720)	
2019-01-08 10:48:21,664 - 10 - training_embed.py - training - Epoch: [3][300/944]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 308.4988 (307.5957)	
2019-01-08 10:48:21,879 - 10 - training_embed.py - training - Epoch: [3][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3980 (307.6316)	
2019-01-08 10:48:22,088 - 10 - training_embed.py - training - Epoch: [3][500/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.2315 (307.6491)	
2019-01-08 10:48:22,322 - 10 - training_embed.py - training - Epoch: [3][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2518 (307.5221)	
2019-01-08 10:48:22,536 - 10 - training_embed.py - training - Epoch: [3][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6328 (307.5097)	
2019-01-08 10:48:22,766 - 10 - training_embed.py - training - Epoch: [3][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7714 (307.5019)	
2019-01-08 10:48:22,981 - 10 - training_embed.py - training - Epoch: [3][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3889 (307.4657)	
2019-01-08 10:48:23,070 - 10 - training_embed.py - training - loss: 307.395950
2019-01-08 10:48:23,070 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:48:23,503 - 10 - training_embed.py - training - Epoch: [4][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3977 (307.1631)	
2019-01-08 10:48:23,707 - 10 - training_embed.py - training - Epoch: [4][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.8306 (307.2013)	
2019-01-08 10:48:23,928 - 10 - training_embed.py - training - Epoch: [4][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8953 (307.2032)	
2019-01-08 10:48:24,134 - 10 - training_embed.py - training - Epoch: [4][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7892 (307.2251)	
2019-01-08 10:48:24,344 - 10 - training_embed.py - training - Epoch: [4][500/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.6112 (307.2415)	
2019-01-08 10:48:24,565 - 10 - training_embed.py - training - Epoch: [4][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8484 (307.2107)	
2019-01-08 10:48:24,781 - 10 - training_embed.py - training - Epoch: [4][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4000 (307.1815)	
2019-01-08 10:48:24,999 - 10 - training_embed.py - training - Epoch: [4][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9885 (307.1683)	
2019-01-08 10:48:25,222 - 10 - training_embed.py - training - Epoch: [4][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0251 (307.1770)	
2019-01-08 10:48:25,315 - 10 - training_embed.py - training - loss: 307.142055
2019-01-08 10:48:25,315 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:48:25,726 - 10 - training_embed.py - training - Epoch: [5][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6893 (307.1263)	
2019-01-08 10:48:25,934 - 10 - training_embed.py - training - Epoch: [5][200/944]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 306.0159 (307.0481)	
2019-01-08 10:48:26,156 - 10 - training_embed.py - training - Epoch: [5][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6149 (307.1698)	
2019-01-08 10:48:26,367 - 10 - training_embed.py - training - Epoch: [5][400/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.9152 (307.1189)	
2019-01-08 10:48:26,586 - 10 - training_embed.py - training - Epoch: [5][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3882 (307.1352)	
2019-01-08 10:48:26,791 - 10 - training_embed.py - training - Epoch: [5][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5796 (307.0462)	
2019-01-08 10:48:27,003 - 10 - training_embed.py - training - Epoch: [5][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6434 (306.9912)	
2019-01-08 10:48:27,213 - 10 - training_embed.py - training - Epoch: [5][800/944]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 308.9401 (306.9804)	
2019-01-08 10:48:27,435 - 10 - training_embed.py - training - Epoch: [5][900/944]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 304.2420 (306.9506)	
2019-01-08 10:48:27,524 - 10 - training_embed.py - training - loss: 306.888067
2019-01-08 10:48:27,524 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:48:27,963 - 10 - training_embed.py - training - Epoch: [6][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1055 (306.3269)	
2019-01-08 10:48:28,180 - 10 - training_embed.py - training - Epoch: [6][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.3040 (306.5221)	
2019-01-08 10:48:28,401 - 10 - training_embed.py - training - Epoch: [6][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1176 (306.6086)	
2019-01-08 10:48:28,619 - 10 - training_embed.py - training - Epoch: [6][400/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.5491 (306.7626)	
2019-01-08 10:48:28,837 - 10 - training_embed.py - training - Epoch: [6][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7569 (306.7758)	
2019-01-08 10:48:29,053 - 10 - training_embed.py - training - Epoch: [6][600/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.1979 (306.8101)	
2019-01-08 10:48:29,264 - 10 - training_embed.py - training - Epoch: [6][700/944]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 304.1829 (306.8008)	
2019-01-08 10:48:29,476 - 10 - training_embed.py - training - Epoch: [6][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.2790 (306.6932)	
2019-01-08 10:48:29,705 - 10 - training_embed.py - training - Epoch: [6][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.5699 (306.6558)	
2019-01-08 10:48:29,793 - 10 - training_embed.py - training - loss: 306.634728
2019-01-08 10:48:29,793 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:48:30,239 - 10 - training_embed.py - training - Epoch: [7][100/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.7621 (306.6137)	
2019-01-08 10:48:30,457 - 10 - training_embed.py - training - Epoch: [7][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7493 (306.6607)	
2019-01-08 10:48:30,674 - 10 - training_embed.py - training - Epoch: [7][300/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 308.1549 (306.5635)	
2019-01-08 10:48:30,895 - 10 - training_embed.py - training - Epoch: [7][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4373 (306.5238)	
2019-01-08 10:48:31,129 - 10 - training_embed.py - training - Epoch: [7][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6138 (306.4791)	
2019-01-08 10:48:31,330 - 10 - training_embed.py - training - Epoch: [7][600/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.3521 (306.5022)	
2019-01-08 10:48:31,543 - 10 - training_embed.py - training - Epoch: [7][700/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 302.6182 (306.4833)	
2019-01-08 10:48:31,765 - 10 - training_embed.py - training - Epoch: [7][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6413 (306.4612)	
2019-01-08 10:48:31,983 - 10 - training_embed.py - training - Epoch: [7][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.9370 (306.4532)	
2019-01-08 10:48:32,080 - 10 - training_embed.py - training - loss: 306.381016
2019-01-08 10:48:32,080 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:48:32,529 - 10 - training_embed.py - training - Epoch: [8][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0319 (306.2479)	
2019-01-08 10:48:32,733 - 10 - training_embed.py - training - Epoch: [8][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2201 (306.3352)	
2019-01-08 10:48:32,960 - 10 - training_embed.py - training - Epoch: [8][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8956 (306.2564)	
2019-01-08 10:48:33,195 - 10 - training_embed.py - training - Epoch: [8][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0302 (306.2423)	
2019-01-08 10:48:33,406 - 10 - training_embed.py - training - Epoch: [8][500/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.7616 (306.1691)	
2019-01-08 10:48:33,623 - 10 - training_embed.py - training - Epoch: [8][600/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.1144 (306.1874)	
2019-01-08 10:48:33,826 - 10 - training_embed.py - training - Epoch: [8][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3247 (306.2133)	
2019-01-08 10:48:34,037 - 10 - training_embed.py - training - Epoch: [8][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.7820 (306.2427)	
2019-01-08 10:48:34,257 - 10 - training_embed.py - training - Epoch: [8][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.0490 (306.2052)	
2019-01-08 10:48:34,343 - 10 - training_embed.py - training - loss: 306.127315
2019-01-08 10:48:34,343 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:48:34,787 - 10 - training_embed.py - training - Epoch: [9][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0154 (305.8444)	
2019-01-08 10:48:34,995 - 10 - training_embed.py - training - Epoch: [9][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.1523 (305.9175)	
2019-01-08 10:48:35,208 - 10 - training_embed.py - training - Epoch: [9][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8917 (305.9602)	
2019-01-08 10:48:35,421 - 10 - training_embed.py - training - Epoch: [9][400/944]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 308.0263 (305.8920)	
2019-01-08 10:48:35,635 - 10 - training_embed.py - training - Epoch: [9][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0519 (305.8524)	
2019-01-08 10:48:35,838 - 10 - training_embed.py - training - Epoch: [9][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7419 (305.8381)	
2019-01-08 10:48:36,048 - 10 - training_embed.py - training - Epoch: [9][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5239 (305.9006)	
2019-01-08 10:48:36,257 - 10 - training_embed.py - training - Epoch: [9][800/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.0800 (305.9222)	
2019-01-08 10:48:36,470 - 10 - training_embed.py - training - Epoch: [9][900/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.1022 (305.9305)	
2019-01-08 10:48:36,557 - 10 - training_embed.py - training - loss: 305.872866
2019-01-08 10:48:36,558 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:48:36,980 - 10 - training_embed.py - training - Epoch: [10][100/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.6522 (305.5705)	
2019-01-08 10:48:37,204 - 10 - training_embed.py - training - Epoch: [10][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.4212 (305.5745)	
2019-01-08 10:48:37,418 - 10 - training_embed.py - training - Epoch: [10][300/944]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 308.4833 (305.7092)	
2019-01-08 10:48:37,640 - 10 - training_embed.py - training - Epoch: [10][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8041 (305.6501)	
2019-01-08 10:48:37,852 - 10 - training_embed.py - training - Epoch: [10][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.5055 (305.7040)	
2019-01-08 10:48:38,075 - 10 - training_embed.py - training - Epoch: [10][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3303 (305.7006)	
2019-01-08 10:48:38,293 - 10 - training_embed.py - training - Epoch: [10][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6289 (305.7074)	
2019-01-08 10:48:38,509 - 10 - training_embed.py - training - Epoch: [10][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.9276 (305.6874)	
2019-01-08 10:48:38,736 - 10 - training_embed.py - training - Epoch: [10][900/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.7743 (305.6842)	
2019-01-08 10:48:38,831 - 10 - training_embed.py - training - loss: 305.621131
2019-01-08 10:48:38,832 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:48:39,272 - 10 - training_embed.py - training - Epoch: [11][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9911 (305.2502)	
2019-01-08 10:48:39,473 - 10 - training_embed.py - training - Epoch: [11][200/944]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 305.1158 (305.3682)	
2019-01-08 10:48:39,678 - 10 - training_embed.py - training - Epoch: [11][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5483 (305.5602)	
2019-01-08 10:48:39,899 - 10 - training_embed.py - training - Epoch: [11][400/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 302.8796 (305.5634)	
2019-01-08 10:48:40,133 - 10 - training_embed.py - training - Epoch: [11][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 300.2200 (305.4407)	
2019-01-08 10:48:40,349 - 10 - training_embed.py - training - Epoch: [11][600/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 301.0193 (305.5096)	
2019-01-08 10:48:40,565 - 10 - training_embed.py - training - Epoch: [11][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3952 (305.5036)	
2019-01-08 10:48:40,783 - 10 - training_embed.py - training - Epoch: [11][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.9218 (305.4512)	
2019-01-08 10:48:40,999 - 10 - training_embed.py - training - Epoch: [11][900/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.1764 (305.4051)	
2019-01-08 10:48:41,086 - 10 - training_embed.py - training - loss: 305.367821
2019-01-08 10:48:41,120 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:48:41,536 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 415.898 ms ~ 0.007 min ~ 0.416 sec
2019-01-08 10:48:41,925 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 805.363 ms ~ 0.013 min ~ 0.805 sec
2019-01-08 10:48:41,925 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:48:41,926 - 10 - corpus.py - subactivity_sampler - 0 / 162
2019-01-08 10:48:41,926 - 10 - corpus.py - subactivity_sampler - [37231. 14139. 59066. 54809.  7655. 35960. 32602.]
2019-01-08 10:48:59,883 - 10 - corpus.py - subactivity_sampler - 20 / 162
2019-01-08 10:48:59,883 - 10 - corpus.py - subactivity_sampler - [37305. 13014. 60565. 54872.  6901. 36249. 32556.]
2019-01-08 10:49:16,147 - 10 - corpus.py - subactivity_sampler - 40 / 162
2019-01-08 10:49:16,147 - 10 - corpus.py - subactivity_sampler - [37562. 12111. 61355. 55259.  6326. 36513. 32336.]
2019-01-08 10:49:28,225 - 10 - corpus.py - subactivity_sampler - 60 / 162
2019-01-08 10:49:28,225 - 10 - corpus.py - subactivity_sampler - [37810. 11263. 62926. 54781.  5766. 36719. 32197.]
2019-01-08 10:49:43,566 - 10 - corpus.py - subactivity_sampler - 80 / 162
2019-01-08 10:49:43,566 - 10 - corpus.py - subactivity_sampler - [38001. 10406. 63940. 54889.  5128. 36997. 32101.]
2019-01-08 10:49:59,135 - 10 - corpus.py - subactivity_sampler - 100 / 162
2019-01-08 10:49:59,136 - 10 - corpus.py - subactivity_sampler - [38018.  9889. 65282. 54142.  4802. 37260. 32069.]
2019-01-08 10:50:14,645 - 10 - corpus.py - subactivity_sampler - 120 / 162
2019-01-08 10:50:14,645 - 10 - corpus.py - subactivity_sampler - [38282.  9182. 65983. 54200.  4451. 37567. 31797.]
2019-01-08 10:50:32,701 - 10 - corpus.py - subactivity_sampler - 140 / 162
2019-01-08 10:50:32,702 - 10 - corpus.py - subactivity_sampler - [38399.  8378. 66691. 54029.  4595. 37725. 31645.]
2019-01-08 10:50:49,367 - 10 - corpus.py - subactivity_sampler - 160 / 162
2019-01-08 10:50:49,367 - 10 - corpus.py - subactivity_sampler - [38346.  7722. 67742. 53500.  4826. 38016. 31310.]
2019-01-08 10:50:50,183 - 10 - corpus.py - subactivity_sampler - [38386.  7675. 67760. 53488.  4826. 38025. 31302.]
2019-01-08 10:50:50,183 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 128257.563 ms ~ 2.138 min ~ 128.258 sec
2019-01-08 10:50:50,183 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:50:51,433 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:50:51,433 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 9. 27.  0.  8. 14.  0.]
2019-01-08 10:50:51,433 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:50:51,453 - 10 - corpus.py - rho_sampling - ['51.5512', '6.2802', '928.4017', '209.2236', '14.6793', '62.3734']
2019-01-08 10:50:51,453 - 10 - pipeline.py - baseline - Iteration 3
2019-01-08 10:50:51,522 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:50:51,527 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 10:50:51,527 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 19', '1: 23', '2: 20', '3: 14', '4: 18', '5: 22', '6: 21']
2019-01-08 10:50:51,535 - 10 - accuracy_class.py - mof_val - frames true: 106743	frames overall : 241462
2019-01-08 10:50:51,535 - 10 - corpus.py - accuracy_corpus - Action: juice
2019-01-08 10:50:51,535 - 10 - corpus.py - accuracy_corpus - MoF val: 0.44206955959944005
2019-01-08 10:50:51,535 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.44206955959944005
2019-01-08 10:50:51,535 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - mof_classes - label 14: 0.000000  0 / 3377
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - mof_classes - label 18: 0.000000  0 / 1411
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - mof_classes - label 19: 0.705090  16234 / 23024
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - mof_classes - label 20: 0.452462  61338 / 135565
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - mof_classes - label 21: 0.506140  19123 / 37782
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - mof_classes - label 22: 0.661533  9837 / 14870
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - mof_classes - label 23: 0.074427  211 / 2835
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - mof_classes - average class mof: 0.299957
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - iou_classes - label 14: 0.000000  0 / 56865
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - iou_classes - label 18: 0.000000  0 / 6237
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - iou_classes - label 19: 0.359350  16234 / 45176
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - iou_classes - label 20: 0.431997  61338 / 141987
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - iou_classes - label 21: 0.382759  19123 / 49961
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - iou_classes - label 22: 0.228459  9837 / 43058
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - iou_classes - label 23: 0.020487  211 / 10299
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - iou_classes - average IoU: 0.203293
2019-01-08 10:50:51,536 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.177882
2019-01-08 10:50:54,635 - 10 - f1_score.py - f1 - f1 score: 0.368160
2019-01-08 10:50:54,645 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3192.021 ms ~ 0.053 min ~ 3.192 sec
2019-01-08 10:50:54,645 - 10 - corpus.py - embedding_training - .
2019-01-08 10:50:54,645 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:50:54,645 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:50:54,645 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:50:56,227 - 10 - training_embed.py - training - create model
2019-01-08 10:50:56,228 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:50:56,228 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:50:56,590 - 10 - training_embed.py - training - Epoch: [0][100/944]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 306.1856 (308.8443)	
2019-01-08 10:50:56,802 - 10 - training_embed.py - training - Epoch: [0][200/944]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 307.3076 (308.8711)	
2019-01-08 10:50:57,020 - 10 - training_embed.py - training - Epoch: [0][300/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 309.8230 (308.8668)	
2019-01-08 10:50:57,222 - 10 - training_embed.py - training - Epoch: [0][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.2480 (308.9328)	
2019-01-08 10:50:57,441 - 10 - training_embed.py - training - Epoch: [0][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4943 (308.9875)	
2019-01-08 10:50:57,659 - 10 - training_embed.py - training - Epoch: [0][600/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 313.0772 (308.9794)	
2019-01-08 10:50:57,874 - 10 - training_embed.py - training - Epoch: [0][700/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.2519 (308.9125)	
2019-01-08 10:50:58,093 - 10 - training_embed.py - training - Epoch: [0][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.5502 (308.9257)	
2019-01-08 10:50:58,322 - 10 - training_embed.py - training - Epoch: [0][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2292 (308.8786)	
2019-01-08 10:50:58,413 - 10 - training_embed.py - training - loss: 308.839261
2019-01-08 10:50:58,413 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:50:58,810 - 10 - training_embed.py - training - Epoch: [1][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7395 (308.9858)	
2019-01-08 10:50:59,036 - 10 - training_embed.py - training - Epoch: [1][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.4564 (308.8756)	
2019-01-08 10:50:59,255 - 10 - training_embed.py - training - Epoch: [1][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2098 (308.7399)	
2019-01-08 10:50:59,472 - 10 - training_embed.py - training - Epoch: [1][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1161 (308.7302)	
2019-01-08 10:50:59,676 - 10 - training_embed.py - training - Epoch: [1][500/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.9265 (308.6815)	
2019-01-08 10:50:59,889 - 10 - training_embed.py - training - Epoch: [1][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4686 (308.6304)	
2019-01-08 10:51:00,100 - 10 - training_embed.py - training - Epoch: [1][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.0573 (308.5944)	
2019-01-08 10:51:00,316 - 10 - training_embed.py - training - Epoch: [1][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.9741 (308.5829)	
2019-01-08 10:51:00,523 - 10 - training_embed.py - training - Epoch: [1][900/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.1572 (308.5730)	
2019-01-08 10:51:00,619 - 10 - training_embed.py - training - loss: 308.522272
2019-01-08 10:51:00,619 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:51:01,006 - 10 - training_embed.py - training - Epoch: [2][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0096 (308.3487)	
2019-01-08 10:51:01,223 - 10 - training_embed.py - training - Epoch: [2][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2490 (308.2772)	
2019-01-08 10:51:01,439 - 10 - training_embed.py - training - Epoch: [2][300/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.8997 (308.2747)	
2019-01-08 10:51:01,676 - 10 - training_embed.py - training - Epoch: [2][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8651 (308.3491)	
2019-01-08 10:51:01,904 - 10 - training_embed.py - training - Epoch: [2][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8335 (308.3044)	
2019-01-08 10:51:02,114 - 10 - training_embed.py - training - Epoch: [2][600/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.2556 (308.2725)	
2019-01-08 10:51:02,323 - 10 - training_embed.py - training - Epoch: [2][700/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.7405 (308.3030)	
2019-01-08 10:51:02,543 - 10 - training_embed.py - training - Epoch: [2][800/944]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 308.0894 (308.2626)	
2019-01-08 10:51:02,763 - 10 - training_embed.py - training - Epoch: [2][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8275 (308.2776)	
2019-01-08 10:51:02,853 - 10 - training_embed.py - training - loss: 308.207634
2019-01-08 10:51:02,854 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:51:03,246 - 10 - training_embed.py - training - Epoch: [3][100/944]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 308.9675 (308.0860)	
2019-01-08 10:51:03,512 - 10 - training_embed.py - training - Epoch: [3][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.7674 (308.1772)	
2019-01-08 10:51:03,743 - 10 - training_embed.py - training - Epoch: [3][300/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.7307 (308.0712)	
2019-01-08 10:51:03,968 - 10 - training_embed.py - training - Epoch: [3][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8223 (308.1300)	
2019-01-08 10:51:04,189 - 10 - training_embed.py - training - Epoch: [3][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.0910 (308.1049)	
2019-01-08 10:51:04,411 - 10 - training_embed.py - training - Epoch: [3][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5358 (307.9903)	
2019-01-08 10:51:04,633 - 10 - training_embed.py - training - Epoch: [3][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.7211 (308.0084)	
2019-01-08 10:51:04,846 - 10 - training_embed.py - training - Epoch: [3][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4321 (308.0115)	
2019-01-08 10:51:05,054 - 10 - training_embed.py - training - Epoch: [3][900/944]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 306.7632 (307.9660)	
2019-01-08 10:51:05,153 - 10 - training_embed.py - training - loss: 307.891863
2019-01-08 10:51:05,153 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:51:05,503 - 10 - training_embed.py - training - Epoch: [4][100/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 309.2906 (307.4931)	
2019-01-08 10:51:05,717 - 10 - training_embed.py - training - Epoch: [4][200/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.1087 (307.5271)	
2019-01-08 10:51:05,934 - 10 - training_embed.py - training - Epoch: [4][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.3467 (307.5568)	
2019-01-08 10:51:06,154 - 10 - training_embed.py - training - Epoch: [4][400/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.5486 (307.6184)	
2019-01-08 10:51:06,362 - 10 - training_embed.py - training - Epoch: [4][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1814 (307.6389)	
2019-01-08 10:51:06,583 - 10 - training_embed.py - training - Epoch: [4][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.3916 (307.6293)	
2019-01-08 10:51:06,801 - 10 - training_embed.py - training - Epoch: [4][700/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.1632 (307.6078)	
2019-01-08 10:51:07,050 - 10 - training_embed.py - training - Epoch: [4][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2831 (307.5901)	
2019-01-08 10:51:07,291 - 10 - training_embed.py - training - Epoch: [4][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8906 (307.6246)	
2019-01-08 10:51:07,386 - 10 - training_embed.py - training - loss: 307.578154
2019-01-08 10:51:07,386 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:51:07,759 - 10 - training_embed.py - training - Epoch: [5][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3690 (307.5100)	
2019-01-08 10:51:07,985 - 10 - training_embed.py - training - Epoch: [5][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3765 (307.4065)	
2019-01-08 10:51:08,201 - 10 - training_embed.py - training - Epoch: [5][300/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.1778 (307.5178)	
2019-01-08 10:51:08,410 - 10 - training_embed.py - training - Epoch: [5][400/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.1954 (307.4751)	
2019-01-08 10:51:08,635 - 10 - training_embed.py - training - Epoch: [5][500/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.7645 (307.4669)	
2019-01-08 10:51:08,844 - 10 - training_embed.py - training - Epoch: [5][600/944]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 304.5150 (307.4123)	
2019-01-08 10:51:09,066 - 10 - training_embed.py - training - Epoch: [5][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6864 (307.3523)	
2019-01-08 10:51:09,280 - 10 - training_embed.py - training - Epoch: [5][800/944]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 308.6775 (307.3395)	
2019-01-08 10:51:09,486 - 10 - training_embed.py - training - Epoch: [5][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3629 (307.3159)	
2019-01-08 10:51:09,581 - 10 - training_embed.py - training - loss: 307.263531
2019-01-08 10:51:09,581 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:51:09,978 - 10 - training_embed.py - training - Epoch: [6][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7361 (306.8254)	
2019-01-08 10:51:10,197 - 10 - training_embed.py - training - Epoch: [6][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3674 (306.9772)	
2019-01-08 10:51:10,412 - 10 - training_embed.py - training - Epoch: [6][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5555 (307.0767)	
2019-01-08 10:51:10,649 - 10 - training_embed.py - training - Epoch: [6][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5795 (307.1760)	
2019-01-08 10:51:10,863 - 10 - training_embed.py - training - Epoch: [6][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5976 (307.1563)	
2019-01-08 10:51:11,074 - 10 - training_embed.py - training - Epoch: [6][600/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.5111 (307.1596)	
2019-01-08 10:51:11,337 - 10 - training_embed.py - training - Epoch: [6][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5540 (307.1152)	
2019-01-08 10:51:11,600 - 10 - training_embed.py - training - Epoch: [6][800/944]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 306.1776 (307.0097)	
2019-01-08 10:51:11,853 - 10 - training_embed.py - training - Epoch: [6][900/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 312.0325 (306.9653)	
2019-01-08 10:51:11,960 - 10 - training_embed.py - training - loss: 306.949968
2019-01-08 10:51:11,960 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:51:12,331 - 10 - training_embed.py - training - Epoch: [7][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4804 (306.9648)	
2019-01-08 10:51:12,566 - 10 - training_embed.py - training - Epoch: [7][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4054 (307.0145)	
2019-01-08 10:51:12,802 - 10 - training_embed.py - training - Epoch: [7][300/944]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.3970 (306.8828)	
2019-01-08 10:51:13,037 - 10 - training_embed.py - training - Epoch: [7][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.2872 (306.8213)	
2019-01-08 10:51:13,260 - 10 - training_embed.py - training - Epoch: [7][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2445 (306.7283)	
2019-01-08 10:51:13,475 - 10 - training_embed.py - training - Epoch: [7][600/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.7350 (306.7429)	
2019-01-08 10:51:13,713 - 10 - training_embed.py - training - Epoch: [7][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.5482 (306.7220)	
2019-01-08 10:51:13,940 - 10 - training_embed.py - training - Epoch: [7][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9821 (306.7129)	
2019-01-08 10:51:14,150 - 10 - training_embed.py - training - Epoch: [7][900/944]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 310.5965 (306.6984)	
2019-01-08 10:51:14,261 - 10 - training_embed.py - training - loss: 306.635242
2019-01-08 10:51:14,261 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:51:14,665 - 10 - training_embed.py - training - Epoch: [8][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2530 (306.5178)	
2019-01-08 10:51:14,893 - 10 - training_embed.py - training - Epoch: [8][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5408 (306.4849)	
2019-01-08 10:51:15,122 - 10 - training_embed.py - training - Epoch: [8][300/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.7183 (306.4480)	
2019-01-08 10:51:15,338 - 10 - training_embed.py - training - Epoch: [8][400/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.8076 (306.4117)	
2019-01-08 10:51:15,564 - 10 - training_embed.py - training - Epoch: [8][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.5337 (306.3430)	
2019-01-08 10:51:15,783 - 10 - training_embed.py - training - Epoch: [8][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.6589 (306.3334)	
2019-01-08 10:51:16,004 - 10 - training_embed.py - training - Epoch: [8][700/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.7939 (306.3885)	
2019-01-08 10:51:16,239 - 10 - training_embed.py - training - Epoch: [8][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.0061 (306.4240)	
2019-01-08 10:51:16,454 - 10 - training_embed.py - training - Epoch: [8][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.7179 (306.3894)	
2019-01-08 10:51:16,544 - 10 - training_embed.py - training - loss: 306.321776
2019-01-08 10:51:16,544 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:51:16,888 - 10 - training_embed.py - training - Epoch: [9][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5435 (306.1845)	
2019-01-08 10:51:17,117 - 10 - training_embed.py - training - Epoch: [9][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4969 (306.1324)	
2019-01-08 10:51:17,349 - 10 - training_embed.py - training - Epoch: [9][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9791 (306.1751)	
2019-01-08 10:51:17,561 - 10 - training_embed.py - training - Epoch: [9][400/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.3875 (306.0932)	
2019-01-08 10:51:17,803 - 10 - training_embed.py - training - Epoch: [9][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2729 (306.0743)	
2019-01-08 10:51:18,020 - 10 - training_embed.py - training - Epoch: [9][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9116 (306.0455)	
2019-01-08 10:51:18,235 - 10 - training_embed.py - training - Epoch: [9][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1108 (306.0889)	
2019-01-08 10:51:18,450 - 10 - training_embed.py - training - Epoch: [9][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.9739 (306.0887)	
2019-01-08 10:51:18,672 - 10 - training_embed.py - training - Epoch: [9][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0177 (306.0681)	
2019-01-08 10:51:18,767 - 10 - training_embed.py - training - loss: 306.007963
2019-01-08 10:51:18,768 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:51:19,148 - 10 - training_embed.py - training - Epoch: [10][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3008 (305.7363)	
2019-01-08 10:51:19,361 - 10 - training_embed.py - training - Epoch: [10][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.2084 (305.6830)	
2019-01-08 10:51:19,570 - 10 - training_embed.py - training - Epoch: [10][300/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.5514 (305.8021)	
2019-01-08 10:51:19,804 - 10 - training_embed.py - training - Epoch: [10][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6175 (305.7382)	
2019-01-08 10:51:20,040 - 10 - training_embed.py - training - Epoch: [10][500/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.7125 (305.8178)	
2019-01-08 10:51:20,255 - 10 - training_embed.py - training - Epoch: [10][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.8818 (305.8194)	
2019-01-08 10:51:20,467 - 10 - training_embed.py - training - Epoch: [10][700/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.6053 (305.7789)	
2019-01-08 10:51:20,717 - 10 - training_embed.py - training - Epoch: [10][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.0376 (305.7599)	
2019-01-08 10:51:20,971 - 10 - training_embed.py - training - Epoch: [10][900/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.4367 (305.7512)	
2019-01-08 10:51:21,078 - 10 - training_embed.py - training - loss: 305.695840
2019-01-08 10:51:21,079 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:51:21,474 - 10 - training_embed.py - training - Epoch: [11][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.9426 (305.3519)	
2019-01-08 10:51:21,702 - 10 - training_embed.py - training - Epoch: [11][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4286 (305.3977)	
2019-01-08 10:51:21,942 - 10 - training_embed.py - training - Epoch: [11][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1223 (305.6008)	
2019-01-08 10:51:22,152 - 10 - training_embed.py - training - Epoch: [11][400/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 302.3426 (305.5955)	
2019-01-08 10:51:22,404 - 10 - training_embed.py - training - Epoch: [11][500/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 302.8781 (305.4928)	
2019-01-08 10:51:22,619 - 10 - training_embed.py - training - Epoch: [11][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.1595 (305.5233)	
2019-01-08 10:51:22,874 - 10 - training_embed.py - training - Epoch: [11][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4146 (305.5079)	
2019-01-08 10:51:23,109 - 10 - training_embed.py - training - Epoch: [11][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4199 (305.4687)	
2019-01-08 10:51:23,342 - 10 - training_embed.py - training - Epoch: [11][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.5654 (305.4178)	
2019-01-08 10:51:23,441 - 10 - training_embed.py - training - loss: 305.383167
2019-01-08 10:51:23,480 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:51:23,891 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 410.989 ms ~ 0.007 min ~ 0.411 sec
2019-01-08 10:51:24,288 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 808.282 ms ~ 0.013 min ~ 0.808 sec
2019-01-08 10:51:24,288 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:51:24,289 - 10 - corpus.py - subactivity_sampler - 0 / 162
2019-01-08 10:51:24,289 - 10 - corpus.py - subactivity_sampler - [38386.  7675. 67760. 53488.  4826. 38025. 31302.]
2019-01-08 10:51:42,100 - 10 - corpus.py - subactivity_sampler - 20 / 162
2019-01-08 10:51:42,100 - 10 - corpus.py - subactivity_sampler - [38398.  7178. 68685. 53114.  4805. 38121. 31161.]
2019-01-08 10:51:58,402 - 10 - corpus.py - subactivity_sampler - 40 / 162
2019-01-08 10:51:58,403 - 10 - corpus.py - subactivity_sampler - [38431.  6528. 70039. 52592.  4776. 38306. 30790.]
2019-01-08 10:52:10,484 - 10 - corpus.py - subactivity_sampler - 60 / 162
2019-01-08 10:52:10,484 - 10 - corpus.py - subactivity_sampler - [38476.  6088. 70390. 52650.  4766. 38907. 30185.]
2019-01-08 10:52:25,759 - 10 - corpus.py - subactivity_sampler - 80 / 162
2019-01-08 10:52:25,760 - 10 - corpus.py - subactivity_sampler - [38551.  5677. 70926. 52377.  4699. 39403. 29829.]
2019-01-08 10:52:41,174 - 10 - corpus.py - subactivity_sampler - 100 / 162
2019-01-08 10:52:41,175 - 10 - corpus.py - subactivity_sampler - [38605.  5414. 71161. 52341.  4671. 39661. 29609.]
2019-01-08 10:52:56,496 - 10 - corpus.py - subactivity_sampler - 120 / 162
2019-01-08 10:52:56,497 - 10 - corpus.py - subactivity_sampler - [38494.  5177. 71751. 52176.  4673. 39967. 29224.]
2019-01-08 10:53:14,476 - 10 - corpus.py - subactivity_sampler - 140 / 162
2019-01-08 10:53:14,477 - 10 - corpus.py - subactivity_sampler - [38081.  4860. 72529. 51238.  5566. 40493. 28695.]
2019-01-08 10:53:30,873 - 10 - corpus.py - subactivity_sampler - 160 / 162
2019-01-08 10:53:30,874 - 10 - corpus.py - subactivity_sampler - [38002.  4593. 72987. 51005.  5693. 40841. 28341.]
2019-01-08 10:53:31,685 - 10 - corpus.py - subactivity_sampler - [38000.  4590. 72993. 51005.  5693. 40842. 28339.]
2019-01-08 10:53:31,685 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 127396.416 ms ~ 2.123 min ~ 127.396 sec
2019-01-08 10:53:31,685 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:53:32,886 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:53:32,886 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 1. 22.  0.  6. 13.  0.]
2019-01-08 10:53:32,886 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:53:32,905 - 10 - corpus.py - rho_sampling - ['62.5872', '24.7747', '51.3459', '180.5025', '5.7431', '17.7993']
2019-01-08 10:53:32,905 - 10 - pipeline.py - baseline - Iteration 4
2019-01-08 10:53:32,974 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:53:32,979 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 10:53:32,979 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 19', '1: 23', '2: 20', '3: 14', '4: 18', '5: 22', '6: 21']
2019-01-08 10:53:32,986 - 10 - accuracy_class.py - mof_val - frames true: 108748	frames overall : 241462
2019-01-08 10:53:32,986 - 10 - corpus.py - accuracy_corpus - Action: juice
2019-01-08 10:53:32,987 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4503731436002352
2019-01-08 10:53:32,987 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4503731436002352
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - mof_classes - label 14: 0.000000  0 / 3377
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - mof_classes - label 18: 0.000000  0 / 1411
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - mof_classes - label 19: 0.711214  16375 / 23024
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - mof_classes - label 20: 0.481459  65269 / 135565
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - mof_classes - label 21: 0.448997  16964 / 37782
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - mof_classes - label 22: 0.675521  10045 / 14870
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - mof_classes - label 23: 0.033510  95 / 2835
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - mof_classes - average class mof: 0.293838
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - iou_classes - label 14: 0.000000  0 / 54382
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - iou_classes - label 18: 0.000000  0 / 7104
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - iou_classes - label 19: 0.366750  16375 / 44649
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - iou_classes - label 20: 0.455506  65269 / 143289
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - iou_classes - label 21: 0.345098  16964 / 49157
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - iou_classes - label 22: 0.219962  10045 / 45667
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - iou_classes - label 23: 0.012960  95 / 7330
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - iou_classes - average IoU: 0.200039
2019-01-08 10:53:32,987 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.175035
2019-01-08 10:53:36,102 - 10 - f1_score.py - f1 - f1 score: 0.364548
2019-01-08 10:53:36,112 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3206.461 ms ~ 0.053 min ~ 3.206 sec
2019-01-08 10:53:36,112 - 10 - corpus.py - embedding_training - .
2019-01-08 10:53:36,112 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:53:36,112 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:53:36,112 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:53:37,752 - 10 - training_embed.py - training - create model
2019-01-08 10:53:37,752 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:53:37,753 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:53:38,104 - 10 - training_embed.py - training - Epoch: [0][100/944]	Time 0.003 (0.003)	Data 0.002 (0.002)	Loss 306.8593 (309.9593)	
2019-01-08 10:53:38,345 - 10 - training_embed.py - training - Epoch: [0][200/944]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 309.6441 (309.9453)	
2019-01-08 10:53:38,565 - 10 - training_embed.py - training - Epoch: [0][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 311.5226 (310.0132)	
2019-01-08 10:53:38,802 - 10 - training_embed.py - training - Epoch: [0][400/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 312.0899 (310.1097)	
2019-01-08 10:53:39,055 - 10 - training_embed.py - training - Epoch: [0][500/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.0677 (310.1365)	
2019-01-08 10:53:39,273 - 10 - training_embed.py - training - Epoch: [0][600/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 314.2291 (310.1030)	
2019-01-08 10:53:39,506 - 10 - training_embed.py - training - Epoch: [0][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1113 (310.0296)	
2019-01-08 10:53:39,744 - 10 - training_embed.py - training - Epoch: [0][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.7598 (310.0390)	
2019-01-08 10:53:40,006 - 10 - training_embed.py - training - Epoch: [0][900/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 311.6010 (309.9844)	
2019-01-08 10:53:40,108 - 10 - training_embed.py - training - loss: 309.947338
2019-01-08 10:53:40,109 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:53:40,507 - 10 - training_embed.py - training - Epoch: [1][100/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 310.4366 (310.0279)	
2019-01-08 10:53:40,748 - 10 - training_embed.py - training - Epoch: [1][200/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 306.2393 (309.9159)	
2019-01-08 10:53:40,978 - 10 - training_embed.py - training - Epoch: [1][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.3643 (309.8288)	
2019-01-08 10:53:41,216 - 10 - training_embed.py - training - Epoch: [1][400/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.8398 (309.8028)	
2019-01-08 10:53:41,433 - 10 - training_embed.py - training - Epoch: [1][500/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 306.1100 (309.7733)	
2019-01-08 10:53:41,641 - 10 - training_embed.py - training - Epoch: [1][600/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 308.8289 (309.7219)	
2019-01-08 10:53:41,845 - 10 - training_embed.py - training - Epoch: [1][700/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 309.6917 (309.6767)	
2019-01-08 10:53:42,064 - 10 - training_embed.py - training - Epoch: [1][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.3127 (309.6621)	
2019-01-08 10:53:42,292 - 10 - training_embed.py - training - Epoch: [1][900/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.6153 (309.6454)	
2019-01-08 10:53:42,392 - 10 - training_embed.py - training - loss: 309.601250
2019-01-08 10:53:42,392 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:53:42,766 - 10 - training_embed.py - training - Epoch: [2][100/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 308.0692 (309.4940)	
2019-01-08 10:53:42,992 - 10 - training_embed.py - training - Epoch: [2][200/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 310.7974 (309.4077)	
2019-01-08 10:53:43,219 - 10 - training_embed.py - training - Epoch: [2][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5786 (309.3577)	
2019-01-08 10:53:43,447 - 10 - training_embed.py - training - Epoch: [2][400/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.5460 (309.4029)	
2019-01-08 10:53:43,691 - 10 - training_embed.py - training - Epoch: [2][500/944]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 310.2415 (309.3506)	
2019-01-08 10:53:43,959 - 10 - training_embed.py - training - Epoch: [2][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.0822 (309.3121)	
2019-01-08 10:53:44,200 - 10 - training_embed.py - training - Epoch: [2][700/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.2157 (309.3357)	
2019-01-08 10:53:44,561 - 10 - training_embed.py - training - Epoch: [2][800/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 308.1145 (309.3103)	
2019-01-08 10:53:44,887 - 10 - training_embed.py - training - Epoch: [2][900/944]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 309.0887 (309.3242)	
2019-01-08 10:53:45,003 - 10 - training_embed.py - training - loss: 309.257131
2019-01-08 10:53:45,004 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:53:45,377 - 10 - training_embed.py - training - Epoch: [3][100/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 308.6935 (309.0916)	
2019-01-08 10:53:45,602 - 10 - training_embed.py - training - Epoch: [3][200/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.3997 (309.1323)	
2019-01-08 10:53:45,819 - 10 - training_embed.py - training - Epoch: [3][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 311.5162 (309.0963)	
2019-01-08 10:53:46,044 - 10 - training_embed.py - training - Epoch: [3][400/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 307.7122 (309.1268)	
2019-01-08 10:53:46,260 - 10 - training_embed.py - training - Epoch: [3][500/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.5807 (309.1287)	
2019-01-08 10:53:46,482 - 10 - training_embed.py - training - Epoch: [3][600/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 306.7831 (309.0030)	
2019-01-08 10:53:46,687 - 10 - training_embed.py - training - Epoch: [3][700/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 311.3825 (309.0250)	
2019-01-08 10:53:46,905 - 10 - training_embed.py - training - Epoch: [3][800/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 305.2630 (309.0231)	
2019-01-08 10:53:47,134 - 10 - training_embed.py - training - Epoch: [3][900/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 306.8691 (308.9839)	
2019-01-08 10:53:47,222 - 10 - training_embed.py - training - loss: 308.912621
2019-01-08 10:53:47,222 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:53:47,623 - 10 - training_embed.py - training - Epoch: [4][100/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.6759 (308.6547)	
2019-01-08 10:53:47,828 - 10 - training_embed.py - training - Epoch: [4][200/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 307.8820 (308.6167)	
2019-01-08 10:53:48,058 - 10 - training_embed.py - training - Epoch: [4][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.6117 (308.6632)	
2019-01-08 10:53:48,286 - 10 - training_embed.py - training - Epoch: [4][400/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 308.6096 (308.6632)	
2019-01-08 10:53:48,509 - 10 - training_embed.py - training - Epoch: [4][500/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.2862 (308.6753)	
2019-01-08 10:53:48,737 - 10 - training_embed.py - training - Epoch: [4][600/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 309.8162 (308.6284)	
2019-01-08 10:53:48,957 - 10 - training_embed.py - training - Epoch: [4][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4521 (308.5966)	
2019-01-08 10:53:49,171 - 10 - training_embed.py - training - Epoch: [4][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.0708 (308.5757)	
2019-01-08 10:53:49,386 - 10 - training_embed.py - training - Epoch: [4][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.1306 (308.6127)	
2019-01-08 10:53:49,473 - 10 - training_embed.py - training - loss: 308.569665
2019-01-08 10:53:49,473 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:53:49,834 - 10 - training_embed.py - training - Epoch: [5][100/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.0614 (308.3951)	
2019-01-08 10:53:50,061 - 10 - training_embed.py - training - Epoch: [5][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5932 (308.3333)	
2019-01-08 10:53:50,292 - 10 - training_embed.py - training - Epoch: [5][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2413 (308.4492)	
2019-01-08 10:53:50,510 - 10 - training_embed.py - training - Epoch: [5][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.4480 (308.4145)	
2019-01-08 10:53:50,747 - 10 - training_embed.py - training - Epoch: [5][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6348 (308.4071)	
2019-01-08 10:53:50,999 - 10 - training_embed.py - training - Epoch: [5][600/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.1132 (308.3726)	
2019-01-08 10:53:51,372 - 10 - training_embed.py - training - Epoch: [5][700/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 308.8858 (308.3134)	
2019-01-08 10:53:51,702 - 10 - training_embed.py - training - Epoch: [5][800/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.7708 (308.2996)	
2019-01-08 10:53:51,936 - 10 - training_embed.py - training - Epoch: [5][900/944]	Time 0.005 (0.003)	Data 0.004 (0.001)	Loss 305.7077 (308.2792)	
2019-01-08 10:53:52,061 - 10 - training_embed.py - training - loss: 308.227015
2019-01-08 10:53:52,061 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:53:52,524 - 10 - training_embed.py - training - Epoch: [6][100/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 305.7221 (307.7494)	
2019-01-08 10:53:52,785 - 10 - training_embed.py - training - Epoch: [6][200/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 308.8941 (307.9105)	
2019-01-08 10:53:53,023 - 10 - training_embed.py - training - Epoch: [6][300/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 308.0200 (308.0353)	
2019-01-08 10:53:53,285 - 10 - training_embed.py - training - Epoch: [6][400/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 310.1852 (308.1164)	
2019-01-08 10:53:53,539 - 10 - training_embed.py - training - Epoch: [6][500/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 310.7302 (308.0977)	
2019-01-08 10:53:53,801 - 10 - training_embed.py - training - Epoch: [6][600/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.1500 (308.1076)	
2019-01-08 10:53:54,030 - 10 - training_embed.py - training - Epoch: [6][700/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 306.9858 (308.0598)	
2019-01-08 10:53:54,271 - 10 - training_embed.py - training - Epoch: [6][800/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 308.0099 (307.9545)	
2019-01-08 10:53:54,475 - 10 - training_embed.py - training - Epoch: [6][900/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 312.3222 (307.9059)	
2019-01-08 10:53:54,564 - 10 - training_embed.py - training - loss: 307.885426
2019-01-08 10:53:54,571 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:53:54,937 - 10 - training_embed.py - training - Epoch: [7][100/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.8961 (307.9787)	
2019-01-08 10:53:55,164 - 10 - training_embed.py - training - Epoch: [7][200/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 306.1375 (307.9712)	
2019-01-08 10:53:55,387 - 10 - training_embed.py - training - Epoch: [7][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 308.4019 (307.8356)	
2019-01-08 10:53:55,606 - 10 - training_embed.py - training - Epoch: [7][400/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.0240 (307.7747)	
2019-01-08 10:53:55,825 - 10 - training_embed.py - training - Epoch: [7][500/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 306.9000 (307.6709)	
2019-01-08 10:53:56,047 - 10 - training_embed.py - training - Epoch: [7][600/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 304.6157 (307.6701)	
2019-01-08 10:53:56,276 - 10 - training_embed.py - training - Epoch: [7][700/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 305.0623 (307.6519)	
2019-01-08 10:53:56,480 - 10 - training_embed.py - training - Epoch: [7][800/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 306.5404 (307.6221)	
2019-01-08 10:53:56,695 - 10 - training_embed.py - training - Epoch: [7][900/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 310.7640 (307.6045)	
2019-01-08 10:53:56,785 - 10 - training_embed.py - training - loss: 307.542561
2019-01-08 10:53:56,786 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:53:57,145 - 10 - training_embed.py - training - Epoch: [8][100/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 308.7763 (307.3849)	
2019-01-08 10:53:57,365 - 10 - training_embed.py - training - Epoch: [8][200/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.2028 (307.4182)	
2019-01-08 10:53:57,616 - 10 - training_embed.py - training - Epoch: [8][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.2497 (307.3868)	
2019-01-08 10:53:57,827 - 10 - training_embed.py - training - Epoch: [8][400/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 305.7217 (307.3201)	
2019-01-08 10:53:58,044 - 10 - training_embed.py - training - Epoch: [8][500/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 304.3234 (307.2517)	
2019-01-08 10:53:58,254 - 10 - training_embed.py - training - Epoch: [8][600/944]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 310.6855 (307.2275)	
2019-01-08 10:53:58,487 - 10 - training_embed.py - training - Epoch: [8][700/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.0365 (307.2855)	
2019-01-08 10:53:58,725 - 10 - training_embed.py - training - Epoch: [8][800/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 305.9825 (307.3069)	
2019-01-08 10:53:58,950 - 10 - training_embed.py - training - Epoch: [8][900/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 305.7313 (307.2620)	
2019-01-08 10:53:59,035 - 10 - training_embed.py - training - loss: 307.201093
2019-01-08 10:53:59,035 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:53:59,425 - 10 - training_embed.py - training - Epoch: [9][100/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 304.5282 (306.9797)	
2019-01-08 10:53:59,644 - 10 - training_embed.py - training - Epoch: [9][200/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.1692 (306.9535)	
2019-01-08 10:53:59,855 - 10 - training_embed.py - training - Epoch: [9][300/944]	Time 0.005 (0.003)	Data 0.003 (0.001)	Loss 308.3556 (306.9984)	
2019-01-08 10:54:00,072 - 10 - training_embed.py - training - Epoch: [9][400/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 306.5604 (306.9297)	
2019-01-08 10:54:00,278 - 10 - training_embed.py - training - Epoch: [9][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9041 (306.9042)	
2019-01-08 10:54:00,493 - 10 - training_embed.py - training - Epoch: [9][600/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.8915 (306.8832)	
2019-01-08 10:54:00,735 - 10 - training_embed.py - training - Epoch: [9][700/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.0917 (306.9247)	
2019-01-08 10:54:00,954 - 10 - training_embed.py - training - Epoch: [9][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.7914 (306.9377)	
2019-01-08 10:54:01,191 - 10 - training_embed.py - training - Epoch: [9][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7284 (306.9200)	
2019-01-08 10:54:01,283 - 10 - training_embed.py - training - loss: 306.859239
2019-01-08 10:54:01,283 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:54:01,673 - 10 - training_embed.py - training - Epoch: [10][100/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 308.1457 (306.6159)	
2019-01-08 10:54:01,902 - 10 - training_embed.py - training - Epoch: [10][200/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 305.7503 (306.5629)	
2019-01-08 10:54:02,104 - 10 - training_embed.py - training - Epoch: [10][300/944]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 310.3167 (306.6605)	
2019-01-08 10:54:02,325 - 10 - training_embed.py - training - Epoch: [10][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0230 (306.6070)	
2019-01-08 10:54:02,537 - 10 - training_embed.py - training - Epoch: [10][500/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.7305 (306.6665)	
2019-01-08 10:54:02,759 - 10 - training_embed.py - training - Epoch: [10][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5479 (306.6635)	
2019-01-08 10:54:02,975 - 10 - training_embed.py - training - Epoch: [10][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5926 (306.6191)	
2019-01-08 10:54:03,192 - 10 - training_embed.py - training - Epoch: [10][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8930 (306.5912)	
2019-01-08 10:54:03,412 - 10 - training_embed.py - training - Epoch: [10][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9429 (306.5790)	
2019-01-08 10:54:03,496 - 10 - training_embed.py - training - loss: 306.519240
2019-01-08 10:54:03,497 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:54:03,909 - 10 - training_embed.py - training - Epoch: [11][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.0916 (306.2016)	
2019-01-08 10:54:04,123 - 10 - training_embed.py - training - Epoch: [11][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0457 (306.2592)	
2019-01-08 10:54:04,342 - 10 - training_embed.py - training - Epoch: [11][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4916 (306.4406)	
2019-01-08 10:54:04,559 - 10 - training_embed.py - training - Epoch: [11][400/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 303.4140 (306.4187)	
2019-01-08 10:54:04,781 - 10 - training_embed.py - training - Epoch: [11][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.7270 (306.3056)	
2019-01-08 10:54:05,003 - 10 - training_embed.py - training - Epoch: [11][600/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.0381 (306.3366)	
2019-01-08 10:54:05,225 - 10 - training_embed.py - training - Epoch: [11][700/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.2527 (306.3144)	
2019-01-08 10:54:05,460 - 10 - training_embed.py - training - Epoch: [11][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9463 (306.2817)	
2019-01-08 10:54:05,676 - 10 - training_embed.py - training - Epoch: [11][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.8896 (306.2169)	
2019-01-08 10:54:05,768 - 10 - training_embed.py - training - loss: 306.178077
2019-01-08 10:54:05,803 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:54:06,211 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 408.610 ms ~ 0.007 min ~ 0.409 sec
2019-01-08 10:54:06,609 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 806.309 ms ~ 0.013 min ~ 0.806 sec
2019-01-08 10:54:06,609 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:54:06,610 - 10 - corpus.py - subactivity_sampler - 0 / 162
2019-01-08 10:54:06,610 - 10 - corpus.py - subactivity_sampler - [38000.  4590. 72993. 51005.  5693. 40842. 28339.]
2019-01-08 10:54:24,410 - 10 - corpus.py - subactivity_sampler - 20 / 162
2019-01-08 10:54:24,411 - 10 - corpus.py - subactivity_sampler - [38023.  4317. 73376. 50627.  6033. 41244. 27842.]
2019-01-08 10:54:40,549 - 10 - corpus.py - subactivity_sampler - 40 / 162
2019-01-08 10:54:40,549 - 10 - corpus.py - subactivity_sampler - [38169.  4008. 73528. 50638.  6020. 42050. 27049.]
2019-01-08 10:54:52,481 - 10 - corpus.py - subactivity_sampler - 60 / 162
2019-01-08 10:54:52,482 - 10 - corpus.py - subactivity_sampler - [38211.  3888. 73572. 50299.  6391. 42591. 26510.]
2019-01-08 10:55:08,078 - 10 - corpus.py - subactivity_sampler - 80 / 162
2019-01-08 10:55:08,078 - 10 - corpus.py - subactivity_sampler - [38190.  3758. 73736. 50282.  6358. 43030. 26108.]
2019-01-08 10:55:24,586 - 10 - corpus.py - subactivity_sampler - 100 / 162
2019-01-08 10:55:24,586 - 10 - corpus.py - subactivity_sampler - [38206.  3604. 73641. 50537.  6358. 43184. 25932.]
2019-01-08 10:55:41,067 - 10 - corpus.py - subactivity_sampler - 120 / 162
2019-01-08 10:55:41,067 - 10 - corpus.py - subactivity_sampler - [38140.  3512. 73682. 50623.  6359. 44007. 25139.]
2019-01-08 10:55:59,699 - 10 - corpus.py - subactivity_sampler - 140 / 162
2019-01-08 10:55:59,699 - 10 - corpus.py - subactivity_sampler - [38159.  3386. 73803. 50601.  6365. 44732. 24416.]
2019-01-08 10:56:16,925 - 10 - corpus.py - subactivity_sampler - 160 / 162
2019-01-08 10:56:16,925 - 10 - corpus.py - subactivity_sampler - [38046.  3312. 73743. 50750.  6439. 45345. 23827.]
2019-01-08 10:56:17,740 - 10 - corpus.py - subactivity_sampler - [38046.  3311. 73743. 50750.  6439. 45347. 23826.]
2019-01-08 10:56:17,741 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 131131.422 ms ~ 2.186 min ~ 131.131 sec
2019-01-08 10:56:17,741 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:56:18,955 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:56:18,955 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 2. 14.  9.  9. 11.  4.]
2019-01-08 10:56:18,955 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:56:18,975 - 10 - corpus.py - rho_sampling - ['59.9687', '8.8633', '791.0993', '69.9444', '11.2901', '38.2277']
2019-01-08 10:56:18,975 - 10 - pipeline.py - baseline - Iteration 5
2019-01-08 10:56:19,046 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:56:19,051 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 10:56:19,051 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 19', '1: 23', '2: 20', '3: 14', '4: 18', '5: 22', '6: 21']
2019-01-08 10:56:19,059 - 10 - accuracy_class.py - mof_val - frames true: 106411	frames overall : 241462
2019-01-08 10:56:19,059 - 10 - corpus.py - accuracy_corpus - Action: juice
2019-01-08 10:56:19,059 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4406946020491837
2019-01-08 10:56:19,059 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4406946020491837
2019-01-08 10:56:19,059 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:56:19,059 - 10 - accuracy_class.py - mof_classes - label 14: 0.000000  0 / 3377
2019-01-08 10:56:19,059 - 10 - accuracy_class.py - mof_classes - label 18: 0.000000  0 / 1411
2019-01-08 10:56:19,059 - 10 - accuracy_class.py - mof_classes - label 19: 0.709825  16343 / 23024
2019-01-08 10:56:19,059 - 10 - accuracy_class.py - mof_classes - label 20: 0.483436  65537 / 135565
2019-01-08 10:56:19,059 - 10 - accuracy_class.py - mof_classes - label 21: 0.375285  14179 / 37782
2019-01-08 10:56:19,059 - 10 - accuracy_class.py - mof_classes - label 22: 0.692737  10301 / 14870
2019-01-08 10:56:19,059 - 10 - accuracy_class.py - mof_classes - label 23: 0.017989  51 / 2835
2019-01-08 10:56:19,059 - 10 - accuracy_class.py - mof_classes - average class mof: 0.284909
2019-01-08 10:56:19,059 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:56:19,059 - 10 - accuracy_class.py - iou_classes - label 14: 0.000000  0 / 54127
2019-01-08 10:56:19,059 - 10 - accuracy_class.py - iou_classes - label 18: 0.000000  0 / 7850
2019-01-08 10:56:19,060 - 10 - accuracy_class.py - iou_classes - label 19: 0.365395  16343 / 44727
2019-01-08 10:56:19,060 - 10 - accuracy_class.py - iou_classes - label 20: 0.455843  65537 / 143771
2019-01-08 10:56:19,060 - 10 - accuracy_class.py - iou_classes - label 21: 0.298952  14179 / 47429
2019-01-08 10:56:19,060 - 10 - accuracy_class.py - iou_classes - label 22: 0.206367  10301 / 49916
2019-01-08 10:56:19,060 - 10 - accuracy_class.py - iou_classes - label 23: 0.008368  51 / 6095
2019-01-08 10:56:19,060 - 10 - accuracy_class.py - iou_classes - average IoU: 0.190703
2019-01-08 10:56:19,060 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.166865
2019-01-08 10:56:22,217 - 10 - f1_score.py - f1 - f1 score: 0.353460
2019-01-08 10:56:22,225 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3250.171 ms ~ 0.054 min ~ 3.250 sec
2019-01-08 10:56:22,226 - 10 - corpus.py - embedding_training - .
2019-01-08 10:56:22,226 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:56:22,226 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:56:22,226 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:56:23,824 - 10 - training_embed.py - training - create model
2019-01-08 10:56:23,824 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:56:23,825 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:56:24,228 - 10 - training_embed.py - training - Epoch: [0][100/944]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 306.9713 (310.5252)	
2019-01-08 10:56:24,462 - 10 - training_embed.py - training - Epoch: [0][200/944]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 310.1690 (310.5422)	
2019-01-08 10:56:24,698 - 10 - training_embed.py - training - Epoch: [0][300/944]	Time 0.004 (0.003)	Data 0.001 (0.002)	Loss 312.0382 (310.5587)	
2019-01-08 10:56:24,949 - 10 - training_embed.py - training - Epoch: [0][400/944]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 312.5107 (310.6433)	
2019-01-08 10:56:25,193 - 10 - training_embed.py - training - Epoch: [0][500/944]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 311.7338 (310.6732)	
2019-01-08 10:56:25,456 - 10 - training_embed.py - training - Epoch: [0][600/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 315.4607 (310.6264)	
2019-01-08 10:56:25,686 - 10 - training_embed.py - training - Epoch: [0][700/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 309.9840 (310.5449)	
2019-01-08 10:56:25,915 - 10 - training_embed.py - training - Epoch: [0][800/944]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 310.7289 (310.5294)	
2019-01-08 10:56:26,152 - 10 - training_embed.py - training - Epoch: [0][900/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 311.2942 (310.4869)	
2019-01-08 10:56:26,249 - 10 - training_embed.py - training - loss: 310.444850
2019-01-08 10:56:26,250 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:56:26,630 - 10 - training_embed.py - training - Epoch: [1][100/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.4532 (310.4442)	
2019-01-08 10:56:26,852 - 10 - training_embed.py - training - Epoch: [1][200/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.1270 (310.3392)	
2019-01-08 10:56:27,089 - 10 - training_embed.py - training - Epoch: [1][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.8391 (310.2509)	
2019-01-08 10:56:27,314 - 10 - training_embed.py - training - Epoch: [1][400/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 309.9111 (310.2536)	
2019-01-08 10:56:27,552 - 10 - training_embed.py - training - Epoch: [1][500/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 307.0237 (310.2487)	
2019-01-08 10:56:27,764 - 10 - training_embed.py - training - Epoch: [1][600/944]	Time 0.005 (0.003)	Data 0.004 (0.001)	Loss 309.8368 (310.2175)	
2019-01-08 10:56:28,001 - 10 - training_embed.py - training - Epoch: [1][700/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 311.2144 (310.1479)	
2019-01-08 10:56:28,251 - 10 - training_embed.py - training - Epoch: [1][800/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 310.7061 (310.1486)	
2019-01-08 10:56:28,498 - 10 - training_embed.py - training - Epoch: [1][900/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 310.1094 (310.1298)	
2019-01-08 10:56:28,598 - 10 - training_embed.py - training - loss: 310.081831
2019-01-08 10:56:28,599 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:56:28,976 - 10 - training_embed.py - training - Epoch: [2][100/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 308.2862 (309.9781)	
2019-01-08 10:56:29,209 - 10 - training_embed.py - training - Epoch: [2][200/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 311.5073 (309.8539)	
2019-01-08 10:56:29,463 - 10 - training_embed.py - training - Epoch: [2][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.8928 (309.8493)	
2019-01-08 10:56:29,703 - 10 - training_embed.py - training - Epoch: [2][400/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.4118 (309.8792)	
2019-01-08 10:56:29,934 - 10 - training_embed.py - training - Epoch: [2][500/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 312.5726 (309.8386)	
2019-01-08 10:56:30,160 - 10 - training_embed.py - training - Epoch: [2][600/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 310.3783 (309.7945)	
2019-01-08 10:56:30,401 - 10 - training_embed.py - training - Epoch: [2][700/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 306.1183 (309.8130)	
2019-01-08 10:56:30,655 - 10 - training_embed.py - training - Epoch: [2][800/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 308.3202 (309.7934)	
2019-01-08 10:56:30,878 - 10 - training_embed.py - training - Epoch: [2][900/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.8539 (309.7888)	
2019-01-08 10:56:30,977 - 10 - training_embed.py - training - loss: 309.720529
2019-01-08 10:56:30,978 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:56:31,358 - 10 - training_embed.py - training - Epoch: [3][100/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 309.6824 (309.4100)	
2019-01-08 10:56:31,621 - 10 - training_embed.py - training - Epoch: [3][200/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.3997 (309.4738)	
2019-01-08 10:56:31,876 - 10 - training_embed.py - training - Epoch: [3][300/944]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 310.9035 (309.4984)	
2019-01-08 10:56:32,118 - 10 - training_embed.py - training - Epoch: [3][400/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 308.1018 (309.5402)	
2019-01-08 10:56:32,405 - 10 - training_embed.py - training - Epoch: [3][500/944]	Time 0.004 (0.003)	Data 0.001 (0.001)	Loss 309.8464 (309.5167)	
2019-01-08 10:56:32,645 - 10 - training_embed.py - training - Epoch: [3][600/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 307.1297 (309.4237)	
2019-01-08 10:56:32,883 - 10 - training_embed.py - training - Epoch: [3][700/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 311.1038 (309.4430)	
2019-01-08 10:56:33,121 - 10 - training_embed.py - training - Epoch: [3][800/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 306.2156 (309.4465)	
2019-01-08 10:56:33,371 - 10 - training_embed.py - training - Epoch: [3][900/944]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 306.1148 (309.4260)	
2019-01-08 10:56:33,472 - 10 - training_embed.py - training - loss: 309.358810
2019-01-08 10:56:33,473 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:56:33,839 - 10 - training_embed.py - training - Epoch: [4][100/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.7262 (309.0447)	
2019-01-08 10:56:34,064 - 10 - training_embed.py - training - Epoch: [4][200/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 309.1434 (309.0216)	
2019-01-08 10:56:34,301 - 10 - training_embed.py - training - Epoch: [4][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.5481 (309.0873)	
2019-01-08 10:56:34,526 - 10 - training_embed.py - training - Epoch: [4][400/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 308.7639 (309.0460)	
2019-01-08 10:56:34,762 - 10 - training_embed.py - training - Epoch: [4][500/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 311.3262 (309.0628)	
2019-01-08 10:56:35,011 - 10 - training_embed.py - training - Epoch: [4][600/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.9799 (309.0343)	
2019-01-08 10:56:35,232 - 10 - training_embed.py - training - Epoch: [4][700/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 308.4005 (309.0053)	
2019-01-08 10:56:35,476 - 10 - training_embed.py - training - Epoch: [4][800/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 311.4835 (308.9873)	
2019-01-08 10:56:35,718 - 10 - training_embed.py - training - Epoch: [4][900/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 311.1252 (309.0318)	
2019-01-08 10:56:35,831 - 10 - training_embed.py - training - loss: 308.998370
2019-01-08 10:56:35,831 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:56:36,198 - 10 - training_embed.py - training - Epoch: [5][100/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.4984 (308.8472)	
2019-01-08 10:56:36,546 - 10 - training_embed.py - training - Epoch: [5][200/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 306.0084 (308.7728)	
2019-01-08 10:56:36,777 - 10 - training_embed.py - training - Epoch: [5][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.7359 (308.8000)	
2019-01-08 10:56:37,007 - 10 - training_embed.py - training - Epoch: [5][400/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.4167 (308.7773)	
2019-01-08 10:56:37,247 - 10 - training_embed.py - training - Epoch: [5][500/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 308.8333 (308.7798)	
2019-01-08 10:56:37,487 - 10 - training_embed.py - training - Epoch: [5][600/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 304.9451 (308.7500)	
2019-01-08 10:56:37,728 - 10 - training_embed.py - training - Epoch: [5][700/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.4037 (308.7131)	
2019-01-08 10:56:37,959 - 10 - training_embed.py - training - Epoch: [5][800/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.0951 (308.7110)	
2019-01-08 10:56:38,207 - 10 - training_embed.py - training - Epoch: [5][900/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 306.1449 (308.6931)	
2019-01-08 10:56:38,302 - 10 - training_embed.py - training - loss: 308.639232
2019-01-08 10:56:38,303 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:56:38,676 - 10 - training_embed.py - training - Epoch: [6][100/944]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 307.6871 (308.2279)	
2019-01-08 10:56:38,917 - 10 - training_embed.py - training - Epoch: [6][200/944]	Time 0.007 (0.003)	Data 0.002 (0.001)	Loss 308.9721 (308.2905)	
2019-01-08 10:56:39,164 - 10 - training_embed.py - training - Epoch: [6][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 308.6390 (308.4453)	
2019-01-08 10:56:39,401 - 10 - training_embed.py - training - Epoch: [6][400/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.5004 (308.5134)	
2019-01-08 10:56:39,634 - 10 - training_embed.py - training - Epoch: [6][500/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.0021 (308.4882)	
2019-01-08 10:56:39,865 - 10 - training_embed.py - training - Epoch: [6][600/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 308.6632 (308.4834)	
2019-01-08 10:56:40,189 - 10 - training_embed.py - training - Epoch: [6][700/944]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 305.8812 (308.4478)	
2019-01-08 10:56:40,425 - 10 - training_embed.py - training - Epoch: [6][800/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.6237 (308.3410)	
2019-01-08 10:56:40,652 - 10 - training_embed.py - training - Epoch: [6][900/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 312.6215 (308.2987)	
2019-01-08 10:56:40,748 - 10 - training_embed.py - training - loss: 308.280821
2019-01-08 10:56:40,748 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:56:41,117 - 10 - training_embed.py - training - Epoch: [7][100/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 309.6158 (308.3018)	
2019-01-08 10:56:41,351 - 10 - training_embed.py - training - Epoch: [7][200/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.5898 (308.3222)	
2019-01-08 10:56:41,575 - 10 - training_embed.py - training - Epoch: [7][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 308.1994 (308.2138)	
2019-01-08 10:56:41,799 - 10 - training_embed.py - training - Epoch: [7][400/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 306.0877 (308.1568)	
2019-01-08 10:56:42,008 - 10 - training_embed.py - training - Epoch: [7][500/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 308.6398 (308.0598)	
2019-01-08 10:56:42,221 - 10 - training_embed.py - training - Epoch: [7][600/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 304.0988 (308.0619)	
2019-01-08 10:56:42,449 - 10 - training_embed.py - training - Epoch: [7][700/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.4874 (308.0455)	
2019-01-08 10:56:42,662 - 10 - training_embed.py - training - Epoch: [7][800/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 307.6066 (308.0015)	
2019-01-08 10:56:42,879 - 10 - training_embed.py - training - Epoch: [7][900/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 310.3987 (307.9782)	
2019-01-08 10:56:42,976 - 10 - training_embed.py - training - loss: 307.920922
2019-01-08 10:56:42,977 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:56:43,362 - 10 - training_embed.py - training - Epoch: [8][100/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.7820 (307.7954)	
2019-01-08 10:56:43,589 - 10 - training_embed.py - training - Epoch: [8][200/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.1901 (307.8042)	
2019-01-08 10:56:43,807 - 10 - training_embed.py - training - Epoch: [8][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 308.8347 (307.7936)	
2019-01-08 10:56:44,024 - 10 - training_embed.py - training - Epoch: [8][400/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 305.0144 (307.7238)	
2019-01-08 10:56:44,248 - 10 - training_embed.py - training - Epoch: [8][500/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 304.8170 (307.6642)	
2019-01-08 10:56:44,469 - 10 - training_embed.py - training - Epoch: [8][600/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 311.9529 (307.6243)	
2019-01-08 10:56:44,693 - 10 - training_embed.py - training - Epoch: [8][700/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.0720 (307.6866)	
2019-01-08 10:56:44,914 - 10 - training_embed.py - training - Epoch: [8][800/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.0139 (307.6814)	
2019-01-08 10:56:45,142 - 10 - training_embed.py - training - Epoch: [8][900/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 306.4511 (307.6247)	
2019-01-08 10:56:45,237 - 10 - training_embed.py - training - loss: 307.561832
2019-01-08 10:56:45,238 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:56:45,628 - 10 - training_embed.py - training - Epoch: [9][100/944]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 305.9828 (307.2600)	
2019-01-08 10:56:45,839 - 10 - training_embed.py - training - Epoch: [9][200/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.5988 (307.2037)	
2019-01-08 10:56:46,050 - 10 - training_embed.py - training - Epoch: [9][300/944]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 310.1044 (307.2658)	
2019-01-08 10:56:46,268 - 10 - training_embed.py - training - Epoch: [9][400/944]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 304.9471 (307.2518)	
2019-01-08 10:56:46,487 - 10 - training_embed.py - training - Epoch: [9][500/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 305.6530 (307.2310)	
2019-01-08 10:56:46,700 - 10 - training_embed.py - training - Epoch: [9][600/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 306.7481 (307.2239)	
2019-01-08 10:56:46,907 - 10 - training_embed.py - training - Epoch: [9][700/944]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 308.0010 (307.2512)	
2019-01-08 10:56:47,132 - 10 - training_embed.py - training - Epoch: [9][800/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.9048 (307.2536)	
2019-01-08 10:56:47,337 - 10 - training_embed.py - training - Epoch: [9][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1431 (307.2515)	
2019-01-08 10:56:47,436 - 10 - training_embed.py - training - loss: 307.203415
2019-01-08 10:56:47,437 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:56:47,821 - 10 - training_embed.py - training - Epoch: [10][100/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 307.8253 (306.9650)	
2019-01-08 10:56:48,043 - 10 - training_embed.py - training - Epoch: [10][200/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 306.3670 (306.9189)	
2019-01-08 10:56:48,269 - 10 - training_embed.py - training - Epoch: [10][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 311.3021 (307.0165)	
2019-01-08 10:56:48,489 - 10 - training_embed.py - training - Epoch: [10][400/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 305.7332 (306.9355)	
2019-01-08 10:56:48,712 - 10 - training_embed.py - training - Epoch: [10][500/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 305.3437 (307.0110)	
2019-01-08 10:56:48,931 - 10 - training_embed.py - training - Epoch: [10][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0179 (306.9988)	
2019-01-08 10:56:49,149 - 10 - training_embed.py - training - Epoch: [10][700/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.4223 (306.9546)	
2019-01-08 10:56:49,377 - 10 - training_embed.py - training - Epoch: [10][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8540 (306.9185)	
2019-01-08 10:56:49,606 - 10 - training_embed.py - training - Epoch: [10][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1954 (306.9048)	
2019-01-08 10:56:49,693 - 10 - training_embed.py - training - loss: 306.846585
2019-01-08 10:56:49,694 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:56:50,081 - 10 - training_embed.py - training - Epoch: [11][100/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.6117 (306.6766)	
2019-01-08 10:56:50,299 - 10 - training_embed.py - training - Epoch: [11][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6638 (306.5734)	
2019-01-08 10:56:50,517 - 10 - training_embed.py - training - Epoch: [11][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3324 (306.7253)	
2019-01-08 10:56:50,734 - 10 - training_embed.py - training - Epoch: [11][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.9175 (306.7284)	
2019-01-08 10:56:50,964 - 10 - training_embed.py - training - Epoch: [11][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.4438 (306.6328)	
2019-01-08 10:56:51,185 - 10 - training_embed.py - training - Epoch: [11][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7417 (306.6237)	
2019-01-08 10:56:51,400 - 10 - training_embed.py - training - Epoch: [11][700/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.2750 (306.5954)	
2019-01-08 10:56:51,607 - 10 - training_embed.py - training - Epoch: [11][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.2997 (306.5647)	
2019-01-08 10:56:51,844 - 10 - training_embed.py - training - Epoch: [11][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.2709 (306.5220)	
2019-01-08 10:56:51,933 - 10 - training_embed.py - training - loss: 306.487862
2019-01-08 10:56:51,967 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:56:52,387 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 420.071 ms ~ 0.007 min ~ 0.420 sec
2019-01-08 10:56:52,781 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 813.514 ms ~ 0.014 min ~ 0.814 sec
2019-01-08 10:56:52,781 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:56:52,781 - 10 - corpus.py - subactivity_sampler - 0 / 162
2019-01-08 10:56:52,781 - 10 - corpus.py - subactivity_sampler - [38046.  3311. 73743. 50750.  6439. 45347. 23826.]
2019-01-08 10:57:10,377 - 10 - corpus.py - subactivity_sampler - 20 / 162
2019-01-08 10:57:10,377 - 10 - corpus.py - subactivity_sampler - [38080.  3261. 74090. 50166.  6713. 45886. 23266.]
2019-01-08 10:57:26,445 - 10 - corpus.py - subactivity_sampler - 40 / 162
2019-01-08 10:57:26,445 - 10 - corpus.py - subactivity_sampler - [37952.  3212. 74332. 50075.  6708. 46496. 22687.]
2019-01-08 10:57:38,300 - 10 - corpus.py - subactivity_sampler - 60 / 162
2019-01-08 10:57:38,300 - 10 - corpus.py - subactivity_sampler - [37842.  3166. 74417. 49646.  7177. 46963. 22251.]
2019-01-08 10:57:53,395 - 10 - corpus.py - subactivity_sampler - 80 / 162
2019-01-08 10:57:53,395 - 10 - corpus.py - subactivity_sampler - [37808.  3124. 74477. 49651.  7176. 47287. 21939.]
2019-01-08 10:58:08,772 - 10 - corpus.py - subactivity_sampler - 100 / 162
2019-01-08 10:58:08,772 - 10 - corpus.py - subactivity_sampler - [37812.  3097. 74192. 49964.  7182. 47944. 21271.]
2019-01-08 10:58:24,636 - 10 - corpus.py - subactivity_sampler - 120 / 162
2019-01-08 10:58:24,636 - 10 - corpus.py - subactivity_sampler - [37801.  3058. 74076. 50107.  7241. 48552. 20627.]
2019-01-08 10:58:42,615 - 10 - corpus.py - subactivity_sampler - 140 / 162
2019-01-08 10:58:42,615 - 10 - corpus.py - subactivity_sampler - [37816.  3021. 74374. 49948.  7236. 49002. 20065.]
2019-01-08 10:58:59,114 - 10 - corpus.py - subactivity_sampler - 160 / 162
2019-01-08 10:58:59,114 - 10 - corpus.py - subactivity_sampler - [37814.  3000. 73969. 50358.  7251. 49254. 19816.]
2019-01-08 10:59:00,045 - 10 - corpus.py - subactivity_sampler - [37816.  2999. 73969. 50358.  7251. 49254. 19815.]
2019-01-08 10:59:00,045 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 127264.642 ms ~ 2.121 min ~ 127.265 sec
2019-01-08 10:59:00,045 - 10 - corpus.py - ordering_sampler - .
2019-01-08 10:59:02,249 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 10:59:02,249 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 0. 15.  0.  8.  8.  5.]
2019-01-08 10:59:02,249 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 10:59:02,265 - 10 - corpus.py - rho_sampling - ['62.5006', '2.1628', '51.8368', '181.2739', '6.1141', '9.5559']
2019-01-08 10:59:02,265 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 10:59:02,334 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 10:59:02,338 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 10:59:02,338 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 19', '1: 23', '2: 20', '3: 14', '4: 18', '5: 21', '6: 22']
2019-01-08 10:59:02,346 - 10 - accuracy_class.py - mof_val - frames true: 106096	frames overall : 241462
2019-01-08 10:59:02,346 - 10 - corpus.py - accuracy_corpus - Action: juice
2019-01-08 10:59:02,346 - 10 - corpus.py - accuracy_corpus - MoF val: 0.43939004895180195
2019-01-08 10:59:02,346 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4291855447233933
2019-01-08 10:59:02,346 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:59:02,346 - 10 - accuracy_class.py - mof_classes - label 14: 0.000000  0 / 3377
2019-01-08 10:59:02,346 - 10 - accuracy_class.py - mof_classes - label 18: 0.000000  0 / 1411
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - mof_classes - label 19: 0.706654  16270 / 23024
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - mof_classes - label 20: 0.482101  65356 / 135565
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - mof_classes - label 21: 0.616828  23305 / 37782
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - mof_classes - label 22: 0.076261  1134 / 14870
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - mof_classes - label 23: 0.010935  31 / 2835
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - mof_classes - average class mof: 0.236597
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22598
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - iou_classes - label 14: 0.000000  0 / 53735
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - iou_classes - label 18: 0.000000  0 / 8662
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - iou_classes - label 19: 0.365044  16270 / 44570
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - iou_classes - label 20: 0.453301  65356 / 144178
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - iou_classes - label 21: 0.365678  23305 / 63731
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - iou_classes - label 22: 0.033799  1134 / 33551
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - iou_classes - label 23: 0.005342  31 / 5803
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - iou_classes - average IoU: 0.174738
2019-01-08 10:59:02,347 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.152895
2019-01-08 10:59:05,434 - 10 - f1_score.py - f1 - f1 score: 0.306422
2019-01-08 10:59:05,443 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3178.157 ms ~ 0.053 min ~ 3.178 sec
2019-01-08 10:59:05,443 - 10 - corpus.py - embedding_training - .
2019-01-08 10:59:05,443 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 10:59:05,443 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 10:59:05,443 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 10:59:07,010 - 10 - training_embed.py - training - create model
2019-01-08 10:59:07,011 - 10 - training_embed.py - training - epochs: 12
2019-01-08 10:59:07,011 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 10:59:07,368 - 10 - training_embed.py - training - Epoch: [0][100/944]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 307.3398 (310.8280)	
2019-01-08 10:59:07,578 - 10 - training_embed.py - training - Epoch: [0][200/944]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 310.5594 (310.9041)	
2019-01-08 10:59:07,796 - 10 - training_embed.py - training - Epoch: [0][300/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 312.7036 (310.9273)	
2019-01-08 10:59:08,017 - 10 - training_embed.py - training - Epoch: [0][400/944]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 313.2189 (311.0117)	
2019-01-08 10:59:08,231 - 10 - training_embed.py - training - Epoch: [0][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.5892 (311.0392)	
2019-01-08 10:59:08,442 - 10 - training_embed.py - training - Epoch: [0][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 316.8099 (311.0095)	
2019-01-08 10:59:08,662 - 10 - training_embed.py - training - Epoch: [0][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.2375 (310.9572)	
2019-01-08 10:59:08,888 - 10 - training_embed.py - training - Epoch: [0][800/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 311.3893 (310.9472)	
2019-01-08 10:59:09,104 - 10 - training_embed.py - training - Epoch: [0][900/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 311.4190 (310.9058)	
2019-01-08 10:59:09,200 - 10 - training_embed.py - training - loss: 310.859984
2019-01-08 10:59:09,201 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 10:59:09,579 - 10 - training_embed.py - training - Epoch: [1][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.7491 (310.7848)	
2019-01-08 10:59:09,794 - 10 - training_embed.py - training - Epoch: [1][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5172 (310.6989)	
2019-01-08 10:59:10,011 - 10 - training_embed.py - training - Epoch: [1][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8059 (310.6724)	
2019-01-08 10:59:10,224 - 10 - training_embed.py - training - Epoch: [1][400/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 310.4557 (310.6763)	
2019-01-08 10:59:10,436 - 10 - training_embed.py - training - Epoch: [1][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8309 (310.6857)	
2019-01-08 10:59:10,663 - 10 - training_embed.py - training - Epoch: [1][600/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 309.8468 (310.6340)	
2019-01-08 10:59:10,874 - 10 - training_embed.py - training - Epoch: [1][700/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.8010 (310.5642)	
2019-01-08 10:59:11,083 - 10 - training_embed.py - training - Epoch: [1][800/944]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 311.1013 (310.5648)	
2019-01-08 10:59:11,293 - 10 - training_embed.py - training - Epoch: [1][900/944]	Time 0.005 (0.002)	Data 0.002 (0.001)	Loss 312.0984 (310.5324)	
2019-01-08 10:59:11,389 - 10 - training_embed.py - training - loss: 310.483541
2019-01-08 10:59:11,389 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 10:59:11,776 - 10 - training_embed.py - training - Epoch: [2][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1811 (310.2777)	
2019-01-08 10:59:11,994 - 10 - training_embed.py - training - Epoch: [2][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.6003 (310.2515)	
2019-01-08 10:59:12,210 - 10 - training_embed.py - training - Epoch: [2][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9593 (310.2375)	
2019-01-08 10:59:12,426 - 10 - training_embed.py - training - Epoch: [2][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.6067 (310.2562)	
2019-01-08 10:59:12,639 - 10 - training_embed.py - training - Epoch: [2][500/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 313.2246 (310.2283)	
2019-01-08 10:59:12,854 - 10 - training_embed.py - training - Epoch: [2][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.2162 (310.1841)	
2019-01-08 10:59:13,063 - 10 - training_embed.py - training - Epoch: [2][700/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.8608 (310.2029)	
2019-01-08 10:59:13,285 - 10 - training_embed.py - training - Epoch: [2][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.0486 (310.1733)	
2019-01-08 10:59:13,508 - 10 - training_embed.py - training - Epoch: [2][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6276 (310.1792)	
2019-01-08 10:59:13,597 - 10 - training_embed.py - training - loss: 310.108628
2019-01-08 10:59:13,598 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 10:59:13,960 - 10 - training_embed.py - training - Epoch: [3][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.8104 (309.8197)	
2019-01-08 10:59:14,165 - 10 - training_embed.py - training - Epoch: [3][200/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.9073 (309.8529)	
2019-01-08 10:59:14,380 - 10 - training_embed.py - training - Epoch: [3][300/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 311.3676 (309.8751)	
2019-01-08 10:59:14,601 - 10 - training_embed.py - training - Epoch: [3][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4920 (309.9541)	
2019-01-08 10:59:14,824 - 10 - training_embed.py - training - Epoch: [3][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.9742 (309.9356)	
2019-01-08 10:59:15,049 - 10 - training_embed.py - training - Epoch: [3][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8871 (309.8250)	
2019-01-08 10:59:15,280 - 10 - training_embed.py - training - Epoch: [3][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.2957 (309.8472)	
2019-01-08 10:59:15,505 - 10 - training_embed.py - training - Epoch: [3][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1907 (309.8318)	
2019-01-08 10:59:15,728 - 10 - training_embed.py - training - Epoch: [3][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3702 (309.7988)	
2019-01-08 10:59:15,836 - 10 - training_embed.py - training - loss: 309.733219
2019-01-08 10:59:15,836 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 10:59:16,257 - 10 - training_embed.py - training - Epoch: [4][100/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.4140 (309.4099)	
2019-01-08 10:59:16,502 - 10 - training_embed.py - training - Epoch: [4][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.7028 (309.3844)	
2019-01-08 10:59:16,756 - 10 - training_embed.py - training - Epoch: [4][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8394 (309.4701)	
2019-01-08 10:59:17,014 - 10 - training_embed.py - training - Epoch: [4][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8545 (309.3948)	
2019-01-08 10:59:17,232 - 10 - training_embed.py - training - Epoch: [4][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.7678 (309.4188)	
2019-01-08 10:59:17,462 - 10 - training_embed.py - training - Epoch: [4][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.1825 (309.3840)	
2019-01-08 10:59:17,673 - 10 - training_embed.py - training - Epoch: [4][700/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.6825 (309.3705)	
2019-01-08 10:59:17,895 - 10 - training_embed.py - training - Epoch: [4][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.1075 (309.3521)	
2019-01-08 10:59:18,100 - 10 - training_embed.py - training - Epoch: [4][900/944]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 309.9891 (309.3874)	
2019-01-08 10:59:18,191 - 10 - training_embed.py - training - loss: 309.359356
2019-01-08 10:59:18,191 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 10:59:18,589 - 10 - training_embed.py - training - Epoch: [5][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2667 (309.1866)	
2019-01-08 10:59:18,805 - 10 - training_embed.py - training - Epoch: [5][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7004 (309.1579)	
2019-01-08 10:59:19,037 - 10 - training_embed.py - training - Epoch: [5][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.0342 (309.1833)	
2019-01-08 10:59:19,277 - 10 - training_embed.py - training - Epoch: [5][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 312.7336 (309.1381)	
2019-01-08 10:59:19,493 - 10 - training_embed.py - training - Epoch: [5][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6483 (309.1630)	
2019-01-08 10:59:19,721 - 10 - training_embed.py - training - Epoch: [5][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5663 (309.1049)	
2019-01-08 10:59:19,936 - 10 - training_embed.py - training - Epoch: [5][700/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.3261 (309.0670)	
2019-01-08 10:59:20,170 - 10 - training_embed.py - training - Epoch: [5][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.2008 (309.0558)	
2019-01-08 10:59:20,376 - 10 - training_embed.py - training - Epoch: [5][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1376 (309.0386)	
2019-01-08 10:59:20,470 - 10 - training_embed.py - training - loss: 308.986345
2019-01-08 10:59:20,470 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 10:59:20,863 - 10 - training_embed.py - training - Epoch: [6][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4663 (308.5824)	
2019-01-08 10:59:21,087 - 10 - training_embed.py - training - Epoch: [6][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.0289 (308.7040)	
2019-01-08 10:59:21,326 - 10 - training_embed.py - training - Epoch: [6][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8877 (308.8074)	
2019-01-08 10:59:21,546 - 10 - training_embed.py - training - Epoch: [6][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4162 (308.8843)	
2019-01-08 10:59:21,784 - 10 - training_embed.py - training - Epoch: [6][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8506 (308.8465)	
2019-01-08 10:59:22,010 - 10 - training_embed.py - training - Epoch: [6][600/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.8302 (308.8488)	
2019-01-08 10:59:22,234 - 10 - training_embed.py - training - Epoch: [6][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8199 (308.7864)	
2019-01-08 10:59:22,482 - 10 - training_embed.py - training - Epoch: [6][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.5938 (308.6750)	
2019-01-08 10:59:22,738 - 10 - training_embed.py - training - Epoch: [6][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 313.1324 (308.6395)	
2019-01-08 10:59:22,835 - 10 - training_embed.py - training - loss: 308.613998
2019-01-08 10:59:22,835 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 10:59:23,208 - 10 - training_embed.py - training - Epoch: [7][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.6560 (308.7326)	
2019-01-08 10:59:23,458 - 10 - training_embed.py - training - Epoch: [7][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.0646 (308.7487)	
2019-01-08 10:59:23,682 - 10 - training_embed.py - training - Epoch: [7][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9687 (308.5856)	
2019-01-08 10:59:23,918 - 10 - training_embed.py - training - Epoch: [7][400/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.5793 (308.5008)	
2019-01-08 10:59:24,165 - 10 - training_embed.py - training - Epoch: [7][500/944]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 308.3493 (308.3992)	
2019-01-08 10:59:24,386 - 10 - training_embed.py - training - Epoch: [7][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1660 (308.3914)	
2019-01-08 10:59:24,601 - 10 - training_embed.py - training - Epoch: [7][700/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.0707 (308.3486)	
2019-01-08 10:59:24,820 - 10 - training_embed.py - training - Epoch: [7][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6669 (308.3089)	
2019-01-08 10:59:25,055 - 10 - training_embed.py - training - Epoch: [7][900/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.8380 (308.3001)	
2019-01-08 10:59:25,154 - 10 - training_embed.py - training - loss: 308.241637
2019-01-08 10:59:25,154 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 10:59:25,539 - 10 - training_embed.py - training - Epoch: [8][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7985 (308.0577)	
2019-01-08 10:59:25,753 - 10 - training_embed.py - training - Epoch: [8][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.7651 (308.1210)	
2019-01-08 10:59:25,968 - 10 - training_embed.py - training - Epoch: [8][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2480 (308.1059)	
2019-01-08 10:59:26,184 - 10 - training_embed.py - training - Epoch: [8][400/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7301 (308.0220)	
2019-01-08 10:59:26,407 - 10 - training_embed.py - training - Epoch: [8][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0339 (307.9555)	
2019-01-08 10:59:26,618 - 10 - training_embed.py - training - Epoch: [8][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 312.2427 (307.9102)	
2019-01-08 10:59:26,821 - 10 - training_embed.py - training - Epoch: [8][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4692 (307.9757)	
2019-01-08 10:59:27,033 - 10 - training_embed.py - training - Epoch: [8][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.4683 (307.9746)	
2019-01-08 10:59:27,257 - 10 - training_embed.py - training - Epoch: [8][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2668 (307.9362)	
2019-01-08 10:59:27,350 - 10 - training_embed.py - training - loss: 307.869474
2019-01-08 10:59:27,351 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 10:59:27,749 - 10 - training_embed.py - training - Epoch: [9][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9435 (307.5825)	
2019-01-08 10:59:27,960 - 10 - training_embed.py - training - Epoch: [9][200/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.3295 (307.5690)	
2019-01-08 10:59:28,176 - 10 - training_embed.py - training - Epoch: [9][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.7396 (307.5990)	
2019-01-08 10:59:28,392 - 10 - training_embed.py - training - Epoch: [9][400/944]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 306.5933 (307.5677)	
2019-01-08 10:59:28,631 - 10 - training_embed.py - training - Epoch: [9][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9651 (307.5504)	
2019-01-08 10:59:28,855 - 10 - training_embed.py - training - Epoch: [9][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1447 (307.5348)	
2019-01-08 10:59:29,080 - 10 - training_embed.py - training - Epoch: [9][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6596 (307.5665)	
2019-01-08 10:59:29,300 - 10 - training_embed.py - training - Epoch: [9][800/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.3720 (307.5652)	
2019-01-08 10:59:29,506 - 10 - training_embed.py - training - Epoch: [9][900/944]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 309.7533 (307.5587)	
2019-01-08 10:59:29,596 - 10 - training_embed.py - training - loss: 307.497072
2019-01-08 10:59:29,597 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 10:59:29,977 - 10 - training_embed.py - training - Epoch: [10][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7915 (307.3154)	
2019-01-08 10:59:30,200 - 10 - training_embed.py - training - Epoch: [10][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6733 (307.1896)	
2019-01-08 10:59:30,426 - 10 - training_embed.py - training - Epoch: [10][300/944]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 312.8600 (307.3006)	
2019-01-08 10:59:30,638 - 10 - training_embed.py - training - Epoch: [10][400/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.2551 (307.2373)	
2019-01-08 10:59:30,848 - 10 - training_embed.py - training - Epoch: [10][500/944]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 304.6624 (307.2972)	
2019-01-08 10:59:31,075 - 10 - training_embed.py - training - Epoch: [10][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8707 (307.2690)	
2019-01-08 10:59:31,293 - 10 - training_embed.py - training - Epoch: [10][700/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.5343 (307.2376)	
2019-01-08 10:59:31,513 - 10 - training_embed.py - training - Epoch: [10][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.3299 (307.1926)	
2019-01-08 10:59:31,735 - 10 - training_embed.py - training - Epoch: [10][900/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0325 (307.1876)	
2019-01-08 10:59:31,829 - 10 - training_embed.py - training - loss: 307.126511
2019-01-08 10:59:31,829 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 10:59:32,209 - 10 - training_embed.py - training - Epoch: [11][100/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.4671 (306.8736)	
2019-01-08 10:59:32,433 - 10 - training_embed.py - training - Epoch: [11][200/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.9019 (306.8166)	
2019-01-08 10:59:32,648 - 10 - training_embed.py - training - Epoch: [11][300/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.0164 (306.9708)	
2019-01-08 10:59:32,872 - 10 - training_embed.py - training - Epoch: [11][400/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 303.2197 (306.9949)	
2019-01-08 10:59:33,106 - 10 - training_embed.py - training - Epoch: [11][500/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.7847 (306.8792)	
2019-01-08 10:59:33,312 - 10 - training_embed.py - training - Epoch: [11][600/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3568 (306.8962)	
2019-01-08 10:59:33,535 - 10 - training_embed.py - training - Epoch: [11][700/944]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6901 (306.8717)	
2019-01-08 10:59:33,747 - 10 - training_embed.py - training - Epoch: [11][800/944]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.1350 (306.8499)	
2019-01-08 10:59:33,951 - 10 - training_embed.py - training - Epoch: [11][900/944]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 303.7575 (306.7945)	
2019-01-08 10:59:34,044 - 10 - training_embed.py - training - loss: 306.754990
2019-01-08 10:59:34,079 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 10:59:34,475 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 395.671 ms ~ 0.007 min ~ 0.396 sec
2019-01-08 10:59:34,868 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 788.807 ms ~ 0.013 min ~ 0.789 sec
2019-01-08 10:59:34,868 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 10:59:34,868 - 10 - corpus.py - subactivity_sampler - 0 / 162
2019-01-08 10:59:34,869 - 10 - corpus.py - subactivity_sampler - [37816.  2999. 73969. 50358.  7251. 49254. 19815.]
2019-01-08 10:59:52,775 - 10 - corpus.py - subactivity_sampler - 20 / 162
2019-01-08 10:59:52,775 - 10 - corpus.py - subactivity_sampler - [37696.  2985. 74210. 50229.  7251. 49535. 19556.]
2019-01-08 11:00:09,012 - 10 - corpus.py - subactivity_sampler - 40 / 162
2019-01-08 11:00:09,012 - 10 - corpus.py - subactivity_sampler - [37621.  2973. 74517. 49989.  7247. 49691. 19424.]
2019-01-08 11:00:21,852 - 10 - corpus.py - subactivity_sampler - 60 / 162
2019-01-08 11:00:21,852 - 10 - corpus.py - subactivity_sampler - [37483.  2964. 74564. 50018.  7320. 49879. 19234.]
2019-01-08 11:00:39,207 - 10 - corpus.py - subactivity_sampler - 80 / 162
2019-01-08 11:00:39,207 - 10 - corpus.py - subactivity_sampler - [37271.  2968. 74736. 50051.  7318. 50329. 18789.]
2019-01-08 11:00:57,174 - 10 - corpus.py - subactivity_sampler - 100 / 162
2019-01-08 11:00:57,175 - 10 - corpus.py - subactivity_sampler - [37130.  2965. 74700. 50231.  7311. 50512. 18613.]
2019-01-08 11:01:13,097 - 10 - corpus.py - subactivity_sampler - 120 / 162
2019-01-08 11:01:13,097 - 10 - corpus.py - subactivity_sampler - [37161.  2960. 74621. 50284.  7295. 50771. 18370.]
2019-01-08 11:01:31,614 - 10 - corpus.py - subactivity_sampler - 140 / 162
2019-01-08 11:01:31,614 - 10 - corpus.py - subactivity_sampler - [37116.  2943. 74776. 50169.  7299. 50992. 18167.]
2019-01-08 11:01:48,212 - 10 - corpus.py - subactivity_sampler - 160 / 162
2019-01-08 11:01:48,212 - 10 - corpus.py - subactivity_sampler - [36987.  2931. 75296. 49809.  7300. 51276. 17863.]
2019-01-08 11:01:49,032 - 10 - corpus.py - subactivity_sampler - [36987.  2931. 75294. 49810.  7300. 51277. 17863.]
2019-01-08 11:01:49,032 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 134163.939 ms ~ 2.236 min ~ 134.164 sec
2019-01-08 11:01:49,032 - 10 - corpus.py - ordering_sampler - .
2019-01-08 11:01:51,221 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 11:01:51,222 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 0. 24.  5.  5.  8. 10.]
2019-01-08 11:01:51,222 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 11:01:51,238 - 10 - corpus.py - rho_sampling - ['63.9319', '22.2382', '29.3157', '180.0643', '2.2452', '3.7225']
2019-01-08 11:01:51,238 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 11:01:51,307 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 11:01:51,312 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 11:01:51,312 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 19', '1: 23', '2: 20', '3: 14', '4: 18', '5: 21', '6: 22']
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - mof_val - frames true: 107937	frames overall : 241462
2019-01-08 11:01:51,320 - 10 - corpus.py - accuracy_corpus - Action: juice
2019-01-08 11:01:51,320 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4470144370542777
2019-01-08 11:01:51,320 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4470144370542777
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 22598
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - mof_classes - label 14: 0.000000  0 / 3377
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - mof_classes - label 18: 0.000000  0 / 1411
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - mof_classes - label 19: 0.683504  15737 / 23024
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - mof_classes - label 20: 0.488482  66221 / 135565
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - mof_classes - label 21: 0.649701  24547 / 37782
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - mof_classes - label 22: 0.094418  1404 / 14870
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - mof_classes - label 23: 0.009877  28 / 2835
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - mof_classes - average class mof: 0.240748
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 22598
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - iou_classes - label 14: 0.000000  0 / 53187
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - iou_classes - label 18: 0.000000  0 / 8711
2019-01-08 11:01:51,320 - 10 - accuracy_class.py - iou_classes - label 19: 0.355446  15737 / 44274
2019-01-08 11:01:51,321 - 10 - accuracy_class.py - iou_classes - label 20: 0.457840  66221 / 144638
2019-01-08 11:01:51,321 - 10 - accuracy_class.py - iou_classes - label 21: 0.380503  24547 / 64512
2019-01-08 11:01:51,321 - 10 - accuracy_class.py - iou_classes - label 22: 0.044815  1404 / 31329
2019-01-08 11:01:51,321 - 10 - accuracy_class.py - iou_classes - label 23: 0.004880  28 / 5738
2019-01-08 11:01:51,321 - 10 - accuracy_class.py - iou_classes - average IoU: 0.177640
2019-01-08 11:01:51,321 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.155435
2019-01-08 11:01:54,427 - 10 - f1_score.py - f1 - f1 score: 0.311536
2019-01-08 11:01:54,437 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3198.897 ms ~ 0.053 min ~ 3.199 sec
2019-01-08 11:01:54,446 - 10 - utils.py - wrap - <function baseline at 0x7f3991d24e18> took 1153754.632 ms ~ 19.229 min ~ 1153.755 sec
2019-01-08 11:01:54,446 - 10 - utils.py - update_opt_str - batch_size: 256
2019-01-08 11:01:54,446 - 10 - utils.py - update_opt_str - bg: False
2019-01-08 11:01:54,446 - 10 - utils.py - update_opt_str - bg_trh: 85
2019-01-08 11:01:54,446 - 10 - utils.py - update_opt_str - data: /media/data/kukleva/lab/Breakfast/feat/s1
2019-01-08 11:01:54,446 - 10 - utils.py - update_opt_str - data_type: 2
2019-01-08 11:01:54,446 - 10 - utils.py - update_opt_str - dataset: bf
2019-01-08 11:01:54,446 - 10 - utils.py - update_opt_str - dataset_root: /media/data/kukleva/lab/Breakfast
2019-01-08 11:01:54,446 - 10 - utils.py - update_opt_str - embed_dim: 30
2019-01-08 11:01:54,446 - 10 - utils.py - update_opt_str - epochs: 12
2019-01-08 11:01:54,446 - 10 - utils.py - update_opt_str - ext: txt
2019-01-08 11:01:54,446 - 10 - utils.py - update_opt_str - feature_dim: 64
2019-01-08 11:01:54,446 - 10 - utils.py - update_opt_str - full: True
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - gmm: 1
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - gmms: one
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - gt: /media/data/kukleva/lab/Breakfast/feat/s1/groundTruth
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - gt_training: False
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - log: DEBUG
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - log_str: slim.mallow._sandwich_!bg_data2_bf_embed30_epochs12_full_gmm1_one_!gt_lr1e-10_!lr_ordering_reg0.1_!viterbi_!zeros_
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - lr: 1e-10
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - lr_adj: False
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - momentum: 0.9
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - num_workers: 4
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - ordering: True
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - prefix: slim.mallow.
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - reg_cov: 0.1
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - resume: False
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - resume_str: 
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - save_embed_feat: False
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - save_likelihood: False
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - save_model: False
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - seed: 0
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - subaction: sandwich
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - vis: False
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - viterbi: False
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - weight_decay: 0.0001
2019-01-08 11:01:54,447 - 10 - utils.py - update_opt_str - zeros: False
2019-01-08 11:01:54,465 - 10 - utils.py - wrap - <function GroundTruth.load_gt at 0x7f39322b36a8> took 17.296 ms ~ 0.000 min ~ 0.017 sec
2019-01-08 11:01:54,535 - 10 - corpus.py - __init__ - sandwich  subactions: 8
2019-01-08 11:01:54,536 - 10 - corpus.py - _init_videos - .
2019-01-08 11:02:04,412 - 10 - corpus.py - _init_videos - gt statistic: Counter({37: 99083, 39: 72048, 36: 58377, 40: 12666, 38: 7111, 18: 6839, 14: 2401, 41: 970})
2019-01-08 11:02:04,412 - 10 - corpus.py - _update_fg_mask - .
2019-01-08 11:02:04,433 - 10 - corpus.py - __init__ - min: -45.483139  max: 41.272209  avg: 0.084439
2019-01-08 11:02:04,433 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 11:02:04,474 - 10 - corpus.py - rho_sampling - ['64.1837', '100.9221', '929.0535', '90.9104', '17.1809', '163.7961', '267.9629']
2019-01-08 11:02:04,474 - 10 - pipeline.py - baseline - Iteration 0
2019-01-08 11:02:04,561 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 11:02:04,565 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 11:02:04,565 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 14', '1: 36', '2: 18', '3: 37', '4: 38', '5: 41', '6: 39', '7: 40']
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - mof_val - frames true: 83195	frames overall : 259495
2019-01-08 11:02:04,574 - 10 - corpus.py - accuracy_corpus - Action: sandwich
2019-01-08 11:02:04,574 - 10 - corpus.py - accuracy_corpus - MoF val: 0.320603479835835
2019-01-08 11:02:04,574 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.0
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - mof_classes - label 14: 0.608673  1193 / 1960
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - mof_classes - label 18: 0.346627  1408 / 4062
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - mof_classes - label 36: 0.455216  20873 / 45853
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - mof_classes - label 37: 0.285412  28211 / 98843
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - mof_classes - label 38: 0.299677  2131 / 7111
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - mof_classes - label 39: 0.395407  25586 / 64708
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - mof_classes - label 40: 0.723027  3793 / 5246
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - mof_classes - label 41: 0.000000  0 / 970
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - mof_classes - average class mof: 0.346004
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - iou_classes - label 14: 0.035848  1193 / 33279
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - iou_classes - label 18: 0.040091  1408 / 35120
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - iou_classes - label 36: 0.363261  20873 / 57460
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - iou_classes - label 37: 0.273697  28211 / 103074
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - iou_classes - label 38: 0.056974  2131 / 37403
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - iou_classes - label 39: 0.357786  25586 / 71512
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - iou_classes - label 40: 0.112126  3793 / 33828
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - iou_classes - label 41: 0.000000  0 / 33377
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - iou_classes - average IoU: 0.154973
2019-01-08 11:02:04,574 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.137754
2019-01-08 11:02:07,676 - 10 - f1_score.py - f1 - f1 score: 0.278112
2019-01-08 11:02:07,688 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3213.836 ms ~ 0.054 min ~ 3.214 sec
2019-01-08 11:02:07,688 - 10 - corpus.py - embedding_training - .
2019-01-08 11:02:07,688 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 11:02:07,688 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 11:02:07,688 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 11:02:09,376 - 10 - training_embed.py - training - create model
2019-01-08 11:02:09,377 - 10 - training_embed.py - training - epochs: 12
2019-01-08 11:02:09,377 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 11:02:09,721 - 10 - training_embed.py - training - Epoch: [0][100/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 355.2539 (358.1517)	
2019-01-08 11:02:09,936 - 10 - training_embed.py - training - Epoch: [0][200/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 358.9037 (357.9566)	
2019-01-08 11:02:10,156 - 10 - training_embed.py - training - Epoch: [0][300/1014]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 356.2940 (358.0602)	
2019-01-08 11:02:10,381 - 10 - training_embed.py - training - Epoch: [0][400/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 359.9774 (358.0763)	
2019-01-08 11:02:10,608 - 10 - training_embed.py - training - Epoch: [0][500/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.3050 (358.0252)	
2019-01-08 11:02:10,835 - 10 - training_embed.py - training - Epoch: [0][600/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 357.0330 (358.1321)	
2019-01-08 11:02:11,055 - 10 - training_embed.py - training - Epoch: [0][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.9608 (358.1437)	
2019-01-08 11:02:11,278 - 10 - training_embed.py - training - Epoch: [0][800/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.7701 (358.1304)	
2019-01-08 11:02:11,480 - 10 - training_embed.py - training - Epoch: [0][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.3863 (358.1123)	
2019-01-08 11:02:11,684 - 10 - training_embed.py - training - Epoch: [0][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 361.9225 (358.0780)	
2019-01-08 11:02:11,721 - 10 - training_embed.py - training - loss: 358.007721
2019-01-08 11:02:11,721 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 11:02:12,120 - 10 - training_embed.py - training - Epoch: [1][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 364.1450 (358.3573)	
2019-01-08 11:02:12,344 - 10 - training_embed.py - training - Epoch: [1][200/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.5001 (358.0762)	
2019-01-08 11:02:12,570 - 10 - training_embed.py - training - Epoch: [1][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.4904 (357.9618)	
2019-01-08 11:02:12,792 - 10 - training_embed.py - training - Epoch: [1][400/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.3701 (358.0334)	
2019-01-08 11:02:13,013 - 10 - training_embed.py - training - Epoch: [1][500/1014]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 359.4022 (358.0340)	
2019-01-08 11:02:13,219 - 10 - training_embed.py - training - Epoch: [1][600/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 361.4542 (358.0266)	
2019-01-08 11:02:13,497 - 10 - training_embed.py - training - Epoch: [1][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.8647 (358.0875)	
2019-01-08 11:02:13,770 - 10 - training_embed.py - training - Epoch: [1][800/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 357.0584 (358.0438)	
2019-01-08 11:02:14,002 - 10 - training_embed.py - training - Epoch: [1][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.8593 (357.9905)	
2019-01-08 11:02:14,236 - 10 - training_embed.py - training - Epoch: [1][1000/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 357.8505 (358.0470)	
2019-01-08 11:02:14,263 - 10 - training_embed.py - training - loss: 357.965996
2019-01-08 11:02:14,269 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 11:02:14,659 - 10 - training_embed.py - training - Epoch: [2][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.2510 (358.2759)	
2019-01-08 11:02:14,879 - 10 - training_embed.py - training - Epoch: [2][200/1014]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 355.5237 (358.0969)	
2019-01-08 11:02:15,092 - 10 - training_embed.py - training - Epoch: [2][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 361.0991 (357.9577)	
2019-01-08 11:02:15,299 - 10 - training_embed.py - training - Epoch: [2][400/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 359.4826 (357.9553)	
2019-01-08 11:02:15,513 - 10 - training_embed.py - training - Epoch: [2][500/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.7076 (357.9964)	
2019-01-08 11:02:15,765 - 10 - training_embed.py - training - Epoch: [2][600/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.3838 (358.0197)	
2019-01-08 11:02:16,021 - 10 - training_embed.py - training - Epoch: [2][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 360.3977 (358.0009)	
2019-01-08 11:02:16,231 - 10 - training_embed.py - training - Epoch: [2][800/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.6352 (358.0115)	
2019-01-08 11:02:16,440 - 10 - training_embed.py - training - Epoch: [2][900/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 358.7320 (358.0350)	
2019-01-08 11:02:16,652 - 10 - training_embed.py - training - Epoch: [2][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.9413 (358.0134)	
2019-01-08 11:02:16,681 - 10 - training_embed.py - training - loss: 357.925792
2019-01-08 11:02:16,681 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 11:02:17,071 - 10 - training_embed.py - training - Epoch: [3][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.4408 (358.1627)	
2019-01-08 11:02:17,288 - 10 - training_embed.py - training - Epoch: [3][200/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.4900 (358.1447)	
2019-01-08 11:02:17,523 - 10 - training_embed.py - training - Epoch: [3][300/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 358.2793 (358.0917)	
2019-01-08 11:02:17,749 - 10 - training_embed.py - training - Epoch: [3][400/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.4683 (358.1171)	
2019-01-08 11:02:17,971 - 10 - training_embed.py - training - Epoch: [3][500/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.5710 (358.1191)	
2019-01-08 11:02:18,189 - 10 - training_embed.py - training - Epoch: [3][600/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.4804 (358.0650)	
2019-01-08 11:02:18,405 - 10 - training_embed.py - training - Epoch: [3][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.3693 (358.0614)	
2019-01-08 11:02:18,623 - 10 - training_embed.py - training - Epoch: [3][800/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.6802 (357.9962)	
2019-01-08 11:02:18,847 - 10 - training_embed.py - training - Epoch: [3][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.4071 (358.0164)	
2019-01-08 11:02:19,079 - 10 - training_embed.py - training - Epoch: [3][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.2061 (357.9797)	
2019-01-08 11:02:19,106 - 10 - training_embed.py - training - loss: 357.885098
2019-01-08 11:02:19,109 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 11:02:19,524 - 10 - training_embed.py - training - Epoch: [4][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.8921 (357.8151)	
2019-01-08 11:02:19,851 - 10 - training_embed.py - training - Epoch: [4][200/1014]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 358.1696 (357.8512)	
2019-01-08 11:02:20,074 - 10 - training_embed.py - training - Epoch: [4][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.7803 (357.8918)	
2019-01-08 11:02:20,302 - 10 - training_embed.py - training - Epoch: [4][400/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 360.7010 (357.8739)	
2019-01-08 11:02:20,513 - 10 - training_embed.py - training - Epoch: [4][500/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 361.5786 (357.9165)	
2019-01-08 11:02:20,737 - 10 - training_embed.py - training - Epoch: [4][600/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.5333 (357.9253)	
2019-01-08 11:02:20,960 - 10 - training_embed.py - training - Epoch: [4][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.4715 (357.9699)	
2019-01-08 11:02:21,197 - 10 - training_embed.py - training - Epoch: [4][800/1014]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 359.0711 (357.9767)	
2019-01-08 11:02:21,520 - 10 - training_embed.py - training - Epoch: [4][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.6385 (357.9612)	
2019-01-08 11:02:21,760 - 10 - training_embed.py - training - Epoch: [4][1000/1014]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 357.9266 (357.9380)	
2019-01-08 11:02:21,786 - 10 - training_embed.py - training - loss: 357.846861
2019-01-08 11:02:21,787 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 11:02:22,157 - 10 - training_embed.py - training - Epoch: [5][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.0507 (357.4297)	
2019-01-08 11:02:22,370 - 10 - training_embed.py - training - Epoch: [5][200/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.1164 (357.5095)	
2019-01-08 11:02:22,597 - 10 - training_embed.py - training - Epoch: [5][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.4687 (357.6531)	
2019-01-08 11:02:22,814 - 10 - training_embed.py - training - Epoch: [5][400/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.7147 (357.7869)	
2019-01-08 11:02:23,047 - 10 - training_embed.py - training - Epoch: [5][500/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.7798 (357.9276)	
2019-01-08 11:02:23,277 - 10 - training_embed.py - training - Epoch: [5][600/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 354.4765 (357.9533)	
2019-01-08 11:02:23,503 - 10 - training_embed.py - training - Epoch: [5][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.8291 (357.9674)	
2019-01-08 11:02:23,735 - 10 - training_embed.py - training - Epoch: [5][800/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.5093 (357.9348)	
2019-01-08 11:02:23,963 - 10 - training_embed.py - training - Epoch: [5][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.8928 (357.9289)	
2019-01-08 11:02:24,189 - 10 - training_embed.py - training - Epoch: [5][1000/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.9798 (357.8871)	
2019-01-08 11:02:24,216 - 10 - training_embed.py - training - loss: 357.806647
2019-01-08 11:02:24,217 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 11:02:24,624 - 10 - training_embed.py - training - Epoch: [6][100/1014]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 357.4684 (358.0139)	
2019-01-08 11:02:24,853 - 10 - training_embed.py - training - Epoch: [6][200/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 360.8073 (357.8938)	
2019-01-08 11:02:25,115 - 10 - training_embed.py - training - Epoch: [6][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.4461 (357.8250)	
2019-01-08 11:02:25,348 - 10 - training_embed.py - training - Epoch: [6][400/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 365.1238 (357.8681)	
2019-01-08 11:02:25,583 - 10 - training_embed.py - training - Epoch: [6][500/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 361.1728 (357.8605)	
2019-01-08 11:02:25,861 - 10 - training_embed.py - training - Epoch: [6][600/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 354.2558 (357.9029)	
2019-01-08 11:02:26,118 - 10 - training_embed.py - training - Epoch: [6][700/1014]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 356.4939 (357.8914)	
2019-01-08 11:02:26,456 - 10 - training_embed.py - training - Epoch: [6][800/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 361.6248 (357.8131)	
2019-01-08 11:02:26,788 - 10 - training_embed.py - training - Epoch: [6][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.1152 (357.7930)	
2019-01-08 11:02:27,032 - 10 - training_embed.py - training - Epoch: [6][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.8609 (357.8449)	
2019-01-08 11:02:27,058 - 10 - training_embed.py - training - loss: 357.764802
2019-01-08 11:02:27,059 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 11:02:27,478 - 10 - training_embed.py - training - Epoch: [7][100/1014]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 356.3387 (357.7688)	
2019-01-08 11:02:27,684 - 10 - training_embed.py - training - Epoch: [7][200/1014]	Time 0.005 (0.003)	Data 0.001 (0.001)	Loss 358.5477 (357.8038)	
2019-01-08 11:02:27,906 - 10 - training_embed.py - training - Epoch: [7][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.2835 (357.8495)	
2019-01-08 11:02:28,137 - 10 - training_embed.py - training - Epoch: [7][400/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 358.4447 (357.9117)	
2019-01-08 11:02:28,366 - 10 - training_embed.py - training - Epoch: [7][500/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 360.2064 (357.8874)	
2019-01-08 11:02:28,589 - 10 - training_embed.py - training - Epoch: [7][600/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.9142 (357.8693)	
2019-01-08 11:02:28,801 - 10 - training_embed.py - training - Epoch: [7][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.8197 (357.8532)	
2019-01-08 11:02:29,043 - 10 - training_embed.py - training - Epoch: [7][800/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.6327 (357.8122)	
2019-01-08 11:02:29,266 - 10 - training_embed.py - training - Epoch: [7][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.7621 (357.8166)	
2019-01-08 11:02:29,495 - 10 - training_embed.py - training - Epoch: [7][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.1349 (357.7864)	
2019-01-08 11:02:29,522 - 10 - training_embed.py - training - loss: 357.723887
2019-01-08 11:02:29,522 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 11:02:29,860 - 10 - training_embed.py - training - Epoch: [8][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0730 (357.5836)	
2019-01-08 11:02:30,099 - 10 - training_embed.py - training - Epoch: [8][200/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.6220 (357.5079)	
2019-01-08 11:02:30,330 - 10 - training_embed.py - training - Epoch: [8][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.5659 (357.5187)	
2019-01-08 11:02:30,665 - 10 - training_embed.py - training - Epoch: [8][400/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.6157 (357.6181)	
2019-01-08 11:02:31,155 - 10 - training_embed.py - training - Epoch: [8][500/1014]	Time 0.011 (0.003)	Data 0.002 (0.001)	Loss 358.5406 (357.5988)	
2019-01-08 11:02:31,515 - 10 - training_embed.py - training - Epoch: [8][600/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 355.6817 (357.6547)	
2019-01-08 11:02:31,781 - 10 - training_embed.py - training - Epoch: [8][700/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 358.4618 (357.6894)	
2019-01-08 11:02:32,042 - 10 - training_embed.py - training - Epoch: [8][800/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 358.7248 (357.6863)	
2019-01-08 11:02:32,378 - 10 - training_embed.py - training - Epoch: [8][900/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 358.7517 (357.7201)	
2019-01-08 11:02:32,738 - 10 - training_embed.py - training - Epoch: [8][1000/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 356.4817 (357.7415)	
2019-01-08 11:02:32,773 - 10 - training_embed.py - training - loss: 357.683591
2019-01-08 11:02:32,773 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 11:02:33,289 - 10 - training_embed.py - training - Epoch: [9][100/1014]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 355.8414 (357.7710)	
2019-01-08 11:02:33,662 - 10 - training_embed.py - training - Epoch: [9][200/1014]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 357.3062 (357.9335)	
2019-01-08 11:02:34,012 - 10 - training_embed.py - training - Epoch: [9][300/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 361.0448 (358.0107)	
2019-01-08 11:02:34,364 - 10 - training_embed.py - training - Epoch: [9][400/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.7159 (357.9245)	
2019-01-08 11:02:34,747 - 10 - training_embed.py - training - Epoch: [9][500/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 356.4160 (357.8489)	
2019-01-08 11:02:35,087 - 10 - training_embed.py - training - Epoch: [9][600/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 359.9728 (357.8583)	
2019-01-08 11:02:35,446 - 10 - training_embed.py - training - Epoch: [9][700/1014]	Time 0.006 (0.003)	Data 0.001 (0.001)	Loss 357.4580 (357.8139)	
2019-01-08 11:02:35,800 - 10 - training_embed.py - training - Epoch: [9][800/1014]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 354.5334 (357.7176)	
2019-01-08 11:02:36,167 - 10 - training_embed.py - training - Epoch: [9][900/1014]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 355.6407 (357.6805)	
2019-01-08 11:02:36,540 - 10 - training_embed.py - training - Epoch: [9][1000/1014]	Time 0.010 (0.003)	Data 0.009 (0.001)	Loss 357.3194 (357.7182)	
2019-01-08 11:02:36,574 - 10 - training_embed.py - training - loss: 357.643925
2019-01-08 11:02:36,574 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 11:02:37,151 - 10 - training_embed.py - training - Epoch: [10][100/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 360.0958 (358.0838)	
2019-01-08 11:02:37,846 - 10 - training_embed.py - training - Epoch: [10][200/1014]	Time 0.006 (0.003)	Data 0.004 (0.001)	Loss 357.9128 (357.8518)	
2019-01-08 11:02:38,182 - 10 - training_embed.py - training - Epoch: [10][300/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 352.6396 (357.7597)	
2019-01-08 11:02:38,441 - 10 - training_embed.py - training - Epoch: [10][400/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 356.6160 (357.7275)	
2019-01-08 11:02:38,707 - 10 - training_embed.py - training - Epoch: [10][500/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 357.4626 (357.6685)	
2019-01-08 11:02:39,295 - 10 - training_embed.py - training - Epoch: [10][600/1014]	Time 0.005 (0.003)	Data 0.003 (0.001)	Loss 355.7498 (357.6354)	
2019-01-08 11:02:39,742 - 10 - training_embed.py - training - Epoch: [10][700/1014]	Time 0.007 (0.003)	Data 0.001 (0.001)	Loss 359.9135 (357.6838)	
2019-01-08 11:02:40,088 - 10 - training_embed.py - training - Epoch: [10][800/1014]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 359.1322 (357.6775)	
2019-01-08 11:02:40,404 - 10 - training_embed.py - training - Epoch: [10][900/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 351.9453 (357.6643)	
2019-01-08 11:02:40,656 - 10 - training_embed.py - training - Epoch: [10][1000/1014]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 359.8317 (357.6866)	
2019-01-08 11:02:40,684 - 10 - training_embed.py - training - loss: 357.604406
2019-01-08 11:02:40,684 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 11:02:41,116 - 10 - training_embed.py - training - Epoch: [11][100/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 360.3009 (357.5571)	
2019-01-08 11:02:41,395 - 10 - training_embed.py - training - Epoch: [11][200/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 358.5971 (357.5718)	
2019-01-08 11:02:41,652 - 10 - training_embed.py - training - Epoch: [11][300/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.1080 (357.6231)	
2019-01-08 11:02:41,916 - 10 - training_embed.py - training - Epoch: [11][400/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 359.4346 (357.6515)	
2019-01-08 11:02:42,175 - 10 - training_embed.py - training - Epoch: [11][500/1014]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 358.8773 (357.6521)	
2019-01-08 11:02:42,428 - 10 - training_embed.py - training - Epoch: [11][600/1014]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 358.8384 (357.6310)	
2019-01-08 11:02:42,814 - 10 - training_embed.py - training - Epoch: [11][700/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.8711 (357.6349)	
2019-01-08 11:02:43,240 - 10 - training_embed.py - training - Epoch: [11][800/1014]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 360.6160 (357.6412)	
2019-01-08 11:02:43,707 - 10 - training_embed.py - training - Epoch: [11][900/1014]	Time 0.005 (0.003)	Data 0.002 (0.001)	Loss 357.0734 (357.6607)	
2019-01-08 11:02:44,222 - 10 - training_embed.py - training - Epoch: [11][1000/1014]	Time 0.007 (0.003)	Data 0.004 (0.001)	Loss 351.0962 (357.6401)	
2019-01-08 11:02:44,269 - 10 - training_embed.py - training - loss: 357.562654
2019-01-08 11:02:44,327 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 11:02:44,827 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 498.927 ms ~ 0.008 min ~ 0.499 sec
2019-01-08 11:02:45,334 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1006.833 ms ~ 0.017 min ~ 1.007 sec
2019-01-08 11:02:45,334 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 11:02:45,335 - 10 - corpus.py - subactivity_sampler - 0 / 169
2019-01-08 11:02:45,335 - 10 - corpus.py - subactivity_sampler - [32512. 32480. 32466. 32442. 32423. 32407. 32390. 32375.]
2019-01-08 11:03:09,074 - 10 - corpus.py - subactivity_sampler - 20 / 169
2019-01-08 11:03:09,075 - 10 - corpus.py - subactivity_sampler - [32668. 32302. 32140. 32501. 32415. 32244. 33295. 31930.]
2019-01-08 11:03:24,726 - 10 - corpus.py - subactivity_sampler - 40 / 169
2019-01-08 11:03:24,726 - 10 - corpus.py - subactivity_sampler - [32772. 32128. 31972. 32539. 32715. 31620. 34002. 31747.]
2019-01-08 11:03:56,728 - 10 - corpus.py - subactivity_sampler - 60 / 169
2019-01-08 11:03:56,728 - 10 - corpus.py - subactivity_sampler - [32715. 32128. 31509. 32547. 32821. 31300. 35039. 31436.]
2019-01-08 11:04:20,270 - 10 - corpus.py - subactivity_sampler - 80 / 169
2019-01-08 11:04:20,271 - 10 - corpus.py - subactivity_sampler - [32608. 32352. 30871. 32642. 33358. 30733. 36157. 30774.]
2019-01-08 11:04:37,550 - 10 - corpus.py - subactivity_sampler - 100 / 169
2019-01-08 11:04:37,551 - 10 - corpus.py - subactivity_sampler - [32450. 33138. 29792. 33042. 33239. 30519. 36953. 30362.]
2019-01-08 11:04:52,019 - 10 - corpus.py - subactivity_sampler - 120 / 169
2019-01-08 11:04:52,019 - 10 - corpus.py - subactivity_sampler - [32244. 33761. 28747. 33016. 33480. 29796. 38660. 29791.]
2019-01-08 11:05:10,886 - 10 - corpus.py - subactivity_sampler - 140 / 169
2019-01-08 11:05:10,887 - 10 - corpus.py - subactivity_sampler - [32006. 34319. 27396. 33289. 33945. 28907. 40547. 29086.]
2019-01-08 11:05:36,958 - 10 - corpus.py - subactivity_sampler - 160 / 169
2019-01-08 11:05:36,958 - 10 - corpus.py - subactivity_sampler - [31867. 35483. 25595. 33671. 33994. 28226. 42819. 27840.]
2019-01-08 11:05:45,846 - 10 - corpus.py - subactivity_sampler - [31464. 35959. 25000. 33839. 34120. 28017. 43855. 27241.]
2019-01-08 11:05:45,846 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 180512.031 ms ~ 3.009 min ~ 180.512 sec
2019-01-08 11:05:45,846 - 10 - corpus.py - ordering_sampler - .
2019-01-08 11:05:48,708 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 11:05:48,708 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 2.  0.  0.  0. 14.  0.  0.]
2019-01-08 11:05:48,708 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 11:05:48,760 - 10 - corpus.py - rho_sampling - ['61.6201', '47.0072', '928.1069', '7.5322', '1.4172', '22.0165', '689.7991']
2019-01-08 11:05:48,760 - 10 - pipeline.py - baseline - Iteration 1
2019-01-08 11:05:48,887 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 11:05:48,893 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 11:05:48,893 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 14', '1: 36', '2: 18', '3: 37', '4: 38', '5: 41', '6: 39', '7: 40']
2019-01-08 11:05:48,910 - 10 - accuracy_class.py - mof_val - frames true: 95016	frames overall : 259495
2019-01-08 11:05:48,911 - 10 - corpus.py - accuracy_corpus - Action: sandwich
2019-01-08 11:05:48,911 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3661573440721401
2019-01-08 11:05:48,911 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3661573440721401
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - mof_classes - label 14: 0.746429  1463 / 1960
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - mof_classes - label 18: 0.289266  1175 / 4062
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - mof_classes - label 36: 0.527359  24181 / 45853
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - mof_classes - label 37: 0.295094  29168 / 98843
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - mof_classes - label 38: 0.279567  1988 / 7111
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - mof_classes - label 39: 0.522053  33781 / 64708
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - mof_classes - label 40: 0.621426  3260 / 5246
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - mof_classes - label 41: 0.000000  0 / 970
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - mof_classes - average class mof: 0.364577
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - iou_classes - label 14: 0.045775  1463 / 31961
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - iou_classes - label 18: 0.042134  1175 / 27887
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - iou_classes - label 36: 0.419583  24181 / 57631
2019-01-08 11:05:48,911 - 10 - accuracy_class.py - iou_classes - label 37: 0.281778  29168 / 103514
2019-01-08 11:05:48,912 - 10 - accuracy_class.py - iou_classes - label 38: 0.050659  1988 / 39243
2019-01-08 11:05:48,912 - 10 - accuracy_class.py - iou_classes - label 39: 0.451726  33781 / 74782
2019-01-08 11:05:48,912 - 10 - accuracy_class.py - iou_classes - label 40: 0.111541  3260 / 29227
2019-01-08 11:05:48,912 - 10 - accuracy_class.py - iou_classes - label 41: 0.000000  0 / 28987
2019-01-08 11:05:48,912 - 10 - accuracy_class.py - iou_classes - average IoU: 0.175400
2019-01-08 11:05:48,912 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.155911
2019-01-08 11:05:52,453 - 10 - f1_score.py - f1 - f1 score: 0.303984
2019-01-08 11:05:52,463 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3702.715 ms ~ 0.062 min ~ 3.703 sec
2019-01-08 11:05:52,463 - 10 - corpus.py - embedding_training - .
2019-01-08 11:05:52,463 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 11:05:52,463 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 11:05:52,463 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 11:05:54,738 - 10 - training_embed.py - training - create model
2019-01-08 11:05:54,739 - 10 - training_embed.py - training - epochs: 12
2019-01-08 11:05:54,739 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 11:05:55,275 - 10 - training_embed.py - training - Epoch: [0][100/1014]	Time 0.003 (0.005)	Data 0.002 (0.003)	Loss 353.1371 (356.8676)	
2019-01-08 11:05:55,611 - 10 - training_embed.py - training - Epoch: [0][200/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 355.4981 (356.6128)	
2019-01-08 11:05:55,999 - 10 - training_embed.py - training - Epoch: [0][300/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 357.5989 (356.6780)	
2019-01-08 11:05:56,381 - 10 - training_embed.py - training - Epoch: [0][400/1014]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 357.8396 (356.7734)	
2019-01-08 11:05:56,779 - 10 - training_embed.py - training - Epoch: [0][500/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 356.0644 (356.7210)	
2019-01-08 11:05:57,131 - 10 - training_embed.py - training - Epoch: [0][600/1014]	Time 0.008 (0.004)	Data 0.000 (0.002)	Loss 353.8250 (356.7697)	
2019-01-08 11:05:57,504 - 10 - training_embed.py - training - Epoch: [0][700/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 357.5166 (356.7579)	
2019-01-08 11:05:57,857 - 10 - training_embed.py - training - Epoch: [0][800/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 350.2969 (356.7575)	
2019-01-08 11:05:58,231 - 10 - training_embed.py - training - Epoch: [0][900/1014]	Time 0.008 (0.004)	Data 0.006 (0.002)	Loss 357.0077 (356.7343)	
2019-01-08 11:05:58,590 - 10 - training_embed.py - training - Epoch: [0][1000/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 359.1562 (356.7014)	
2019-01-08 11:05:58,629 - 10 - training_embed.py - training - loss: 356.627219
2019-01-08 11:05:58,630 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 11:05:59,176 - 10 - training_embed.py - training - Epoch: [1][100/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 362.8524 (356.7260)	
2019-01-08 11:05:59,556 - 10 - training_embed.py - training - Epoch: [1][200/1014]	Time 0.005 (0.004)	Data 0.004 (0.002)	Loss 356.9622 (356.6411)	
2019-01-08 11:05:59,907 - 10 - training_embed.py - training - Epoch: [1][300/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 356.3641 (356.5694)	
2019-01-08 11:06:00,300 - 10 - training_embed.py - training - Epoch: [1][400/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 354.1307 (356.6098)	
2019-01-08 11:06:00,704 - 10 - training_embed.py - training - Epoch: [1][500/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 356.7096 (356.6081)	
2019-01-08 11:06:01,073 - 10 - training_embed.py - training - Epoch: [1][600/1014]	Time 0.006 (0.004)	Data 0.000 (0.002)	Loss 358.4355 (356.5863)	
2019-01-08 11:06:01,449 - 10 - training_embed.py - training - Epoch: [1][700/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 359.4280 (356.6289)	
2019-01-08 11:06:01,842 - 10 - training_embed.py - training - Epoch: [1][800/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 356.3812 (356.5976)	
2019-01-08 11:06:02,225 - 10 - training_embed.py - training - Epoch: [1][900/1014]	Time 0.010 (0.004)	Data 0.002 (0.002)	Loss 359.8293 (356.5833)	
2019-01-08 11:06:02,584 - 10 - training_embed.py - training - Epoch: [1][1000/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 357.5138 (356.6146)	
2019-01-08 11:06:02,622 - 10 - training_embed.py - training - loss: 356.536755
2019-01-08 11:06:02,623 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 11:06:03,241 - 10 - training_embed.py - training - Epoch: [2][100/1014]	Time 0.015 (0.004)	Data 0.008 (0.002)	Loss 354.0237 (356.7120)	
2019-01-08 11:06:03,643 - 10 - training_embed.py - training - Epoch: [2][200/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 353.3172 (356.5836)	
2019-01-08 11:06:04,055 - 10 - training_embed.py - training - Epoch: [2][300/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 357.6174 (356.5345)	
2019-01-08 11:06:04,461 - 10 - training_embed.py - training - Epoch: [2][400/1014]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 356.2015 (356.5246)	
2019-01-08 11:06:04,859 - 10 - training_embed.py - training - Epoch: [2][500/1014]	Time 0.005 (0.004)	Data 0.000 (0.002)	Loss 357.3509 (356.5170)	
2019-01-08 11:06:05,292 - 10 - training_embed.py - training - Epoch: [2][600/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 353.6004 (356.5551)	
2019-01-08 11:06:05,713 - 10 - training_embed.py - training - Epoch: [2][700/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 356.8139 (356.5420)	
2019-01-08 11:06:06,101 - 10 - training_embed.py - training - Epoch: [2][800/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 357.3842 (356.5453)	
2019-01-08 11:06:06,460 - 10 - training_embed.py - training - Epoch: [2][900/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 358.5829 (356.5544)	
2019-01-08 11:06:06,857 - 10 - training_embed.py - training - Epoch: [2][1000/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 351.2421 (356.5362)	
2019-01-08 11:06:06,902 - 10 - training_embed.py - training - loss: 356.446659
2019-01-08 11:06:06,902 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 11:06:07,485 - 10 - training_embed.py - training - Epoch: [3][100/1014]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 356.3344 (356.5669)	
2019-01-08 11:06:07,899 - 10 - training_embed.py - training - Epoch: [3][200/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 354.9993 (356.4831)	
2019-01-08 11:06:08,328 - 10 - training_embed.py - training - Epoch: [3][300/1014]	Time 0.005 (0.004)	Data 0.004 (0.002)	Loss 355.8368 (356.4665)	
2019-01-08 11:06:08,689 - 10 - training_embed.py - training - Epoch: [3][400/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 353.1781 (356.5352)	
2019-01-08 11:06:09,073 - 10 - training_embed.py - training - Epoch: [3][500/1014]	Time 0.010 (0.004)	Data 0.007 (0.002)	Loss 356.2455 (356.5720)	
2019-01-08 11:06:09,474 - 10 - training_embed.py - training - Epoch: [3][600/1014]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 358.5500 (356.5122)	
2019-01-08 11:06:09,889 - 10 - training_embed.py - training - Epoch: [3][700/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 355.7492 (356.5011)	
2019-01-08 11:06:10,264 - 10 - training_embed.py - training - Epoch: [3][800/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 354.5220 (356.4627)	
2019-01-08 11:06:10,642 - 10 - training_embed.py - training - Epoch: [3][900/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 355.5872 (356.4708)	
2019-01-08 11:06:11,006 - 10 - training_embed.py - training - Epoch: [3][1000/1014]	Time 0.005 (0.004)	Data 0.004 (0.002)	Loss 356.6515 (356.4445)	
2019-01-08 11:06:11,044 - 10 - training_embed.py - training - loss: 356.359470
2019-01-08 11:06:11,044 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 11:06:11,646 - 10 - training_embed.py - training - Epoch: [4][100/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 354.9682 (356.1640)	
2019-01-08 11:06:12,036 - 10 - training_embed.py - training - Epoch: [4][200/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 357.4031 (356.3227)	
2019-01-08 11:06:12,467 - 10 - training_embed.py - training - Epoch: [4][300/1014]	Time 0.008 (0.004)	Data 0.000 (0.002)	Loss 355.8233 (356.3935)	
2019-01-08 11:06:12,896 - 10 - training_embed.py - training - Epoch: [4][400/1014]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 360.7509 (356.3537)	
2019-01-08 11:06:13,279 - 10 - training_embed.py - training - Epoch: [4][500/1014]	Time 0.009 (0.004)	Data 0.001 (0.002)	Loss 357.7255 (356.3230)	
2019-01-08 11:06:13,597 - 10 - training_embed.py - training - Epoch: [4][600/1014]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 354.5470 (356.3040)	
2019-01-08 11:06:14,039 - 10 - training_embed.py - training - Epoch: [4][700/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 356.4504 (356.3559)	
2019-01-08 11:06:14,447 - 10 - training_embed.py - training - Epoch: [4][800/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 357.7266 (356.3500)	
2019-01-08 11:06:14,911 - 10 - training_embed.py - training - Epoch: [4][900/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 358.1401 (356.3769)	
2019-01-08 11:06:15,285 - 10 - training_embed.py - training - Epoch: [4][1000/1014]	Time 0.017 (0.004)	Data 0.015 (0.002)	Loss 355.1992 (356.3632)	
2019-01-08 11:06:15,336 - 10 - training_embed.py - training - loss: 356.271438
2019-01-08 11:06:15,336 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 11:06:15,889 - 10 - training_embed.py - training - Epoch: [5][100/1014]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 355.7102 (355.9075)	
2019-01-08 11:06:16,237 - 10 - training_embed.py - training - Epoch: [5][200/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 357.0053 (355.8771)	
2019-01-08 11:06:16,691 - 10 - training_embed.py - training - Epoch: [5][300/1014]	Time 0.008 (0.004)	Data 0.000 (0.002)	Loss 354.6147 (356.0070)	
2019-01-08 11:06:17,118 - 10 - training_embed.py - training - Epoch: [5][400/1014]	Time 0.005 (0.004)	Data 0.001 (0.002)	Loss 353.5790 (356.1898)	
2019-01-08 11:06:17,597 - 10 - training_embed.py - training - Epoch: [5][500/1014]	Time 0.007 (0.004)	Data 0.001 (0.002)	Loss 356.6962 (356.3101)	
2019-01-08 11:06:18,065 - 10 - training_embed.py - training - Epoch: [5][600/1014]	Time 0.011 (0.004)	Data 0.002 (0.002)	Loss 354.2663 (356.3273)	
2019-01-08 11:06:18,655 - 10 - training_embed.py - training - Epoch: [5][700/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 356.5695 (356.3301)	
2019-01-08 11:06:19,232 - 10 - training_embed.py - training - Epoch: [5][800/1014]	Time 0.008 (0.004)	Data 0.006 (0.002)	Loss 356.8498 (356.2622)	
2019-01-08 11:06:19,786 - 10 - training_embed.py - training - Epoch: [5][900/1014]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 352.9543 (356.2881)	
2019-01-08 11:06:20,296 - 10 - training_embed.py - training - Epoch: [5][1000/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 354.5834 (356.2605)	
2019-01-08 11:06:20,377 - 10 - training_embed.py - training - loss: 356.182375
2019-01-08 11:06:20,377 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 11:06:21,125 - 10 - training_embed.py - training - Epoch: [6][100/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 358.6399 (356.4489)	
2019-01-08 11:06:21,755 - 10 - training_embed.py - training - Epoch: [6][200/1014]	Time 0.005 (0.004)	Data 0.003 (0.002)	Loss 357.1517 (356.2484)	
2019-01-08 11:06:22,318 - 10 - training_embed.py - training - Epoch: [6][300/1014]	Time 0.010 (0.004)	Data 0.001 (0.002)	Loss 357.9758 (356.2395)	
2019-01-08 11:06:22,828 - 10 - training_embed.py - training - Epoch: [6][400/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 361.9567 (356.2046)	
2019-01-08 11:06:23,398 - 10 - training_embed.py - training - Epoch: [6][500/1014]	Time 0.010 (0.004)	Data 0.002 (0.002)	Loss 357.9879 (356.1987)	
2019-01-08 11:06:23,974 - 10 - training_embed.py - training - Epoch: [6][600/1014]	Time 0.008 (0.004)	Data 0.003 (0.002)	Loss 355.2966 (356.2431)	
2019-01-08 11:06:24,483 - 10 - training_embed.py - training - Epoch: [6][700/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 355.8388 (356.2208)	
2019-01-08 11:06:24,885 - 10 - training_embed.py - training - Epoch: [6][800/1014]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 359.7881 (356.1490)	
2019-01-08 11:06:25,371 - 10 - training_embed.py - training - Epoch: [6][900/1014]	Time 0.009 (0.004)	Data 0.001 (0.002)	Loss 358.7867 (356.1389)	
2019-01-08 11:06:25,857 - 10 - training_embed.py - training - Epoch: [6][1000/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 357.2993 (356.1765)	
2019-01-08 11:06:25,929 - 10 - training_embed.py - training - loss: 356.091285
2019-01-08 11:06:25,929 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 11:06:26,641 - 10 - training_embed.py - training - Epoch: [7][100/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 354.3527 (356.0166)	
2019-01-08 11:06:27,355 - 10 - training_embed.py - training - Epoch: [7][200/1014]	Time 0.009 (0.004)	Data 0.003 (0.002)	Loss 357.3259 (356.0436)	
2019-01-08 11:06:27,956 - 10 - training_embed.py - training - Epoch: [7][300/1014]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 355.5974 (356.1241)	
2019-01-08 11:06:28,673 - 10 - training_embed.py - training - Epoch: [7][400/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 356.2673 (356.1305)	
2019-01-08 11:06:29,311 - 10 - training_embed.py - training - Epoch: [7][500/1014]	Time 0.013 (0.005)	Data 0.000 (0.002)	Loss 356.1644 (356.0846)	
2019-01-08 11:06:29,936 - 10 - training_embed.py - training - Epoch: [7][600/1014]	Time 0.023 (0.005)	Data 0.014 (0.002)	Loss 357.8599 (356.0991)	
2019-01-08 11:06:30,385 - 10 - training_embed.py - training - Epoch: [7][700/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 353.6110 (356.1191)	
2019-01-08 11:06:30,929 - 10 - training_embed.py - training - Epoch: [7][800/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 353.0011 (356.0897)	
2019-01-08 11:06:31,429 - 10 - training_embed.py - training - Epoch: [7][900/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 357.0764 (356.1004)	
2019-01-08 11:06:31,970 - 10 - training_embed.py - training - Epoch: [7][1000/1014]	Time 0.011 (0.005)	Data 0.004 (0.002)	Loss 356.8333 (356.0669)	
2019-01-08 11:06:32,037 - 10 - training_embed.py - training - loss: 356.002487
2019-01-08 11:06:32,037 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 11:06:32,935 - 10 - training_embed.py - training - Epoch: [8][100/1014]	Time 0.009 (0.005)	Data 0.001 (0.002)	Loss 355.4417 (355.7957)	
2019-01-08 11:06:33,501 - 10 - training_embed.py - training - Epoch: [8][200/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 356.0341 (355.7318)	
2019-01-08 11:06:34,188 - 10 - training_embed.py - training - Epoch: [8][300/1014]	Time 0.006 (0.005)	Data 0.004 (0.002)	Loss 356.9586 (355.7899)	
2019-01-08 11:06:34,851 - 10 - training_embed.py - training - Epoch: [8][400/1014]	Time 0.007 (0.005)	Data 0.000 (0.002)	Loss 355.8674 (355.8702)	
2019-01-08 11:06:35,414 - 10 - training_embed.py - training - Epoch: [8][500/1014]	Time 0.022 (0.005)	Data 0.018 (0.002)	Loss 358.9017 (355.8708)	
2019-01-08 11:06:35,986 - 10 - training_embed.py - training - Epoch: [8][600/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 351.5959 (355.8821)	
2019-01-08 11:06:36,474 - 10 - training_embed.py - training - Epoch: [8][700/1014]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 356.6047 (355.9155)	
2019-01-08 11:06:36,936 - 10 - training_embed.py - training - Epoch: [8][800/1014]	Time 0.005 (0.005)	Data 0.003 (0.002)	Loss 358.6159 (355.9363)	
2019-01-08 11:06:37,411 - 10 - training_embed.py - training - Epoch: [8][900/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 358.7413 (355.9613)	
2019-01-08 11:06:37,866 - 10 - training_embed.py - training - Epoch: [8][1000/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 357.4191 (355.9731)	
2019-01-08 11:06:37,904 - 10 - training_embed.py - training - loss: 355.912901
2019-01-08 11:06:37,905 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 11:06:38,857 - 10 - training_embed.py - training - Epoch: [9][100/1014]	Time 0.010 (0.005)	Data 0.002 (0.002)	Loss 356.0070 (355.9135)	
2019-01-08 11:06:39,413 - 10 - training_embed.py - training - Epoch: [9][200/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 355.7174 (356.0787)	
2019-01-08 11:06:40,009 - 10 - training_embed.py - training - Epoch: [9][300/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 358.4098 (356.2377)	
2019-01-08 11:06:40,715 - 10 - training_embed.py - training - Epoch: [9][400/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 352.1706 (356.1055)	
2019-01-08 11:06:41,384 - 10 - training_embed.py - training - Epoch: [9][500/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 351.2796 (356.0322)	
2019-01-08 11:06:41,944 - 10 - training_embed.py - training - Epoch: [9][600/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 356.6864 (356.0252)	
2019-01-08 11:06:42,461 - 10 - training_embed.py - training - Epoch: [9][700/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 357.8928 (355.9633)	
2019-01-08 11:06:42,951 - 10 - training_embed.py - training - Epoch: [9][800/1014]	Time 0.014 (0.005)	Data 0.013 (0.002)	Loss 353.8374 (355.9104)	
2019-01-08 11:06:43,576 - 10 - training_embed.py - training - Epoch: [9][900/1014]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 357.8690 (355.8823)	
2019-01-08 11:06:44,081 - 10 - training_embed.py - training - Epoch: [9][1000/1014]	Time 0.009 (0.005)	Data 0.007 (0.002)	Loss 355.5323 (355.9045)	
2019-01-08 11:06:44,141 - 10 - training_embed.py - training - loss: 355.824774
2019-01-08 11:06:44,141 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 11:06:45,012 - 10 - training_embed.py - training - Epoch: [10][100/1014]	Time 0.009 (0.005)	Data 0.001 (0.002)	Loss 357.9375 (356.0900)	
2019-01-08 11:06:45,571 - 10 - training_embed.py - training - Epoch: [10][200/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 354.7007 (355.9254)	
2019-01-08 11:06:46,123 - 10 - training_embed.py - training - Epoch: [10][300/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 350.5518 (355.8997)	
2019-01-08 11:06:46,766 - 10 - training_embed.py - training - Epoch: [10][400/1014]	Time 0.008 (0.005)	Data 0.002 (0.002)	Loss 353.4933 (355.8923)	
2019-01-08 11:06:47,434 - 10 - training_embed.py - training - Epoch: [10][500/1014]	Time 0.007 (0.005)	Data 0.002 (0.002)	Loss 355.8748 (355.8454)	
2019-01-08 11:06:48,414 - 10 - training_embed.py - training - Epoch: [10][600/1014]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 355.7408 (355.8140)	
2019-01-08 11:06:48,941 - 10 - training_embed.py - training - Epoch: [10][700/1014]	Time 0.006 (0.005)	Data 0.005 (0.002)	Loss 359.0243 (355.8666)	
2019-01-08 11:06:49,453 - 10 - training_embed.py - training - Epoch: [10][800/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 356.7953 (355.8460)	
2019-01-08 11:06:50,047 - 10 - training_embed.py - training - Epoch: [10][900/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 351.4109 (355.8140)	
2019-01-08 11:06:50,722 - 10 - training_embed.py - training - Epoch: [10][1000/1014]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 360.5134 (355.8188)	
2019-01-08 11:06:50,787 - 10 - training_embed.py - training - loss: 355.734442
2019-01-08 11:06:50,787 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 11:06:51,968 - 10 - training_embed.py - training - Epoch: [11][100/1014]	Time 0.010 (0.005)	Data 0.002 (0.002)	Loss 357.8952 (355.7219)	
2019-01-08 11:06:52,936 - 10 - training_embed.py - training - Epoch: [11][200/1014]	Time 0.009 (0.005)	Data 0.001 (0.002)	Loss 356.8089 (355.6793)	
2019-01-08 11:06:53,841 - 10 - training_embed.py - training - Epoch: [11][300/1014]	Time 0.010 (0.005)	Data 0.002 (0.002)	Loss 349.8411 (355.6466)	
2019-01-08 11:06:54,618 - 10 - training_embed.py - training - Epoch: [11][400/1014]	Time 0.010 (0.005)	Data 0.002 (0.002)	Loss 358.0749 (355.6806)	
2019-01-08 11:06:55,441 - 10 - training_embed.py - training - Epoch: [11][500/1014]	Time 0.010 (0.005)	Data 0.002 (0.002)	Loss 354.8617 (355.7230)	
2019-01-08 11:06:56,218 - 10 - training_embed.py - training - Epoch: [11][600/1014]	Time 0.013 (0.005)	Data 0.001 (0.002)	Loss 356.5931 (355.7327)	
2019-01-08 11:06:56,920 - 10 - training_embed.py - training - Epoch: [11][700/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 354.9636 (355.7135)	
2019-01-08 11:06:57,396 - 10 - training_embed.py - training - Epoch: [11][800/1014]	Time 0.006 (0.005)	Data 0.000 (0.002)	Loss 356.7408 (355.7153)	
2019-01-08 11:06:57,837 - 10 - training_embed.py - training - Epoch: [11][900/1014]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 357.9838 (355.7605)	
2019-01-08 11:06:58,363 - 10 - training_embed.py - training - Epoch: [11][1000/1014]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 350.0249 (355.7321)	
2019-01-08 11:06:58,473 - 10 - training_embed.py - training - loss: 355.644836
2019-01-08 11:06:58,610 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 11:06:59,507 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 896.577 ms ~ 0.015 min ~ 0.897 sec
2019-01-08 11:07:00,606 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1996.299 ms ~ 0.033 min ~ 1.996 sec
2019-01-08 11:07:00,607 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 11:07:00,607 - 10 - corpus.py - subactivity_sampler - 0 / 169
2019-01-08 11:07:00,608 - 10 - corpus.py - subactivity_sampler - [31464. 35959. 25000. 33839. 34120. 28017. 43855. 27241.]
2019-01-08 11:07:31,861 - 10 - corpus.py - subactivity_sampler - 20 / 169
2019-01-08 11:07:31,861 - 10 - corpus.py - subactivity_sampler - [31306. 36397. 24428. 34263. 33750. 27529. 45315. 26507.]
2019-01-08 11:07:52,806 - 10 - corpus.py - subactivity_sampler - 40 / 169
2019-01-08 11:07:52,807 - 10 - corpus.py - subactivity_sampler - [31103. 37751. 22893. 34540. 33805. 27219. 46873. 25311.]
2019-01-08 11:08:25,028 - 10 - corpus.py - subactivity_sampler - 60 / 169
2019-01-08 11:08:25,028 - 10 - corpus.py - subactivity_sampler - [30806. 38616. 22069. 35243. 33060. 26639. 48621. 24441.]
2019-01-08 11:08:58,855 - 10 - corpus.py - subactivity_sampler - 80 / 169
2019-01-08 11:08:58,856 - 10 - corpus.py - subactivity_sampler - [30555. 39286. 20756. 36271. 32725. 25920. 50924. 23058.]
2019-01-08 11:09:24,463 - 10 - corpus.py - subactivity_sampler - 100 / 169
2019-01-08 11:09:24,464 - 10 - corpus.py - subactivity_sampler - [30328. 40702. 19254. 36972. 32049. 25420. 53026. 21744.]
2019-01-08 11:09:43,819 - 10 - corpus.py - subactivity_sampler - 120 / 169
2019-01-08 11:09:43,819 - 10 - corpus.py - subactivity_sampler - [30009. 41420. 18446. 37575. 31493. 24957. 55099. 20496.]
2019-01-08 11:10:06,543 - 10 - corpus.py - subactivity_sampler - 140 / 169
2019-01-08 11:10:06,543 - 10 - corpus.py - subactivity_sampler - [29537. 42580. 17451. 38086. 31638. 24420. 56713. 19070.]
2019-01-08 11:10:31,283 - 10 - corpus.py - subactivity_sampler - 160 / 169
2019-01-08 11:10:31,283 - 10 - corpus.py - subactivity_sampler - [29126. 43501. 16136. 39208. 31061. 24059. 58677. 17727.]
2019-01-08 11:10:39,192 - 10 - corpus.py - subactivity_sampler - [28981. 44055. 15539. 39305. 30837. 23901. 59948. 16929.]
2019-01-08 11:10:39,192 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 218585.830 ms ~ 3.643 min ~ 218.586 sec
2019-01-08 11:10:39,192 - 10 - corpus.py - ordering_sampler - .
2019-01-08 11:10:41,403 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 11:10:41,403 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 1.  0.  0. 59. 45. 12.  0.]
2019-01-08 11:10:41,404 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 11:10:41,443 - 10 - corpus.py - rho_sampling - ['60.2746', '32.4985', '928.5380', '142.4679', '0.4088', '274.3317', '755.8528']
2019-01-08 11:10:41,443 - 10 - pipeline.py - baseline - Iteration 2
2019-01-08 11:10:41,548 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 11:10:41,554 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 11:10:41,555 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 18', '1: 36', '2: 14', '3: 37', '4: 38', '5: 41', '6: 39', '7: 40']
2019-01-08 11:10:41,567 - 10 - accuracy_class.py - mof_val - frames true: 105633	frames overall : 259495
2019-01-08 11:10:41,567 - 10 - corpus.py - accuracy_corpus - Action: sandwich
2019-01-08 11:10:41,567 - 10 - corpus.py - accuracy_corpus - MoF val: 0.407071427195129
2019-01-08 11:10:41,567 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4070059153355556
2019-01-08 11:10:41,567 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:10:41,567 - 10 - accuracy_class.py - mof_classes - label 14: 0.071429  140 / 1960
2019-01-08 11:10:41,567 - 10 - accuracy_class.py - mof_classes - label 18: 0.515510  2094 / 4062
2019-01-08 11:10:41,567 - 10 - accuracy_class.py - mof_classes - label 36: 0.602316  27618 / 45853
2019-01-08 11:10:41,567 - 10 - accuracy_class.py - mof_classes - label 37: 0.310371  30678 / 98843
2019-01-08 11:10:41,567 - 10 - accuracy_class.py - mof_classes - label 38: 0.232738  1655 / 7111
2019-01-08 11:10:41,567 - 10 - accuracy_class.py - mof_classes - label 39: 0.645453  41766 / 64708
2019-01-08 11:10:41,567 - 10 - accuracy_class.py - mof_classes - label 40: 0.320625  1682 / 5246
2019-01-08 11:10:41,568 - 10 - accuracy_class.py - mof_classes - label 41: 0.000000  0 / 970
2019-01-08 11:10:41,568 - 10 - accuracy_class.py - mof_classes - average class mof: 0.299827
2019-01-08 11:10:41,568 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:10:41,568 - 10 - accuracy_class.py - iou_classes - label 14: 0.008065  140 / 17359
2019-01-08 11:10:41,568 - 10 - accuracy_class.py - iou_classes - label 18: 0.067660  2094 / 30949
2019-01-08 11:10:41,568 - 10 - accuracy_class.py - iou_classes - label 36: 0.443378  27618 / 62290
2019-01-08 11:10:41,568 - 10 - accuracy_class.py - iou_classes - label 37: 0.285456  30678 / 107470
2019-01-08 11:10:41,568 - 10 - accuracy_class.py - iou_classes - label 38: 0.045601  1655 / 36293
2019-01-08 11:10:41,568 - 10 - accuracy_class.py - iou_classes - label 39: 0.503873  41766 / 82890
2019-01-08 11:10:41,568 - 10 - accuracy_class.py - iou_classes - label 40: 0.082077  1682 / 20493
2019-01-08 11:10:41,568 - 10 - accuracy_class.py - iou_classes - label 41: 0.000000  0 / 24871
2019-01-08 11:10:41,568 - 10 - accuracy_class.py - iou_classes - average IoU: 0.179514
2019-01-08 11:10:41,568 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.159568
2019-01-08 11:10:46,065 - 10 - f1_score.py - f1 - f1 score: 0.304084
2019-01-08 11:10:46,076 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 4633.573 ms ~ 0.077 min ~ 4.634 sec
2019-01-08 11:10:46,077 - 10 - corpus.py - embedding_training - .
2019-01-08 11:10:46,077 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 11:10:46,077 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 11:10:46,077 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 11:10:48,655 - 10 - training_embed.py - training - create model
2019-01-08 11:10:48,663 - 10 - training_embed.py - training - epochs: 12
2019-01-08 11:10:48,663 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 11:10:49,266 - 10 - training_embed.py - training - Epoch: [0][100/1014]	Time 0.003 (0.006)	Data 0.002 (0.004)	Loss 352.8308 (355.6777)	
2019-01-08 11:10:49,666 - 10 - training_embed.py - training - Epoch: [0][200/1014]	Time 0.002 (0.005)	Data 0.000 (0.003)	Loss 352.6458 (355.4140)	
2019-01-08 11:10:50,088 - 10 - training_embed.py - training - Epoch: [0][300/1014]	Time 0.008 (0.005)	Data 0.001 (0.003)	Loss 355.2153 (355.3981)	
2019-01-08 11:10:50,520 - 10 - training_embed.py - training - Epoch: [0][400/1014]	Time 0.001 (0.005)	Data 0.000 (0.003)	Loss 357.0191 (355.4837)	
2019-01-08 11:10:50,948 - 10 - training_embed.py - training - Epoch: [0][500/1014]	Time 0.003 (0.005)	Data 0.002 (0.003)	Loss 354.0006 (355.3856)	
2019-01-08 11:10:51,368 - 10 - training_embed.py - training - Epoch: [0][600/1014]	Time 0.006 (0.004)	Data 0.005 (0.002)	Loss 353.8402 (355.4167)	
2019-01-08 11:10:51,795 - 10 - training_embed.py - training - Epoch: [0][700/1014]	Time 0.005 (0.004)	Data 0.003 (0.002)	Loss 354.3059 (355.3755)	
2019-01-08 11:10:52,222 - 10 - training_embed.py - training - Epoch: [0][800/1014]	Time 0.004 (0.004)	Data 0.001 (0.002)	Loss 350.2647 (355.3378)	
2019-01-08 11:10:52,640 - 10 - training_embed.py - training - Epoch: [0][900/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 355.2737 (355.2831)	
2019-01-08 11:10:53,062 - 10 - training_embed.py - training - Epoch: [0][1000/1014]	Time 0.012 (0.004)	Data 0.010 (0.002)	Loss 355.2465 (355.2073)	
2019-01-08 11:10:53,099 - 10 - training_embed.py - training - loss: 355.138215
2019-01-08 11:10:53,100 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 11:10:53,758 - 10 - training_embed.py - training - Epoch: [1][100/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 361.2095 (355.3200)	
2019-01-08 11:10:54,210 - 10 - training_embed.py - training - Epoch: [1][200/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 355.3086 (355.0450)	
2019-01-08 11:10:54,674 - 10 - training_embed.py - training - Epoch: [1][300/1014]	Time 0.005 (0.005)	Data 0.003 (0.002)	Loss 354.2926 (355.0013)	
2019-01-08 11:10:55,150 - 10 - training_embed.py - training - Epoch: [1][400/1014]	Time 0.002 (0.005)	Data 0.000 (0.002)	Loss 354.2063 (355.0647)	
2019-01-08 11:10:55,670 - 10 - training_embed.py - training - Epoch: [1][500/1014]	Time 0.008 (0.005)	Data 0.000 (0.002)	Loss 355.5645 (355.0402)	
2019-01-08 11:10:56,135 - 10 - training_embed.py - training - Epoch: [1][600/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 357.2472 (355.0234)	
2019-01-08 11:10:56,597 - 10 - training_embed.py - training - Epoch: [1][700/1014]	Time 0.004 (0.005)	Data 0.003 (0.002)	Loss 357.9951 (355.0357)	
2019-01-08 11:10:57,059 - 10 - training_embed.py - training - Epoch: [1][800/1014]	Time 0.006 (0.005)	Data 0.001 (0.002)	Loss 356.9153 (355.0372)	
2019-01-08 11:10:57,531 - 10 - training_embed.py - training - Epoch: [1][900/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 357.7704 (354.9990)	
2019-01-08 11:10:57,994 - 10 - training_embed.py - training - Epoch: [1][1000/1014]	Time 0.005 (0.005)	Data 0.003 (0.002)	Loss 355.1828 (355.0144)	
2019-01-08 11:10:58,047 - 10 - training_embed.py - training - loss: 354.943810
2019-01-08 11:10:58,047 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 11:10:58,763 - 10 - training_embed.py - training - Epoch: [2][100/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 354.8046 (354.8994)	
2019-01-08 11:10:59,233 - 10 - training_embed.py - training - Epoch: [2][200/1014]	Time 0.005 (0.005)	Data 0.003 (0.002)	Loss 352.3961 (354.6597)	
2019-01-08 11:10:59,719 - 10 - training_embed.py - training - Epoch: [2][300/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 355.5022 (354.6930)	
2019-01-08 11:11:00,200 - 10 - training_embed.py - training - Epoch: [2][400/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 354.6830 (354.8426)	
2019-01-08 11:11:00,671 - 10 - training_embed.py - training - Epoch: [2][500/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 355.6527 (354.8077)	
2019-01-08 11:11:01,150 - 10 - training_embed.py - training - Epoch: [2][600/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 352.6611 (354.8352)	
2019-01-08 11:11:01,613 - 10 - training_embed.py - training - Epoch: [2][700/1014]	Time 0.004 (0.005)	Data 0.003 (0.002)	Loss 354.0417 (354.7842)	
2019-01-08 11:11:02,050 - 10 - training_embed.py - training - Epoch: [2][800/1014]	Time 0.011 (0.005)	Data 0.010 (0.002)	Loss 355.4071 (354.7744)	
2019-01-08 11:11:02,539 - 10 - training_embed.py - training - Epoch: [2][900/1014]	Time 0.017 (0.005)	Data 0.015 (0.002)	Loss 355.3377 (354.8302)	
2019-01-08 11:11:02,954 - 10 - training_embed.py - training - Epoch: [2][1000/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 350.9159 (354.8309)	
2019-01-08 11:11:03,011 - 10 - training_embed.py - training - loss: 354.747612
2019-01-08 11:11:03,011 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 11:11:03,660 - 10 - training_embed.py - training - Epoch: [3][100/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 353.2162 (354.5497)	
2019-01-08 11:11:04,114 - 10 - training_embed.py - training - Epoch: [3][200/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 356.8571 (354.4863)	
2019-01-08 11:11:04,581 - 10 - training_embed.py - training - Epoch: [3][300/1014]	Time 0.005 (0.005)	Data 0.000 (0.002)	Loss 355.1115 (354.6345)	
2019-01-08 11:11:05,018 - 10 - training_embed.py - training - Epoch: [3][400/1014]	Time 0.004 (0.005)	Data 0.000 (0.002)	Loss 350.6165 (354.6712)	
2019-01-08 11:11:05,450 - 10 - training_embed.py - training - Epoch: [3][500/1014]	Time 0.007 (0.005)	Data 0.005 (0.002)	Loss 355.1527 (354.6863)	
2019-01-08 11:11:05,918 - 10 - training_embed.py - training - Epoch: [3][600/1014]	Time 0.009 (0.005)	Data 0.002 (0.002)	Loss 358.2303 (354.6531)	
2019-01-08 11:11:06,375 - 10 - training_embed.py - training - Epoch: [3][700/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 353.2180 (354.6564)	
2019-01-08 11:11:06,854 - 10 - training_embed.py - training - Epoch: [3][800/1014]	Time 0.006 (0.005)	Data 0.005 (0.002)	Loss 352.4655 (354.6292)	
2019-01-08 11:11:07,240 - 10 - training_embed.py - training - Epoch: [3][900/1014]	Time 0.006 (0.005)	Data 0.002 (0.002)	Loss 352.8460 (354.6661)	
2019-01-08 11:11:07,655 - 10 - training_embed.py - training - Epoch: [3][1000/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 353.9279 (354.6393)	
2019-01-08 11:11:07,712 - 10 - training_embed.py - training - loss: 354.553209
2019-01-08 11:11:07,712 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 11:11:08,373 - 10 - training_embed.py - training - Epoch: [4][100/1014]	Time 0.009 (0.005)	Data 0.001 (0.002)	Loss 353.4969 (354.2578)	
2019-01-08 11:11:08,854 - 10 - training_embed.py - training - Epoch: [4][200/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 354.8961 (354.4382)	
2019-01-08 11:11:09,356 - 10 - training_embed.py - training - Epoch: [4][300/1014]	Time 0.006 (0.005)	Data 0.000 (0.002)	Loss 355.5354 (354.4600)	
2019-01-08 11:11:09,851 - 10 - training_embed.py - training - Epoch: [4][400/1014]	Time 0.007 (0.005)	Data 0.003 (0.002)	Loss 359.2966 (354.5087)	
2019-01-08 11:11:10,278 - 10 - training_embed.py - training - Epoch: [4][500/1014]	Time 0.008 (0.005)	Data 0.006 (0.002)	Loss 353.9637 (354.4435)	
2019-01-08 11:11:10,720 - 10 - training_embed.py - training - Epoch: [4][600/1014]	Time 0.006 (0.005)	Data 0.000 (0.002)	Loss 351.7800 (354.4404)	
2019-01-08 11:11:11,143 - 10 - training_embed.py - training - Epoch: [4][700/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 354.7583 (354.4652)	
2019-01-08 11:11:11,606 - 10 - training_embed.py - training - Epoch: [4][800/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 356.1943 (354.4336)	
2019-01-08 11:11:12,060 - 10 - training_embed.py - training - Epoch: [4][900/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 355.9860 (354.4500)	
2019-01-08 11:11:12,524 - 10 - training_embed.py - training - Epoch: [4][1000/1014]	Time 0.010 (0.005)	Data 0.008 (0.002)	Loss 356.7170 (354.4557)	
2019-01-08 11:11:12,575 - 10 - training_embed.py - training - loss: 354.359454
2019-01-08 11:11:12,582 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 11:11:13,222 - 10 - training_embed.py - training - Epoch: [5][100/1014]	Time 0.010 (0.005)	Data 0.002 (0.002)	Loss 352.9986 (353.8078)	
2019-01-08 11:11:13,639 - 10 - training_embed.py - training - Epoch: [5][200/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 355.4195 (353.9741)	
2019-01-08 11:11:14,077 - 10 - training_embed.py - training - Epoch: [5][300/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 352.2914 (353.9554)	
2019-01-08 11:11:14,514 - 10 - training_embed.py - training - Epoch: [5][400/1014]	Time 0.009 (0.005)	Data 0.008 (0.002)	Loss 349.7409 (354.1420)	
2019-01-08 11:11:14,972 - 10 - training_embed.py - training - Epoch: [5][500/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 355.8644 (354.2958)	
2019-01-08 11:11:15,447 - 10 - training_embed.py - training - Epoch: [5][600/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 353.0180 (354.2786)	
2019-01-08 11:11:15,941 - 10 - training_embed.py - training - Epoch: [5][700/1014]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 352.8578 (354.2846)	
2019-01-08 11:11:16,389 - 10 - training_embed.py - training - Epoch: [5][800/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 354.6305 (354.2225)	
2019-01-08 11:11:16,864 - 10 - training_embed.py - training - Epoch: [5][900/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 349.9832 (354.2646)	
2019-01-08 11:11:17,331 - 10 - training_embed.py - training - Epoch: [5][1000/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 351.5910 (354.2378)	
2019-01-08 11:11:17,386 - 10 - training_embed.py - training - loss: 354.163435
2019-01-08 11:11:17,386 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 11:11:18,007 - 10 - training_embed.py - training - Epoch: [6][100/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 358.0625 (354.0157)	
2019-01-08 11:11:18,424 - 10 - training_embed.py - training - Epoch: [6][200/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 353.5756 (353.9739)	
2019-01-08 11:11:18,846 - 10 - training_embed.py - training - Epoch: [6][300/1014]	Time 0.004 (0.005)	Data 0.003 (0.002)	Loss 352.6631 (354.0516)	
2019-01-08 11:11:19,308 - 10 - training_embed.py - training - Epoch: [6][400/1014]	Time 0.009 (0.005)	Data 0.001 (0.002)	Loss 359.2090 (353.9999)	
2019-01-08 11:11:19,707 - 10 - training_embed.py - training - Epoch: [6][500/1014]	Time 0.004 (0.005)	Data 0.003 (0.002)	Loss 354.0190 (354.0767)	
2019-01-08 11:11:20,130 - 10 - training_embed.py - training - Epoch: [6][600/1014]	Time 0.008 (0.005)	Data 0.000 (0.002)	Loss 353.4643 (354.1623)	
2019-01-08 11:11:20,574 - 10 - training_embed.py - training - Epoch: [6][700/1014]	Time 0.004 (0.005)	Data 0.000 (0.002)	Loss 352.4245 (354.1435)	
2019-01-08 11:11:21,046 - 10 - training_embed.py - training - Epoch: [6][800/1014]	Time 0.002 (0.005)	Data 0.000 (0.002)	Loss 356.9988 (354.0408)	
2019-01-08 11:11:21,493 - 10 - training_embed.py - training - Epoch: [6][900/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 355.3564 (354.0292)	
2019-01-08 11:11:21,913 - 10 - training_embed.py - training - Epoch: [6][1000/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 355.4973 (354.0625)	
2019-01-08 11:11:21,961 - 10 - training_embed.py - training - loss: 353.966511
2019-01-08 11:11:21,961 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 11:11:22,616 - 10 - training_embed.py - training - Epoch: [7][100/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 353.6329 (353.9921)	
2019-01-08 11:11:23,052 - 10 - training_embed.py - training - Epoch: [7][200/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 353.3281 (354.0858)	
2019-01-08 11:11:23,514 - 10 - training_embed.py - training - Epoch: [7][300/1014]	Time 0.008 (0.005)	Data 0.000 (0.002)	Loss 352.0001 (353.9794)	
2019-01-08 11:11:23,985 - 10 - training_embed.py - training - Epoch: [7][400/1014]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 353.0589 (353.9496)	
2019-01-08 11:11:24,468 - 10 - training_embed.py - training - Epoch: [7][500/1014]	Time 0.005 (0.005)	Data 0.004 (0.002)	Loss 354.1397 (353.8793)	
2019-01-08 11:11:24,928 - 10 - training_embed.py - training - Epoch: [7][600/1014]	Time 0.004 (0.005)	Data 0.003 (0.002)	Loss 353.4854 (353.9005)	
2019-01-08 11:11:25,388 - 10 - training_embed.py - training - Epoch: [7][700/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 355.1890 (353.8837)	
2019-01-08 11:11:25,879 - 10 - training_embed.py - training - Epoch: [7][800/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 353.9381 (353.8635)	
2019-01-08 11:11:26,332 - 10 - training_embed.py - training - Epoch: [7][900/1014]	Time 0.002 (0.005)	Data 0.000 (0.002)	Loss 353.6277 (353.8505)	
2019-01-08 11:11:26,803 - 10 - training_embed.py - training - Epoch: [7][1000/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 355.1106 (353.8362)	
2019-01-08 11:11:26,871 - 10 - training_embed.py - training - loss: 353.770964
2019-01-08 11:11:26,871 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 11:11:27,490 - 10 - training_embed.py - training - Epoch: [8][100/1014]	Time 0.006 (0.005)	Data 0.001 (0.002)	Loss 356.1378 (353.3105)	
2019-01-08 11:11:27,916 - 10 - training_embed.py - training - Epoch: [8][200/1014]	Time 0.006 (0.005)	Data 0.005 (0.002)	Loss 352.1722 (353.4533)	
2019-01-08 11:11:28,393 - 10 - training_embed.py - training - Epoch: [8][300/1014]	Time 0.004 (0.005)	Data 0.000 (0.002)	Loss 358.2493 (353.5142)	
2019-01-08 11:11:28,817 - 10 - training_embed.py - training - Epoch: [8][400/1014]	Time 0.005 (0.005)	Data 0.002 (0.002)	Loss 352.2942 (353.5469)	
2019-01-08 11:11:29,217 - 10 - training_embed.py - training - Epoch: [8][500/1014]	Time 0.006 (0.005)	Data 0.004 (0.002)	Loss 357.8377 (353.5572)	
2019-01-08 11:11:29,663 - 10 - training_embed.py - training - Epoch: [8][600/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 349.3896 (353.5649)	
2019-01-08 11:11:30,131 - 10 - training_embed.py - training - Epoch: [8][700/1014]	Time 0.005 (0.005)	Data 0.004 (0.002)	Loss 354.2083 (353.6043)	
2019-01-08 11:11:30,578 - 10 - training_embed.py - training - Epoch: [8][800/1014]	Time 0.004 (0.005)	Data 0.003 (0.002)	Loss 353.5874 (353.6370)	
2019-01-08 11:11:31,040 - 10 - training_embed.py - training - Epoch: [8][900/1014]	Time 0.014 (0.005)	Data 0.013 (0.002)	Loss 356.5923 (353.6302)	
2019-01-08 11:11:31,439 - 10 - training_embed.py - training - Epoch: [8][1000/1014]	Time 0.008 (0.005)	Data 0.006 (0.002)	Loss 352.6413 (353.6333)	
2019-01-08 11:11:31,474 - 10 - training_embed.py - training - loss: 353.572978
2019-01-08 11:11:31,474 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 11:11:32,158 - 10 - training_embed.py - training - Epoch: [9][100/1014]	Time 0.005 (0.005)	Data 0.002 (0.002)	Loss 354.8573 (353.9460)	
2019-01-08 11:11:32,590 - 10 - training_embed.py - training - Epoch: [9][200/1014]	Time 0.004 (0.005)	Data 0.001 (0.002)	Loss 357.7112 (353.6757)	
2019-01-08 11:11:33,009 - 10 - training_embed.py - training - Epoch: [9][300/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 353.1666 (353.7917)	
2019-01-08 11:11:33,427 - 10 - training_embed.py - training - Epoch: [9][400/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 349.9843 (353.7037)	
2019-01-08 11:11:33,830 - 10 - training_embed.py - training - Epoch: [9][500/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 349.4227 (353.6461)	
2019-01-08 11:11:34,273 - 10 - training_embed.py - training - Epoch: [9][600/1014]	Time 0.010 (0.005)	Data 0.000 (0.002)	Loss 355.6939 (353.6095)	
2019-01-08 11:11:34,728 - 10 - training_embed.py - training - Epoch: [9][700/1014]	Time 0.010 (0.005)	Data 0.003 (0.002)	Loss 350.4275 (353.5414)	
2019-01-08 11:11:35,140 - 10 - training_embed.py - training - Epoch: [9][800/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 353.2477 (353.4910)	
2019-01-08 11:11:35,599 - 10 - training_embed.py - training - Epoch: [9][900/1014]	Time 0.003 (0.005)	Data 0.000 (0.002)	Loss 354.6839 (353.4939)	
2019-01-08 11:11:36,085 - 10 - training_embed.py - training - Epoch: [9][1000/1014]	Time 0.008 (0.005)	Data 0.007 (0.002)	Loss 354.6034 (353.4587)	
2019-01-08 11:11:36,149 - 10 - training_embed.py - training - loss: 353.376869
2019-01-08 11:11:36,149 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 11:11:36,747 - 10 - training_embed.py - training - Epoch: [10][100/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 357.4807 (353.6759)	
2019-01-08 11:11:37,221 - 10 - training_embed.py - training - Epoch: [10][200/1014]	Time 0.007 (0.005)	Data 0.000 (0.002)	Loss 351.1291 (353.4920)	
2019-01-08 11:11:37,643 - 10 - training_embed.py - training - Epoch: [10][300/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 350.2675 (353.4114)	
2019-01-08 11:11:38,071 - 10 - training_embed.py - training - Epoch: [10][400/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 352.8912 (353.3848)	
2019-01-08 11:11:38,506 - 10 - training_embed.py - training - Epoch: [10][500/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 350.2452 (353.3015)	
2019-01-08 11:11:38,924 - 10 - training_embed.py - training - Epoch: [10][600/1014]	Time 0.004 (0.005)	Data 0.003 (0.002)	Loss 357.3043 (353.2444)	
2019-01-08 11:11:39,341 - 10 - training_embed.py - training - Epoch: [10][700/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 355.9463 (353.2805)	
2019-01-08 11:11:39,760 - 10 - training_embed.py - training - Epoch: [10][800/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 356.0984 (353.2867)	
2019-01-08 11:11:40,173 - 10 - training_embed.py - training - Epoch: [10][900/1014]	Time 0.007 (0.005)	Data 0.006 (0.002)	Loss 350.8589 (353.2646)	
2019-01-08 11:11:40,600 - 10 - training_embed.py - training - Epoch: [10][1000/1014]	Time 0.007 (0.005)	Data 0.005 (0.002)	Loss 357.7447 (353.2615)	
2019-01-08 11:11:40,633 - 10 - training_embed.py - training - loss: 353.179654
2019-01-08 11:11:40,634 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 11:11:41,238 - 10 - training_embed.py - training - Epoch: [11][100/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 353.1875 (352.9950)	
2019-01-08 11:11:41,714 - 10 - training_embed.py - training - Epoch: [11][200/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 355.2249 (352.9335)	
2019-01-08 11:11:42,162 - 10 - training_embed.py - training - Epoch: [11][300/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 348.1497 (352.8600)	
2019-01-08 11:11:42,621 - 10 - training_embed.py - training - Epoch: [11][400/1014]	Time 0.009 (0.005)	Data 0.008 (0.002)	Loss 354.5114 (352.9742)	
2019-01-08 11:11:43,076 - 10 - training_embed.py - training - Epoch: [11][500/1014]	Time 0.007 (0.005)	Data 0.006 (0.002)	Loss 353.1518 (353.0192)	
2019-01-08 11:11:43,563 - 10 - training_embed.py - training - Epoch: [11][600/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 352.0920 (353.0637)	
2019-01-08 11:11:44,031 - 10 - training_embed.py - training - Epoch: [11][700/1014]	Time 0.009 (0.005)	Data 0.003 (0.002)	Loss 350.5891 (353.0697)	
2019-01-08 11:11:44,517 - 10 - training_embed.py - training - Epoch: [11][800/1014]	Time 0.014 (0.005)	Data 0.013 (0.002)	Loss 354.3590 (353.0631)	
2019-01-08 11:11:44,992 - 10 - training_embed.py - training - Epoch: [11][900/1014]	Time 0.004 (0.005)	Data 0.001 (0.002)	Loss 354.8019 (353.0772)	
2019-01-08 11:11:45,482 - 10 - training_embed.py - training - Epoch: [11][1000/1014]	Time 0.007 (0.005)	Data 0.005 (0.002)	Loss 346.6158 (353.0705)	
2019-01-08 11:11:45,522 - 10 - training_embed.py - training - loss: 352.981589
2019-01-08 11:11:45,586 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 11:11:46,418 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 831.472 ms ~ 0.014 min ~ 0.831 sec
2019-01-08 11:11:47,499 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1912.956 ms ~ 0.032 min ~ 1.913 sec
2019-01-08 11:11:47,499 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 11:11:47,500 - 10 - corpus.py - subactivity_sampler - 0 / 169
2019-01-08 11:11:47,501 - 10 - corpus.py - subactivity_sampler - [28981. 44055. 15539. 39305. 30837. 23901. 59948. 16929.]
2019-01-08 11:12:17,647 - 10 - corpus.py - subactivity_sampler - 20 / 169
2019-01-08 11:12:17,647 - 10 - corpus.py - subactivity_sampler - [28728. 44939. 14596. 39891. 29792. 22704. 63540. 15305.]
2019-01-08 11:12:34,994 - 10 - corpus.py - subactivity_sampler - 40 / 169
2019-01-08 11:12:34,994 - 10 - corpus.py - subactivity_sampler - [28632. 45597. 14045. 39918. 29489. 22311. 65247. 14256.]
2019-01-08 11:12:58,418 - 10 - corpus.py - subactivity_sampler - 60 / 169
2019-01-08 11:12:58,418 - 10 - corpus.py - subactivity_sampler - [28537. 46546. 12936. 40871. 28786. 21544. 67912. 12363.]
2019-01-08 11:13:21,465 - 10 - corpus.py - subactivity_sampler - 80 / 169
2019-01-08 11:13:21,465 - 10 - corpus.py - subactivity_sampler - [28336. 47129. 12364. 41278. 28216. 21412. 69730. 11030.]
2019-01-08 11:13:41,103 - 10 - corpus.py - subactivity_sampler - 100 / 169
2019-01-08 11:13:41,103 - 10 - corpus.py - subactivity_sampler - [28163. 47525. 11970. 41672. 27620. 20560. 72192.  9793.]
2019-01-08 11:13:58,412 - 10 - corpus.py - subactivity_sampler - 120 / 169
2019-01-08 11:13:58,412 - 10 - corpus.py - subactivity_sampler - [28007. 48495. 11221. 41709. 27006. 20096. 73987.  8974.]
2019-01-08 11:14:20,089 - 10 - corpus.py - subactivity_sampler - 140 / 169
2019-01-08 11:14:20,090 - 10 - corpus.py - subactivity_sampler - [27796. 49088. 10786. 42312. 26434. 19269. 75920.  7890.]
2019-01-08 11:14:44,230 - 10 - corpus.py - subactivity_sampler - 160 / 169
2019-01-08 11:14:44,230 - 10 - corpus.py - subactivity_sampler - [27669. 49345. 10342. 43265. 26034. 18525. 77152.  7163.]
2019-01-08 11:14:52,444 - 10 - corpus.py - subactivity_sampler - [27606. 49576. 10175. 43540. 25864. 18019. 77633.  7082.]
2019-01-08 11:14:52,444 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 184944.741 ms ~ 3.082 min ~ 184.945 sec
2019-01-08 11:14:52,444 - 10 - corpus.py - ordering_sampler - .
2019-01-08 11:14:54,694 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 11:14:54,695 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 4.  0.  0.  5. 66.  0.  0.]
2019-01-08 11:14:54,695 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 11:14:54,719 - 10 - corpus.py - rho_sampling - ['59.0766', '33.9150', '928.4017', '72.5629', '31.5325', '38.5765', '27.8368']
2019-01-08 11:14:54,719 - 10 - pipeline.py - baseline - Iteration 3
2019-01-08 11:14:54,816 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 11:14:54,822 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 11:14:54,822 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 18', '1: 36', '2: 14', '3: 37', '4: 38', '5: 41', '6: 39', '7: 40']
2019-01-08 11:14:54,833 - 10 - accuracy_class.py - mof_val - frames true: 118222	frames overall : 259495
2019-01-08 11:14:54,833 - 10 - corpus.py - accuracy_corpus - Action: sandwich
2019-01-08 11:14:54,833 - 10 - corpus.py - accuracy_corpus - MoF val: 0.45558488602863256
2019-01-08 11:14:54,833 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.45558488602863256
2019-01-08 11:14:54,833 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:14:54,833 - 10 - accuracy_class.py - mof_classes - label 14: 0.048469  95 / 1960
2019-01-08 11:14:54,833 - 10 - accuracy_class.py - mof_classes - label 18: 0.474151  1926 / 4062
2019-01-08 11:14:54,833 - 10 - accuracy_class.py - mof_classes - label 36: 0.641114  29397 / 45853
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - mof_classes - label 37: 0.357678  35354 / 98843
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - mof_classes - label 38: 0.155393  1105 / 7111
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - mof_classes - label 39: 0.770647  49867 / 64708
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - mof_classes - label 40: 0.091117  478 / 5246
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - mof_classes - label 41: 0.000000  0 / 970
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - mof_classes - average class mof: 0.282063
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - iou_classes - label 14: 0.007890  95 / 12040
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - iou_classes - label 18: 0.064757  1926 / 29742
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - iou_classes - label 36: 0.445193  29397 / 66032
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - iou_classes - label 37: 0.330322  35354 / 107029
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - iou_classes - label 38: 0.034672  1105 / 31870
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - iou_classes - label 39: 0.539254  49867 / 92474
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - iou_classes - label 40: 0.040338  478 / 11850
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - iou_classes - label 41: 0.000000  0 / 18989
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - iou_classes - average IoU: 0.182803
2019-01-08 11:14:54,834 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.162492
2019-01-08 11:14:58,535 - 10 - f1_score.py - f1 - f1 score: 0.311966
2019-01-08 11:14:58,549 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3829.600 ms ~ 0.064 min ~ 3.830 sec
2019-01-08 11:14:58,549 - 10 - corpus.py - embedding_training - .
2019-01-08 11:14:58,549 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 11:14:58,549 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 11:14:58,549 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 11:15:01,065 - 10 - training_embed.py - training - create model
2019-01-08 11:15:01,069 - 10 - training_embed.py - training - epochs: 12
2019-01-08 11:15:01,070 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 11:15:01,601 - 10 - training_embed.py - training - Epoch: [0][100/1014]	Time 0.008 (0.005)	Data 0.007 (0.003)	Loss 350.7760 (354.1296)	
2019-01-08 11:15:01,971 - 10 - training_embed.py - training - Epoch: [0][200/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 349.3472 (354.0022)	
2019-01-08 11:15:02,374 - 10 - training_embed.py - training - Epoch: [0][300/1014]	Time 0.007 (0.004)	Data 0.006 (0.002)	Loss 354.1622 (353.9330)	
2019-01-08 11:15:02,782 - 10 - training_embed.py - training - Epoch: [0][400/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 358.3062 (353.9422)	
2019-01-08 11:15:03,177 - 10 - training_embed.py - training - Epoch: [0][500/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 353.9160 (353.8698)	
2019-01-08 11:15:03,594 - 10 - training_embed.py - training - Epoch: [0][600/1014]	Time 0.021 (0.004)	Data 0.019 (0.002)	Loss 351.7670 (353.8899)	
2019-01-08 11:15:03,987 - 10 - training_embed.py - training - Epoch: [0][700/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 352.9303 (353.8389)	
2019-01-08 11:15:04,404 - 10 - training_embed.py - training - Epoch: [0][800/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 348.8441 (353.8317)	
2019-01-08 11:15:04,817 - 10 - training_embed.py - training - Epoch: [0][900/1014]	Time 0.009 (0.004)	Data 0.007 (0.002)	Loss 355.8407 (353.7782)	
2019-01-08 11:15:05,187 - 10 - training_embed.py - training - Epoch: [0][1000/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 353.3650 (353.6916)	
2019-01-08 11:15:05,229 - 10 - training_embed.py - training - loss: 353.608801
2019-01-08 11:15:05,230 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 11:15:05,818 - 10 - training_embed.py - training - Epoch: [1][100/1014]	Time 0.008 (0.004)	Data 0.000 (0.002)	Loss 357.5926 (353.5620)	
2019-01-08 11:15:06,217 - 10 - training_embed.py - training - Epoch: [1][200/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 355.5050 (353.3002)	
2019-01-08 11:15:06,590 - 10 - training_embed.py - training - Epoch: [1][300/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 353.1427 (353.3264)	
2019-01-08 11:15:07,021 - 10 - training_embed.py - training - Epoch: [1][400/1014]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 351.0006 (353.3824)	
2019-01-08 11:15:07,394 - 10 - training_embed.py - training - Epoch: [1][500/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 351.3896 (353.3916)	
2019-01-08 11:15:07,783 - 10 - training_embed.py - training - Epoch: [1][600/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 354.4206 (353.3934)	
2019-01-08 11:15:08,158 - 10 - training_embed.py - training - Epoch: [1][700/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 353.5553 (353.3902)	
2019-01-08 11:15:08,538 - 10 - training_embed.py - training - Epoch: [1][800/1014]	Time 0.009 (0.004)	Data 0.007 (0.002)	Loss 351.6402 (353.4025)	
2019-01-08 11:15:08,949 - 10 - training_embed.py - training - Epoch: [1][900/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 355.0633 (353.3292)	
2019-01-08 11:15:09,364 - 10 - training_embed.py - training - Epoch: [1][1000/1014]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 356.1746 (353.3707)	
2019-01-08 11:15:09,403 - 10 - training_embed.py - training - loss: 353.288657
2019-01-08 11:15:09,404 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 11:15:09,980 - 10 - training_embed.py - training - Epoch: [2][100/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 349.6464 (353.0733)	
2019-01-08 11:15:10,416 - 10 - training_embed.py - training - Epoch: [2][200/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 351.1867 (352.9943)	
2019-01-08 11:15:10,843 - 10 - training_embed.py - training - Epoch: [2][300/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 351.9361 (353.0476)	
2019-01-08 11:15:11,288 - 10 - training_embed.py - training - Epoch: [2][400/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 351.2400 (353.0872)	
2019-01-08 11:15:11,662 - 10 - training_embed.py - training - Epoch: [2][500/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 354.3441 (353.0247)	
2019-01-08 11:15:12,067 - 10 - training_embed.py - training - Epoch: [2][600/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 351.0991 (353.0760)	
2019-01-08 11:15:12,462 - 10 - training_embed.py - training - Epoch: [2][700/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 352.5169 (353.0220)	
2019-01-08 11:15:12,869 - 10 - training_embed.py - training - Epoch: [2][800/1014]	Time 0.004 (0.004)	Data 0.001 (0.002)	Loss 353.6206 (352.9987)	
2019-01-08 11:15:13,263 - 10 - training_embed.py - training - Epoch: [2][900/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 354.4922 (353.0666)	
2019-01-08 11:15:13,650 - 10 - training_embed.py - training - Epoch: [2][1000/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 351.0829 (353.0538)	
2019-01-08 11:15:13,686 - 10 - training_embed.py - training - loss: 352.967407
2019-01-08 11:15:13,687 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 11:15:14,295 - 10 - training_embed.py - training - Epoch: [3][100/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 351.8565 (352.5631)	
2019-01-08 11:15:14,706 - 10 - training_embed.py - training - Epoch: [3][200/1014]	Time 0.006 (0.004)	Data 0.002 (0.002)	Loss 350.6088 (352.7516)	
2019-01-08 11:15:15,110 - 10 - training_embed.py - training - Epoch: [3][300/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 353.8362 (352.7902)	
2019-01-08 11:15:15,536 - 10 - training_embed.py - training - Epoch: [3][400/1014]	Time 0.012 (0.004)	Data 0.003 (0.002)	Loss 349.9969 (352.8260)	
2019-01-08 11:15:15,961 - 10 - training_embed.py - training - Epoch: [3][500/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 352.0843 (352.8610)	
2019-01-08 11:15:16,380 - 10 - training_embed.py - training - Epoch: [3][600/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 353.0967 (352.8223)	
2019-01-08 11:15:16,802 - 10 - training_embed.py - training - Epoch: [3][700/1014]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 352.3330 (352.7860)	
2019-01-08 11:15:17,209 - 10 - training_embed.py - training - Epoch: [3][800/1014]	Time 0.005 (0.004)	Data 0.000 (0.002)	Loss 351.3142 (352.7268)	
2019-01-08 11:15:17,585 - 10 - training_embed.py - training - Epoch: [3][900/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 348.4391 (352.7478)	
2019-01-08 11:15:17,977 - 10 - training_embed.py - training - Epoch: [3][1000/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 352.4215 (352.7363)	
2019-01-08 11:15:18,014 - 10 - training_embed.py - training - loss: 352.647220
2019-01-08 11:15:18,018 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 11:15:18,631 - 10 - training_embed.py - training - Epoch: [4][100/1014]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 352.6752 (352.3914)	
2019-01-08 11:15:19,026 - 10 - training_embed.py - training - Epoch: [4][200/1014]	Time 0.008 (0.004)	Data 0.007 (0.002)	Loss 353.4208 (352.4369)	
2019-01-08 11:15:19,436 - 10 - training_embed.py - training - Epoch: [4][300/1014]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 355.4220 (352.5534)	
2019-01-08 11:15:19,836 - 10 - training_embed.py - training - Epoch: [4][400/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 356.6823 (352.5652)	
2019-01-08 11:15:20,271 - 10 - training_embed.py - training - Epoch: [4][500/1014]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 354.8348 (352.4841)	
2019-01-08 11:15:20,676 - 10 - training_embed.py - training - Epoch: [4][600/1014]	Time 0.005 (0.004)	Data 0.003 (0.002)	Loss 349.3502 (352.4920)	
2019-01-08 11:15:21,079 - 10 - training_embed.py - training - Epoch: [4][700/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 351.7903 (352.5095)	
2019-01-08 11:15:21,473 - 10 - training_embed.py - training - Epoch: [4][800/1014]	Time 0.006 (0.004)	Data 0.001 (0.002)	Loss 351.2644 (352.4602)	
2019-01-08 11:15:21,881 - 10 - training_embed.py - training - Epoch: [4][900/1014]	Time 0.008 (0.004)	Data 0.006 (0.002)	Loss 353.3728 (352.4581)	
2019-01-08 11:15:22,297 - 10 - training_embed.py - training - Epoch: [4][1000/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 357.7924 (352.4202)	
2019-01-08 11:15:22,347 - 10 - training_embed.py - training - loss: 352.324747
2019-01-08 11:15:22,347 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 11:15:22,922 - 10 - training_embed.py - training - Epoch: [5][100/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 349.8746 (351.6825)	
2019-01-08 11:15:23,329 - 10 - training_embed.py - training - Epoch: [5][200/1014]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 353.6775 (351.8284)	
2019-01-08 11:15:23,720 - 10 - training_embed.py - training - Epoch: [5][300/1014]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 353.5140 (351.8206)	
2019-01-08 11:15:24,125 - 10 - training_embed.py - training - Epoch: [5][400/1014]	Time 0.011 (0.004)	Data 0.002 (0.002)	Loss 347.8803 (352.0349)	
2019-01-08 11:15:24,530 - 10 - training_embed.py - training - Epoch: [5][500/1014]	Time 0.009 (0.004)	Data 0.001 (0.002)	Loss 352.8107 (352.1668)	
2019-01-08 11:15:24,888 - 10 - training_embed.py - training - Epoch: [5][600/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 348.5276 (352.1876)	
2019-01-08 11:15:25,263 - 10 - training_embed.py - training - Epoch: [5][700/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 350.0249 (352.1463)	
2019-01-08 11:15:25,647 - 10 - training_embed.py - training - Epoch: [5][800/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 354.5410 (352.0830)	
2019-01-08 11:15:26,068 - 10 - training_embed.py - training - Epoch: [5][900/1014]	Time 0.006 (0.004)	Data 0.002 (0.002)	Loss 348.9813 (352.0886)	
2019-01-08 11:15:26,455 - 10 - training_embed.py - training - Epoch: [5][1000/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 349.3837 (352.0908)	
2019-01-08 11:15:26,493 - 10 - training_embed.py - training - loss: 352.003061
2019-01-08 11:15:26,493 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 11:15:27,087 - 10 - training_embed.py - training - Epoch: [6][100/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 354.9534 (351.9835)	
2019-01-08 11:15:27,490 - 10 - training_embed.py - training - Epoch: [6][200/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 351.3183 (351.8998)	
2019-01-08 11:15:27,919 - 10 - training_embed.py - training - Epoch: [6][300/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 353.3325 (351.9683)	
2019-01-08 11:15:28,338 - 10 - training_embed.py - training - Epoch: [6][400/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 353.6408 (351.8568)	
2019-01-08 11:15:28,773 - 10 - training_embed.py - training - Epoch: [6][500/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 352.2710 (351.9534)	
2019-01-08 11:15:29,195 - 10 - training_embed.py - training - Epoch: [6][600/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 350.3235 (351.9497)	
2019-01-08 11:15:29,597 - 10 - training_embed.py - training - Epoch: [6][700/1014]	Time 0.005 (0.004)	Data 0.001 (0.002)	Loss 349.0217 (351.8764)	
2019-01-08 11:15:29,985 - 10 - training_embed.py - training - Epoch: [6][800/1014]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 352.4841 (351.7785)	
2019-01-08 11:15:30,362 - 10 - training_embed.py - training - Epoch: [6][900/1014]	Time 0.006 (0.004)	Data 0.000 (0.002)	Loss 353.6996 (351.7875)	
2019-01-08 11:15:30,789 - 10 - training_embed.py - training - Epoch: [6][1000/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 351.6330 (351.7689)	
2019-01-08 11:15:30,830 - 10 - training_embed.py - training - loss: 351.676551
2019-01-08 11:15:30,831 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 11:15:31,478 - 10 - training_embed.py - training - Epoch: [7][100/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 354.2044 (351.8113)	
2019-01-08 11:15:31,956 - 10 - training_embed.py - training - Epoch: [7][200/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 351.1956 (351.7260)	
2019-01-08 11:15:32,369 - 10 - training_embed.py - training - Epoch: [7][300/1014]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 350.4623 (351.6201)	
2019-01-08 11:15:32,843 - 10 - training_embed.py - training - Epoch: [7][400/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 350.8165 (351.5377)	
2019-01-08 11:15:33,265 - 10 - training_embed.py - training - Epoch: [7][500/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 352.0490 (351.4546)	
2019-01-08 11:15:33,735 - 10 - training_embed.py - training - Epoch: [7][600/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 349.7770 (351.4721)	
2019-01-08 11:15:34,167 - 10 - training_embed.py - training - Epoch: [7][700/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 350.4718 (351.4395)	
2019-01-08 11:15:34,612 - 10 - training_embed.py - training - Epoch: [7][800/1014]	Time 0.007 (0.004)	Data 0.006 (0.002)	Loss 352.0806 (351.4288)	
2019-01-08 11:15:35,032 - 10 - training_embed.py - training - Epoch: [7][900/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 354.4093 (351.4149)	
2019-01-08 11:15:35,415 - 10 - training_embed.py - training - Epoch: [7][1000/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 353.0771 (351.4228)	
2019-01-08 11:15:35,469 - 10 - training_embed.py - training - loss: 351.352745
2019-01-08 11:15:35,470 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 11:15:36,058 - 10 - training_embed.py - training - Epoch: [8][100/1014]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 354.3592 (351.0666)	
2019-01-08 11:15:36,479 - 10 - training_embed.py - training - Epoch: [8][200/1014]	Time 0.006 (0.004)	Data 0.005 (0.002)	Loss 347.6353 (351.1227)	
2019-01-08 11:15:36,925 - 10 - training_embed.py - training - Epoch: [8][300/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 353.7731 (351.2349)	
2019-01-08 11:15:37,320 - 10 - training_embed.py - training - Epoch: [8][400/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 349.0435 (351.2486)	
2019-01-08 11:15:37,712 - 10 - training_embed.py - training - Epoch: [8][500/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 355.5685 (351.1697)	
2019-01-08 11:15:38,142 - 10 - training_embed.py - training - Epoch: [8][600/1014]	Time 0.009 (0.004)	Data 0.002 (0.002)	Loss 346.4761 (351.1655)	
2019-01-08 11:15:38,567 - 10 - training_embed.py - training - Epoch: [8][700/1014]	Time 0.005 (0.004)	Data 0.003 (0.002)	Loss 348.9854 (351.1985)	
2019-01-08 11:15:38,982 - 10 - training_embed.py - training - Epoch: [8][800/1014]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 348.5049 (351.1519)	
2019-01-08 11:15:39,371 - 10 - training_embed.py - training - Epoch: [8][900/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 348.8956 (351.1138)	
2019-01-08 11:15:39,792 - 10 - training_embed.py - training - Epoch: [8][1000/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 347.9276 (351.0822)	
2019-01-08 11:15:39,833 - 10 - training_embed.py - training - loss: 351.025796
2019-01-08 11:15:39,833 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 11:15:40,478 - 10 - training_embed.py - training - Epoch: [9][100/1014]	Time 0.007 (0.004)	Data 0.001 (0.002)	Loss 351.0828 (351.1955)	
2019-01-08 11:15:40,910 - 10 - training_embed.py - training - Epoch: [9][200/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 353.4655 (350.9887)	
2019-01-08 11:15:41,310 - 10 - training_embed.py - training - Epoch: [9][300/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 351.6501 (351.0717)	
2019-01-08 11:15:41,711 - 10 - training_embed.py - training - Epoch: [9][400/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 348.3563 (351.0522)	
2019-01-08 11:15:42,094 - 10 - training_embed.py - training - Epoch: [9][500/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 347.8424 (351.0176)	
2019-01-08 11:15:42,474 - 10 - training_embed.py - training - Epoch: [9][600/1014]	Time 0.005 (0.004)	Data 0.001 (0.002)	Loss 353.5820 (350.9423)	
2019-01-08 11:15:42,886 - 10 - training_embed.py - training - Epoch: [9][700/1014]	Time 0.006 (0.004)	Data 0.004 (0.002)	Loss 350.1677 (350.8799)	
2019-01-08 11:15:43,277 - 10 - training_embed.py - training - Epoch: [9][800/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 349.7046 (350.7918)	
2019-01-08 11:15:43,650 - 10 - training_embed.py - training - Epoch: [9][900/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 354.7474 (350.7784)	
2019-01-08 11:15:44,018 - 10 - training_embed.py - training - Epoch: [9][1000/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 351.0356 (350.7701)	
2019-01-08 11:15:44,058 - 10 - training_embed.py - training - loss: 350.699497
2019-01-08 11:15:44,058 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 11:15:44,684 - 10 - training_embed.py - training - Epoch: [10][100/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 352.2146 (350.8807)	
2019-01-08 11:15:45,101 - 10 - training_embed.py - training - Epoch: [10][200/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 348.9318 (350.7204)	
2019-01-08 11:15:45,502 - 10 - training_embed.py - training - Epoch: [10][300/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 348.6039 (350.5724)	
2019-01-08 11:15:45,917 - 10 - training_embed.py - training - Epoch: [10][400/1014]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 348.7692 (350.6075)	
2019-01-08 11:15:46,329 - 10 - training_embed.py - training - Epoch: [10][500/1014]	Time 0.007 (0.004)	Data 0.006 (0.002)	Loss 348.8529 (350.5411)	
2019-01-08 11:15:46,721 - 10 - training_embed.py - training - Epoch: [10][600/1014]	Time 0.006 (0.004)	Data 0.004 (0.002)	Loss 353.1646 (350.4718)	
2019-01-08 11:15:47,130 - 10 - training_embed.py - training - Epoch: [10][700/1014]	Time 0.005 (0.004)	Data 0.004 (0.002)	Loss 352.9360 (350.4706)	
2019-01-08 11:15:47,510 - 10 - training_embed.py - training - Epoch: [10][800/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 352.5673 (350.4480)	
2019-01-08 11:15:47,878 - 10 - training_embed.py - training - Epoch: [10][900/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 347.4828 (350.4505)	
2019-01-08 11:15:48,270 - 10 - training_embed.py - training - Epoch: [10][1000/1014]	Time 0.007 (0.004)	Data 0.005 (0.002)	Loss 353.7578 (350.4408)	
2019-01-08 11:15:48,313 - 10 - training_embed.py - training - loss: 350.370462
2019-01-08 11:15:48,314 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 11:15:48,907 - 10 - training_embed.py - training - Epoch: [11][100/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 349.4075 (350.1925)	
2019-01-08 11:15:49,328 - 10 - training_embed.py - training - Epoch: [11][200/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 348.9367 (350.0421)	
2019-01-08 11:15:49,760 - 10 - training_embed.py - training - Epoch: [11][300/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 346.4693 (349.9252)	
2019-01-08 11:15:50,160 - 10 - training_embed.py - training - Epoch: [11][400/1014]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 349.9972 (350.0633)	
2019-01-08 11:15:50,537 - 10 - training_embed.py - training - Epoch: [11][500/1014]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 349.3496 (350.0851)	
2019-01-08 11:15:50,930 - 10 - training_embed.py - training - Epoch: [11][600/1014]	Time 0.008 (0.004)	Data 0.001 (0.002)	Loss 350.5696 (350.1405)	
2019-01-08 11:15:51,317 - 10 - training_embed.py - training - Epoch: [11][700/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 348.0838 (350.1333)	
2019-01-08 11:15:51,744 - 10 - training_embed.py - training - Epoch: [11][800/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 350.8091 (350.1232)	
2019-01-08 11:15:52,133 - 10 - training_embed.py - training - Epoch: [11][900/1014]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 354.9825 (350.1436)	
2019-01-08 11:15:52,545 - 10 - training_embed.py - training - Epoch: [11][1000/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 346.4144 (350.1322)	
2019-01-08 11:15:52,587 - 10 - training_embed.py - training - loss: 350.041370
2019-01-08 11:15:52,675 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 11:15:53,351 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 675.592 ms ~ 0.011 min ~ 0.676 sec
2019-01-08 11:15:53,975 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1300.077 ms ~ 0.022 min ~ 1.300 sec
2019-01-08 11:15:53,976 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 11:15:53,976 - 10 - corpus.py - subactivity_sampler - 0 / 169
2019-01-08 11:15:53,977 - 10 - corpus.py - subactivity_sampler - [27606. 49576. 10175. 43540. 25864. 18019. 77633.  7082.]
2019-01-08 11:16:22,921 - 10 - corpus.py - subactivity_sampler - 20 / 169
2019-01-08 11:16:22,921 - 10 - corpus.py - subactivity_sampler - [27504. 49899.  9490. 43898. 25157. 17566. 79645.  6336.]
2019-01-08 11:16:40,962 - 10 - corpus.py - subactivity_sampler - 40 / 169
2019-01-08 11:16:40,962 - 10 - corpus.py - subactivity_sampler - [27429. 49806.  9305. 44335. 24795. 17100. 80762.  5963.]
2019-01-08 11:17:04,852 - 10 - corpus.py - subactivity_sampler - 60 / 169
2019-01-08 11:17:04,852 - 10 - corpus.py - subactivity_sampler - [27328. 50294.  8664. 44849. 24589. 16626. 81561.  5584.]
2019-01-08 11:17:28,858 - 10 - corpus.py - subactivity_sampler - 80 / 169
2019-01-08 11:17:28,859 - 10 - corpus.py - subactivity_sampler - [27292. 50476.  8266. 45318. 24477. 16154. 81947.  5565.]
2019-01-08 11:17:49,184 - 10 - corpus.py - subactivity_sampler - 100 / 169
2019-01-08 11:17:49,184 - 10 - corpus.py - subactivity_sampler - [27086. 50643.  8001. 45712. 23979. 15511. 83142.  5421.]
2019-01-08 11:18:09,060 - 10 - corpus.py - subactivity_sampler - 120 / 169
2019-01-08 11:18:09,060 - 10 - corpus.py - subactivity_sampler - [26978. 51170.  7948. 45659. 23649. 15025. 83725.  5341.]
2019-01-08 11:18:30,677 - 10 - corpus.py - subactivity_sampler - 140 / 169
2019-01-08 11:18:30,677 - 10 - corpus.py - subactivity_sampler - [26959. 51329.  7700. 46125. 23300. 14540. 84342.  5200.]
2019-01-08 11:18:53,209 - 10 - corpus.py - subactivity_sampler - 160 / 169
2019-01-08 11:18:53,210 - 10 - corpus.py - subactivity_sampler - [26898. 51363.  7450. 47265. 22579. 13785. 85181.  4974.]
2019-01-08 11:19:00,161 - 10 - corpus.py - subactivity_sampler - [26810. 51552.  7322. 47493. 22198. 13413. 85771.  4936.]
2019-01-08 11:19:00,162 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 186186.083 ms ~ 3.103 min ~ 186.186 sec
2019-01-08 11:19:00,162 - 10 - corpus.py - ordering_sampler - .
2019-01-08 11:19:02,053 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 11:19:02,054 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 4.  2.  0.  9. 30.  0.  2.]
2019-01-08 11:19:02,054 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 11:19:02,091 - 10 - corpus.py - rho_sampling - ['59.7804', '25.9632', '928.2654', '73.0454', '11.0603', '162.2552', '3.9156']
2019-01-08 11:19:02,092 - 10 - pipeline.py - baseline - Iteration 4
2019-01-08 11:19:02,194 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 11:19:02,200 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 11:19:02,200 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 18', '1: 36', '2: 14', '3: 37', '4: 38', '5: 41', '6: 39', '7: 40']
2019-01-08 11:19:02,211 - 10 - accuracy_class.py - mof_val - frames true: 125317	frames overall : 259495
2019-01-08 11:19:02,212 - 10 - corpus.py - accuracy_corpus - Action: sandwich
2019-01-08 11:19:02,212 - 10 - corpus.py - accuracy_corpus - MoF val: 0.48292645330353184
2019-01-08 11:19:02,212 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.48292645330353184
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - mof_classes - label 14: 0.043367  85 / 1960
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - mof_classes - label 18: 0.472674  1920 / 4062
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - mof_classes - label 36: 0.661941  30352 / 45853
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - mof_classes - label 37: 0.387392  38291 / 98843
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - mof_classes - label 38: 0.107580  765 / 7111
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - mof_classes - label 39: 0.825725  53431 / 64708
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - mof_classes - label 40: 0.090164  473 / 5246
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - mof_classes - label 41: 0.000000  0 / 970
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - mof_classes - average class mof: 0.287649
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - iou_classes - label 14: 0.009242  85 / 9197
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - iou_classes - label 18: 0.066317  1920 / 28952
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - iou_classes - label 36: 0.452657  30352 / 67053
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - iou_classes - label 37: 0.354399  38291 / 108045
2019-01-08 11:19:02,212 - 10 - accuracy_class.py - iou_classes - label 38: 0.026801  765 / 28544
2019-01-08 11:19:02,213 - 10 - accuracy_class.py - iou_classes - label 39: 0.550563  53431 / 97048
2019-01-08 11:19:02,213 - 10 - accuracy_class.py - iou_classes - label 40: 0.048718  473 / 9709
2019-01-08 11:19:02,213 - 10 - accuracy_class.py - iou_classes - label 41: 0.000000  0 / 14383
2019-01-08 11:19:02,213 - 10 - accuracy_class.py - iou_classes - average IoU: 0.188587
2019-01-08 11:19:02,213 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.167633
2019-01-08 11:19:06,049 - 10 - f1_score.py - f1 - f1 score: 0.323777
2019-01-08 11:19:06,060 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3968.633 ms ~ 0.066 min ~ 3.969 sec
2019-01-08 11:19:06,061 - 10 - corpus.py - embedding_training - .
2019-01-08 11:19:06,061 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 11:19:06,061 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 11:19:06,061 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 11:19:08,658 - 10 - training_embed.py - training - create model
2019-01-08 11:19:08,667 - 10 - training_embed.py - training - epochs: 12
2019-01-08 11:19:08,667 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 11:19:09,235 - 10 - training_embed.py - training - Epoch: [0][100/1014]	Time 0.007 (0.006)	Data 0.006 (0.003)	Loss 349.2539 (352.5821)	
2019-01-08 11:19:09,622 - 10 - training_embed.py - training - Epoch: [0][200/1014]	Time 0.005 (0.005)	Data 0.003 (0.003)	Loss 349.2130 (352.5266)	
2019-01-08 11:19:10,000 - 10 - training_embed.py - training - Epoch: [0][300/1014]	Time 0.002 (0.004)	Data 0.001 (0.003)	Loss 353.5963 (352.4406)	
2019-01-08 11:19:10,405 - 10 - training_embed.py - training - Epoch: [0][400/1014]	Time 0.005 (0.004)	Data 0.002 (0.002)	Loss 354.6586 (352.4855)	
2019-01-08 11:19:10,792 - 10 - training_embed.py - training - Epoch: [0][500/1014]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 354.1293 (352.4171)	
2019-01-08 11:19:11,182 - 10 - training_embed.py - training - Epoch: [0][600/1014]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 350.9485 (352.4048)	
2019-01-08 11:19:11,531 - 10 - training_embed.py - training - Epoch: [0][700/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 351.7345 (352.3317)	
2019-01-08 11:19:11,924 - 10 - training_embed.py - training - Epoch: [0][800/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 347.2658 (352.3035)	
2019-01-08 11:19:12,302 - 10 - training_embed.py - training - Epoch: [0][900/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 356.4485 (352.2497)	
2019-01-08 11:19:12,673 - 10 - training_embed.py - training - Epoch: [0][1000/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 353.1003 (352.1729)	
2019-01-08 11:19:12,715 - 10 - training_embed.py - training - loss: 352.095759
2019-01-08 11:19:12,715 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 11:19:13,343 - 10 - training_embed.py - training - Epoch: [1][100/1014]	Time 0.006 (0.004)	Data 0.004 (0.002)	Loss 356.8098 (351.9793)	
2019-01-08 11:19:13,727 - 10 - training_embed.py - training - Epoch: [1][200/1014]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 353.7300 (351.7384)	
2019-01-08 11:19:14,075 - 10 - training_embed.py - training - Epoch: [1][300/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 349.8357 (351.7333)	
2019-01-08 11:19:14,463 - 10 - training_embed.py - training - Epoch: [1][400/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 348.0884 (351.7771)	
2019-01-08 11:19:14,843 - 10 - training_embed.py - training - Epoch: [1][500/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 349.0838 (351.8292)	
2019-01-08 11:19:15,354 - 10 - training_embed.py - training - Epoch: [1][600/1014]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 353.4926 (351.8374)	
2019-01-08 11:19:15,778 - 10 - training_embed.py - training - Epoch: [1][700/1014]	Time 0.004 (0.004)	Data 0.001 (0.002)	Loss 352.6357 (351.8387)	
2019-01-08 11:19:16,053 - 10 - training_embed.py - training - Epoch: [1][800/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 349.2843 (351.8489)	
2019-01-08 11:19:16,297 - 10 - training_embed.py - training - Epoch: [1][900/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 353.8600 (351.7595)	
2019-01-08 11:19:16,531 - 10 - training_embed.py - training - Epoch: [1][1000/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 355.8324 (351.7789)	
2019-01-08 11:19:16,565 - 10 - training_embed.py - training - loss: 351.694381
2019-01-08 11:19:16,566 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 11:19:16,952 - 10 - training_embed.py - training - Epoch: [2][100/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 349.9100 (351.6166)	
2019-01-08 11:19:17,176 - 10 - training_embed.py - training - Epoch: [2][200/1014]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 349.8388 (351.3864)	
2019-01-08 11:19:17,393 - 10 - training_embed.py - training - Epoch: [2][300/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 349.9754 (351.4084)	
2019-01-08 11:19:17,633 - 10 - training_embed.py - training - Epoch: [2][400/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 350.0376 (351.4514)	
2019-01-08 11:19:17,866 - 10 - training_embed.py - training - Epoch: [2][500/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 353.6159 (351.3925)	
2019-01-08 11:19:18,107 - 10 - training_embed.py - training - Epoch: [2][600/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 348.8058 (351.3992)	
2019-01-08 11:19:18,352 - 10 - training_embed.py - training - Epoch: [2][700/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 351.5916 (351.3620)	
2019-01-08 11:19:18,596 - 10 - training_embed.py - training - Epoch: [2][800/1014]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 351.6112 (351.3253)	
2019-01-08 11:19:18,893 - 10 - training_embed.py - training - Epoch: [2][900/1014]	Time 0.005 (0.003)	Data 0.004 (0.002)	Loss 353.1627 (351.3947)	
2019-01-08 11:19:19,185 - 10 - training_embed.py - training - Epoch: [2][1000/1014]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 351.0430 (351.3844)	
2019-01-08 11:19:19,223 - 10 - training_embed.py - training - loss: 351.292240
2019-01-08 11:19:19,224 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 11:19:19,598 - 10 - training_embed.py - training - Epoch: [3][100/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 348.8803 (350.9982)	
2019-01-08 11:19:19,833 - 10 - training_embed.py - training - Epoch: [3][200/1014]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 349.9863 (351.0720)	
2019-01-08 11:19:20,068 - 10 - training_embed.py - training - Epoch: [3][300/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 353.9763 (351.0774)	
2019-01-08 11:19:20,380 - 10 - training_embed.py - training - Epoch: [3][400/1014]	Time 0.009 (0.003)	Data 0.001 (0.002)	Loss 348.3470 (351.1192)	
2019-01-08 11:19:20,615 - 10 - training_embed.py - training - Epoch: [3][500/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 349.7239 (351.1242)	
2019-01-08 11:19:20,846 - 10 - training_embed.py - training - Epoch: [3][600/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 350.1104 (351.0527)	
2019-01-08 11:19:21,071 - 10 - training_embed.py - training - Epoch: [3][700/1014]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 351.2162 (351.0168)	
2019-01-08 11:19:21,295 - 10 - training_embed.py - training - Epoch: [3][800/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 349.7603 (350.9703)	
2019-01-08 11:19:21,536 - 10 - training_embed.py - training - Epoch: [3][900/1014]	Time 0.001 (0.003)	Data 0.000 (0.002)	Loss 348.5240 (350.9877)	
2019-01-08 11:19:21,771 - 10 - training_embed.py - training - Epoch: [3][1000/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 351.8706 (350.9766)	
2019-01-08 11:19:21,798 - 10 - training_embed.py - training - loss: 350.890021
2019-01-08 11:19:21,798 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 11:19:22,187 - 10 - training_embed.py - training - Epoch: [4][100/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 348.5926 (350.3291)	
2019-01-08 11:19:22,424 - 10 - training_embed.py - training - Epoch: [4][200/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 351.9951 (350.5357)	
2019-01-08 11:19:22,657 - 10 - training_embed.py - training - Epoch: [4][300/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 353.4950 (350.6437)	
2019-01-08 11:19:22,890 - 10 - training_embed.py - training - Epoch: [4][400/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 355.5450 (350.7076)	
2019-01-08 11:19:23,121 - 10 - training_embed.py - training - Epoch: [4][500/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 351.5797 (350.6425)	
2019-01-08 11:19:23,351 - 10 - training_embed.py - training - Epoch: [4][600/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 345.3882 (350.6359)	
2019-01-08 11:19:23,587 - 10 - training_embed.py - training - Epoch: [4][700/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 352.4144 (350.6300)	
2019-01-08 11:19:23,823 - 10 - training_embed.py - training - Epoch: [4][800/1014]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 348.3507 (350.5971)	
2019-01-08 11:19:24,054 - 10 - training_embed.py - training - Epoch: [4][900/1014]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 351.1413 (350.6191)	
2019-01-08 11:19:24,265 - 10 - training_embed.py - training - Epoch: [4][1000/1014]	Time 0.004 (0.003)	Data 0.003 (0.002)	Loss 356.1091 (350.5786)	
2019-01-08 11:19:24,294 - 10 - training_embed.py - training - loss: 350.485143
2019-01-08 11:19:24,294 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 11:19:24,674 - 10 - training_embed.py - training - Epoch: [5][100/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 348.3456 (349.9317)	
2019-01-08 11:19:24,882 - 10 - training_embed.py - training - Epoch: [5][200/1014]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 350.5052 (350.0272)	
2019-01-08 11:19:25,109 - 10 - training_embed.py - training - Epoch: [5][300/1014]	Time 0.003 (0.003)	Data 0.002 (0.002)	Loss 350.9819 (349.9942)	
2019-01-08 11:19:25,336 - 10 - training_embed.py - training - Epoch: [5][400/1014]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 346.2768 (350.1809)	
2019-01-08 11:19:25,575 - 10 - training_embed.py - training - Epoch: [5][500/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 349.7069 (350.2787)	
2019-01-08 11:19:25,813 - 10 - training_embed.py - training - Epoch: [5][600/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 346.5917 (350.2553)	
2019-01-08 11:19:26,047 - 10 - training_embed.py - training - Epoch: [5][700/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 345.1023 (350.2135)	
2019-01-08 11:19:26,270 - 10 - training_embed.py - training - Epoch: [5][800/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 352.2749 (350.1459)	
2019-01-08 11:19:26,510 - 10 - training_embed.py - training - Epoch: [5][900/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 346.1177 (350.1568)	
2019-01-08 11:19:26,741 - 10 - training_embed.py - training - Epoch: [5][1000/1014]	Time 0.004 (0.003)	Data 0.001 (0.002)	Loss 347.4357 (350.1578)	
2019-01-08 11:19:26,770 - 10 - training_embed.py - training - loss: 350.079592
2019-01-08 11:19:26,770 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 11:19:27,146 - 10 - training_embed.py - training - Epoch: [6][100/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 355.0126 (349.8545)	
2019-01-08 11:19:27,381 - 10 - training_embed.py - training - Epoch: [6][200/1014]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 348.9229 (349.8241)	
2019-01-08 11:19:27,616 - 10 - training_embed.py - training - Epoch: [6][300/1014]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 350.2367 (349.9193)	
2019-01-08 11:19:27,854 - 10 - training_embed.py - training - Epoch: [6][400/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 350.9203 (349.7858)	
2019-01-08 11:19:28,083 - 10 - training_embed.py - training - Epoch: [6][500/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 349.2301 (349.9088)	
2019-01-08 11:19:28,322 - 10 - training_embed.py - training - Epoch: [6][600/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 348.4684 (349.9300)	
2019-01-08 11:19:28,563 - 10 - training_embed.py - training - Epoch: [6][700/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 345.3957 (349.8991)	
2019-01-08 11:19:28,803 - 10 - training_embed.py - training - Epoch: [6][800/1014]	Time 0.004 (0.003)	Data 0.001 (0.002)	Loss 351.2229 (349.7976)	
2019-01-08 11:19:29,004 - 10 - training_embed.py - training - Epoch: [6][900/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.6741 (349.7802)	
2019-01-08 11:19:29,244 - 10 - training_embed.py - training - Epoch: [6][1000/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.1559 (349.7658)	
2019-01-08 11:19:29,270 - 10 - training_embed.py - training - loss: 349.670012
2019-01-08 11:19:29,271 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 11:19:29,600 - 10 - training_embed.py - training - Epoch: [7][100/1014]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 351.2535 (349.7226)	
2019-01-08 11:19:29,831 - 10 - training_embed.py - training - Epoch: [7][200/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.3928 (349.6203)	
2019-01-08 11:19:30,059 - 10 - training_embed.py - training - Epoch: [7][300/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.7443 (349.5688)	
2019-01-08 11:19:30,299 - 10 - training_embed.py - training - Epoch: [7][400/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.3857 (349.4733)	
2019-01-08 11:19:30,529 - 10 - training_embed.py - training - Epoch: [7][500/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.5218 (349.4275)	
2019-01-08 11:19:30,744 - 10 - training_embed.py - training - Epoch: [7][600/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.0886 (349.4446)	
2019-01-08 11:19:30,975 - 10 - training_embed.py - training - Epoch: [7][700/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 347.8459 (349.3939)	
2019-01-08 11:19:31,196 - 10 - training_embed.py - training - Epoch: [7][800/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.3263 (349.3614)	
2019-01-08 11:19:31,427 - 10 - training_embed.py - training - Epoch: [7][900/1014]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 350.7590 (349.3272)	
2019-01-08 11:19:31,634 - 10 - training_embed.py - training - Epoch: [7][1000/1014]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 350.5627 (349.3350)	
2019-01-08 11:19:31,666 - 10 - training_embed.py - training - loss: 349.261796
2019-01-08 11:19:31,666 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 11:19:32,046 - 10 - training_embed.py - training - Epoch: [8][100/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.0902 (349.0625)	
2019-01-08 11:19:32,268 - 10 - training_embed.py - training - Epoch: [8][200/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 345.9079 (349.0339)	
2019-01-08 11:19:32,491 - 10 - training_embed.py - training - Epoch: [8][300/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.6476 (349.0871)	
2019-01-08 11:19:32,706 - 10 - training_embed.py - training - Epoch: [8][400/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 348.2932 (349.1022)	
2019-01-08 11:19:32,949 - 10 - training_embed.py - training - Epoch: [8][500/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.3867 (349.0431)	
2019-01-08 11:19:33,170 - 10 - training_embed.py - training - Epoch: [8][600/1014]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 344.5009 (348.9919)	
2019-01-08 11:19:33,405 - 10 - training_embed.py - training - Epoch: [8][700/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.1224 (349.0010)	
2019-01-08 11:19:33,645 - 10 - training_embed.py - training - Epoch: [8][800/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 346.7977 (348.9795)	
2019-01-08 11:19:33,878 - 10 - training_embed.py - training - Epoch: [8][900/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.1880 (348.9455)	
2019-01-08 11:19:34,119 - 10 - training_embed.py - training - Epoch: [8][1000/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 344.1186 (348.9083)	
2019-01-08 11:19:34,146 - 10 - training_embed.py - training - loss: 348.850017
2019-01-08 11:19:34,146 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 11:19:34,536 - 10 - training_embed.py - training - Epoch: [9][100/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.6918 (348.9469)	
2019-01-08 11:19:34,740 - 10 - training_embed.py - training - Epoch: [9][200/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.7583 (348.8181)	
2019-01-08 11:19:34,976 - 10 - training_embed.py - training - Epoch: [9][300/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.6248 (348.8661)	
2019-01-08 11:19:35,200 - 10 - training_embed.py - training - Epoch: [9][400/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 347.1727 (348.8564)	
2019-01-08 11:19:35,426 - 10 - training_embed.py - training - Epoch: [9][500/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 345.7136 (348.7999)	
2019-01-08 11:19:35,663 - 10 - training_embed.py - training - Epoch: [9][600/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 352.0195 (348.7092)	
2019-01-08 11:19:35,906 - 10 - training_embed.py - training - Epoch: [9][700/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 346.6064 (348.6569)	
2019-01-08 11:19:36,145 - 10 - training_embed.py - training - Epoch: [9][800/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.8660 (348.5510)	
2019-01-08 11:19:36,386 - 10 - training_embed.py - training - Epoch: [9][900/1014]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 353.8513 (348.5424)	
2019-01-08 11:19:36,656 - 10 - training_embed.py - training - Epoch: [9][1000/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 348.2175 (348.5120)	
2019-01-08 11:19:36,691 - 10 - training_embed.py - training - loss: 348.439137
2019-01-08 11:19:36,691 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 11:19:37,087 - 10 - training_embed.py - training - Epoch: [10][100/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.3426 (348.6095)	
2019-01-08 11:19:37,314 - 10 - training_embed.py - training - Epoch: [10][200/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 343.4195 (348.4275)	
2019-01-08 11:19:37,549 - 10 - training_embed.py - training - Epoch: [10][300/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.5953 (348.2568)	
2019-01-08 11:19:37,772 - 10 - training_embed.py - training - Epoch: [10][400/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 344.9345 (348.2703)	
2019-01-08 11:19:38,002 - 10 - training_embed.py - training - Epoch: [10][500/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 345.0984 (348.2434)	
2019-01-08 11:19:38,236 - 10 - training_embed.py - training - Epoch: [10][600/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 350.4886 (348.1670)	
2019-01-08 11:19:38,472 - 10 - training_embed.py - training - Epoch: [10][700/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.1155 (348.1730)	
2019-01-08 11:19:38,703 - 10 - training_embed.py - training - Epoch: [10][800/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.3871 (348.1301)	
2019-01-08 11:19:38,938 - 10 - training_embed.py - training - Epoch: [10][900/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 344.7172 (348.1275)	
2019-01-08 11:19:39,179 - 10 - training_embed.py - training - Epoch: [10][1000/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 350.2833 (348.1031)	
2019-01-08 11:19:39,206 - 10 - training_embed.py - training - loss: 348.023809
2019-01-08 11:19:39,207 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 11:19:39,585 - 10 - training_embed.py - training - Epoch: [11][100/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 345.8416 (347.6625)	
2019-01-08 11:19:39,817 - 10 - training_embed.py - training - Epoch: [11][200/1014]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 348.6991 (347.6028)	
2019-01-08 11:19:40,059 - 10 - training_embed.py - training - Epoch: [11][300/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 343.9971 (347.4810)	
2019-01-08 11:19:40,293 - 10 - training_embed.py - training - Epoch: [11][400/1014]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 347.1046 (347.6032)	
2019-01-08 11:19:40,535 - 10 - training_embed.py - training - Epoch: [11][500/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 346.0583 (347.6199)	
2019-01-08 11:19:40,777 - 10 - training_embed.py - training - Epoch: [11][600/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.6548 (347.6542)	
2019-01-08 11:19:41,025 - 10 - training_embed.py - training - Epoch: [11][700/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 347.1327 (347.6617)	
2019-01-08 11:19:41,251 - 10 - training_embed.py - training - Epoch: [11][800/1014]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 348.2478 (347.6746)	
2019-01-08 11:19:41,495 - 10 - training_embed.py - training - Epoch: [11][900/1014]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 353.5994 (347.6986)	
2019-01-08 11:19:41,722 - 10 - training_embed.py - training - Epoch: [11][1000/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 343.5794 (347.6954)	
2019-01-08 11:19:41,749 - 10 - training_embed.py - training - loss: 347.607229
2019-01-08 11:19:41,802 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 11:19:42,284 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 481.858 ms ~ 0.008 min ~ 0.482 sec
2019-01-08 11:19:42,808 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1005.957 ms ~ 0.017 min ~ 1.006 sec
2019-01-08 11:19:42,808 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 11:19:42,808 - 10 - corpus.py - subactivity_sampler - 0 / 169
2019-01-08 11:19:42,809 - 10 - corpus.py - subactivity_sampler - [26810. 51552.  7322. 47493. 22198. 13413. 85771.  4936.]
2019-01-08 11:20:06,964 - 10 - corpus.py - subactivity_sampler - 20 / 169
2019-01-08 11:20:06,964 - 10 - corpus.py - subactivity_sampler - [26648. 51809.  6983. 47594. 21897. 13117. 86656.  4791.]
2019-01-08 11:20:22,790 - 10 - corpus.py - subactivity_sampler - 40 / 169
2019-01-08 11:20:22,791 - 10 - corpus.py - subactivity_sampler - [26502. 51919.  6907. 47729. 21812. 12809. 87260.  4557.]
2019-01-08 11:20:44,738 - 10 - corpus.py - subactivity_sampler - 60 / 169
2019-01-08 11:20:44,738 - 10 - corpus.py - subactivity_sampler - [26507. 52327.  6383. 48196. 21598. 12381. 87658.  4445.]
2019-01-08 11:21:05,106 - 10 - corpus.py - subactivity_sampler - 80 / 169
2019-01-08 11:21:05,107 - 10 - corpus.py - subactivity_sampler - [26478. 52659.  5896. 48490. 21399. 12037. 88107.  4429.]
2019-01-08 11:21:22,861 - 10 - corpus.py - subactivity_sampler - 100 / 169
2019-01-08 11:21:22,861 - 10 - corpus.py - subactivity_sampler - [26371. 52810.  5695. 49121. 21090. 11351. 88735.  4322.]
2019-01-08 11:21:39,912 - 10 - corpus.py - subactivity_sampler - 120 / 169
2019-01-08 11:21:39,913 - 10 - corpus.py - subactivity_sampler - [26296. 53113.  5615. 49153. 20831. 11069. 89109.  4309.]
2019-01-08 11:22:03,779 - 10 - corpus.py - subactivity_sampler - 140 / 169
2019-01-08 11:22:03,780 - 10 - corpus.py - subactivity_sampler - [26342. 53572.  5467. 49397. 20363. 10645. 89459.  4250.]
2019-01-08 11:22:22,939 - 10 - corpus.py - subactivity_sampler - 160 / 169
2019-01-08 11:22:22,939 - 10 - corpus.py - subactivity_sampler - [26304. 53609.  5345. 50544. 19973.  9823. 89634.  4263.]
2019-01-08 11:22:29,642 - 10 - corpus.py - subactivity_sampler - [26283. 53871.  5329. 50220. 19868.  9682. 89979.  4263.]
2019-01-08 11:22:29,642 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 166834.060 ms ~ 2.781 min ~ 166.834 sec
2019-01-08 11:22:29,642 - 10 - corpus.py - ordering_sampler - .
2019-01-08 11:22:31,934 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 11:22:31,934 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 4.  7.  0.  8. 26.  0.  3.]
2019-01-08 11:22:31,934 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 11:22:31,982 - 10 - corpus.py - rho_sampling - ['58.0067', '10.5144', '928.1292', '71.5524', '3.3860', '40.0092', '581.3417']
2019-01-08 11:22:31,982 - 10 - pipeline.py - baseline - Iteration 5
2019-01-08 11:22:32,122 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 11:22:32,131 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 11:22:32,131 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 18', '1: 36', '2: 14', '3: 37', '4: 38', '5: 41', '6: 39', '7: 40']
2019-01-08 11:22:32,160 - 10 - accuracy_class.py - mof_val - frames true: 128214	frames overall : 259495
2019-01-08 11:22:32,160 - 10 - corpus.py - accuracy_corpus - Action: sandwich
2019-01-08 11:22:32,160 - 10 - corpus.py - accuracy_corpus - MoF val: 0.4940904449025993
2019-01-08 11:22:32,160 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.4940904449025993
2019-01-08 11:22:32,160 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:22:32,160 - 10 - accuracy_class.py - mof_classes - label 14: 0.044898  88 / 1960
2019-01-08 11:22:32,160 - 10 - accuracy_class.py - mof_classes - label 18: 0.470458  1911 / 4062
2019-01-08 11:22:32,160 - 10 - accuracy_class.py - mof_classes - label 36: 0.663163  30408 / 45853
2019-01-08 11:22:32,160 - 10 - accuracy_class.py - mof_classes - label 37: 0.408395  40367 / 98843
2019-01-08 11:22:32,160 - 10 - accuracy_class.py - mof_classes - label 38: 0.076642  545 / 7111
2019-01-08 11:22:32,160 - 10 - accuracy_class.py - mof_classes - label 39: 0.840885  54412 / 64708
2019-01-08 11:22:32,161 - 10 - accuracy_class.py - mof_classes - label 40: 0.092070  483 / 5246
2019-01-08 11:22:32,161 - 10 - accuracy_class.py - mof_classes - label 41: 0.000000  0 / 970
2019-01-08 11:22:32,161 - 10 - accuracy_class.py - mof_classes - average class mof: 0.288501
2019-01-08 11:22:32,161 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:22:32,161 - 10 - accuracy_class.py - iou_classes - label 14: 0.012221  88 / 7201
2019-01-08 11:22:32,161 - 10 - accuracy_class.py - iou_classes - label 18: 0.067208  1911 / 28434
2019-01-08 11:22:32,161 - 10 - accuracy_class.py - iou_classes - label 36: 0.438687  30408 / 69316
2019-01-08 11:22:32,161 - 10 - accuracy_class.py - iou_classes - label 37: 0.371375  40367 / 108696
2019-01-08 11:22:32,161 - 10 - accuracy_class.py - iou_classes - label 38: 0.020617  545 / 26434
2019-01-08 11:22:32,161 - 10 - accuracy_class.py - iou_classes - label 39: 0.542628  54412 / 100275
2019-01-08 11:22:32,161 - 10 - accuracy_class.py - iou_classes - label 40: 0.053512  483 / 9026
2019-01-08 11:22:32,161 - 10 - accuracy_class.py - iou_classes - label 41: 0.000000  0 / 10652
2019-01-08 11:22:32,161 - 10 - accuracy_class.py - iou_classes - average IoU: 0.188281
2019-01-08 11:22:32,162 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.167361
2019-01-08 11:22:36,430 - 10 - f1_score.py - f1 - f1 score: 0.328179
2019-01-08 11:22:36,440 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 4457.583 ms ~ 0.074 min ~ 4.458 sec
2019-01-08 11:22:36,440 - 10 - corpus.py - embedding_training - .
2019-01-08 11:22:36,440 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 11:22:36,440 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 11:22:36,440 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 11:22:38,667 - 10 - training_embed.py - training - create model
2019-01-08 11:22:38,674 - 10 - training_embed.py - training - epochs: 12
2019-01-08 11:22:38,674 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 11:22:39,284 - 10 - training_embed.py - training - Epoch: [0][100/1014]	Time 0.005 (0.006)	Data 0.003 (0.004)	Loss 349.2037 (351.6470)	
2019-01-08 11:22:39,785 - 10 - training_embed.py - training - Epoch: [0][200/1014]	Time 0.004 (0.006)	Data 0.001 (0.003)	Loss 348.6647 (351.5967)	
2019-01-08 11:22:40,256 - 10 - training_embed.py - training - Epoch: [0][300/1014]	Time 0.003 (0.005)	Data 0.002 (0.003)	Loss 353.2507 (351.5479)	
2019-01-08 11:22:40,725 - 10 - training_embed.py - training - Epoch: [0][400/1014]	Time 0.005 (0.005)	Data 0.004 (0.002)	Loss 354.2987 (351.5876)	
2019-01-08 11:22:41,185 - 10 - training_embed.py - training - Epoch: [0][500/1014]	Time 0.010 (0.005)	Data 0.001 (0.003)	Loss 351.5084 (351.5255)	
2019-01-08 11:22:41,632 - 10 - training_embed.py - training - Epoch: [0][600/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 348.5301 (351.5253)	
2019-01-08 11:22:42,116 - 10 - training_embed.py - training - Epoch: [0][700/1014]	Time 0.001 (0.005)	Data 0.000 (0.003)	Loss 351.1299 (351.4571)	
2019-01-08 11:22:42,559 - 10 - training_embed.py - training - Epoch: [0][800/1014]	Time 0.001 (0.005)	Data 0.000 (0.003)	Loss 347.1760 (351.4109)	
2019-01-08 11:22:42,982 - 10 - training_embed.py - training - Epoch: [0][900/1014]	Time 0.004 (0.005)	Data 0.000 (0.003)	Loss 355.7232 (351.3419)	
2019-01-08 11:22:43,410 - 10 - training_embed.py - training - Epoch: [0][1000/1014]	Time 0.007 (0.005)	Data 0.006 (0.003)	Loss 351.5423 (351.2778)	
2019-01-08 11:22:43,452 - 10 - training_embed.py - training - loss: 351.201516
2019-01-08 11:22:43,453 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 11:22:44,129 - 10 - training_embed.py - training - Epoch: [1][100/1014]	Time 0.003 (0.005)	Data 0.002 (0.003)	Loss 355.7786 (351.2242)	
2019-01-08 11:22:44,572 - 10 - training_embed.py - training - Epoch: [1][200/1014]	Time 0.004 (0.005)	Data 0.003 (0.003)	Loss 353.0107 (350.8778)	
2019-01-08 11:22:45,020 - 10 - training_embed.py - training - Epoch: [1][300/1014]	Time 0.001 (0.005)	Data 0.000 (0.003)	Loss 349.4134 (350.8466)	
2019-01-08 11:22:45,464 - 10 - training_embed.py - training - Epoch: [1][400/1014]	Time 0.002 (0.005)	Data 0.000 (0.002)	Loss 346.9617 (350.8651)	
2019-01-08 11:22:45,927 - 10 - training_embed.py - training - Epoch: [1][500/1014]	Time 0.004 (0.005)	Data 0.000 (0.002)	Loss 348.7823 (350.9001)	
2019-01-08 11:22:46,418 - 10 - training_embed.py - training - Epoch: [1][600/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 351.4157 (350.9099)	
2019-01-08 11:22:46,873 - 10 - training_embed.py - training - Epoch: [1][700/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 352.1530 (350.8888)	
2019-01-08 11:22:47,333 - 10 - training_embed.py - training - Epoch: [1][800/1014]	Time 0.008 (0.005)	Data 0.001 (0.002)	Loss 346.9638 (350.8994)	
2019-01-08 11:22:47,763 - 10 - training_embed.py - training - Epoch: [1][900/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 352.0697 (350.8148)	
2019-01-08 11:22:48,231 - 10 - training_embed.py - training - Epoch: [1][1000/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 352.3740 (350.8262)	
2019-01-08 11:22:48,285 - 10 - training_embed.py - training - loss: 350.743465
2019-01-08 11:22:48,285 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 11:22:48,871 - 10 - training_embed.py - training - Epoch: [2][100/1014]	Time 0.009 (0.005)	Data 0.005 (0.002)	Loss 348.3381 (350.5602)	
2019-01-08 11:22:49,314 - 10 - training_embed.py - training - Epoch: [2][200/1014]	Time 0.006 (0.005)	Data 0.003 (0.002)	Loss 347.3693 (350.3823)	
2019-01-08 11:22:49,755 - 10 - training_embed.py - training - Epoch: [2][300/1014]	Time 0.005 (0.005)	Data 0.002 (0.002)	Loss 349.0659 (350.3750)	
2019-01-08 11:22:50,215 - 10 - training_embed.py - training - Epoch: [2][400/1014]	Time 0.009 (0.005)	Data 0.007 (0.002)	Loss 349.8921 (350.4503)	
2019-01-08 11:22:50,678 - 10 - training_embed.py - training - Epoch: [2][500/1014]	Time 0.004 (0.005)	Data 0.003 (0.002)	Loss 351.0764 (350.4070)	
2019-01-08 11:22:51,230 - 10 - training_embed.py - training - Epoch: [2][600/1014]	Time 0.004 (0.005)	Data 0.001 (0.002)	Loss 348.8928 (350.4574)	
2019-01-08 11:22:51,698 - 10 - training_embed.py - training - Epoch: [2][700/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 350.8072 (350.3812)	
2019-01-08 11:22:52,125 - 10 - training_embed.py - training - Epoch: [2][800/1014]	Time 0.009 (0.005)	Data 0.002 (0.002)	Loss 350.8701 (350.3362)	
2019-01-08 11:22:52,559 - 10 - training_embed.py - training - Epoch: [2][900/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 350.7649 (350.4023)	
2019-01-08 11:22:52,975 - 10 - training_embed.py - training - Epoch: [2][1000/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 351.4543 (350.3767)	
2019-01-08 11:22:53,014 - 10 - training_embed.py - training - loss: 350.283622
2019-01-08 11:22:53,014 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 11:22:53,660 - 10 - training_embed.py - training - Epoch: [3][100/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 348.6284 (349.9203)	
2019-01-08 11:22:54,102 - 10 - training_embed.py - training - Epoch: [3][200/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 345.7990 (350.0188)	
2019-01-08 11:22:54,507 - 10 - training_embed.py - training - Epoch: [3][300/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 353.4383 (350.0317)	
2019-01-08 11:22:54,924 - 10 - training_embed.py - training - Epoch: [3][400/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 348.4940 (350.0961)	
2019-01-08 11:22:55,342 - 10 - training_embed.py - training - Epoch: [3][500/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 348.3251 (350.1153)	
2019-01-08 11:22:55,747 - 10 - training_embed.py - training - Epoch: [3][600/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 349.5681 (350.0634)	
2019-01-08 11:22:56,231 - 10 - training_embed.py - training - Epoch: [3][700/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 350.1693 (350.0015)	
2019-01-08 11:22:56,629 - 10 - training_embed.py - training - Epoch: [3][800/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 348.7039 (349.9333)	
2019-01-08 11:22:57,069 - 10 - training_embed.py - training - Epoch: [3][900/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 345.4892 (349.9170)	
2019-01-08 11:22:57,517 - 10 - training_embed.py - training - Epoch: [3][1000/1014]	Time 0.008 (0.005)	Data 0.000 (0.002)	Loss 350.2166 (349.9142)	
2019-01-08 11:22:57,552 - 10 - training_embed.py - training - loss: 349.823524
2019-01-08 11:22:57,552 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 11:22:58,272 - 10 - training_embed.py - training - Epoch: [4][100/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 348.3868 (349.2570)	
2019-01-08 11:22:58,861 - 10 - training_embed.py - training - Epoch: [4][200/1014]	Time 0.006 (0.005)	Data 0.004 (0.002)	Loss 349.5761 (349.4160)	
2019-01-08 11:22:59,366 - 10 - training_embed.py - training - Epoch: [4][300/1014]	Time 0.008 (0.005)	Data 0.001 (0.002)	Loss 352.0312 (349.4965)	
2019-01-08 11:22:59,851 - 10 - training_embed.py - training - Epoch: [4][400/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 352.4852 (349.5358)	
2019-01-08 11:23:00,329 - 10 - training_embed.py - training - Epoch: [4][500/1014]	Time 0.005 (0.005)	Data 0.004 (0.002)	Loss 349.2204 (349.4864)	
2019-01-08 11:23:00,783 - 10 - training_embed.py - training - Epoch: [4][600/1014]	Time 0.008 (0.005)	Data 0.000 (0.002)	Loss 345.1364 (349.4910)	
2019-01-08 11:23:01,212 - 10 - training_embed.py - training - Epoch: [4][700/1014]	Time 0.002 (0.005)	Data 0.000 (0.002)	Loss 348.6475 (349.5007)	
2019-01-08 11:23:01,633 - 10 - training_embed.py - training - Epoch: [4][800/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 346.8023 (349.4774)	
2019-01-08 11:23:02,058 - 10 - training_embed.py - training - Epoch: [4][900/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 351.5407 (349.4844)	
2019-01-08 11:23:02,499 - 10 - training_embed.py - training - Epoch: [4][1000/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 354.6884 (349.4442)	
2019-01-08 11:23:02,549 - 10 - training_embed.py - training - loss: 349.359562
2019-01-08 11:23:02,560 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 11:23:03,195 - 10 - training_embed.py - training - Epoch: [5][100/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 347.3347 (348.8155)	
2019-01-08 11:23:03,619 - 10 - training_embed.py - training - Epoch: [5][200/1014]	Time 0.011 (0.005)	Data 0.009 (0.002)	Loss 348.4362 (348.8746)	
2019-01-08 11:23:04,038 - 10 - training_embed.py - training - Epoch: [5][300/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 350.9312 (348.8147)	
2019-01-08 11:23:04,481 - 10 - training_embed.py - training - Epoch: [5][400/1014]	Time 0.004 (0.005)	Data 0.003 (0.002)	Loss 345.7627 (348.9868)	
2019-01-08 11:23:04,942 - 10 - training_embed.py - training - Epoch: [5][500/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 347.8822 (349.0901)	
2019-01-08 11:23:05,393 - 10 - training_embed.py - training - Epoch: [5][600/1014]	Time 0.008 (0.005)	Data 0.002 (0.002)	Loss 346.1031 (349.0830)	
2019-01-08 11:23:05,797 - 10 - training_embed.py - training - Epoch: [5][700/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 344.8105 (349.0536)	
2019-01-08 11:23:06,219 - 10 - training_embed.py - training - Epoch: [5][800/1014]	Time 0.003 (0.005)	Data 0.000 (0.002)	Loss 350.2173 (348.9961)	
2019-01-08 11:23:06,622 - 10 - training_embed.py - training - Epoch: [5][900/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 345.6915 (348.9960)	
2019-01-08 11:23:07,044 - 10 - training_embed.py - training - Epoch: [5][1000/1014]	Time 0.007 (0.005)	Data 0.000 (0.002)	Loss 345.6095 (348.9754)	
2019-01-08 11:23:07,099 - 10 - training_embed.py - training - loss: 348.895291
2019-01-08 11:23:07,104 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 11:23:07,708 - 10 - training_embed.py - training - Epoch: [6][100/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 354.5874 (348.6921)	
2019-01-08 11:23:08,154 - 10 - training_embed.py - training - Epoch: [6][200/1014]	Time 0.011 (0.005)	Data 0.001 (0.002)	Loss 347.5827 (348.6167)	
2019-01-08 11:23:08,617 - 10 - training_embed.py - training - Epoch: [6][300/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 348.7141 (348.7081)	
2019-01-08 11:23:09,040 - 10 - training_embed.py - training - Epoch: [6][400/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 347.8220 (348.5876)	
2019-01-08 11:23:09,521 - 10 - training_embed.py - training - Epoch: [6][500/1014]	Time 0.003 (0.005)	Data 0.000 (0.002)	Loss 348.0898 (348.6986)	
2019-01-08 11:23:09,946 - 10 - training_embed.py - training - Epoch: [6][600/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 348.3437 (348.6702)	
2019-01-08 11:23:10,372 - 10 - training_embed.py - training - Epoch: [6][700/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 342.9133 (348.6170)	
2019-01-08 11:23:10,812 - 10 - training_embed.py - training - Epoch: [6][800/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 349.6349 (348.5374)	
2019-01-08 11:23:11,234 - 10 - training_embed.py - training - Epoch: [6][900/1014]	Time 0.004 (0.005)	Data 0.003 (0.002)	Loss 351.2181 (348.5241)	
2019-01-08 11:23:11,707 - 10 - training_embed.py - training - Epoch: [6][1000/1014]	Time 0.008 (0.005)	Data 0.000 (0.002)	Loss 348.4164 (348.5223)	
2019-01-08 11:23:11,747 - 10 - training_embed.py - training - loss: 348.426349
2019-01-08 11:23:11,747 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 11:23:12,357 - 10 - training_embed.py - training - Epoch: [7][100/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 348.6739 (348.5415)	
2019-01-08 11:23:12,796 - 10 - training_embed.py - training - Epoch: [7][200/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 343.7772 (348.4081)	
2019-01-08 11:23:13,215 - 10 - training_embed.py - training - Epoch: [7][300/1014]	Time 0.007 (0.005)	Data 0.006 (0.002)	Loss 347.8544 (348.2666)	
2019-01-08 11:23:13,630 - 10 - training_embed.py - training - Epoch: [7][400/1014]	Time 0.006 (0.005)	Data 0.000 (0.002)	Loss 344.8721 (348.1433)	
2019-01-08 11:23:14,065 - 10 - training_embed.py - training - Epoch: [7][500/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 347.8973 (348.0967)	
2019-01-08 11:23:14,506 - 10 - training_embed.py - training - Epoch: [7][600/1014]	Time 0.004 (0.005)	Data 0.000 (0.002)	Loss 345.5801 (348.1431)	
2019-01-08 11:23:14,981 - 10 - training_embed.py - training - Epoch: [7][700/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 347.7047 (348.1088)	
2019-01-08 11:23:15,397 - 10 - training_embed.py - training - Epoch: [7][800/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 348.3959 (348.0844)	
2019-01-08 11:23:15,807 - 10 - training_embed.py - training - Epoch: [7][900/1014]	Time 0.010 (0.005)	Data 0.009 (0.002)	Loss 349.8272 (348.0439)	
2019-01-08 11:23:16,217 - 10 - training_embed.py - training - Epoch: [7][1000/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 349.2631 (348.0303)	
2019-01-08 11:23:16,256 - 10 - training_embed.py - training - loss: 347.958321
2019-01-08 11:23:16,256 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 11:23:16,892 - 10 - training_embed.py - training - Epoch: [8][100/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 349.5917 (347.8207)	
2019-01-08 11:23:17,411 - 10 - training_embed.py - training - Epoch: [8][200/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 343.6035 (347.7084)	
2019-01-08 11:23:17,851 - 10 - training_embed.py - training - Epoch: [8][300/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 350.8130 (347.7102)	
2019-01-08 11:23:18,264 - 10 - training_embed.py - training - Epoch: [8][400/1014]	Time 0.008 (0.005)	Data 0.001 (0.002)	Loss 346.9142 (347.7143)	
2019-01-08 11:23:18,674 - 10 - training_embed.py - training - Epoch: [8][500/1014]	Time 0.009 (0.005)	Data 0.001 (0.002)	Loss 350.3846 (347.6670)	
2019-01-08 11:23:19,089 - 10 - training_embed.py - training - Epoch: [8][600/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 343.9506 (347.6052)	
2019-01-08 11:23:19,527 - 10 - training_embed.py - training - Epoch: [8][700/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 346.8301 (347.6212)	
2019-01-08 11:23:20,032 - 10 - training_embed.py - training - Epoch: [8][800/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 346.1655 (347.6125)	
2019-01-08 11:23:20,446 - 10 - training_embed.py - training - Epoch: [8][900/1014]	Time 0.015 (0.005)	Data 0.013 (0.002)	Loss 346.9541 (347.5801)	
2019-01-08 11:23:20,852 - 10 - training_embed.py - training - Epoch: [8][1000/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 340.8052 (347.5511)	
2019-01-08 11:23:20,912 - 10 - training_embed.py - training - loss: 347.486331
2019-01-08 11:23:20,912 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 11:23:21,478 - 10 - training_embed.py - training - Epoch: [9][100/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 346.3014 (347.5036)	
2019-01-08 11:23:21,905 - 10 - training_embed.py - training - Epoch: [9][200/1014]	Time 0.002 (0.005)	Data 0.000 (0.002)	Loss 350.2622 (347.3998)	
2019-01-08 11:23:22,385 - 10 - training_embed.py - training - Epoch: [9][300/1014]	Time 0.008 (0.005)	Data 0.000 (0.002)	Loss 349.5014 (347.3636)	
2019-01-08 11:23:22,840 - 10 - training_embed.py - training - Epoch: [9][400/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 344.6347 (347.3421)	
2019-01-08 11:23:23,297 - 10 - training_embed.py - training - Epoch: [9][500/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 343.3915 (347.3211)	
2019-01-08 11:23:23,739 - 10 - training_embed.py - training - Epoch: [9][600/1014]	Time 0.008 (0.005)	Data 0.000 (0.002)	Loss 351.8055 (347.2555)	
2019-01-08 11:23:24,210 - 10 - training_embed.py - training - Epoch: [9][700/1014]	Time 0.013 (0.005)	Data 0.012 (0.002)	Loss 343.9189 (347.2273)	
2019-01-08 11:23:24,620 - 10 - training_embed.py - training - Epoch: [9][800/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 347.2488 (347.1205)	
2019-01-08 11:23:25,037 - 10 - training_embed.py - training - Epoch: [9][900/1014]	Time 0.006 (0.005)	Data 0.000 (0.002)	Loss 351.2729 (347.1111)	
2019-01-08 11:23:25,445 - 10 - training_embed.py - training - Epoch: [9][1000/1014]	Time 0.014 (0.005)	Data 0.012 (0.002)	Loss 348.9348 (347.0831)	
2019-01-08 11:23:25,505 - 10 - training_embed.py - training - loss: 347.014034
2019-01-08 11:23:25,505 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 11:23:26,128 - 10 - training_embed.py - training - Epoch: [10][100/1014]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 347.9506 (347.2187)	
2019-01-08 11:23:26,564 - 10 - training_embed.py - training - Epoch: [10][200/1014]	Time 0.002 (0.005)	Data 0.000 (0.002)	Loss 344.1367 (347.0239)	
2019-01-08 11:23:26,997 - 10 - training_embed.py - training - Epoch: [10][300/1014]	Time 0.008 (0.005)	Data 0.007 (0.002)	Loss 345.0660 (346.7928)	
2019-01-08 11:23:27,541 - 10 - training_embed.py - training - Epoch: [10][400/1014]	Time 0.005 (0.005)	Data 0.003 (0.002)	Loss 342.8864 (346.8452)	
2019-01-08 11:23:28,005 - 10 - training_embed.py - training - Epoch: [10][500/1014]	Time 0.009 (0.005)	Data 0.001 (0.002)	Loss 341.4975 (346.8007)	
2019-01-08 11:23:28,417 - 10 - training_embed.py - training - Epoch: [10][600/1014]	Time 0.007 (0.005)	Data 0.002 (0.002)	Loss 349.8808 (346.7161)	
2019-01-08 11:23:28,818 - 10 - training_embed.py - training - Epoch: [10][700/1014]	Time 0.005 (0.005)	Data 0.001 (0.002)	Loss 351.6632 (346.6921)	
2019-01-08 11:23:29,243 - 10 - training_embed.py - training - Epoch: [10][800/1014]	Time 0.008 (0.005)	Data 0.007 (0.002)	Loss 351.1677 (346.6379)	
2019-01-08 11:23:29,688 - 10 - training_embed.py - training - Epoch: [10][900/1014]	Time 0.013 (0.005)	Data 0.011 (0.002)	Loss 343.8020 (346.6211)	
2019-01-08 11:23:30,112 - 10 - training_embed.py - training - Epoch: [10][1000/1014]	Time 0.006 (0.005)	Data 0.005 (0.002)	Loss 345.9838 (346.6163)	
2019-01-08 11:23:30,153 - 10 - training_embed.py - training - loss: 346.536843
2019-01-08 11:23:30,154 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 11:23:30,771 - 10 - training_embed.py - training - Epoch: [11][100/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 345.0159 (346.1991)	
2019-01-08 11:23:31,366 - 10 - training_embed.py - training - Epoch: [11][200/1014]	Time 0.010 (0.005)	Data 0.006 (0.002)	Loss 347.7357 (346.1872)	
2019-01-08 11:23:31,854 - 10 - training_embed.py - training - Epoch: [11][300/1014]	Time 0.006 (0.005)	Data 0.000 (0.002)	Loss 343.2150 (346.0189)	
2019-01-08 11:23:32,302 - 10 - training_embed.py - training - Epoch: [11][400/1014]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 344.7315 (346.1021)	
2019-01-08 11:23:32,760 - 10 - training_embed.py - training - Epoch: [11][500/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 343.6061 (346.1420)	
2019-01-08 11:23:33,208 - 10 - training_embed.py - training - Epoch: [11][600/1014]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 346.4182 (346.1437)	
2019-01-08 11:23:33,684 - 10 - training_embed.py - training - Epoch: [11][700/1014]	Time 0.005 (0.005)	Data 0.004 (0.002)	Loss 344.8182 (346.1277)	
2019-01-08 11:23:34,100 - 10 - training_embed.py - training - Epoch: [11][800/1014]	Time 0.011 (0.005)	Data 0.001 (0.002)	Loss 348.8213 (346.1406)	
2019-01-08 11:23:34,518 - 10 - training_embed.py - training - Epoch: [11][900/1014]	Time 0.008 (0.005)	Data 0.001 (0.002)	Loss 351.0644 (346.1687)	
2019-01-08 11:23:34,927 - 10 - training_embed.py - training - Epoch: [11][1000/1014]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 344.0541 (346.1502)	
2019-01-08 11:23:34,971 - 10 - training_embed.py - training - loss: 346.058535
2019-01-08 11:23:35,053 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 11:23:35,742 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 688.829 ms ~ 0.011 min ~ 0.689 sec
2019-01-08 11:23:36,633 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1580.237 ms ~ 0.026 min ~ 1.580 sec
2019-01-08 11:23:36,633 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 11:23:36,634 - 10 - corpus.py - subactivity_sampler - 0 / 169
2019-01-08 11:23:36,634 - 10 - corpus.py - subactivity_sampler - [26283. 53871.  5329. 50220. 19868.  9682. 89979.  4263.]
2019-01-08 11:24:06,789 - 10 - corpus.py - subactivity_sampler - 20 / 169
2019-01-08 11:24:06,789 - 10 - corpus.py - subactivity_sampler - [26211. 54211.  5068. 50535. 19515.  9279. 90407.  4269.]
2019-01-08 11:24:27,557 - 10 - corpus.py - subactivity_sampler - 40 / 169
2019-01-08 11:24:27,558 - 10 - corpus.py - subactivity_sampler - [26159. 54628.  5032. 50845. 19040.  8765. 90760.  4266.]
2019-01-08 11:24:54,976 - 10 - corpus.py - subactivity_sampler - 60 / 169
2019-01-08 11:24:54,977 - 10 - corpus.py - subactivity_sampler - [26145. 55031.  4869. 51075. 18766.  8424. 90936.  4249.]
2019-01-08 11:25:16,300 - 10 - corpus.py - subactivity_sampler - 80 / 169
2019-01-08 11:25:16,300 - 10 - corpus.py - subactivity_sampler - [26080. 55297.  4639. 51010. 18657.  8155. 91429.  4228.]
2019-01-08 11:25:32,775 - 10 - corpus.py - subactivity_sampler - 100 / 169
2019-01-08 11:25:32,775 - 10 - corpus.py - subactivity_sampler - [26049. 55367.  4542. 51109. 18319.  7873. 92030.  4206.]
2019-01-08 11:25:47,097 - 10 - corpus.py - subactivity_sampler - 120 / 169
2019-01-08 11:25:47,097 - 10 - corpus.py - subactivity_sampler - [26018. 55578.  4522. 51426. 18070.  7454. 92225.  4202.]
2019-01-08 11:26:04,581 - 10 - corpus.py - subactivity_sampler - 140 / 169
2019-01-08 11:26:04,581 - 10 - corpus.py - subactivity_sampler - [25967. 55643.  4455. 51614. 17933.  7256. 92298.  4329.]
2019-01-08 11:26:23,570 - 10 - corpus.py - subactivity_sampler - 160 / 169
2019-01-08 11:26:23,570 - 10 - corpus.py - subactivity_sampler - [25941. 55447.  4421. 52172. 17738.  7046. 92391.  4339.]
2019-01-08 11:26:29,638 - 10 - corpus.py - subactivity_sampler - [25937. 55450.  4406. 52333. 17524.  6962. 92544.  4339.]
2019-01-08 11:26:29,638 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 173004.988 ms ~ 2.883 min ~ 173.005 sec
2019-01-08 11:26:29,638 - 10 - corpus.py - ordering_sampler - .
2019-01-08 11:26:32,646 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 11:26:32,646 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 4. 25.  0. 15. 27.  3.  0.]
2019-01-08 11:26:32,646 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 11:26:32,671 - 10 - corpus.py - rho_sampling - ['58.7949', '0.8191', '53.3694', '179.7163', '15.7507', '9.9755', '587.5208']
2019-01-08 11:26:32,671 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 11:26:32,750 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 11:26:32,755 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 11:26:32,755 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 18', '1: 36', '2: 14', '3: 37', '4: 41', '5: 38', '6: 39', '7: 40']
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - mof_val - frames true: 129426	frames overall : 259495
2019-01-08 11:26:32,764 - 10 - corpus.py - accuracy_corpus - Action: sandwich
2019-01-08 11:26:32,764 - 10 - corpus.py - accuracy_corpus - MoF val: 0.49876105512630303
2019-01-08 11:26:32,764 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.49868783598913274
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - mof_classes - label 14: 0.043878  86 / 1960
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - mof_classes - label 18: 0.469719  1908 / 4062
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - mof_classes - label 36: 0.651713  29883 / 45853
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - mof_classes - label 37: 0.420445  41558 / 98843
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - mof_classes - label 38: 0.063142  449 / 7111
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - mof_classes - label 39: 0.851610  55106 / 64708
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - mof_classes - label 40: 0.083111  436 / 5246
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - mof_classes - label 41: 0.000000  0 / 970
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - mof_classes - average class mof: 0.287069
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - iou_classes - label 14: 0.013694  86 / 6280
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - iou_classes - label 18: 0.067922  1908 / 28091
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - iou_classes - label 36: 0.418412  29883 / 71420
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - iou_classes - label 37: 0.379117  41558 / 109618
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - iou_classes - label 38: 0.032957  449 / 13624
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - iou_classes - label 39: 0.539483  55106 / 102146
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - iou_classes - label 40: 0.047655  436 / 9149
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - iou_classes - label 41: 0.000000  0 / 18494
2019-01-08 11:26:32,764 - 10 - accuracy_class.py - iou_classes - average IoU: 0.187405
2019-01-08 11:26:32,765 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.166582
2019-01-08 11:26:35,831 - 10 - f1_score.py - f1 - f1 score: 0.329524
2019-01-08 11:26:35,842 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3171.127 ms ~ 0.053 min ~ 3.171 sec
2019-01-08 11:26:35,842 - 10 - corpus.py - embedding_training - .
2019-01-08 11:26:35,842 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 11:26:35,842 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 11:26:35,842 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 11:26:37,396 - 10 - training_embed.py - training - create model
2019-01-08 11:26:37,398 - 10 - training_embed.py - training - epochs: 12
2019-01-08 11:26:37,398 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 11:26:37,736 - 10 - training_embed.py - training - Epoch: [0][100/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 347.7074 (351.2165)	
2019-01-08 11:26:37,937 - 10 - training_embed.py - training - Epoch: [0][200/1014]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 347.6873 (351.1392)	
2019-01-08 11:26:38,161 - 10 - training_embed.py - training - Epoch: [0][300/1014]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.5824 (351.0411)	
2019-01-08 11:26:38,376 - 10 - training_embed.py - training - Epoch: [0][400/1014]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 352.4498 (351.0866)	
2019-01-08 11:26:38,601 - 10 - training_embed.py - training - Epoch: [0][500/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 349.9209 (351.0037)	
2019-01-08 11:26:38,817 - 10 - training_embed.py - training - Epoch: [0][600/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.8846 (350.9927)	
2019-01-08 11:26:39,049 - 10 - training_embed.py - training - Epoch: [0][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.3752 (350.9438)	
2019-01-08 11:26:39,269 - 10 - training_embed.py - training - Epoch: [0][800/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.4808 (350.8951)	
2019-01-08 11:26:39,488 - 10 - training_embed.py - training - Epoch: [0][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.8794 (350.8271)	
2019-01-08 11:26:39,702 - 10 - training_embed.py - training - Epoch: [0][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.6355 (350.7633)	
2019-01-08 11:26:39,729 - 10 - training_embed.py - training - loss: 350.683430
2019-01-08 11:26:39,729 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 11:26:40,101 - 10 - training_embed.py - training - Epoch: [1][100/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 354.3671 (350.6556)	
2019-01-08 11:26:40,322 - 10 - training_embed.py - training - Epoch: [1][200/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.7967 (350.3588)	
2019-01-08 11:26:40,550 - 10 - training_embed.py - training - Epoch: [1][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.1925 (350.3312)	
2019-01-08 11:26:40,771 - 10 - training_embed.py - training - Epoch: [1][400/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.6900 (350.3480)	
2019-01-08 11:26:40,993 - 10 - training_embed.py - training - Epoch: [1][500/1014]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 348.9247 (350.3638)	
2019-01-08 11:26:41,228 - 10 - training_embed.py - training - Epoch: [1][600/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.4074 (350.3652)	
2019-01-08 11:26:41,432 - 10 - training_embed.py - training - Epoch: [1][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.8953 (350.3368)	
2019-01-08 11:26:41,655 - 10 - training_embed.py - training - Epoch: [1][800/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.2976 (350.3490)	
2019-01-08 11:26:41,872 - 10 - training_embed.py - training - Epoch: [1][900/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.7073 (350.2732)	
2019-01-08 11:26:42,098 - 10 - training_embed.py - training - Epoch: [1][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.1640 (350.2775)	
2019-01-08 11:26:42,126 - 10 - training_embed.py - training - loss: 350.188538
2019-01-08 11:26:42,127 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 11:26:42,523 - 10 - training_embed.py - training - Epoch: [2][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.7594 (349.9140)	
2019-01-08 11:26:42,736 - 10 - training_embed.py - training - Epoch: [2][200/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 347.8108 (349.7215)	
2019-01-08 11:26:42,952 - 10 - training_embed.py - training - Epoch: [2][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.0853 (349.7704)	
2019-01-08 11:26:43,177 - 10 - training_embed.py - training - Epoch: [2][400/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 349.7081 (349.8618)	
2019-01-08 11:26:43,399 - 10 - training_embed.py - training - Epoch: [2][500/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.8633 (349.8182)	
2019-01-08 11:26:43,622 - 10 - training_embed.py - training - Epoch: [2][600/1014]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 347.5062 (349.8660)	
2019-01-08 11:26:43,850 - 10 - training_embed.py - training - Epoch: [2][700/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.0338 (349.7676)	
2019-01-08 11:26:44,073 - 10 - training_embed.py - training - Epoch: [2][800/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.9764 (349.7325)	
2019-01-08 11:26:44,294 - 10 - training_embed.py - training - Epoch: [2][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.5639 (349.7938)	
2019-01-08 11:26:44,538 - 10 - training_embed.py - training - Epoch: [2][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.0487 (349.7810)	
2019-01-08 11:26:44,565 - 10 - training_embed.py - training - loss: 349.691626
2019-01-08 11:26:44,566 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 11:26:44,940 - 10 - training_embed.py - training - Epoch: [3][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.7932 (349.2561)	
2019-01-08 11:26:45,140 - 10 - training_embed.py - training - Epoch: [3][200/1014]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 345.4638 (349.3291)	
2019-01-08 11:26:45,364 - 10 - training_embed.py - training - Epoch: [3][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.0019 (349.4118)	
2019-01-08 11:26:45,580 - 10 - training_embed.py - training - Epoch: [3][400/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 348.9005 (349.4807)	
2019-01-08 11:26:45,819 - 10 - training_embed.py - training - Epoch: [3][500/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.2402 (349.4612)	
2019-01-08 11:26:46,054 - 10 - training_embed.py - training - Epoch: [3][600/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.9158 (349.4191)	
2019-01-08 11:26:46,269 - 10 - training_embed.py - training - Epoch: [3][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.3721 (349.3871)	
2019-01-08 11:26:46,493 - 10 - training_embed.py - training - Epoch: [3][800/1014]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.0486 (349.3095)	
2019-01-08 11:26:46,718 - 10 - training_embed.py - training - Epoch: [3][900/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 345.2839 (349.2926)	
2019-01-08 11:26:46,925 - 10 - training_embed.py - training - Epoch: [3][1000/1014]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 350.9516 (349.2815)	
2019-01-08 11:26:46,954 - 10 - training_embed.py - training - loss: 349.193861
2019-01-08 11:26:46,955 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 11:26:47,359 - 10 - training_embed.py - training - Epoch: [4][100/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 347.3824 (348.6261)	
2019-01-08 11:26:47,575 - 10 - training_embed.py - training - Epoch: [4][200/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 348.2694 (348.7745)	
2019-01-08 11:26:47,805 - 10 - training_embed.py - training - Epoch: [4][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.3642 (348.8552)	
2019-01-08 11:26:48,022 - 10 - training_embed.py - training - Epoch: [4][400/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.9832 (348.8645)	
2019-01-08 11:26:48,226 - 10 - training_embed.py - training - Epoch: [4][500/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.5764 (348.8414)	
2019-01-08 11:26:48,447 - 10 - training_embed.py - training - Epoch: [4][600/1014]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 345.3383 (348.8199)	
2019-01-08 11:26:48,675 - 10 - training_embed.py - training - Epoch: [4][700/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.3390 (348.8366)	
2019-01-08 11:26:48,902 - 10 - training_embed.py - training - Epoch: [4][800/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.5915 (348.8131)	
2019-01-08 11:26:49,132 - 10 - training_embed.py - training - Epoch: [4][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.4692 (348.8157)	
2019-01-08 11:26:49,352 - 10 - training_embed.py - training - Epoch: [4][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.4359 (348.7776)	
2019-01-08 11:26:49,379 - 10 - training_embed.py - training - loss: 348.691778
2019-01-08 11:26:49,379 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 11:26:49,763 - 10 - training_embed.py - training - Epoch: [5][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.0269 (348.1507)	
2019-01-08 11:26:49,988 - 10 - training_embed.py - training - Epoch: [5][200/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.9938 (348.1958)	
2019-01-08 11:26:50,214 - 10 - training_embed.py - training - Epoch: [5][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.0180 (348.1341)	
2019-01-08 11:26:50,444 - 10 - training_embed.py - training - Epoch: [5][400/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.9321 (348.2944)	
2019-01-08 11:26:50,648 - 10 - training_embed.py - training - Epoch: [5][500/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.6047 (348.3773)	
2019-01-08 11:26:50,870 - 10 - training_embed.py - training - Epoch: [5][600/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.8825 (348.3589)	
2019-01-08 11:26:51,088 - 10 - training_embed.py - training - Epoch: [5][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.6696 (348.3422)	
2019-01-08 11:26:51,294 - 10 - training_embed.py - training - Epoch: [5][800/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.5685 (348.2956)	
2019-01-08 11:26:51,527 - 10 - training_embed.py - training - Epoch: [5][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.4253 (348.2959)	
2019-01-08 11:26:51,752 - 10 - training_embed.py - training - Epoch: [5][1000/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 343.1062 (348.2735)	
2019-01-08 11:26:51,781 - 10 - training_embed.py - training - loss: 348.189518
2019-01-08 11:26:51,782 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 11:26:52,170 - 10 - training_embed.py - training - Epoch: [6][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.4089 (347.8881)	
2019-01-08 11:26:52,377 - 10 - training_embed.py - training - Epoch: [6][200/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.7497 (347.9280)	
2019-01-08 11:26:52,598 - 10 - training_embed.py - training - Epoch: [6][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.6456 (348.0113)	
2019-01-08 11:26:52,819 - 10 - training_embed.py - training - Epoch: [6][400/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.5856 (347.9219)	
2019-01-08 11:26:53,055 - 10 - training_embed.py - training - Epoch: [6][500/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 347.3021 (347.9764)	
2019-01-08 11:26:53,274 - 10 - training_embed.py - training - Epoch: [6][600/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.8231 (347.9608)	
2019-01-08 11:26:53,493 - 10 - training_embed.py - training - Epoch: [6][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 342.9183 (347.8710)	
2019-01-08 11:26:53,713 - 10 - training_embed.py - training - Epoch: [6][800/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.0427 (347.7912)	
2019-01-08 11:26:53,942 - 10 - training_embed.py - training - Epoch: [6][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.6423 (347.7689)	
2019-01-08 11:26:54,157 - 10 - training_embed.py - training - Epoch: [6][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.3603 (347.7708)	
2019-01-08 11:26:54,186 - 10 - training_embed.py - training - loss: 347.682282
2019-01-08 11:26:54,187 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 11:26:54,559 - 10 - training_embed.py - training - Epoch: [7][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.0285 (347.7286)	
2019-01-08 11:26:54,780 - 10 - training_embed.py - training - Epoch: [7][200/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 343.4812 (347.6543)	
2019-01-08 11:26:55,007 - 10 - training_embed.py - training - Epoch: [7][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.0917 (347.5215)	
2019-01-08 11:26:55,216 - 10 - training_embed.py - training - Epoch: [7][400/1014]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 346.1833 (347.4002)	
2019-01-08 11:26:55,436 - 10 - training_embed.py - training - Epoch: [7][500/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.9372 (347.3492)	
2019-01-08 11:26:55,660 - 10 - training_embed.py - training - Epoch: [7][600/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 346.1005 (347.3981)	
2019-01-08 11:26:55,873 - 10 - training_embed.py - training - Epoch: [7][700/1014]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 348.1282 (347.3827)	
2019-01-08 11:26:56,094 - 10 - training_embed.py - training - Epoch: [7][800/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.8541 (347.3473)	
2019-01-08 11:26:56,324 - 10 - training_embed.py - training - Epoch: [7][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.8176 (347.2899)	
2019-01-08 11:26:56,532 - 10 - training_embed.py - training - Epoch: [7][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.5726 (347.2476)	
2019-01-08 11:26:56,563 - 10 - training_embed.py - training - loss: 347.175221
2019-01-08 11:26:56,563 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 11:26:56,934 - 10 - training_embed.py - training - Epoch: [8][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.8904 (346.9449)	
2019-01-08 11:26:57,153 - 10 - training_embed.py - training - Epoch: [8][200/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.0415 (346.9114)	
2019-01-08 11:26:57,379 - 10 - training_embed.py - training - Epoch: [8][300/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.4669 (346.9434)	
2019-01-08 11:26:57,599 - 10 - training_embed.py - training - Epoch: [8][400/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.3190 (346.9446)	
2019-01-08 11:26:57,820 - 10 - training_embed.py - training - Epoch: [8][500/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.0447 (346.8975)	
2019-01-08 11:26:58,037 - 10 - training_embed.py - training - Epoch: [8][600/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 343.3869 (346.8353)	
2019-01-08 11:26:58,247 - 10 - training_embed.py - training - Epoch: [8][700/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 345.9383 (346.8403)	
2019-01-08 11:26:58,483 - 10 - training_embed.py - training - Epoch: [8][800/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.2329 (346.8138)	
2019-01-08 11:26:58,706 - 10 - training_embed.py - training - Epoch: [8][900/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 345.2539 (346.7675)	
2019-01-08 11:26:58,943 - 10 - training_embed.py - training - Epoch: [8][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 339.8055 (346.7344)	
2019-01-08 11:26:58,969 - 10 - training_embed.py - training - loss: 346.664015
2019-01-08 11:26:58,972 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 11:26:59,357 - 10 - training_embed.py - training - Epoch: [9][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.1974 (346.6116)	
2019-01-08 11:26:59,576 - 10 - training_embed.py - training - Epoch: [9][200/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.8288 (346.5453)	
2019-01-08 11:26:59,792 - 10 - training_embed.py - training - Epoch: [9][300/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.5488 (346.5271)	
2019-01-08 11:27:00,011 - 10 - training_embed.py - training - Epoch: [9][400/1014]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 342.4567 (346.4878)	
2019-01-08 11:27:00,229 - 10 - training_embed.py - training - Epoch: [9][500/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 342.8286 (346.4662)	
2019-01-08 11:27:00,469 - 10 - training_embed.py - training - Epoch: [9][600/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.4601 (346.3939)	
2019-01-08 11:27:00,699 - 10 - training_embed.py - training - Epoch: [9][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 342.2661 (346.3608)	
2019-01-08 11:27:00,910 - 10 - training_embed.py - training - Epoch: [9][800/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.7379 (346.2519)	
2019-01-08 11:27:01,136 - 10 - training_embed.py - training - Epoch: [9][900/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 350.1525 (346.2466)	
2019-01-08 11:27:01,359 - 10 - training_embed.py - training - Epoch: [9][1000/1014]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 348.0208 (346.2174)	
2019-01-08 11:27:01,388 - 10 - training_embed.py - training - loss: 346.151833
2019-01-08 11:27:01,389 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 11:27:01,763 - 10 - training_embed.py - training - Epoch: [10][100/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.6662 (346.3268)	
2019-01-08 11:27:01,983 - 10 - training_embed.py - training - Epoch: [10][200/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 341.8901 (346.1002)	
2019-01-08 11:27:02,220 - 10 - training_embed.py - training - Epoch: [10][300/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 344.7401 (345.9041)	
2019-01-08 11:27:02,444 - 10 - training_embed.py - training - Epoch: [10][400/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 340.6990 (345.9480)	
2019-01-08 11:27:02,651 - 10 - training_embed.py - training - Epoch: [10][500/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 339.7637 (345.9207)	
2019-01-08 11:27:02,880 - 10 - training_embed.py - training - Epoch: [10][600/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.7579 (345.8299)	
2019-01-08 11:27:03,102 - 10 - training_embed.py - training - Epoch: [10][700/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.2809 (345.8030)	
2019-01-08 11:27:03,315 - 10 - training_embed.py - training - Epoch: [10][800/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.7328 (345.7441)	
2019-01-08 11:27:03,539 - 10 - training_embed.py - training - Epoch: [10][900/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 342.4210 (345.7255)	
2019-01-08 11:27:03,761 - 10 - training_embed.py - training - Epoch: [10][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.5591 (345.7105)	
2019-01-08 11:27:03,788 - 10 - training_embed.py - training - loss: 345.634978
2019-01-08 11:27:03,788 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 11:27:04,161 - 10 - training_embed.py - training - Epoch: [11][100/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 343.7577 (345.1360)	
2019-01-08 11:27:04,385 - 10 - training_embed.py - training - Epoch: [11][200/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.8696 (345.1488)	
2019-01-08 11:27:04,598 - 10 - training_embed.py - training - Epoch: [11][300/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 344.0862 (345.0537)	
2019-01-08 11:27:04,802 - 10 - training_embed.py - training - Epoch: [11][400/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.7366 (345.1131)	
2019-01-08 11:27:05,030 - 10 - training_embed.py - training - Epoch: [11][500/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 342.1030 (345.1784)	
2019-01-08 11:27:05,256 - 10 - training_embed.py - training - Epoch: [11][600/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 345.4941 (345.1903)	
2019-01-08 11:27:05,478 - 10 - training_embed.py - training - Epoch: [11][700/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.2299 (345.1750)	
2019-01-08 11:27:05,698 - 10 - training_embed.py - training - Epoch: [11][800/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.5261 (345.1906)	
2019-01-08 11:27:05,925 - 10 - training_embed.py - training - Epoch: [11][900/1014]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.4408 (345.2211)	
2019-01-08 11:27:06,143 - 10 - training_embed.py - training - Epoch: [11][1000/1014]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 343.8866 (345.2039)	
2019-01-08 11:27:06,172 - 10 - training_embed.py - training - loss: 345.116135
2019-01-08 11:27:06,205 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 11:27:06,657 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 451.792 ms ~ 0.008 min ~ 0.452 sec
2019-01-08 11:27:07,130 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 925.541 ms ~ 0.015 min ~ 0.926 sec
2019-01-08 11:27:07,130 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 11:27:07,131 - 10 - corpus.py - subactivity_sampler - 0 / 169
2019-01-08 11:27:07,131 - 10 - corpus.py - subactivity_sampler - [25937. 55450.  4406. 52333. 17524.  6962. 92544.  4339.]
2019-01-08 11:27:29,830 - 10 - corpus.py - subactivity_sampler - 20 / 169
2019-01-08 11:27:29,831 - 10 - corpus.py - subactivity_sampler - [25851. 55585.  4396. 52338. 17354.  6524. 93091.  4356.]
2019-01-08 11:27:44,698 - 10 - corpus.py - subactivity_sampler - 40 / 169
2019-01-08 11:27:44,698 - 10 - corpus.py - subactivity_sampler - [25834. 55874.  4301. 52525. 16987.  6405. 93206.  4363.]
2019-01-08 11:28:05,372 - 10 - corpus.py - subactivity_sampler - 60 / 169
2019-01-08 11:28:05,372 - 10 - corpus.py - subactivity_sampler - [25823. 55946.  4229. 52596. 16953.  6242. 93515.  4191.]
2019-01-08 11:28:25,133 - 10 - corpus.py - subactivity_sampler - 80 / 169
2019-01-08 11:28:25,134 - 10 - corpus.py - subactivity_sampler - [25763. 56091.  4153. 52595. 16885.  6122. 93732.  4154.]
2019-01-08 11:28:41,753 - 10 - corpus.py - subactivity_sampler - 100 / 169
2019-01-08 11:28:41,754 - 10 - corpus.py - subactivity_sampler - [25687. 56139.  4120. 52720. 16681.  6007. 93980.  4161.]
2019-01-08 11:28:56,594 - 10 - corpus.py - subactivity_sampler - 120 / 169
2019-01-08 11:28:56,594 - 10 - corpus.py - subactivity_sampler - [25645. 56351.  4023. 52848. 16530.  5900. 94056.  4142.]
2019-01-08 11:29:16,170 - 10 - corpus.py - subactivity_sampler - 140 / 169
2019-01-08 11:29:16,171 - 10 - corpus.py - subactivity_sampler - [25640. 56405.  4016. 52825. 16286.  5867. 94400.  4056.]
2019-01-08 11:29:40,471 - 10 - corpus.py - subactivity_sampler - 160 / 169
2019-01-08 11:29:40,472 - 10 - corpus.py - subactivity_sampler - [25634. 56200.  3999. 52994. 16253.  5800. 94562.  4053.]
2019-01-08 11:29:47,317 - 10 - corpus.py - subactivity_sampler - [25632. 56202.  3995. 52929. 16133.  5794. 94757.  4053.]
2019-01-08 11:29:47,317 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 160186.912 ms ~ 2.670 min ~ 160.187 sec
2019-01-08 11:29:47,317 - 10 - corpus.py - ordering_sampler - .
2019-01-08 11:29:49,979 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 11:29:49,979 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 4. 43. 16.  5. 17.  4.  1.]
2019-01-08 11:29:49,979 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 11:29:49,998 - 10 - corpus.py - rho_sampling - ['59.1795', '13.4517', '35.0597', '185.3351', '3.8376', '23.3805', '5.5670']
2019-01-08 11:29:49,998 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 11:29:50,079 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 11:29:50,084 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 11:29:50,084 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 18', '1: 36', '2: 14', '3: 37', '4: 41', '5: 38', '6: 39', '7: 40']
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - mof_val - frames true: 127759	frames overall : 259495
2019-01-08 11:29:50,095 - 10 - corpus.py - accuracy_corpus - Action: sandwich
2019-01-08 11:29:50,095 - 10 - corpus.py - accuracy_corpus - MoF val: 0.49233703924931116
2019-01-08 11:29:50,095 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.49233703924931116
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - mof_classes - label 14: 0.043367  85 / 1960
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - mof_classes - label 18: 0.469719  1908 / 4062
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - mof_classes - label 36: 0.631976  28978 / 45853
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - mof_classes - label 37: 0.412452  40768 / 98843
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - mof_classes - label 38: 0.059626  424 / 7111
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - mof_classes - label 39: 0.852599  55170 / 64708
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - mof_classes - label 40: 0.081205  426 / 5246
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - mof_classes - label 41: 0.000000  0 / 970
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - mof_classes - average class mof: 0.283438
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 30742
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - iou_classes - label 14: 0.014480  85 / 5870
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - iou_classes - label 18: 0.068668  1908 / 27786
2019-01-08 11:29:50,095 - 10 - accuracy_class.py - iou_classes - label 36: 0.396541  28978 / 73077
2019-01-08 11:29:50,096 - 10 - accuracy_class.py - iou_classes - label 37: 0.367266  40768 / 111004
2019-01-08 11:29:50,096 - 10 - accuracy_class.py - iou_classes - label 38: 0.033972  424 / 12481
2019-01-08 11:29:50,096 - 10 - accuracy_class.py - iou_classes - label 39: 0.528980  55170 / 104295
2019-01-08 11:29:50,096 - 10 - accuracy_class.py - iou_classes - label 40: 0.048011  426 / 8873
2019-01-08 11:29:50,096 - 10 - accuracy_class.py - iou_classes - label 41: 0.000000  0 / 17103
2019-01-08 11:29:50,096 - 10 - accuracy_class.py - iou_classes - average IoU: 0.182240
2019-01-08 11:29:50,096 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.161991
2019-01-08 11:29:53,394 - 10 - f1_score.py - f1 - f1 score: 0.323449
2019-01-08 11:29:53,404 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 3406.071 ms ~ 0.057 min ~ 3.406 sec
2019-01-08 11:29:53,415 - 10 - utils.py - wrap - <function baseline at 0x7f3991d24e18> took 1678967.682 ms ~ 27.983 min ~ 1678.968 sec
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - batch_size: 256
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - bg: False
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - bg_trh: 85
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - data: /media/data/kukleva/lab/Breakfast/feat/s1
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - data_type: 2
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - dataset: bf
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - dataset_root: /media/data/kukleva/lab/Breakfast
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - embed_dim: 30
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - epochs: 12
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - ext: txt
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - feature_dim: 64
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - full: True
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - gmm: 1
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - gmms: one
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - gt: /media/data/kukleva/lab/Breakfast/feat/s1/groundTruth
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - gt_training: False
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - log: DEBUG
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - log_str: slim.mallow._scrambledegg_!bg_data2_bf_embed30_epochs12_full_gmm1_one_!gt_lr1e-10_!lr_ordering_reg0.1_!viterbi_!zeros_
2019-01-08 11:29:53,416 - 10 - utils.py - update_opt_str - lr: 1e-10
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - lr_adj: False
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - momentum: 0.9
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - num_workers: 4
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - ordering: True
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - prefix: slim.mallow.
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - reg_cov: 0.1
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - resume: False
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - resume_str: 
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - save_embed_feat: False
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - save_likelihood: False
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - save_model: False
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - seed: 0
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - subaction: scrambledegg
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - vis: False
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - viterbi: False
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - weight_decay: 0.0001
2019-01-08 11:29:53,417 - 10 - utils.py - update_opt_str - zeros: False
2019-01-08 11:29:53,441 - 10 - utils.py - wrap - <function GroundTruth.load_gt at 0x7f39322b36a8> took 23.364 ms ~ 0.000 min ~ 0.023 sec
2019-01-08 11:29:53,588 - 10 - corpus.py - __init__ - scrambledegg  subactions: 11
2019-01-08 11:29:53,589 - 10 - corpus.py - _init_videos - .
2019-01-08 11:30:17,098 - 10 - corpus.py - _init_videos - gt statistic: Counter({44: 214839, 11: 62734, 17: 55328, 15: 51971, 12: 42008, 42: 32129, 43: 22977, 14: 18527, 10: 14414, 4: 1536, 16: 1015})
2019-01-08 11:30:17,099 - 10 - corpus.py - _update_fg_mask - .
2019-01-08 11:30:17,155 - 10 - corpus.py - __init__ - min: -37.329617  max: 36.483841  avg: 0.127730
2019-01-08 11:30:17,155 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 11:30:17,238 - 10 - corpus.py - rho_sampling - ['64.1837', '100.9221', '929.0535', '90.9104', '17.1809', '163.7961', '56.0422', '55.1381', '89.4672', '406.3854']
2019-01-08 11:30:17,238 - 10 - pipeline.py - baseline - Iteration 0
2019-01-08 11:30:17,506 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 11:30:17,519 - 10 - accuracy_class.py - mof - # gt_labels: 12   # pr_labels: 11
2019-01-08 11:30:17,519 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 10', '1: 11', '2: 42', '3: 17', '4: 43', '5: 16', '6: 12', '7: 4', '8: 44', '9: 14', '10: 15']
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_val - frames true: 133245	frames overall : 517478
2019-01-08 11:30:17,549 - 10 - corpus.py - accuracy_corpus - Action: scrambledegg
2019-01-08 11:30:17,549 - 10 - corpus.py - accuracy_corpus - MoF val: 0.2574892072706473
2019-01-08 11:30:17,549 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.0
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 29280
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1218
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_classes - label 10: 0.367078  4014 / 10935
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_classes - label 11: 0.284075  15967 / 56207
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_classes - label 12: 0.157976  6356 / 40234
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_classes - label 14: 0.436545  7688 / 17611
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_classes - label 15: 0.665263  28917 / 43467
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 1003
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_classes - label 17: 0.212073  10466 / 49351
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_classes - label 42: 0.370662  11909 / 32129
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_classes - label 43: 0.281934  6478 / 22977
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_classes - label 44: 0.194541  41450 / 213066
2019-01-08 11:30:17,549 - 10 - accuracy_class.py - mof_classes - average class mof: 0.247512
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 29280
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 48222
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - label 10: 0.074270  4014 / 54046
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - label 11: 0.182800  15967 / 87347
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - label 12: 0.078562  6356 / 80904
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - label 14: 0.135107  7688 / 56903
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - label 15: 0.470073  28917 / 61516
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 48046
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - label 17: 0.121742  10466 / 85969
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - label 42: 0.176907  11909 / 67318
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - label 43: 0.101919  6478 / 63560
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - label 44: 0.189616  41450 / 218600
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - average IoU: 0.139181
2019-01-08 11:30:17,550 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.127583
2019-01-08 11:30:26,984 - 10 - f1_score.py - f1 - f1 score: 0.245524
2019-01-08 11:30:27,006 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 9768.520 ms ~ 0.163 min ~ 9.769 sec
2019-01-08 11:30:27,006 - 10 - corpus.py - embedding_training - .
2019-01-08 11:30:27,007 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 11:30:27,007 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 11:30:27,007 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 11:30:31,318 - 10 - training_embed.py - training - create model
2019-01-08 11:30:31,328 - 10 - training_embed.py - training - epochs: 12
2019-01-08 11:30:31,328 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 11:30:31,871 - 10 - training_embed.py - training - Epoch: [0][100/2022]	Time 0.006 (0.005)	Data 0.005 (0.004)	Loss 513.5873 (510.8195)	
2019-01-08 11:30:32,224 - 10 - training_embed.py - training - Epoch: [0][200/2022]	Time 0.002 (0.004)	Data 0.001 (0.003)	Loss 510.3109 (510.9755)	
2019-01-08 11:30:32,587 - 10 - training_embed.py - training - Epoch: [0][300/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 513.2444 (510.8383)	
2019-01-08 11:30:32,990 - 10 - training_embed.py - training - Epoch: [0][400/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 514.6234 (510.8345)	
2019-01-08 11:30:33,394 - 10 - training_embed.py - training - Epoch: [0][500/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 504.4084 (510.8832)	
2019-01-08 11:30:33,775 - 10 - training_embed.py - training - Epoch: [0][600/2022]	Time 0.009 (0.004)	Data 0.001 (0.002)	Loss 504.5052 (510.8219)	
2019-01-08 11:30:34,192 - 10 - training_embed.py - training - Epoch: [0][700/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 516.9611 (510.7616)	
2019-01-08 11:30:34,545 - 10 - training_embed.py - training - Epoch: [0][800/2022]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 513.3622 (510.7451)	
2019-01-08 11:30:34,881 - 10 - training_embed.py - training - Epoch: [0][900/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 519.8738 (510.7317)	
2019-01-08 11:30:35,209 - 10 - training_embed.py - training - Epoch: [0][1000/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 511.9714 (510.7292)	
2019-01-08 11:30:35,546 - 10 - training_embed.py - training - Epoch: [0][1100/2022]	Time 0.008 (0.004)	Data 0.005 (0.002)	Loss 512.3251 (510.8042)	
2019-01-08 11:30:35,871 - 10 - training_embed.py - training - Epoch: [0][1200/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 512.2864 (510.8298)	
2019-01-08 11:30:36,208 - 10 - training_embed.py - training - Epoch: [0][1300/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 512.3159 (510.7348)	
2019-01-08 11:30:36,533 - 10 - training_embed.py - training - Epoch: [0][1400/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 521.3128 (510.7577)	
2019-01-08 11:30:36,872 - 10 - training_embed.py - training - Epoch: [0][1500/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 521.5649 (510.7516)	
2019-01-08 11:30:37,210 - 10 - training_embed.py - training - Epoch: [0][1600/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 510.8622 (510.7762)	
2019-01-08 11:30:37,545 - 10 - training_embed.py - training - Epoch: [0][1700/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 510.2507 (510.7640)	
2019-01-08 11:30:37,880 - 10 - training_embed.py - training - Epoch: [0][1800/2022]	Time 0.010 (0.004)	Data 0.009 (0.002)	Loss 502.9565 (510.7816)	
2019-01-08 11:30:38,222 - 10 - training_embed.py - training - Epoch: [0][1900/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 512.5522 (510.7784)	
2019-01-08 11:30:38,542 - 10 - training_embed.py - training - Epoch: [0][2000/2022]	Time 0.005 (0.004)	Data 0.001 (0.002)	Loss 514.9650 (510.7705)	
2019-01-08 11:30:38,601 - 10 - training_embed.py - training - loss: 510.702389
2019-01-08 11:30:38,602 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 11:30:39,180 - 10 - training_embed.py - training - Epoch: [1][100/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 514.5957 (511.0474)	
2019-01-08 11:30:39,536 - 10 - training_embed.py - training - Epoch: [1][200/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 514.3446 (510.9005)	
2019-01-08 11:30:39,928 - 10 - training_embed.py - training - Epoch: [1][300/2022]	Time 0.008 (0.004)	Data 0.002 (0.002)	Loss 511.9932 (510.6238)	
2019-01-08 11:30:40,288 - 10 - training_embed.py - training - Epoch: [1][400/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 508.4586 (510.4390)	
2019-01-08 11:30:40,650 - 10 - training_embed.py - training - Epoch: [1][500/2022]	Time 0.008 (0.004)	Data 0.006 (0.002)	Loss 505.7704 (510.2604)	
2019-01-08 11:30:40,987 - 10 - training_embed.py - training - Epoch: [1][600/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 517.2458 (510.4436)	
2019-01-08 11:30:41,329 - 10 - training_embed.py - training - Epoch: [1][700/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 507.7722 (510.5069)	
2019-01-08 11:30:41,735 - 10 - training_embed.py - training - Epoch: [1][800/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 514.0979 (510.5706)	
2019-01-08 11:30:42,099 - 10 - training_embed.py - training - Epoch: [1][900/2022]	Time 0.008 (0.004)	Data 0.006 (0.002)	Loss 507.0028 (510.5506)	
2019-01-08 11:30:42,456 - 10 - training_embed.py - training - Epoch: [1][1000/2022]	Time 0.005 (0.004)	Data 0.003 (0.002)	Loss 510.7370 (510.5240)	
2019-01-08 11:30:42,816 - 10 - training_embed.py - training - Epoch: [1][1100/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 506.2347 (510.5167)	
2019-01-08 11:30:43,161 - 10 - training_embed.py - training - Epoch: [1][1200/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 518.9890 (510.5270)	
2019-01-08 11:30:43,473 - 10 - training_embed.py - training - Epoch: [1][1300/2022]	Time 0.008 (0.004)	Data 0.003 (0.002)	Loss 510.6241 (510.5388)	
2019-01-08 11:30:43,813 - 10 - training_embed.py - training - Epoch: [1][1400/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 516.4919 (510.5847)	
2019-01-08 11:30:44,128 - 10 - training_embed.py - training - Epoch: [1][1500/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 513.5641 (510.5754)	
2019-01-08 11:30:44,496 - 10 - training_embed.py - training - Epoch: [1][1600/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 512.8290 (510.5743)	
2019-01-08 11:30:44,840 - 10 - training_embed.py - training - Epoch: [1][1700/2022]	Time 0.008 (0.004)	Data 0.007 (0.002)	Loss 505.6503 (510.5730)	
2019-01-08 11:30:45,181 - 10 - training_embed.py - training - Epoch: [1][1800/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 511.5186 (510.5652)	
2019-01-08 11:30:45,499 - 10 - training_embed.py - training - Epoch: [1][1900/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 516.7651 (510.5815)	
2019-01-08 11:30:45,849 - 10 - training_embed.py - training - Epoch: [1][2000/2022]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 512.5991 (510.5759)	
2019-01-08 11:30:45,928 - 10 - training_embed.py - training - loss: 510.521606
2019-01-08 11:30:45,929 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 11:30:46,523 - 10 - training_embed.py - training - Epoch: [2][100/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 511.6614 (510.1168)	
2019-01-08 11:30:46,895 - 10 - training_embed.py - training - Epoch: [2][200/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 514.6623 (510.1193)	
2019-01-08 11:30:47,247 - 10 - training_embed.py - training - Epoch: [2][300/2022]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 512.3492 (510.2567)	
2019-01-08 11:30:47,609 - 10 - training_embed.py - training - Epoch: [2][400/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 510.7006 (510.2261)	
2019-01-08 11:30:47,979 - 10 - training_embed.py - training - Epoch: [2][500/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 506.0525 (510.1831)	
2019-01-08 11:30:48,341 - 10 - training_embed.py - training - Epoch: [2][600/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 514.2442 (510.1913)	
2019-01-08 11:30:48,706 - 10 - training_embed.py - training - Epoch: [2][700/2022]	Time 0.008 (0.004)	Data 0.001 (0.002)	Loss 510.6546 (510.2309)	
2019-01-08 11:30:49,084 - 10 - training_embed.py - training - Epoch: [2][800/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 509.8007 (510.2799)	
2019-01-08 11:30:49,460 - 10 - training_embed.py - training - Epoch: [2][900/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 511.7552 (510.3090)	
2019-01-08 11:30:49,805 - 10 - training_embed.py - training - Epoch: [2][1000/2022]	Time 0.006 (0.004)	Data 0.001 (0.002)	Loss 505.7568 (510.2977)	
2019-01-08 11:30:50,166 - 10 - training_embed.py - training - Epoch: [2][1100/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 505.4763 (510.3079)	
2019-01-08 11:30:50,506 - 10 - training_embed.py - training - Epoch: [2][1200/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 511.7230 (510.3598)	
2019-01-08 11:30:50,864 - 10 - training_embed.py - training - Epoch: [2][1300/2022]	Time 0.009 (0.004)	Data 0.000 (0.002)	Loss 517.9154 (510.3681)	
2019-01-08 11:30:51,229 - 10 - training_embed.py - training - Epoch: [2][1400/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 510.1103 (510.4070)	
2019-01-08 11:30:51,572 - 10 - training_embed.py - training - Epoch: [2][1500/2022]	Time 0.008 (0.004)	Data 0.007 (0.002)	Loss 512.9444 (510.3725)	
2019-01-08 11:30:51,909 - 10 - training_embed.py - training - Epoch: [2][1600/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 510.7585 (510.3707)	
2019-01-08 11:30:52,249 - 10 - training_embed.py - training - Epoch: [2][1700/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 508.6226 (510.3813)	
2019-01-08 11:30:52,600 - 10 - training_embed.py - training - Epoch: [2][1800/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 508.8837 (510.4108)	
2019-01-08 11:30:52,954 - 10 - training_embed.py - training - Epoch: [2][1900/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 507.3897 (510.4123)	
2019-01-08 11:30:53,300 - 10 - training_embed.py - training - Epoch: [2][2000/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 512.4625 (510.4005)	
2019-01-08 11:30:53,363 - 10 - training_embed.py - training - loss: 510.341041
2019-01-08 11:30:53,363 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 11:30:53,911 - 10 - training_embed.py - training - Epoch: [3][100/2022]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 498.7001 (509.6258)	
2019-01-08 11:30:54,269 - 10 - training_embed.py - training - Epoch: [3][200/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 510.6202 (509.9578)	
2019-01-08 11:30:54,616 - 10 - training_embed.py - training - Epoch: [3][300/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 503.6456 (509.8777)	
2019-01-08 11:30:54,958 - 10 - training_embed.py - training - Epoch: [3][400/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 511.0598 (510.0119)	
2019-01-08 11:30:55,310 - 10 - training_embed.py - training - Epoch: [3][500/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 518.1857 (510.0205)	
2019-01-08 11:30:55,685 - 10 - training_embed.py - training - Epoch: [3][600/2022]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 510.9258 (510.0250)	
2019-01-08 11:30:56,053 - 10 - training_embed.py - training - Epoch: [3][700/2022]	Time 0.007 (0.004)	Data 0.005 (0.002)	Loss 507.8505 (509.9898)	
2019-01-08 11:30:56,424 - 10 - training_embed.py - training - Epoch: [3][800/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 512.3644 (510.0973)	
2019-01-08 11:30:56,826 - 10 - training_embed.py - training - Epoch: [3][900/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 507.0166 (510.1712)	
2019-01-08 11:30:57,226 - 10 - training_embed.py - training - Epoch: [3][1000/2022]	Time 0.003 (0.004)	Data 0.000 (0.002)	Loss 513.0478 (510.1505)	
2019-01-08 11:30:57,578 - 10 - training_embed.py - training - Epoch: [3][1100/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 511.3033 (510.2039)	
2019-01-08 11:30:57,901 - 10 - training_embed.py - training - Epoch: [3][1200/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 507.9091 (510.2368)	
2019-01-08 11:30:58,242 - 10 - training_embed.py - training - Epoch: [3][1300/2022]	Time 0.009 (0.004)	Data 0.001 (0.002)	Loss 511.1109 (510.2552)	
2019-01-08 11:30:58,564 - 10 - training_embed.py - training - Epoch: [3][1400/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 510.9323 (510.2364)	
2019-01-08 11:30:58,873 - 10 - training_embed.py - training - Epoch: [3][1500/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 513.1697 (510.2296)	
2019-01-08 11:30:59,200 - 10 - training_embed.py - training - Epoch: [3][1600/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 510.7785 (510.2304)	
2019-01-08 11:30:59,538 - 10 - training_embed.py - training - Epoch: [3][1700/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 508.0079 (510.2526)	
2019-01-08 11:30:59,888 - 10 - training_embed.py - training - Epoch: [3][1800/2022]	Time 0.009 (0.004)	Data 0.008 (0.002)	Loss 509.6053 (510.2518)	
2019-01-08 11:31:00,219 - 10 - training_embed.py - training - Epoch: [3][1900/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 511.5220 (510.2367)	
2019-01-08 11:31:00,542 - 10 - training_embed.py - training - Epoch: [3][2000/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 505.4798 (510.2209)	
2019-01-08 11:31:00,599 - 10 - training_embed.py - training - loss: 510.159685
2019-01-08 11:31:00,599 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 11:31:01,137 - 10 - training_embed.py - training - Epoch: [4][100/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 512.0768 (510.1440)	
2019-01-08 11:31:01,500 - 10 - training_embed.py - training - Epoch: [4][200/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 505.7731 (510.0389)	
2019-01-08 11:31:01,869 - 10 - training_embed.py - training - Epoch: [4][300/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 512.8621 (509.9176)	
2019-01-08 11:31:02,224 - 10 - training_embed.py - training - Epoch: [4][400/2022]	Time 0.005 (0.004)	Data 0.003 (0.002)	Loss 505.5911 (509.9914)	
2019-01-08 11:31:02,580 - 10 - training_embed.py - training - Epoch: [4][500/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 509.5019 (509.9612)	
2019-01-08 11:31:02,946 - 10 - training_embed.py - training - Epoch: [4][600/2022]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 511.3482 (509.9431)	
2019-01-08 11:31:03,286 - 10 - training_embed.py - training - Epoch: [4][700/2022]	Time 0.005 (0.004)	Data 0.004 (0.002)	Loss 504.7209 (509.8751)	
2019-01-08 11:31:03,655 - 10 - training_embed.py - training - Epoch: [4][800/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 506.4579 (509.8875)	
2019-01-08 11:31:04,038 - 10 - training_embed.py - training - Epoch: [4][900/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 510.7740 (509.9319)	
2019-01-08 11:31:04,407 - 10 - training_embed.py - training - Epoch: [4][1000/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 509.7650 (509.9902)	
2019-01-08 11:31:04,762 - 10 - training_embed.py - training - Epoch: [4][1100/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 508.9319 (509.9806)	
2019-01-08 11:31:05,109 - 10 - training_embed.py - training - Epoch: [4][1200/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 509.5480 (509.9929)	
2019-01-08 11:31:05,452 - 10 - training_embed.py - training - Epoch: [4][1300/2022]	Time 0.012 (0.004)	Data 0.011 (0.002)	Loss 512.6905 (510.0030)	
2019-01-08 11:31:05,777 - 10 - training_embed.py - training - Epoch: [4][1400/2022]	Time 0.004 (0.004)	Data 0.000 (0.002)	Loss 507.1516 (510.0080)	
2019-01-08 11:31:06,107 - 10 - training_embed.py - training - Epoch: [4][1500/2022]	Time 0.007 (0.004)	Data 0.006 (0.002)	Loss 514.8375 (510.0080)	
2019-01-08 11:31:06,445 - 10 - training_embed.py - training - Epoch: [4][1600/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 501.9117 (509.9885)	
2019-01-08 11:31:06,779 - 10 - training_embed.py - training - Epoch: [4][1700/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 512.3238 (510.0088)	
2019-01-08 11:31:07,107 - 10 - training_embed.py - training - Epoch: [4][1800/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 510.5577 (510.0325)	
2019-01-08 11:31:07,472 - 10 - training_embed.py - training - Epoch: [4][1900/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 512.6623 (510.0028)	
2019-01-08 11:31:07,805 - 10 - training_embed.py - training - Epoch: [4][2000/2022]	Time 0.014 (0.004)	Data 0.013 (0.002)	Loss 515.8598 (510.0324)	
2019-01-08 11:31:07,859 - 10 - training_embed.py - training - loss: 509.977672
2019-01-08 11:31:07,859 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 11:31:08,397 - 10 - training_embed.py - training - Epoch: [5][100/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 510.0176 (509.7672)	
2019-01-08 11:31:08,767 - 10 - training_embed.py - training - Epoch: [5][200/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 509.0863 (509.8523)	
2019-01-08 11:31:09,124 - 10 - training_embed.py - training - Epoch: [5][300/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 512.1533 (509.7885)	
2019-01-08 11:31:09,489 - 10 - training_embed.py - training - Epoch: [5][400/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 505.0031 (509.9085)	
2019-01-08 11:31:09,847 - 10 - training_embed.py - training - Epoch: [5][500/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 504.2745 (509.8521)	
2019-01-08 11:31:10,236 - 10 - training_embed.py - training - Epoch: [5][600/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 514.3542 (509.8095)	
2019-01-08 11:31:10,605 - 10 - training_embed.py - training - Epoch: [5][700/2022]	Time 0.002 (0.004)	Data 0.000 (0.002)	Loss 512.5998 (509.8412)	
2019-01-08 11:31:10,965 - 10 - training_embed.py - training - Epoch: [5][800/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 515.5610 (509.9299)	
2019-01-08 11:31:11,300 - 10 - training_embed.py - training - Epoch: [5][900/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 516.2368 (509.8849)	
2019-01-08 11:31:11,626 - 10 - training_embed.py - training - Epoch: [5][1000/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 515.4729 (509.9184)	
2019-01-08 11:31:11,979 - 10 - training_embed.py - training - Epoch: [5][1100/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 506.1806 (509.8582)	
2019-01-08 11:31:12,329 - 10 - training_embed.py - training - Epoch: [5][1200/2022]	Time 0.005 (0.004)	Data 0.003 (0.002)	Loss 509.8129 (509.8594)	
2019-01-08 11:31:12,673 - 10 - training_embed.py - training - Epoch: [5][1300/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 501.8953 (509.8853)	
2019-01-08 11:31:13,026 - 10 - training_embed.py - training - Epoch: [5][1400/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 508.2509 (509.8610)	
2019-01-08 11:31:13,368 - 10 - training_embed.py - training - Epoch: [5][1500/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 507.2039 (509.8712)	
2019-01-08 11:31:13,746 - 10 - training_embed.py - training - Epoch: [5][1600/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 508.3705 (509.8669)	
2019-01-08 11:31:14,104 - 10 - training_embed.py - training - Epoch: [5][1700/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 507.3472 (509.8995)	
2019-01-08 11:31:14,449 - 10 - training_embed.py - training - Epoch: [5][1800/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 514.8013 (509.9070)	
2019-01-08 11:31:14,785 - 10 - training_embed.py - training - Epoch: [5][1900/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 510.7008 (509.8755)	
2019-01-08 11:31:15,116 - 10 - training_embed.py - training - Epoch: [5][2000/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 516.9588 (509.8596)	
2019-01-08 11:31:15,179 - 10 - training_embed.py - training - loss: 509.794841
2019-01-08 11:31:15,179 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 11:31:15,758 - 10 - training_embed.py - training - Epoch: [6][100/2022]	Time 0.006 (0.004)	Data 0.001 (0.002)	Loss 506.3453 (509.1711)	
2019-01-08 11:31:16,108 - 10 - training_embed.py - training - Epoch: [6][200/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 502.6788 (509.4377)	
2019-01-08 11:31:16,497 - 10 - training_embed.py - training - Epoch: [6][300/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 510.9939 (509.5927)	
2019-01-08 11:31:16,859 - 10 - training_embed.py - training - Epoch: [6][400/2022]	Time 0.006 (0.004)	Data 0.000 (0.002)	Loss 510.4346 (509.4800)	
2019-01-08 11:31:17,212 - 10 - training_embed.py - training - Epoch: [6][500/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 509.6813 (509.4458)	
2019-01-08 11:31:17,575 - 10 - training_embed.py - training - Epoch: [6][600/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 514.0607 (509.5321)	
2019-01-08 11:31:17,942 - 10 - training_embed.py - training - Epoch: [6][700/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 507.8484 (509.5563)	
2019-01-08 11:31:18,282 - 10 - training_embed.py - training - Epoch: [6][800/2022]	Time 0.005 (0.004)	Data 0.002 (0.002)	Loss 513.6824 (509.5689)	
2019-01-08 11:31:18,628 - 10 - training_embed.py - training - Epoch: [6][900/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 515.3229 (509.6418)	
2019-01-08 11:31:18,949 - 10 - training_embed.py - training - Epoch: [6][1000/2022]	Time 0.009 (0.004)	Data 0.007 (0.002)	Loss 507.4564 (509.6528)	
2019-01-08 11:31:19,290 - 10 - training_embed.py - training - Epoch: [6][1100/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 504.5224 (509.6733)	
2019-01-08 11:31:19,640 - 10 - training_embed.py - training - Epoch: [6][1200/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 517.2936 (509.6459)	
2019-01-08 11:31:19,961 - 10 - training_embed.py - training - Epoch: [6][1300/2022]	Time 0.007 (0.004)	Data 0.006 (0.002)	Loss 508.4653 (509.6790)	
2019-01-08 11:31:20,296 - 10 - training_embed.py - training - Epoch: [6][1400/2022]	Time 0.016 (0.004)	Data 0.015 (0.002)	Loss 511.2671 (509.6505)	
2019-01-08 11:31:20,656 - 10 - training_embed.py - training - Epoch: [6][1500/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 508.5018 (509.6757)	
2019-01-08 11:31:20,990 - 10 - training_embed.py - training - Epoch: [6][1600/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 513.4889 (509.6684)	
2019-01-08 11:31:21,339 - 10 - training_embed.py - training - Epoch: [6][1700/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 513.1635 (509.6507)	
2019-01-08 11:31:21,676 - 10 - training_embed.py - training - Epoch: [6][1800/2022]	Time 0.005 (0.004)	Data 0.004 (0.002)	Loss 510.8591 (509.6561)	
2019-01-08 11:31:22,020 - 10 - training_embed.py - training - Epoch: [6][1900/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 512.9583 (509.6651)	
2019-01-08 11:31:22,361 - 10 - training_embed.py - training - Epoch: [6][2000/2022]	Time 0.003 (0.004)	Data 0.000 (0.002)	Loss 510.6979 (509.6566)	
2019-01-08 11:31:22,427 - 10 - training_embed.py - training - loss: 509.611670
2019-01-08 11:31:22,427 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 11:31:22,986 - 10 - training_embed.py - training - Epoch: [7][100/2022]	Time 0.004 (0.004)	Data 0.001 (0.002)	Loss 512.4392 (509.5466)	
2019-01-08 11:31:23,340 - 10 - training_embed.py - training - Epoch: [7][200/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 510.4351 (509.6306)	
2019-01-08 11:31:23,705 - 10 - training_embed.py - training - Epoch: [7][300/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 508.6771 (509.6783)	
2019-01-08 11:31:24,097 - 10 - training_embed.py - training - Epoch: [7][400/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 506.4530 (509.6182)	
2019-01-08 11:31:24,438 - 10 - training_embed.py - training - Epoch: [7][500/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 508.7368 (509.4944)	
2019-01-08 11:31:24,781 - 10 - training_embed.py - training - Epoch: [7][600/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 507.2570 (509.4658)	
2019-01-08 11:31:25,170 - 10 - training_embed.py - training - Epoch: [7][700/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 501.1206 (509.4631)	
2019-01-08 11:31:25,563 - 10 - training_embed.py - training - Epoch: [7][800/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 511.7880 (509.4332)	
2019-01-08 11:31:25,930 - 10 - training_embed.py - training - Epoch: [7][900/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 508.5997 (509.4553)	
2019-01-08 11:31:26,292 - 10 - training_embed.py - training - Epoch: [7][1000/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 509.2844 (509.4646)	
2019-01-08 11:31:26,639 - 10 - training_embed.py - training - Epoch: [7][1100/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 509.1614 (509.4464)	
2019-01-08 11:31:27,003 - 10 - training_embed.py - training - Epoch: [7][1200/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 516.4730 (509.4373)	
2019-01-08 11:31:27,346 - 10 - training_embed.py - training - Epoch: [7][1300/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 501.5339 (509.4013)	
2019-01-08 11:31:27,694 - 10 - training_embed.py - training - Epoch: [7][1400/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 507.8087 (509.4437)	
2019-01-08 11:31:27,996 - 10 - training_embed.py - training - Epoch: [7][1500/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 508.7159 (509.4465)	
2019-01-08 11:31:28,334 - 10 - training_embed.py - training - Epoch: [7][1600/2022]	Time 0.010 (0.004)	Data 0.009 (0.002)	Loss 512.7595 (509.4735)	
2019-01-08 11:31:28,693 - 10 - training_embed.py - training - Epoch: [7][1700/2022]	Time 0.006 (0.004)	Data 0.004 (0.002)	Loss 507.2684 (509.5104)	
2019-01-08 11:31:29,051 - 10 - training_embed.py - training - Epoch: [7][1800/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 510.5366 (509.5029)	
2019-01-08 11:31:29,393 - 10 - training_embed.py - training - Epoch: [7][1900/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 510.0262 (509.5004)	
2019-01-08 11:31:29,739 - 10 - training_embed.py - training - Epoch: [7][2000/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 507.3853 (509.4904)	
2019-01-08 11:31:29,796 - 10 - training_embed.py - training - loss: 509.428890
2019-01-08 11:31:29,796 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 11:31:30,345 - 10 - training_embed.py - training - Epoch: [8][100/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 502.6524 (509.5545)	
2019-01-08 11:31:30,721 - 10 - training_embed.py - training - Epoch: [8][200/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 512.3820 (509.2709)	
2019-01-08 11:31:31,087 - 10 - training_embed.py - training - Epoch: [8][300/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 507.8051 (509.2496)	
2019-01-08 11:31:31,454 - 10 - training_embed.py - training - Epoch: [8][400/2022]	Time 0.005 (0.004)	Data 0.004 (0.002)	Loss 511.2556 (509.3564)	
2019-01-08 11:31:31,824 - 10 - training_embed.py - training - Epoch: [8][500/2022]	Time 0.009 (0.004)	Data 0.008 (0.002)	Loss 510.0495 (509.3726)	
2019-01-08 11:31:32,194 - 10 - training_embed.py - training - Epoch: [8][600/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 508.5754 (509.4497)	
2019-01-08 11:31:32,573 - 10 - training_embed.py - training - Epoch: [8][700/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 510.0853 (509.4663)	
2019-01-08 11:31:32,920 - 10 - training_embed.py - training - Epoch: [8][800/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 511.6962 (509.4618)	
2019-01-08 11:31:33,275 - 10 - training_embed.py - training - Epoch: [8][900/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 507.6909 (509.4028)	
2019-01-08 11:31:33,629 - 10 - training_embed.py - training - Epoch: [8][1000/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 506.1266 (509.2853)	
2019-01-08 11:31:33,961 - 10 - training_embed.py - training - Epoch: [8][1100/2022]	Time 0.007 (0.004)	Data 0.005 (0.002)	Loss 516.6130 (509.2799)	
2019-01-08 11:31:34,296 - 10 - training_embed.py - training - Epoch: [8][1200/2022]	Time 0.010 (0.004)	Data 0.009 (0.002)	Loss 510.3788 (509.2934)	
2019-01-08 11:31:34,642 - 10 - training_embed.py - training - Epoch: [8][1300/2022]	Time 0.009 (0.004)	Data 0.007 (0.002)	Loss 511.3571 (509.2907)	
2019-01-08 11:31:34,973 - 10 - training_embed.py - training - Epoch: [8][1400/2022]	Time 0.009 (0.004)	Data 0.008 (0.002)	Loss 512.4513 (509.2879)	
2019-01-08 11:31:35,317 - 10 - training_embed.py - training - Epoch: [8][1500/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 512.4239 (509.2663)	
2019-01-08 11:31:35,648 - 10 - training_embed.py - training - Epoch: [8][1600/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 507.0708 (509.2700)	
2019-01-08 11:31:36,011 - 10 - training_embed.py - training - Epoch: [8][1700/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 507.1932 (509.2603)	
2019-01-08 11:31:36,361 - 10 - training_embed.py - training - Epoch: [8][1800/2022]	Time 0.008 (0.004)	Data 0.000 (0.002)	Loss 497.4335 (509.2991)	
2019-01-08 11:31:36,708 - 10 - training_embed.py - training - Epoch: [8][1900/2022]	Time 0.004 (0.004)	Data 0.002 (0.002)	Loss 514.2950 (509.3064)	
2019-01-08 11:31:37,043 - 10 - training_embed.py - training - Epoch: [8][2000/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 507.2739 (509.3174)	
2019-01-08 11:31:37,109 - 10 - training_embed.py - training - loss: 509.246660
2019-01-08 11:31:37,109 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 11:31:37,649 - 10 - training_embed.py - training - Epoch: [9][100/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 517.6257 (509.1692)	
2019-01-08 11:31:38,024 - 10 - training_embed.py - training - Epoch: [9][200/2022]	Time 0.005 (0.004)	Data 0.004 (0.002)	Loss 514.4562 (508.7269)	
2019-01-08 11:31:38,381 - 10 - training_embed.py - training - Epoch: [9][300/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 509.5082 (508.9825)	
2019-01-08 11:31:38,739 - 10 - training_embed.py - training - Epoch: [9][400/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 511.9338 (509.1108)	
2019-01-08 11:31:39,098 - 10 - training_embed.py - training - Epoch: [9][500/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 508.3148 (509.1185)	
2019-01-08 11:31:39,482 - 10 - training_embed.py - training - Epoch: [9][600/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 511.9203 (509.0381)	
2019-01-08 11:31:39,844 - 10 - training_embed.py - training - Epoch: [9][700/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 510.9341 (509.0947)	
2019-01-08 11:31:40,221 - 10 - training_embed.py - training - Epoch: [9][800/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 506.5964 (509.0810)	
2019-01-08 11:31:40,613 - 10 - training_embed.py - training - Epoch: [9][900/2022]	Time 0.006 (0.004)	Data 0.005 (0.002)	Loss 513.1976 (509.1098)	
2019-01-08 11:31:40,938 - 10 - training_embed.py - training - Epoch: [9][1000/2022]	Time 0.009 (0.004)	Data 0.001 (0.002)	Loss 510.4820 (509.1683)	
2019-01-08 11:31:41,279 - 10 - training_embed.py - training - Epoch: [9][1100/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 508.8258 (509.1436)	
2019-01-08 11:31:41,626 - 10 - training_embed.py - training - Epoch: [9][1200/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 513.4796 (509.1335)	
2019-01-08 11:31:41,970 - 10 - training_embed.py - training - Epoch: [9][1300/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 518.9677 (509.1369)	
2019-01-08 11:31:42,305 - 10 - training_embed.py - training - Epoch: [9][1400/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 509.6412 (509.0907)	
2019-01-08 11:31:42,676 - 10 - training_embed.py - training - Epoch: [9][1500/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 508.1394 (509.1298)	
2019-01-08 11:31:43,005 - 10 - training_embed.py - training - Epoch: [9][1600/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 510.3483 (509.1385)	
2019-01-08 11:31:43,327 - 10 - training_embed.py - training - Epoch: [9][1700/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 508.0743 (509.1674)	
2019-01-08 11:31:43,697 - 10 - training_embed.py - training - Epoch: [9][1800/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 513.9854 (509.1357)	
2019-01-08 11:31:44,034 - 10 - training_embed.py - training - Epoch: [9][1900/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 506.8297 (509.1309)	
2019-01-08 11:31:44,380 - 10 - training_embed.py - training - Epoch: [9][2000/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 515.4857 (509.1320)	
2019-01-08 11:31:44,447 - 10 - training_embed.py - training - loss: 509.062314
2019-01-08 11:31:44,447 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 11:31:45,016 - 10 - training_embed.py - training - Epoch: [10][100/2022]	Time 0.004 (0.004)	Data 0.001 (0.002)	Loss 509.7911 (508.7812)	
2019-01-08 11:31:45,374 - 10 - training_embed.py - training - Epoch: [10][200/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 520.5401 (508.8540)	
2019-01-08 11:31:45,748 - 10 - training_embed.py - training - Epoch: [10][300/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 508.6744 (509.0480)	
2019-01-08 11:31:46,122 - 10 - training_embed.py - training - Epoch: [10][400/2022]	Time 0.008 (0.004)	Data 0.000 (0.002)	Loss 506.6428 (509.0605)	
2019-01-08 11:31:46,479 - 10 - training_embed.py - training - Epoch: [10][500/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 508.1612 (509.0025)	
2019-01-08 11:31:46,830 - 10 - training_embed.py - training - Epoch: [10][600/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 515.6934 (509.0684)	
2019-01-08 11:31:47,181 - 10 - training_embed.py - training - Epoch: [10][700/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 510.4932 (509.0737)	
2019-01-08 11:31:47,525 - 10 - training_embed.py - training - Epoch: [10][800/2022]	Time 0.008 (0.004)	Data 0.001 (0.002)	Loss 510.4367 (508.9886)	
2019-01-08 11:31:47,897 - 10 - training_embed.py - training - Epoch: [10][900/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 511.0300 (508.9855)	
2019-01-08 11:31:48,232 - 10 - training_embed.py - training - Epoch: [10][1000/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 504.7475 (508.9173)	
2019-01-08 11:31:48,565 - 10 - training_embed.py - training - Epoch: [10][1100/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 505.2462 (508.8619)	
2019-01-08 11:31:48,932 - 10 - training_embed.py - training - Epoch: [10][1200/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 507.2263 (508.8884)	
2019-01-08 11:31:49,293 - 10 - training_embed.py - training - Epoch: [10][1300/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 512.6772 (508.8781)	
2019-01-08 11:31:49,642 - 10 - training_embed.py - training - Epoch: [10][1400/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 508.8212 (508.8898)	
2019-01-08 11:31:49,975 - 10 - training_embed.py - training - Epoch: [10][1500/2022]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 508.3526 (508.9328)	
2019-01-08 11:31:50,310 - 10 - training_embed.py - training - Epoch: [10][1600/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 508.6420 (508.9125)	
2019-01-08 11:31:50,643 - 10 - training_embed.py - training - Epoch: [10][1700/2022]	Time 0.004 (0.004)	Data 0.003 (0.002)	Loss 510.9377 (508.9181)	
2019-01-08 11:31:50,996 - 10 - training_embed.py - training - Epoch: [10][1800/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 511.6437 (508.9772)	
2019-01-08 11:31:51,332 - 10 - training_embed.py - training - Epoch: [10][1900/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 509.1771 (508.9674)	
2019-01-08 11:31:51,670 - 10 - training_embed.py - training - Epoch: [10][2000/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 507.8285 (508.9442)	
2019-01-08 11:31:51,747 - 10 - training_embed.py - training - loss: 508.877552
2019-01-08 11:31:51,748 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 11:31:52,307 - 10 - training_embed.py - training - Epoch: [11][100/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 509.3958 (509.1829)	
2019-01-08 11:31:52,656 - 10 - training_embed.py - training - Epoch: [11][200/2022]	Time 0.006 (0.004)	Data 0.005 (0.002)	Loss 515.7582 (509.0739)	
2019-01-08 11:31:53,007 - 10 - training_embed.py - training - Epoch: [11][300/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 512.6445 (509.1201)	
2019-01-08 11:31:53,381 - 10 - training_embed.py - training - Epoch: [11][400/2022]	Time 0.008 (0.004)	Data 0.007 (0.002)	Loss 509.5752 (508.9872)	
2019-01-08 11:31:53,717 - 10 - training_embed.py - training - Epoch: [11][500/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 506.4144 (508.8486)	
2019-01-08 11:31:54,061 - 10 - training_embed.py - training - Epoch: [11][600/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 511.8491 (508.8296)	
2019-01-08 11:31:54,422 - 10 - training_embed.py - training - Epoch: [11][700/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 503.5891 (508.7730)	
2019-01-08 11:31:54,819 - 10 - training_embed.py - training - Epoch: [11][800/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 509.5325 (508.7521)	
2019-01-08 11:31:55,186 - 10 - training_embed.py - training - Epoch: [11][900/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 501.2026 (508.6863)	
2019-01-08 11:31:55,535 - 10 - training_embed.py - training - Epoch: [11][1000/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 512.7530 (508.7958)	
2019-01-08 11:31:55,877 - 10 - training_embed.py - training - Epoch: [11][1100/2022]	Time 0.007 (0.004)	Data 0.001 (0.002)	Loss 508.0632 (508.7107)	
2019-01-08 11:31:56,240 - 10 - training_embed.py - training - Epoch: [11][1200/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 504.0098 (508.7298)	
2019-01-08 11:31:56,591 - 10 - training_embed.py - training - Epoch: [11][1300/2022]	Time 0.008 (0.004)	Data 0.007 (0.002)	Loss 504.7943 (508.7284)	
2019-01-08 11:31:56,970 - 10 - training_embed.py - training - Epoch: [11][1400/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 504.7111 (508.7657)	
2019-01-08 11:31:57,343 - 10 - training_embed.py - training - Epoch: [11][1500/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 503.4590 (508.7600)	
2019-01-08 11:31:57,703 - 10 - training_embed.py - training - Epoch: [11][1600/2022]	Time 0.006 (0.004)	Data 0.004 (0.002)	Loss 506.8395 (508.7652)	
2019-01-08 11:31:58,026 - 10 - training_embed.py - training - Epoch: [11][1700/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 505.7430 (508.7751)	
2019-01-08 11:31:58,357 - 10 - training_embed.py - training - Epoch: [11][1800/2022]	Time 0.012 (0.004)	Data 0.010 (0.002)	Loss 509.9782 (508.7627)	
2019-01-08 11:31:58,702 - 10 - training_embed.py - training - Epoch: [11][1900/2022]	Time 0.003 (0.004)	Data 0.001 (0.002)	Loss 505.3960 (508.7334)	
2019-01-08 11:31:59,044 - 10 - training_embed.py - training - Epoch: [11][2000/2022]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 502.6508 (508.7557)	
2019-01-08 11:31:59,108 - 10 - training_embed.py - training - loss: 508.694080
2019-01-08 11:31:59,206 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 11:32:00,296 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1089.339 ms ~ 0.018 min ~ 1.089 sec
2019-01-08 11:32:01,634 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 2428.291 ms ~ 0.040 min ~ 2.428 sec
2019-01-08 11:32:01,635 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 11:32:01,635 - 10 - corpus.py - subactivity_sampler - 0 / 166
2019-01-08 11:32:01,636 - 10 - corpus.py - subactivity_sampler - [47125. 47107. 47098. 47084. 47061. 47043. 47026. 47004. 46984. 46980.
 46966.]
2019-01-08 11:33:34,646 - 10 - corpus.py - subactivity_sampler - 20 / 166
2019-01-08 11:33:34,646 - 10 - corpus.py - subactivity_sampler - [47885. 46139. 46863. 47051. 46925. 47500. 46715. 47334. 46908. 47086.
 47072.]
2019-01-08 11:34:53,109 - 10 - corpus.py - subactivity_sampler - 40 / 166
2019-01-08 11:34:53,110 - 10 - corpus.py - subactivity_sampler - [48085. 46215. 45785. 46993. 46629. 48271. 45971. 48285. 47180. 46637.
 47427.]
2019-01-08 11:36:44,857 - 10 - corpus.py - subactivity_sampler - 60 / 166
2019-01-08 11:36:44,858 - 10 - corpus.py - subactivity_sampler - [48667. 45496. 45092. 47195. 46207. 49535. 44774. 49729. 46486. 46698.
 47599.]
2019-01-08 11:38:25,745 - 10 - corpus.py - subactivity_sampler - 80 / 166
2019-01-08 11:38:25,746 - 10 - corpus.py - subactivity_sampler - [49726. 44551. 44090. 47439. 45818. 50679. 43483. 50933. 46556. 46574.
 47629.]
2019-01-08 11:39:37,271 - 10 - corpus.py - subactivity_sampler - 100 / 166
2019-01-08 11:39:37,271 - 10 - corpus.py - subactivity_sampler - [50888. 43453. 42994. 48145. 45079. 51525. 41639. 53507. 45994. 46612.
 47642.]
2019-01-08 11:41:04,310 - 10 - corpus.py - subactivity_sampler - 120 / 166
2019-01-08 11:41:04,310 - 10 - corpus.py - subactivity_sampler - [54390. 41265. 40992. 48247. 44371. 53464. 39440. 55523. 44871. 46631.
 48284.]
2019-01-08 11:42:28,195 - 10 - corpus.py - subactivity_sampler - 140 / 166
2019-01-08 11:42:28,195 - 10 - corpus.py - subactivity_sampler - [55378. 39899. 39917. 49516. 43464. 55177. 37140. 58797. 42808. 46717.
 48665.]
2019-01-08 11:43:49,209 - 10 - corpus.py - subactivity_sampler - 160 / 166
2019-01-08 11:43:49,209 - 10 - corpus.py - subactivity_sampler - [58185. 37984. 38178. 50535. 41927. 58578. 33412. 62323. 40572. 47013.
 48771.]
2019-01-08 11:44:07,778 - 10 - corpus.py - subactivity_sampler - [58651. 37381. 37949. 50832. 41344. 59859. 32366. 63265. 39589. 47158.
 49084.]
2019-01-08 11:44:07,778 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 726143.193 ms ~ 12.102 min ~ 726.143 sec
2019-01-08 11:44:07,778 - 10 - corpus.py - ordering_sampler - .
2019-01-08 11:44:12,571 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 11:44:12,571 - 10 - corpus.py - ordering_sampler - inv_count_vec: [10.  0.  0.  1. 36.  0.  4.  9.  7.  0.]
2019-01-08 11:44:12,572 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 11:44:12,608 - 10 - corpus.py - rho_sampling - ['52.7045', '47.0072', '928.1069', '87.0123', '8.6855', '162.1174', '17.8479', '49.3512', '16.0249', '4.7672']
2019-01-08 11:44:12,608 - 10 - pipeline.py - baseline - Iteration 1
2019-01-08 11:44:12,800 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 11:44:12,810 - 10 - accuracy_class.py - mof - # gt_labels: 12   # pr_labels: 11
2019-01-08 11:44:12,810 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 11', '1: 4', '2: 42', '3: 17', '4: 10', '5: 43', '6: 12', '7: 44', '8: 16', '9: 14', '10: 15']
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_val - frames true: 151352	frames overall : 517478
2019-01-08 11:44:12,835 - 10 - corpus.py - accuracy_corpus - Action: scrambledegg
2019-01-08 11:44:12,835 - 10 - corpus.py - accuracy_corpus - MoF val: 0.29248006678544786
2019-01-08 11:44:12,835 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.24298231035908774
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 29280
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_classes - label 4: 0.066502  81 / 1218
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_classes - label 10: 0.100686  1101 / 10935
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_classes - label 11: 0.354974  19952 / 56207
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_classes - label 12: 0.115773  4658 / 40234
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_classes - label 14: 0.451422  7950 / 17611
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_classes - label 15: 0.697357  30312 / 43467
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 1003
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_classes - label 17: 0.235943  11644 / 49351
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_classes - label 42: 0.362476  11646 / 32129
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_classes - label 43: 0.318449  7317 / 22977
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_classes - label 44: 0.266072  56691 / 213066
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - mof_classes - average class mof: 0.247471
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 29280
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - iou_classes - label 4: 0.002103  81 / 38518
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - iou_classes - label 10: 0.021513  1101 / 51178
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - iou_classes - label 11: 0.210229  19952 / 94906
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - iou_classes - label 12: 0.068558  4658 / 67942
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - iou_classes - label 14: 0.139918  7950 / 56819
2019-01-08 11:44:12,835 - 10 - accuracy_class.py - iou_classes - label 15: 0.487026  30312 / 62239
2019-01-08 11:44:12,836 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 40592
2019-01-08 11:44:12,836 - 10 - accuracy_class.py - iou_classes - label 17: 0.131513  11644 / 88539
2019-01-08 11:44:12,836 - 10 - accuracy_class.py - iou_classes - label 42: 0.199309  11646 / 58432
2019-01-08 11:44:12,836 - 10 - accuracy_class.py - iou_classes - label 43: 0.096890  7317 / 75519
2019-01-08 11:44:12,836 - 10 - accuracy_class.py - iou_classes - label 44: 0.258109  56691 / 219640
2019-01-08 11:44:12,836 - 10 - accuracy_class.py - iou_classes - average IoU: 0.146833
2019-01-08 11:44:12,836 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.134597
2019-01-08 11:44:20,537 - 10 - f1_score.py - f1 - f1 score: 0.258427
2019-01-08 11:44:20,557 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 7948.864 ms ~ 0.132 min ~ 7.949 sec
2019-01-08 11:44:20,558 - 10 - corpus.py - embedding_training - .
2019-01-08 11:44:20,558 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 11:44:20,558 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 11:44:20,558 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 11:44:23,763 - 10 - training_embed.py - training - create model
2019-01-08 11:44:23,763 - 10 - training_embed.py - training - epochs: 12
2019-01-08 11:44:23,764 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 11:44:24,118 - 10 - training_embed.py - training - Epoch: [0][100/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 506.1355 (508.3702)	
2019-01-08 11:44:24,329 - 10 - training_embed.py - training - Epoch: [0][200/2022]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 507.3949 (508.5902)	
2019-01-08 11:44:24,561 - 10 - training_embed.py - training - Epoch: [0][300/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 510.2003 (508.3499)	
2019-01-08 11:44:24,767 - 10 - training_embed.py - training - Epoch: [0][400/2022]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 513.5390 (508.2526)	
2019-01-08 11:44:24,976 - 10 - training_embed.py - training - Epoch: [0][500/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 497.9283 (508.3381)	
2019-01-08 11:44:25,178 - 10 - training_embed.py - training - Epoch: [0][600/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 508.3973 (508.1715)	
2019-01-08 11:44:25,403 - 10 - training_embed.py - training - Epoch: [0][700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 514.0036 (508.1180)	
2019-01-08 11:44:25,635 - 10 - training_embed.py - training - Epoch: [0][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 509.3589 (508.1201)	
2019-01-08 11:44:25,866 - 10 - training_embed.py - training - Epoch: [0][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 515.5063 (508.1116)	
2019-01-08 11:44:26,094 - 10 - training_embed.py - training - Epoch: [0][1000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 511.3992 (508.1421)	
2019-01-08 11:44:26,321 - 10 - training_embed.py - training - Epoch: [0][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 510.3818 (508.1712)	
2019-01-08 11:44:26,525 - 10 - training_embed.py - training - Epoch: [0][1200/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 511.5234 (508.1714)	
2019-01-08 11:44:26,739 - 10 - training_embed.py - training - Epoch: [0][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 512.2954 (508.1105)	
2019-01-08 11:44:26,954 - 10 - training_embed.py - training - Epoch: [0][1400/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 512.7162 (508.1169)	
2019-01-08 11:44:27,174 - 10 - training_embed.py - training - Epoch: [0][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 519.0132 (508.1221)	
2019-01-08 11:44:27,415 - 10 - training_embed.py - training - Epoch: [0][1600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 507.0569 (508.1196)	
2019-01-08 11:44:27,651 - 10 - training_embed.py - training - Epoch: [0][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.8043 (508.1087)	
2019-01-08 11:44:27,875 - 10 - training_embed.py - training - Epoch: [0][1800/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 500.4233 (508.1185)	
2019-01-08 11:44:28,088 - 10 - training_embed.py - training - Epoch: [0][1900/2022]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 509.0835 (508.1123)	
2019-01-08 11:44:28,311 - 10 - training_embed.py - training - Epoch: [0][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 514.1278 (508.1050)	
2019-01-08 11:44:28,358 - 10 - training_embed.py - training - loss: 508.031686
2019-01-08 11:44:28,358 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 11:44:28,766 - 10 - training_embed.py - training - Epoch: [1][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 510.1570 (508.0953)	
2019-01-08 11:44:28,989 - 10 - training_embed.py - training - Epoch: [1][200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 513.9156 (508.1549)	
2019-01-08 11:44:29,201 - 10 - training_embed.py - training - Epoch: [1][300/2022]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 509.6555 (507.9406)	
2019-01-08 11:44:29,434 - 10 - training_embed.py - training - Epoch: [1][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.4283 (507.8293)	
2019-01-08 11:44:29,653 - 10 - training_embed.py - training - Epoch: [1][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.7400 (507.6420)	
2019-01-08 11:44:29,878 - 10 - training_embed.py - training - Epoch: [1][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 513.7490 (507.7969)	
2019-01-08 11:44:30,109 - 10 - training_embed.py - training - Epoch: [1][700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 505.2875 (507.8532)	
2019-01-08 11:44:30,312 - 10 - training_embed.py - training - Epoch: [1][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 512.5615 (507.8876)	
2019-01-08 11:44:30,533 - 10 - training_embed.py - training - Epoch: [1][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 506.1533 (507.8624)	
2019-01-08 11:44:30,759 - 10 - training_embed.py - training - Epoch: [1][1000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 509.0655 (507.8136)	
2019-01-08 11:44:30,984 - 10 - training_embed.py - training - Epoch: [1][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 504.9589 (507.7947)	
2019-01-08 11:44:31,205 - 10 - training_embed.py - training - Epoch: [1][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 511.8654 (507.7886)	
2019-01-08 11:44:31,422 - 10 - training_embed.py - training - Epoch: [1][1300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 507.5694 (507.7601)	
2019-01-08 11:44:31,638 - 10 - training_embed.py - training - Epoch: [1][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 511.8589 (507.7728)	
2019-01-08 11:44:31,857 - 10 - training_embed.py - training - Epoch: [1][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 513.6599 (507.7410)	
2019-01-08 11:44:32,081 - 10 - training_embed.py - training - Epoch: [1][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 508.6053 (507.7324)	
2019-01-08 11:44:32,302 - 10 - training_embed.py - training - Epoch: [1][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.9625 (507.7200)	
2019-01-08 11:44:32,531 - 10 - training_embed.py - training - Epoch: [1][1800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 508.1564 (507.7212)	
2019-01-08 11:44:32,759 - 10 - training_embed.py - training - Epoch: [1][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 511.3635 (507.7231)	
2019-01-08 11:44:32,984 - 10 - training_embed.py - training - Epoch: [1][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 510.6252 (507.7157)	
2019-01-08 11:44:33,030 - 10 - training_embed.py - training - loss: 507.651032
2019-01-08 11:44:33,031 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 11:44:33,445 - 10 - training_embed.py - training - Epoch: [2][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 513.0595 (507.2394)	
2019-01-08 11:44:33,656 - 10 - training_embed.py - training - Epoch: [2][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 516.9095 (507.1399)	
2019-01-08 11:44:33,881 - 10 - training_embed.py - training - Epoch: [2][300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 508.3096 (507.3787)	
2019-01-08 11:44:34,122 - 10 - training_embed.py - training - Epoch: [2][400/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 501.9940 (507.3572)	
2019-01-08 11:44:34,351 - 10 - training_embed.py - training - Epoch: [2][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.3655 (507.2862)	
2019-01-08 11:44:34,566 - 10 - training_embed.py - training - Epoch: [2][600/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 508.6217 (507.3452)	
2019-01-08 11:44:34,785 - 10 - training_embed.py - training - Epoch: [2][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 506.0848 (507.3695)	
2019-01-08 11:44:35,011 - 10 - training_embed.py - training - Epoch: [2][800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 506.6517 (507.3742)	
2019-01-08 11:44:35,245 - 10 - training_embed.py - training - Epoch: [2][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 511.2070 (507.3102)	
2019-01-08 11:44:35,474 - 10 - training_embed.py - training - Epoch: [2][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.6382 (507.3113)	
2019-01-08 11:44:35,697 - 10 - training_embed.py - training - Epoch: [2][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 503.9938 (507.3169)	
2019-01-08 11:44:35,921 - 10 - training_embed.py - training - Epoch: [2][1200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 508.7259 (507.3307)	
2019-01-08 11:44:36,143 - 10 - training_embed.py - training - Epoch: [2][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 510.9072 (507.3382)	
2019-01-08 11:44:36,356 - 10 - training_embed.py - training - Epoch: [2][1400/2022]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 505.5807 (507.3518)	
2019-01-08 11:44:36,570 - 10 - training_embed.py - training - Epoch: [2][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 511.4964 (507.3530)	
2019-01-08 11:44:36,792 - 10 - training_embed.py - training - Epoch: [2][1600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 509.1616 (507.3203)	
2019-01-08 11:44:37,003 - 10 - training_embed.py - training - Epoch: [2][1700/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 502.8408 (507.3129)	
2019-01-08 11:44:37,224 - 10 - training_embed.py - training - Epoch: [2][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 504.1749 (507.3360)	
2019-01-08 11:44:37,454 - 10 - training_embed.py - training - Epoch: [2][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.1140 (507.3406)	
2019-01-08 11:44:37,669 - 10 - training_embed.py - training - Epoch: [2][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 509.0440 (507.3289)	
2019-01-08 11:44:37,717 - 10 - training_embed.py - training - loss: 507.269688
2019-01-08 11:44:37,717 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 11:44:38,126 - 10 - training_embed.py - training - Epoch: [3][100/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 498.7427 (506.9464)	
2019-01-08 11:44:38,352 - 10 - training_embed.py - training - Epoch: [3][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.2267 (506.9086)	
2019-01-08 11:44:38,555 - 10 - training_embed.py - training - Epoch: [3][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.0815 (506.7955)	
2019-01-08 11:44:38,791 - 10 - training_embed.py - training - Epoch: [3][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 506.5405 (506.9593)	
2019-01-08 11:44:39,007 - 10 - training_embed.py - training - Epoch: [3][500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 516.4899 (506.9549)	
2019-01-08 11:44:39,229 - 10 - training_embed.py - training - Epoch: [3][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 508.3091 (506.9348)	
2019-01-08 11:44:39,451 - 10 - training_embed.py - training - Epoch: [3][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.9785 (506.9225)	
2019-01-08 11:44:39,674 - 10 - training_embed.py - training - Epoch: [3][800/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 510.9361 (506.9728)	
2019-01-08 11:44:39,901 - 10 - training_embed.py - training - Epoch: [3][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 506.8496 (507.0230)	
2019-01-08 11:44:40,131 - 10 - training_embed.py - training - Epoch: [3][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.8864 (506.9767)	
2019-01-08 11:44:40,364 - 10 - training_embed.py - training - Epoch: [3][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 507.9498 (506.9962)	
2019-01-08 11:44:40,584 - 10 - training_embed.py - training - Epoch: [3][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.5309 (506.9640)	
2019-01-08 11:44:40,803 - 10 - training_embed.py - training - Epoch: [3][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.4830 (507.0334)	
2019-01-08 11:44:41,018 - 10 - training_embed.py - training - Epoch: [3][1400/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 505.5374 (507.0134)	
2019-01-08 11:44:41,230 - 10 - training_embed.py - training - Epoch: [3][1500/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 507.6059 (507.0038)	
2019-01-08 11:44:41,462 - 10 - training_embed.py - training - Epoch: [3][1600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 509.7204 (506.9959)	
2019-01-08 11:44:41,685 - 10 - training_embed.py - training - Epoch: [3][1700/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 501.1772 (506.9935)	
2019-01-08 11:44:41,894 - 10 - training_embed.py - training - Epoch: [3][1800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 503.7000 (506.9862)	
2019-01-08 11:44:42,102 - 10 - training_embed.py - training - Epoch: [3][1900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 506.9439 (506.9760)	
2019-01-08 11:44:42,321 - 10 - training_embed.py - training - Epoch: [3][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.6004 (506.9481)	
2019-01-08 11:44:42,366 - 10 - training_embed.py - training - loss: 506.884818
2019-01-08 11:44:42,366 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 11:44:42,779 - 10 - training_embed.py - training - Epoch: [4][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 509.2300 (506.6798)	
2019-01-08 11:44:43,001 - 10 - training_embed.py - training - Epoch: [4][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.8891 (506.7356)	
2019-01-08 11:44:43,207 - 10 - training_embed.py - training - Epoch: [4][300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 509.3371 (506.6100)	
2019-01-08 11:44:43,426 - 10 - training_embed.py - training - Epoch: [4][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.7259 (506.8099)	
2019-01-08 11:44:43,655 - 10 - training_embed.py - training - Epoch: [4][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.4483 (506.7938)	
2019-01-08 11:44:43,877 - 10 - training_embed.py - training - Epoch: [4][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.6641 (506.6790)	
2019-01-08 11:44:44,098 - 10 - training_embed.py - training - Epoch: [4][700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 502.9798 (506.6637)	
2019-01-08 11:44:44,332 - 10 - training_embed.py - training - Epoch: [4][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.6870 (506.6144)	
2019-01-08 11:44:44,565 - 10 - training_embed.py - training - Epoch: [4][900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 507.7974 (506.6970)	
2019-01-08 11:44:44,782 - 10 - training_embed.py - training - Epoch: [4][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 506.5797 (506.7084)	
2019-01-08 11:44:44,995 - 10 - training_embed.py - training - Epoch: [4][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 508.9833 (506.6941)	
2019-01-08 11:44:45,235 - 10 - training_embed.py - training - Epoch: [4][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 506.8075 (506.6792)	
2019-01-08 11:44:45,457 - 10 - training_embed.py - training - Epoch: [4][1300/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 513.4432 (506.6371)	
2019-01-08 11:44:45,700 - 10 - training_embed.py - training - Epoch: [4][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.7801 (506.6289)	
2019-01-08 11:44:45,938 - 10 - training_embed.py - training - Epoch: [4][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.6253 (506.5828)	
2019-01-08 11:44:46,151 - 10 - training_embed.py - training - Epoch: [4][1600/2022]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 502.6466 (506.5366)	
2019-01-08 11:44:46,363 - 10 - training_embed.py - training - Epoch: [4][1700/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 512.5995 (506.5695)	
2019-01-08 11:44:46,582 - 10 - training_embed.py - training - Epoch: [4][1800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 511.9166 (506.5770)	
2019-01-08 11:44:46,799 - 10 - training_embed.py - training - Epoch: [4][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.6400 (506.5487)	
2019-01-08 11:44:47,012 - 10 - training_embed.py - training - Epoch: [4][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 511.8353 (506.5629)	
2019-01-08 11:44:47,059 - 10 - training_embed.py - training - loss: 506.499391
2019-01-08 11:44:47,060 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 11:44:47,479 - 10 - training_embed.py - training - Epoch: [5][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 504.6256 (506.0716)	
2019-01-08 11:44:47,697 - 10 - training_embed.py - training - Epoch: [5][200/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 510.8583 (506.2915)	
2019-01-08 11:44:47,907 - 10 - training_embed.py - training - Epoch: [5][300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 507.0207 (506.2785)	
2019-01-08 11:44:48,125 - 10 - training_embed.py - training - Epoch: [5][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.9886 (506.3630)	
2019-01-08 11:44:48,363 - 10 - training_embed.py - training - Epoch: [5][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.7414 (506.3458)	
2019-01-08 11:44:48,591 - 10 - training_embed.py - training - Epoch: [5][600/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 507.8860 (506.2874)	
2019-01-08 11:44:48,818 - 10 - training_embed.py - training - Epoch: [5][700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 514.6191 (506.2965)	
2019-01-08 11:44:49,048 - 10 - training_embed.py - training - Epoch: [5][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.7919 (506.3937)	
2019-01-08 11:44:49,257 - 10 - training_embed.py - training - Epoch: [5][900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 512.8251 (506.3298)	
2019-01-08 11:44:49,480 - 10 - training_embed.py - training - Epoch: [5][1000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 509.6109 (506.3442)	
2019-01-08 11:44:49,692 - 10 - training_embed.py - training - Epoch: [5][1100/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 501.3812 (506.2745)	
2019-01-08 11:44:49,908 - 10 - training_embed.py - training - Epoch: [5][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 504.9241 (506.2770)	
2019-01-08 11:44:50,125 - 10 - training_embed.py - training - Epoch: [5][1300/2022]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 500.7113 (506.2970)	
2019-01-08 11:44:50,347 - 10 - training_embed.py - training - Epoch: [5][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.5174 (506.2756)	
2019-01-08 11:44:50,556 - 10 - training_embed.py - training - Epoch: [5][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 505.1828 (506.2503)	
2019-01-08 11:44:50,772 - 10 - training_embed.py - training - Epoch: [5][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.6588 (506.2704)	
2019-01-08 11:44:50,987 - 10 - training_embed.py - training - Epoch: [5][1700/2022]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 507.0094 (506.2589)	
2019-01-08 11:44:51,209 - 10 - training_embed.py - training - Epoch: [5][1800/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 512.3596 (506.2528)	
2019-01-08 11:44:51,426 - 10 - training_embed.py - training - Epoch: [5][1900/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 504.5615 (506.2181)	
2019-01-08 11:44:51,657 - 10 - training_embed.py - training - Epoch: [5][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 512.7657 (506.1827)	
2019-01-08 11:44:51,706 - 10 - training_embed.py - training - loss: 506.110674
2019-01-08 11:44:51,706 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 11:44:52,136 - 10 - training_embed.py - training - Epoch: [6][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.1892 (505.4984)	
2019-01-08 11:44:52,366 - 10 - training_embed.py - training - Epoch: [6][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.6876 (505.8246)	
2019-01-08 11:44:52,589 - 10 - training_embed.py - training - Epoch: [6][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 509.7758 (505.8324)	
2019-01-08 11:44:52,815 - 10 - training_embed.py - training - Epoch: [6][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.7565 (505.6301)	
2019-01-08 11:44:53,024 - 10 - training_embed.py - training - Epoch: [6][500/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 503.9235 (505.5860)	
2019-01-08 11:44:53,247 - 10 - training_embed.py - training - Epoch: [6][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 511.4893 (505.7335)	
2019-01-08 11:44:53,495 - 10 - training_embed.py - training - Epoch: [6][700/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 503.8095 (505.7495)	
2019-01-08 11:44:53,743 - 10 - training_embed.py - training - Epoch: [6][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 506.5314 (505.7940)	
2019-01-08 11:44:53,990 - 10 - training_embed.py - training - Epoch: [6][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 512.1819 (505.7932)	
2019-01-08 11:44:54,193 - 10 - training_embed.py - training - Epoch: [6][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 506.0974 (505.8594)	
2019-01-08 11:44:54,432 - 10 - training_embed.py - training - Epoch: [6][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 503.8668 (505.8923)	
2019-01-08 11:44:54,658 - 10 - training_embed.py - training - Epoch: [6][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 514.6171 (505.8587)	
2019-01-08 11:44:54,868 - 10 - training_embed.py - training - Epoch: [6][1300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 502.7191 (505.8922)	
2019-01-08 11:44:55,090 - 10 - training_embed.py - training - Epoch: [6][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.8914 (505.8556)	
2019-01-08 11:44:55,293 - 10 - training_embed.py - training - Epoch: [6][1500/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 504.4570 (505.8576)	
2019-01-08 11:44:55,518 - 10 - training_embed.py - training - Epoch: [6][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 509.9671 (505.8339)	
2019-01-08 11:44:55,734 - 10 - training_embed.py - training - Epoch: [6][1700/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 506.9724 (505.7976)	
2019-01-08 11:44:55,937 - 10 - training_embed.py - training - Epoch: [6][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 509.5893 (505.7938)	
2019-01-08 11:44:56,155 - 10 - training_embed.py - training - Epoch: [6][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 508.9156 (505.8001)	
2019-01-08 11:44:56,371 - 10 - training_embed.py - training - Epoch: [6][2000/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 508.9518 (505.7621)	
2019-01-08 11:44:56,424 - 10 - training_embed.py - training - loss: 505.720408
2019-01-08 11:44:56,424 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 11:44:56,830 - 10 - training_embed.py - training - Epoch: [7][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.6882 (505.2320)	
2019-01-08 11:44:57,050 - 10 - training_embed.py - training - Epoch: [7][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.2919 (505.3746)	
2019-01-08 11:44:57,256 - 10 - training_embed.py - training - Epoch: [7][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 509.0692 (505.4279)	
2019-01-08 11:44:57,481 - 10 - training_embed.py - training - Epoch: [7][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.4395 (505.3895)	
2019-01-08 11:44:57,695 - 10 - training_embed.py - training - Epoch: [7][500/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 507.8970 (505.4115)	
2019-01-08 11:44:57,913 - 10 - training_embed.py - training - Epoch: [7][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.6469 (505.3233)	
2019-01-08 11:44:58,118 - 10 - training_embed.py - training - Epoch: [7][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.7844 (505.4042)	
2019-01-08 11:44:58,336 - 10 - training_embed.py - training - Epoch: [7][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 510.4522 (505.3642)	
2019-01-08 11:44:58,545 - 10 - training_embed.py - training - Epoch: [7][900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 508.1890 (505.3724)	
2019-01-08 11:44:58,781 - 10 - training_embed.py - training - Epoch: [7][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.5244 (505.4045)	
2019-01-08 11:44:59,013 - 10 - training_embed.py - training - Epoch: [7][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 506.2663 (505.3594)	
2019-01-08 11:44:59,232 - 10 - training_embed.py - training - Epoch: [7][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 510.0724 (505.3779)	
2019-01-08 11:44:59,461 - 10 - training_embed.py - training - Epoch: [7][1300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 499.0675 (505.3444)	
2019-01-08 11:44:59,695 - 10 - training_embed.py - training - Epoch: [7][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.4976 (505.3719)	
2019-01-08 11:44:59,907 - 10 - training_embed.py - training - Epoch: [7][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.5103 (505.3587)	
2019-01-08 11:45:00,134 - 10 - training_embed.py - training - Epoch: [7][1600/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 505.2498 (505.3795)	
2019-01-08 11:45:00,351 - 10 - training_embed.py - training - Epoch: [7][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.0008 (505.4066)	
2019-01-08 11:45:00,581 - 10 - training_embed.py - training - Epoch: [7][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 508.7226 (505.3959)	
2019-01-08 11:45:00,803 - 10 - training_embed.py - training - Epoch: [7][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.6927 (505.3966)	
2019-01-08 11:45:01,023 - 10 - training_embed.py - training - Epoch: [7][2000/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 505.9761 (505.3834)	
2019-01-08 11:45:01,071 - 10 - training_embed.py - training - loss: 505.328058
2019-01-08 11:45:01,071 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 11:45:01,492 - 10 - training_embed.py - training - Epoch: [8][100/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 504.0002 (505.4294)	
2019-01-08 11:45:01,703 - 10 - training_embed.py - training - Epoch: [8][200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 512.2512 (505.1110)	
2019-01-08 11:45:01,915 - 10 - training_embed.py - training - Epoch: [8][300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 503.4677 (505.0034)	
2019-01-08 11:45:02,132 - 10 - training_embed.py - training - Epoch: [8][400/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 507.2432 (505.1308)	
2019-01-08 11:45:02,360 - 10 - training_embed.py - training - Epoch: [8][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.6299 (505.1398)	
2019-01-08 11:45:02,580 - 10 - training_embed.py - training - Epoch: [8][600/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 503.5740 (505.1658)	
2019-01-08 11:45:02,808 - 10 - training_embed.py - training - Epoch: [8][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 511.7747 (505.1956)	
2019-01-08 11:45:03,043 - 10 - training_embed.py - training - Epoch: [8][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.0425 (505.1813)	
2019-01-08 11:45:03,264 - 10 - training_embed.py - training - Epoch: [8][900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 500.4240 (505.1193)	
2019-01-08 11:45:03,478 - 10 - training_embed.py - training - Epoch: [8][1000/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 504.6730 (504.9995)	
2019-01-08 11:45:03,696 - 10 - training_embed.py - training - Epoch: [8][1100/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 508.3148 (505.0078)	
2019-01-08 11:45:03,930 - 10 - training_embed.py - training - Epoch: [8][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 509.0232 (505.0120)	
2019-01-08 11:45:04,136 - 10 - training_embed.py - training - Epoch: [8][1300/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 505.5328 (505.0074)	
2019-01-08 11:45:04,360 - 10 - training_embed.py - training - Epoch: [8][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.9029 (505.0099)	
2019-01-08 11:45:04,591 - 10 - training_embed.py - training - Epoch: [8][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 505.8436 (504.9953)	
2019-01-08 11:45:04,821 - 10 - training_embed.py - training - Epoch: [8][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 504.1168 (504.9947)	
2019-01-08 11:45:05,064 - 10 - training_embed.py - training - Epoch: [8][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 504.6868 (504.9675)	
2019-01-08 11:45:05,300 - 10 - training_embed.py - training - Epoch: [8][1800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 490.5220 (504.9905)	
2019-01-08 11:45:05,542 - 10 - training_embed.py - training - Epoch: [8][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 510.1586 (505.0068)	
2019-01-08 11:45:05,778 - 10 - training_embed.py - training - Epoch: [8][2000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 501.4763 (505.0046)	
2019-01-08 11:45:05,826 - 10 - training_embed.py - training - loss: 504.934737
2019-01-08 11:45:05,827 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 11:45:06,234 - 10 - training_embed.py - training - Epoch: [9][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 515.4817 (504.6196)	
2019-01-08 11:45:06,463 - 10 - training_embed.py - training - Epoch: [9][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 510.6530 (504.3314)	
2019-01-08 11:45:06,675 - 10 - training_embed.py - training - Epoch: [9][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 509.4260 (504.5097)	
2019-01-08 11:45:06,888 - 10 - training_embed.py - training - Epoch: [9][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 508.3430 (504.5662)	
2019-01-08 11:45:07,105 - 10 - training_embed.py - training - Epoch: [9][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 504.7724 (504.6266)	
2019-01-08 11:45:07,340 - 10 - training_embed.py - training - Epoch: [9][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 512.1157 (504.5243)	
2019-01-08 11:45:07,570 - 10 - training_embed.py - training - Epoch: [9][700/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 506.4244 (504.5458)	
2019-01-08 11:45:07,786 - 10 - training_embed.py - training - Epoch: [9][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.1577 (504.5609)	
2019-01-08 11:45:08,004 - 10 - training_embed.py - training - Epoch: [9][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.3994 (504.6298)	
2019-01-08 11:45:08,226 - 10 - training_embed.py - training - Epoch: [9][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.2756 (504.6223)	
2019-01-08 11:45:08,465 - 10 - training_embed.py - training - Epoch: [9][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 501.6070 (504.6327)	
2019-01-08 11:45:08,682 - 10 - training_embed.py - training - Epoch: [9][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 508.6893 (504.6342)	
2019-01-08 11:45:08,902 - 10 - training_embed.py - training - Epoch: [9][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 510.6876 (504.5971)	
2019-01-08 11:45:09,118 - 10 - training_embed.py - training - Epoch: [9][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.4058 (504.5632)	
2019-01-08 11:45:09,335 - 10 - training_embed.py - training - Epoch: [9][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.0872 (504.5954)	
2019-01-08 11:45:09,549 - 10 - training_embed.py - training - Epoch: [9][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.7923 (504.6189)	
2019-01-08 11:45:09,759 - 10 - training_embed.py - training - Epoch: [9][1700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 504.2910 (504.6211)	
2019-01-08 11:45:09,977 - 10 - training_embed.py - training - Epoch: [9][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 508.6144 (504.6227)	
2019-01-08 11:45:10,190 - 10 - training_embed.py - training - Epoch: [9][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.2248 (504.5959)	
2019-01-08 11:45:10,410 - 10 - training_embed.py - training - Epoch: [9][2000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 505.5957 (504.6045)	
2019-01-08 11:45:10,459 - 10 - training_embed.py - training - loss: 504.538716
2019-01-08 11:45:10,460 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 11:45:10,873 - 10 - training_embed.py - training - Epoch: [10][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.9763 (504.3163)	
2019-01-08 11:45:11,103 - 10 - training_embed.py - training - Epoch: [10][200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 512.9026 (504.2689)	
2019-01-08 11:45:11,330 - 10 - training_embed.py - training - Epoch: [10][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.0386 (504.4882)	
2019-01-08 11:45:11,549 - 10 - training_embed.py - training - Epoch: [10][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.5258 (504.4017)	
2019-01-08 11:45:11,776 - 10 - training_embed.py - training - Epoch: [10][500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 503.9919 (504.2850)	
2019-01-08 11:45:12,008 - 10 - training_embed.py - training - Epoch: [10][600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 512.0775 (504.3253)	
2019-01-08 11:45:12,249 - 10 - training_embed.py - training - Epoch: [10][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.0353 (504.2809)	
2019-01-08 11:45:12,470 - 10 - training_embed.py - training - Epoch: [10][800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 509.3507 (504.2463)	
2019-01-08 11:45:12,692 - 10 - training_embed.py - training - Epoch: [10][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 506.3061 (504.2662)	
2019-01-08 11:45:12,911 - 10 - training_embed.py - training - Epoch: [10][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.9843 (504.2134)	
2019-01-08 11:45:13,133 - 10 - training_embed.py - training - Epoch: [10][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 502.8210 (504.2071)	
2019-01-08 11:45:13,362 - 10 - training_embed.py - training - Epoch: [10][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.1904 (504.2137)	
2019-01-08 11:45:13,576 - 10 - training_embed.py - training - Epoch: [10][1300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 507.2704 (504.2042)	
2019-01-08 11:45:13,802 - 10 - training_embed.py - training - Epoch: [10][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.9440 (504.1993)	
2019-01-08 11:45:14,014 - 10 - training_embed.py - training - Epoch: [10][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 499.4892 (504.2083)	
2019-01-08 11:45:14,229 - 10 - training_embed.py - training - Epoch: [10][1600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 501.9767 (504.1902)	
2019-01-08 11:45:14,444 - 10 - training_embed.py - training - Epoch: [10][1700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 511.8318 (504.1913)	
2019-01-08 11:45:14,679 - 10 - training_embed.py - training - Epoch: [10][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.4805 (504.2535)	
2019-01-08 11:45:14,901 - 10 - training_embed.py - training - Epoch: [10][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 510.0078 (504.2327)	
2019-01-08 11:45:15,133 - 10 - training_embed.py - training - Epoch: [10][2000/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 503.6712 (504.2108)	
2019-01-08 11:45:15,183 - 10 - training_embed.py - training - loss: 504.138735
2019-01-08 11:45:15,183 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 11:45:15,596 - 10 - training_embed.py - training - Epoch: [11][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.6241 (504.2226)	
2019-01-08 11:45:15,800 - 10 - training_embed.py - training - Epoch: [11][200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 510.9051 (504.1485)	
2019-01-08 11:45:16,028 - 10 - training_embed.py - training - Epoch: [11][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.1898 (504.0905)	
2019-01-08 11:45:16,246 - 10 - training_embed.py - training - Epoch: [11][400/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 509.5195 (504.1709)	
2019-01-08 11:45:16,478 - 10 - training_embed.py - training - Epoch: [11][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.9681 (504.0469)	
2019-01-08 11:45:16,708 - 10 - training_embed.py - training - Epoch: [11][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.7038 (504.0039)	
2019-01-08 11:45:16,945 - 10 - training_embed.py - training - Epoch: [11][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.8901 (503.9759)	
2019-01-08 11:45:17,174 - 10 - training_embed.py - training - Epoch: [11][800/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 499.8066 (503.9455)	
2019-01-08 11:45:17,406 - 10 - training_embed.py - training - Epoch: [11][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.8714 (503.9069)	
2019-01-08 11:45:17,628 - 10 - training_embed.py - training - Epoch: [11][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 508.5033 (503.9785)	
2019-01-08 11:45:17,844 - 10 - training_embed.py - training - Epoch: [11][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.2976 (503.9322)	
2019-01-08 11:45:18,066 - 10 - training_embed.py - training - Epoch: [11][1200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 496.6629 (503.8761)	
2019-01-08 11:45:18,271 - 10 - training_embed.py - training - Epoch: [11][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.9704 (503.8539)	
2019-01-08 11:45:18,485 - 10 - training_embed.py - training - Epoch: [11][1400/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 499.8464 (503.8614)	
2019-01-08 11:45:18,698 - 10 - training_embed.py - training - Epoch: [11][1500/2022]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 498.0870 (503.8589)	
2019-01-08 11:45:18,923 - 10 - training_embed.py - training - Epoch: [11][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.9426 (503.8581)	
2019-01-08 11:45:19,133 - 10 - training_embed.py - training - Epoch: [11][1700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 502.9629 (503.8636)	
2019-01-08 11:45:19,365 - 10 - training_embed.py - training - Epoch: [11][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 504.2691 (503.8398)	
2019-01-08 11:45:19,575 - 10 - training_embed.py - training - Epoch: [11][1900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 501.3757 (503.8021)	
2019-01-08 11:45:19,786 - 10 - training_embed.py - training - Epoch: [11][2000/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 495.7903 (503.8099)	
2019-01-08 11:45:19,834 - 10 - training_embed.py - training - loss: 503.739165
2019-01-08 11:45:19,901 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 11:45:20,896 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 995.652 ms ~ 0.017 min ~ 0.996 sec
2019-01-08 11:45:21,913 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 2012.505 ms ~ 0.034 min ~ 2.013 sec
2019-01-08 11:45:21,913 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 11:45:21,914 - 10 - corpus.py - subactivity_sampler - 0 / 166
2019-01-08 11:45:21,914 - 10 - corpus.py - subactivity_sampler - [58651. 37381. 37949. 50832. 41344. 59859. 32366. 63265. 39589. 47158.
 49084.]
2019-01-08 11:47:15,130 - 10 - corpus.py - subactivity_sampler - 20 / 166
2019-01-08 11:47:15,131 - 10 - corpus.py - subactivity_sampler - [59409. 36871. 37121. 51605. 39803. 63845. 29231. 65408. 37995. 47052.
 49138.]
2019-01-08 11:48:26,680 - 10 - corpus.py - subactivity_sampler - 40 / 166
2019-01-08 11:48:26,680 - 10 - corpus.py - subactivity_sampler - [59523. 36409. 36416. 52299. 38912. 65962. 27055. 68085. 36399. 47262.
 49156.]
2019-01-08 11:49:55,351 - 10 - corpus.py - subactivity_sampler - 60 / 166
2019-01-08 11:49:55,352 - 10 - corpus.py - subactivity_sampler - [60546. 35773. 35515. 52771. 37380. 68787. 24676. 72013. 33496. 47319.
 49202.]
2019-01-08 11:52:12,690 - 10 - corpus.py - subactivity_sampler - 80 / 166
2019-01-08 11:52:12,690 - 10 - corpus.py - subactivity_sampler - [61197. 34874. 35026. 53746. 35382. 72679. 21825. 75065. 30472. 48081.
 49131.]
2019-01-08 11:54:07,522 - 10 - corpus.py - subactivity_sampler - 100 / 166
2019-01-08 11:54:07,522 - 10 - corpus.py - subactivity_sampler - [62158. 34371. 33860. 54149. 33219. 76935. 19143. 79695. 26861. 47834.
 49253.]
2019-01-08 11:55:34,447 - 10 - corpus.py - subactivity_sampler - 120 / 166
2019-01-08 11:55:34,447 - 10 - corpus.py - subactivity_sampler - [63199. 33686. 32806. 55319. 31260. 80155. 16705. 83010. 24121. 47925.
 49292.]
2019-01-08 11:56:59,862 - 10 - corpus.py - subactivity_sampler - 140 / 166
2019-01-08 11:56:59,862 - 10 - corpus.py - subactivity_sampler - [63484. 33138. 31914. 56815. 29219. 83107. 14576. 86121. 21448. 48322.
 49334.]
2019-01-08 11:59:25,114 - 10 - corpus.py - subactivity_sampler - 160 / 166
2019-01-08 11:59:25,115 - 10 - corpus.py - subactivity_sampler - [63902. 32739. 30762. 58166. 26941. 87298. 12883. 88090. 18955. 48474.
 49268.]
2019-01-08 11:59:59,158 - 10 - corpus.py - subactivity_sampler - [63932. 32654. 30369. 58514. 26425. 88903. 12530. 88426. 18436. 47881.
 49408.]
2019-01-08 11:59:59,158 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 877245.099 ms ~ 14.621 min ~ 877.245 sec
2019-01-08 11:59:59,158 - 10 - corpus.py - ordering_sampler - .
2019-01-08 12:00:07,547 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 12:00:07,547 - 10 - corpus.py - ordering_sampler - inv_count_vec: [13.  5.  0.  8. 68.  0. 38. 11. 27. 17.]
2019-01-08 12:00:07,547 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 12:00:07,605 - 10 - corpus.py - rho_sampling - ['50.2405', '18.2085', '928.5380', '209.6312', '3.2929', '39.8714', '5.5759', '7.6016', '42.7034', '159.8650']
2019-01-08 12:00:07,606 - 10 - pipeline.py - baseline - Iteration 2
2019-01-08 12:00:08,005 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 12:00:08,023 - 10 - accuracy_class.py - mof - # gt_labels: 12   # pr_labels: 11
2019-01-08 12:00:08,023 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 11', '1: 17', '2: 42', '3: 12', '4: 10', '5: 43', '6: 16', '7: 44', '8: 4', '9: 14', '10: 15']
2019-01-08 12:00:08,087 - 10 - accuracy_class.py - mof_val - frames true: 181843	frames overall : 517478
2019-01-08 12:00:08,088 - 10 - corpus.py - accuracy_corpus - Action: scrambledegg
2019-01-08 12:00:08,088 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3514023784586011
2019-01-08 12:00:08,088 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3467915544235697
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 29280
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1218
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - mof_classes - label 10: 0.050572  553 / 10935
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - mof_classes - label 11: 0.376928  21186 / 56207
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - mof_classes - label 12: 0.203162  8174 / 40234
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - mof_classes - label 14: 0.429504  7564 / 17611
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - mof_classes - label 15: 0.703936  30598 / 43467
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 1003
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - mof_classes - label 17: 0.184900  9125 / 49351
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - mof_classes - label 42: 0.381556  12259 / 32129
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - mof_classes - label 43: 0.480611  11043 / 22977
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - mof_classes - label 44: 0.381764  81341 / 213066
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - mof_classes - average class mof: 0.266078
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 29280
2019-01-08 12:00:08,088 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 19654
2019-01-08 12:00:08,089 - 10 - accuracy_class.py - iou_classes - label 10: 0.015024  553 / 36807
2019-01-08 12:00:08,089 - 10 - accuracy_class.py - iou_classes - label 11: 0.214102  21186 / 98953
2019-01-08 12:00:08,089 - 10 - accuracy_class.py - iou_classes - label 12: 0.090247  8174 / 90574
2019-01-08 12:00:08,092 - 10 - accuracy_class.py - iou_classes - label 14: 0.130576  7564 / 57928
2019-01-08 12:00:08,092 - 10 - accuracy_class.py - iou_classes - label 15: 0.491321  30598 / 62277
2019-01-08 12:00:08,092 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 13533
2019-01-08 12:00:08,092 - 10 - accuracy_class.py - iou_classes - label 17: 0.125206  9125 / 72880
2019-01-08 12:00:08,092 - 10 - accuracy_class.py - iou_classes - label 42: 0.244014  12259 / 50239
2019-01-08 12:00:08,092 - 10 - accuracy_class.py - iou_classes - label 43: 0.109513  11043 / 100837
2019-01-08 12:00:08,092 - 10 - accuracy_class.py - iou_classes - label 44: 0.369478  81341 / 220151
2019-01-08 12:00:08,092 - 10 - accuracy_class.py - iou_classes - average IoU: 0.162680
2019-01-08 12:00:08,092 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.149123
2019-01-08 12:00:21,931 - 10 - f1_score.py - f1 - f1 score: 0.291098
2019-01-08 12:00:21,967 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 14361.056 ms ~ 0.239 min ~ 14.361 sec
2019-01-08 12:00:21,967 - 10 - corpus.py - embedding_training - .
2019-01-08 12:00:21,967 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 12:00:21,967 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 12:00:21,968 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 12:00:28,726 - 10 - training_embed.py - training - create model
2019-01-08 12:00:28,731 - 10 - training_embed.py - training - epochs: 12
2019-01-08 12:00:28,733 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 12:00:29,577 - 10 - training_embed.py - training - Epoch: [0][100/2022]	Time 0.003 (0.008)	Data 0.002 (0.004)	Loss 501.7134 (504.7810)	
2019-01-08 12:00:30,274 - 10 - training_embed.py - training - Epoch: [0][200/2022]	Time 0.010 (0.008)	Data 0.001 (0.003)	Loss 504.7155 (504.5234)	
2019-01-08 12:00:31,000 - 10 - training_embed.py - training - Epoch: [0][300/2022]	Time 0.010 (0.008)	Data 0.001 (0.003)	Loss 506.5831 (504.1915)	
2019-01-08 12:00:31,597 - 10 - training_embed.py - training - Epoch: [0][400/2022]	Time 0.003 (0.007)	Data 0.001 (0.002)	Loss 504.2864 (504.1279)	
2019-01-08 12:00:32,201 - 10 - training_embed.py - training - Epoch: [0][500/2022]	Time 0.003 (0.007)	Data 0.001 (0.002)	Loss 494.9379 (504.1115)	
2019-01-08 12:00:32,778 - 10 - training_embed.py - training - Epoch: [0][600/2022]	Time 0.003 (0.007)	Data 0.002 (0.002)	Loss 505.7963 (504.0556)	
2019-01-08 12:00:33,256 - 10 - training_embed.py - training - Epoch: [0][700/2022]	Time 0.002 (0.006)	Data 0.001 (0.002)	Loss 507.7755 (503.9331)	
2019-01-08 12:00:33,754 - 10 - training_embed.py - training - Epoch: [0][800/2022]	Time 0.008 (0.006)	Data 0.006 (0.002)	Loss 506.4092 (503.9307)	
2019-01-08 12:00:34,290 - 10 - training_embed.py - training - Epoch: [0][900/2022]	Time 0.001 (0.006)	Data 0.000 (0.002)	Loss 511.6462 (503.8851)	
2019-01-08 12:00:34,881 - 10 - training_embed.py - training - Epoch: [0][1000/2022]	Time 0.009 (0.006)	Data 0.002 (0.002)	Loss 505.2954 (503.8925)	
2019-01-08 12:00:35,526 - 10 - training_embed.py - training - Epoch: [0][1100/2022]	Time 0.009 (0.006)	Data 0.003 (0.002)	Loss 506.0241 (503.9423)	
2019-01-08 12:00:36,023 - 10 - training_embed.py - training - Epoch: [0][1200/2022]	Time 0.003 (0.006)	Data 0.001 (0.002)	Loss 507.0503 (503.9669)	
2019-01-08 12:00:36,493 - 10 - training_embed.py - training - Epoch: [0][1300/2022]	Time 0.001 (0.006)	Data 0.000 (0.002)	Loss 503.4680 (503.8624)	
2019-01-08 12:00:36,930 - 10 - training_embed.py - training - Epoch: [0][1400/2022]	Time 0.002 (0.006)	Data 0.001 (0.002)	Loss 507.0272 (503.8433)	
2019-01-08 12:00:37,394 - 10 - training_embed.py - training - Epoch: [0][1500/2022]	Time 0.003 (0.006)	Data 0.001 (0.002)	Loss 508.6141 (503.8345)	
2019-01-08 12:00:37,854 - 10 - training_embed.py - training - Epoch: [0][1600/2022]	Time 0.008 (0.006)	Data 0.001 (0.002)	Loss 506.4419 (503.8279)	
2019-01-08 12:00:38,309 - 10 - training_embed.py - training - Epoch: [0][1700/2022]	Time 0.002 (0.006)	Data 0.001 (0.002)	Loss 502.3173 (503.7969)	
2019-01-08 12:00:38,785 - 10 - training_embed.py - training - Epoch: [0][1800/2022]	Time 0.005 (0.006)	Data 0.003 (0.002)	Loss 496.7871 (503.7688)	
2019-01-08 12:00:39,264 - 10 - training_embed.py - training - Epoch: [0][1900/2022]	Time 0.003 (0.006)	Data 0.000 (0.002)	Loss 505.6009 (503.7410)	
2019-01-08 12:00:39,804 - 10 - training_embed.py - training - Epoch: [0][2000/2022]	Time 0.003 (0.006)	Data 0.000 (0.002)	Loss 506.0474 (503.7253)	
2019-01-08 12:00:39,977 - 10 - training_embed.py - training - loss: 503.648572
2019-01-08 12:00:39,986 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 12:00:41,516 - 10 - training_embed.py - training - Epoch: [1][100/2022]	Time 0.011 (0.006)	Data 0.002 (0.003)	Loss 503.0093 (503.2092)	
2019-01-08 12:00:42,310 - 10 - training_embed.py - training - Epoch: [1][200/2022]	Time 0.004 (0.006)	Data 0.002 (0.003)	Loss 510.5053 (503.2807)	
2019-01-08 12:00:43,131 - 10 - training_embed.py - training - Epoch: [1][300/2022]	Time 0.004 (0.006)	Data 0.002 (0.003)	Loss 506.4760 (503.0621)	
2019-01-08 12:00:44,064 - 10 - training_embed.py - training - Epoch: [1][400/2022]	Time 0.019 (0.006)	Data 0.003 (0.003)	Loss 499.2349 (502.8443)	
2019-01-08 12:00:45,075 - 10 - training_embed.py - training - Epoch: [1][500/2022]	Time 0.015 (0.006)	Data 0.013 (0.003)	Loss 501.8044 (502.7365)	
2019-01-08 12:00:45,838 - 10 - training_embed.py - training - Epoch: [1][600/2022]	Time 0.012 (0.007)	Data 0.000 (0.003)	Loss 506.7597 (502.8359)	
2019-01-08 12:00:46,454 - 10 - training_embed.py - training - Epoch: [1][700/2022]	Time 0.016 (0.006)	Data 0.014 (0.003)	Loss 503.5504 (502.9014)	
2019-01-08 12:00:47,022 - 10 - training_embed.py - training - Epoch: [1][800/2022]	Time 0.013 (0.006)	Data 0.012 (0.003)	Loss 503.5487 (502.9372)	
2019-01-08 12:00:47,667 - 10 - training_embed.py - training - Epoch: [1][900/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 497.5120 (502.9281)	
2019-01-08 12:00:48,376 - 10 - training_embed.py - training - Epoch: [1][1000/2022]	Time 0.002 (0.006)	Data 0.000 (0.003)	Loss 503.4802 (502.8635)	
2019-01-08 12:00:49,313 - 10 - training_embed.py - training - Epoch: [1][1100/2022]	Time 0.006 (0.007)	Data 0.002 (0.003)	Loss 500.3996 (502.8175)	
2019-01-08 12:00:50,022 - 10 - training_embed.py - training - Epoch: [1][1200/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 506.3372 (502.7843)	
2019-01-08 12:00:50,584 - 10 - training_embed.py - training - Epoch: [1][1300/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 505.3622 (502.7390)	
2019-01-08 12:00:51,309 - 10 - training_embed.py - training - Epoch: [1][1400/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 504.2980 (502.7310)	
2019-01-08 12:00:51,942 - 10 - training_embed.py - training - Epoch: [1][1500/2022]	Time 0.001 (0.007)	Data 0.000 (0.003)	Loss 506.6492 (502.6986)	
2019-01-08 12:00:52,572 - 10 - training_embed.py - training - Epoch: [1][1600/2022]	Time 0.013 (0.007)	Data 0.012 (0.003)	Loss 502.2308 (502.6888)	
2019-01-08 12:00:53,056 - 10 - training_embed.py - training - Epoch: [1][1700/2022]	Time 0.010 (0.007)	Data 0.000 (0.003)	Loss 496.0630 (502.6706)	
2019-01-08 12:00:53,529 - 10 - training_embed.py - training - Epoch: [1][1800/2022]	Time 0.011 (0.006)	Data 0.010 (0.003)	Loss 504.9433 (502.6558)	
2019-01-08 12:00:54,065 - 10 - training_embed.py - training - Epoch: [1][1900/2022]	Time 0.004 (0.006)	Data 0.002 (0.003)	Loss 504.9822 (502.6619)	
2019-01-08 12:00:54,770 - 10 - training_embed.py - training - Epoch: [1][2000/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 505.3090 (502.6300)	
2019-01-08 12:00:54,895 - 10 - training_embed.py - training - loss: 502.564225
2019-01-08 12:00:54,896 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 12:00:56,090 - 10 - training_embed.py - training - Epoch: [2][100/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 506.1946 (502.0442)	
2019-01-08 12:00:56,915 - 10 - training_embed.py - training - Epoch: [2][200/2022]	Time 0.002 (0.007)	Data 0.001 (0.003)	Loss 508.6817 (501.9579)	
2019-01-08 12:00:57,890 - 10 - training_embed.py - training - Epoch: [2][300/2022]	Time 0.012 (0.007)	Data 0.002 (0.003)	Loss 502.6146 (501.9847)	
2019-01-08 12:00:58,729 - 10 - training_embed.py - training - Epoch: [2][400/2022]	Time 0.014 (0.007)	Data 0.002 (0.003)	Loss 493.3569 (501.9103)	
2019-01-08 12:00:59,578 - 10 - training_embed.py - training - Epoch: [2][500/2022]	Time 0.004 (0.007)	Data 0.003 (0.003)	Loss 496.9967 (501.8840)	
2019-01-08 12:01:00,200 - 10 - training_embed.py - training - Epoch: [2][600/2022]	Time 0.003 (0.007)	Data 0.002 (0.003)	Loss 506.5550 (501.9233)	
2019-01-08 12:01:00,771 - 10 - training_embed.py - training - Epoch: [2][700/2022]	Time 0.010 (0.007)	Data 0.002 (0.003)	Loss 505.4786 (501.8883)	
2019-01-08 12:01:01,380 - 10 - training_embed.py - training - Epoch: [2][800/2022]	Time 0.008 (0.007)	Data 0.005 (0.003)	Loss 498.1767 (501.8170)	
2019-01-08 12:01:01,927 - 10 - training_embed.py - training - Epoch: [2][900/2022]	Time 0.006 (0.007)	Data 0.003 (0.003)	Loss 506.3489 (501.7677)	
2019-01-08 12:01:02,510 - 10 - training_embed.py - training - Epoch: [2][1000/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 494.9283 (501.7070)	
2019-01-08 12:01:03,070 - 10 - training_embed.py - training - Epoch: [2][1100/2022]	Time 0.015 (0.007)	Data 0.000 (0.003)	Loss 498.4648 (501.7128)	
2019-01-08 12:01:03,795 - 10 - training_embed.py - training - Epoch: [2][1200/2022]	Time 0.010 (0.007)	Data 0.002 (0.003)	Loss 503.2068 (501.7138)	
2019-01-08 12:01:04,400 - 10 - training_embed.py - training - Epoch: [2][1300/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 506.7947 (501.7253)	
2019-01-08 12:01:04,974 - 10 - training_embed.py - training - Epoch: [2][1400/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 500.9888 (501.7347)	
2019-01-08 12:01:05,710 - 10 - training_embed.py - training - Epoch: [2][1500/2022]	Time 0.006 (0.007)	Data 0.004 (0.003)	Loss 506.2428 (501.7062)	
2019-01-08 12:01:06,488 - 10 - training_embed.py - training - Epoch: [2][1600/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 499.6639 (501.6376)	
2019-01-08 12:01:07,190 - 10 - training_embed.py - training - Epoch: [2][1700/2022]	Time 0.010 (0.007)	Data 0.000 (0.003)	Loss 496.1292 (501.6083)	
2019-01-08 12:01:07,974 - 10 - training_embed.py - training - Epoch: [2][1800/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 500.1031 (501.6112)	
2019-01-08 12:01:08,577 - 10 - training_embed.py - training - Epoch: [2][1900/2022]	Time 0.002 (0.007)	Data 0.001 (0.003)	Loss 498.3450 (501.5643)	
2019-01-08 12:01:09,205 - 10 - training_embed.py - training - Epoch: [2][2000/2022]	Time 0.014 (0.007)	Data 0.002 (0.003)	Loss 505.8856 (501.5285)	
2019-01-08 12:01:09,315 - 10 - training_embed.py - training - loss: 501.473976
2019-01-08 12:01:09,316 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 12:01:10,620 - 10 - training_embed.py - training - Epoch: [3][100/2022]	Time 0.003 (0.007)	Data 0.002 (0.003)	Loss 495.3047 (501.0021)	
2019-01-08 12:01:11,478 - 10 - training_embed.py - training - Epoch: [3][200/2022]	Time 0.008 (0.007)	Data 0.005 (0.003)	Loss 500.7769 (500.9443)	
2019-01-08 12:01:12,603 - 10 - training_embed.py - training - Epoch: [3][300/2022]	Time 0.017 (0.007)	Data 0.016 (0.003)	Loss 499.5804 (500.7501)	
2019-01-08 12:01:13,486 - 10 - training_embed.py - training - Epoch: [3][400/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 502.5540 (500.7360)	
2019-01-08 12:01:14,189 - 10 - training_embed.py - training - Epoch: [3][500/2022]	Time 0.014 (0.007)	Data 0.012 (0.003)	Loss 504.7542 (500.7042)	
2019-01-08 12:01:14,729 - 10 - training_embed.py - training - Epoch: [3][600/2022]	Time 0.002 (0.007)	Data 0.001 (0.003)	Loss 503.5317 (500.7008)	
2019-01-08 12:01:15,293 - 10 - training_embed.py - training - Epoch: [3][700/2022]	Time 0.004 (0.007)	Data 0.002 (0.003)	Loss 499.0280 (500.6687)	
2019-01-08 12:01:15,858 - 10 - training_embed.py - training - Epoch: [3][800/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 501.3890 (500.6675)	
2019-01-08 12:01:16,651 - 10 - training_embed.py - training - Epoch: [3][900/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 499.5858 (500.6702)	
2019-01-08 12:01:17,483 - 10 - training_embed.py - training - Epoch: [3][1000/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 499.9926 (500.6252)	
2019-01-08 12:01:18,072 - 10 - training_embed.py - training - Epoch: [3][1100/2022]	Time 0.014 (0.007)	Data 0.002 (0.003)	Loss 495.6024 (500.6369)	
2019-01-08 12:01:18,772 - 10 - training_embed.py - training - Epoch: [3][1200/2022]	Time 0.003 (0.007)	Data 0.000 (0.003)	Loss 499.4166 (500.5824)	
2019-01-08 12:01:19,366 - 10 - training_embed.py - training - Epoch: [3][1300/2022]	Time 0.005 (0.007)	Data 0.000 (0.003)	Loss 498.0945 (500.6181)	
2019-01-08 12:01:19,944 - 10 - training_embed.py - training - Epoch: [3][1400/2022]	Time 0.015 (0.007)	Data 0.013 (0.003)	Loss 500.7697 (500.5665)	
2019-01-08 12:01:20,443 - 10 - training_embed.py - training - Epoch: [3][1500/2022]	Time 0.001 (0.007)	Data 0.000 (0.003)	Loss 500.8781 (500.5704)	
2019-01-08 12:01:21,222 - 10 - training_embed.py - training - Epoch: [3][1600/2022]	Time 0.002 (0.007)	Data 0.001 (0.003)	Loss 502.2482 (500.5544)	
2019-01-08 12:01:21,794 - 10 - training_embed.py - training - Epoch: [3][1700/2022]	Time 0.007 (0.007)	Data 0.002 (0.003)	Loss 495.8365 (500.5303)	
2019-01-08 12:01:22,401 - 10 - training_embed.py - training - Epoch: [3][1800/2022]	Time 0.010 (0.007)	Data 0.002 (0.003)	Loss 496.6793 (500.5065)	
2019-01-08 12:01:23,052 - 10 - training_embed.py - training - Epoch: [3][1900/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 499.2282 (500.4853)	
2019-01-08 12:01:23,735 - 10 - training_embed.py - training - Epoch: [3][2000/2022]	Time 0.019 (0.007)	Data 0.009 (0.003)	Loss 492.9640 (500.4400)	
2019-01-08 12:01:23,886 - 10 - training_embed.py - training - loss: 500.370691
2019-01-08 12:01:23,886 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 12:01:25,130 - 10 - training_embed.py - training - Epoch: [4][100/2022]	Time 0.003 (0.007)	Data 0.002 (0.003)	Loss 499.9289 (500.0526)	
2019-01-08 12:01:25,952 - 10 - training_embed.py - training - Epoch: [4][200/2022]	Time 0.014 (0.007)	Data 0.002 (0.003)	Loss 495.7169 (499.9934)	
2019-01-08 12:01:26,780 - 10 - training_embed.py - training - Epoch: [4][300/2022]	Time 0.004 (0.007)	Data 0.002 (0.003)	Loss 501.3361 (499.8725)	
2019-01-08 12:01:27,754 - 10 - training_embed.py - training - Epoch: [4][400/2022]	Time 0.018 (0.007)	Data 0.016 (0.003)	Loss 498.0248 (499.9089)	
2019-01-08 12:01:28,582 - 10 - training_embed.py - training - Epoch: [4][500/2022]	Time 0.010 (0.007)	Data 0.001 (0.003)	Loss 499.8947 (499.9157)	
2019-01-08 12:01:29,340 - 10 - training_embed.py - training - Epoch: [4][600/2022]	Time 0.012 (0.007)	Data 0.000 (0.003)	Loss 502.3279 (499.8312)	
2019-01-08 12:01:29,939 - 10 - training_embed.py - training - Epoch: [4][700/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 497.2405 (499.7578)	
2019-01-08 12:01:30,551 - 10 - training_embed.py - training - Epoch: [4][800/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 491.4612 (499.6562)	
2019-01-08 12:01:31,344 - 10 - training_embed.py - training - Epoch: [4][900/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 497.7565 (499.6937)	
2019-01-08 12:01:32,008 - 10 - training_embed.py - training - Epoch: [4][1000/2022]	Time 0.009 (0.007)	Data 0.006 (0.003)	Loss 496.7979 (499.6670)	
2019-01-08 12:01:32,627 - 10 - training_embed.py - training - Epoch: [4][1100/2022]	Time 0.020 (0.007)	Data 0.017 (0.003)	Loss 502.7106 (499.5866)	
2019-01-08 12:01:33,243 - 10 - training_embed.py - training - Epoch: [4][1200/2022]	Time 0.009 (0.007)	Data 0.008 (0.003)	Loss 502.9139 (499.5798)	
2019-01-08 12:01:33,934 - 10 - training_embed.py - training - Epoch: [4][1300/2022]	Time 0.006 (0.007)	Data 0.004 (0.003)	Loss 505.3055 (499.5118)	
2019-01-08 12:01:34,639 - 10 - training_embed.py - training - Epoch: [4][1400/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 496.8177 (499.4727)	
2019-01-08 12:01:35,375 - 10 - training_embed.py - training - Epoch: [4][1500/2022]	Time 0.023 (0.007)	Data 0.002 (0.003)	Loss 505.4637 (499.4181)	
2019-01-08 12:01:35,933 - 10 - training_embed.py - training - Epoch: [4][1600/2022]	Time 0.011 (0.007)	Data 0.000 (0.003)	Loss 492.5667 (499.3558)	
2019-01-08 12:01:36,538 - 10 - training_embed.py - training - Epoch: [4][1700/2022]	Time 0.015 (0.007)	Data 0.000 (0.003)	Loss 500.0034 (499.3751)	
2019-01-08 12:01:37,203 - 10 - training_embed.py - training - Epoch: [4][1800/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 500.9479 (499.3631)	
2019-01-08 12:01:37,835 - 10 - training_embed.py - training - Epoch: [4][1900/2022]	Time 0.003 (0.007)	Data 0.002 (0.003)	Loss 496.5648 (499.3272)	
2019-01-08 12:01:38,569 - 10 - training_embed.py - training - Epoch: [4][2000/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 499.6971 (499.3231)	
2019-01-08 12:01:38,749 - 10 - training_embed.py - training - loss: 499.257578
2019-01-08 12:01:38,749 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 12:01:39,924 - 10 - training_embed.py - training - Epoch: [5][100/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 500.8234 (498.2274)	
2019-01-08 12:01:40,778 - 10 - training_embed.py - training - Epoch: [5][200/2022]	Time 0.004 (0.007)	Data 0.002 (0.003)	Loss 499.3466 (498.4552)	
2019-01-08 12:01:41,556 - 10 - training_embed.py - training - Epoch: [5][300/2022]	Time 0.012 (0.007)	Data 0.002 (0.003)	Loss 499.0858 (498.4550)	
2019-01-08 12:01:42,525 - 10 - training_embed.py - training - Epoch: [5][400/2022]	Time 0.011 (0.007)	Data 0.002 (0.003)	Loss 492.7418 (498.5705)	
2019-01-08 12:01:43,532 - 10 - training_embed.py - training - Epoch: [5][500/2022]	Time 0.003 (0.007)	Data 0.002 (0.003)	Loss 494.2251 (498.5232)	
2019-01-08 12:01:44,212 - 10 - training_embed.py - training - Epoch: [5][600/2022]	Time 0.001 (0.007)	Data 0.000 (0.003)	Loss 503.5008 (498.5198)	
2019-01-08 12:01:44,902 - 10 - training_embed.py - training - Epoch: [5][700/2022]	Time 0.018 (0.007)	Data 0.002 (0.003)	Loss 504.1207 (498.6266)	
2019-01-08 12:01:45,471 - 10 - training_embed.py - training - Epoch: [5][800/2022]	Time 0.003 (0.007)	Data 0.000 (0.003)	Loss 503.2428 (498.6567)	
2019-01-08 12:01:46,147 - 10 - training_embed.py - training - Epoch: [5][900/2022]	Time 0.005 (0.007)	Data 0.002 (0.003)	Loss 502.8638 (498.5820)	
2019-01-08 12:01:46,727 - 10 - training_embed.py - training - Epoch: [5][1000/2022]	Time 0.006 (0.007)	Data 0.005 (0.003)	Loss 500.3640 (498.5407)	
2019-01-08 12:01:47,336 - 10 - training_embed.py - training - Epoch: [5][1100/2022]	Time 0.004 (0.007)	Data 0.002 (0.003)	Loss 496.7427 (498.4801)	
2019-01-08 12:01:48,008 - 10 - training_embed.py - training - Epoch: [5][1200/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 499.0637 (498.4475)	
2019-01-08 12:01:48,767 - 10 - training_embed.py - training - Epoch: [5][1300/2022]	Time 0.001 (0.007)	Data 0.000 (0.003)	Loss 492.8469 (498.4304)	
2019-01-08 12:01:49,422 - 10 - training_embed.py - training - Epoch: [5][1400/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 493.0242 (498.3795)	
2019-01-08 12:01:49,998 - 10 - training_embed.py - training - Epoch: [5][1500/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 491.8882 (498.3083)	
2019-01-08 12:01:50,676 - 10 - training_embed.py - training - Epoch: [5][1600/2022]	Time 0.017 (0.007)	Data 0.014 (0.003)	Loss 497.1306 (498.2940)	
2019-01-08 12:01:51,340 - 10 - training_embed.py - training - Epoch: [5][1700/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 497.5582 (498.2938)	
2019-01-08 12:01:51,911 - 10 - training_embed.py - training - Epoch: [5][1800/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 507.5593 (498.2712)	
2019-01-08 12:01:52,505 - 10 - training_embed.py - training - Epoch: [5][1900/2022]	Time 0.005 (0.007)	Data 0.003 (0.003)	Loss 500.9123 (498.2484)	
2019-01-08 12:01:53,182 - 10 - training_embed.py - training - Epoch: [5][2000/2022]	Time 0.001 (0.007)	Data 0.000 (0.003)	Loss 503.8829 (498.2112)	
2019-01-08 12:01:53,290 - 10 - training_embed.py - training - loss: 498.131462
2019-01-08 12:01:53,290 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 12:01:54,306 - 10 - training_embed.py - training - Epoch: [6][100/2022]	Time 0.013 (0.007)	Data 0.003 (0.003)	Loss 492.8616 (496.9153)	
2019-01-08 12:01:54,853 - 10 - training_embed.py - training - Epoch: [6][200/2022]	Time 0.002 (0.007)	Data 0.000 (0.003)	Loss 490.3858 (497.2952)	
2019-01-08 12:01:55,399 - 10 - training_embed.py - training - Epoch: [6][300/2022]	Time 0.003 (0.007)	Data 0.002 (0.003)	Loss 503.4789 (497.4345)	
2019-01-08 12:01:55,888 - 10 - training_embed.py - training - Epoch: [6][400/2022]	Time 0.003 (0.007)	Data 0.002 (0.003)	Loss 496.8796 (497.2312)	
2019-01-08 12:01:56,406 - 10 - training_embed.py - training - Epoch: [6][500/2022]	Time 0.004 (0.007)	Data 0.002 (0.003)	Loss 495.0434 (497.1182)	
2019-01-08 12:01:56,934 - 10 - training_embed.py - training - Epoch: [6][600/2022]	Time 0.008 (0.007)	Data 0.000 (0.003)	Loss 508.5580 (497.1495)	
2019-01-08 12:01:57,442 - 10 - training_embed.py - training - Epoch: [6][700/2022]	Time 0.014 (0.007)	Data 0.013 (0.003)	Loss 501.7492 (497.1146)	
2019-01-08 12:01:58,096 - 10 - training_embed.py - training - Epoch: [6][800/2022]	Time 0.004 (0.007)	Data 0.003 (0.003)	Loss 499.3549 (497.1580)	
2019-01-08 12:01:58,699 - 10 - training_embed.py - training - Epoch: [6][900/2022]	Time 0.011 (0.007)	Data 0.006 (0.003)	Loss 500.1313 (497.1761)	
2019-01-08 12:01:59,319 - 10 - training_embed.py - training - Epoch: [6][1000/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 492.6013 (497.2128)	
2019-01-08 12:01:59,914 - 10 - training_embed.py - training - Epoch: [6][1100/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 500.3080 (497.2314)	
2019-01-08 12:02:00,555 - 10 - training_embed.py - training - Epoch: [6][1200/2022]	Time 0.005 (0.007)	Data 0.003 (0.003)	Loss 508.6719 (497.2046)	
2019-01-08 12:02:00,994 - 10 - training_embed.py - training - Epoch: [6][1300/2022]	Time 0.004 (0.007)	Data 0.002 (0.003)	Loss 490.9324 (497.2371)	
2019-01-08 12:02:01,417 - 10 - training_embed.py - training - Epoch: [6][1400/2022]	Time 0.010 (0.007)	Data 0.008 (0.003)	Loss 498.6797 (497.1772)	
2019-01-08 12:02:01,844 - 10 - training_embed.py - training - Epoch: [6][1500/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 499.1285 (497.1758)	
2019-01-08 12:02:02,226 - 10 - training_embed.py - training - Epoch: [6][1600/2022]	Time 0.005 (0.007)	Data 0.004 (0.003)	Loss 499.4733 (497.1511)	
2019-01-08 12:02:02,572 - 10 - training_embed.py - training - Epoch: [6][1700/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 495.1066 (497.1191)	
2019-01-08 12:02:02,832 - 10 - training_embed.py - training - Epoch: [6][1800/2022]	Time 0.004 (0.007)	Data 0.002 (0.003)	Loss 497.1548 (497.0785)	
2019-01-08 12:02:03,099 - 10 - training_embed.py - training - Epoch: [6][1900/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 502.9523 (497.0585)	
2019-01-08 12:02:03,371 - 10 - training_embed.py - training - Epoch: [6][2000/2022]	Time 0.002 (0.007)	Data 0.001 (0.003)	Loss 500.4125 (497.0320)	
2019-01-08 12:02:03,446 - 10 - training_embed.py - training - loss: 496.992801
2019-01-08 12:02:03,447 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 12:02:03,991 - 10 - training_embed.py - training - Epoch: [7][100/2022]	Time 0.013 (0.007)	Data 0.011 (0.003)	Loss 496.7470 (496.5979)	
2019-01-08 12:02:04,326 - 10 - training_embed.py - training - Epoch: [7][200/2022]	Time 0.002 (0.007)	Data 0.001 (0.003)	Loss 494.3530 (496.4288)	
2019-01-08 12:02:04,625 - 10 - training_embed.py - training - Epoch: [7][300/2022]	Time 0.003 (0.007)	Data 0.001 (0.003)	Loss 503.7869 (496.3066)	
2019-01-08 12:02:04,998 - 10 - training_embed.py - training - Epoch: [7][400/2022]	Time 0.006 (0.007)	Data 0.003 (0.003)	Loss 493.0792 (496.2389)	
2019-01-08 12:02:05,377 - 10 - training_embed.py - training - Epoch: [7][500/2022]	Time 0.004 (0.007)	Data 0.002 (0.003)	Loss 491.8873 (496.2961)	
2019-01-08 12:02:05,746 - 10 - training_embed.py - training - Epoch: [7][600/2022]	Time 0.003 (0.007)	Data 0.002 (0.003)	Loss 495.1492 (496.1717)	
2019-01-08 12:02:06,090 - 10 - training_embed.py - training - Epoch: [7][700/2022]	Time 0.004 (0.007)	Data 0.003 (0.003)	Loss 494.4995 (496.1894)	
2019-01-08 12:02:06,407 - 10 - training_embed.py - training - Epoch: [7][800/2022]	Time 0.002 (0.007)	Data 0.001 (0.003)	Loss 499.3320 (496.1469)	
2019-01-08 12:02:06,721 - 10 - training_embed.py - training - Epoch: [7][900/2022]	Time 0.003 (0.006)	Data 0.002 (0.003)	Loss 503.3560 (496.0624)	
2019-01-08 12:02:07,097 - 10 - training_embed.py - training - Epoch: [7][1000/2022]	Time 0.003 (0.006)	Data 0.002 (0.003)	Loss 494.0623 (496.0954)	
2019-01-08 12:02:07,419 - 10 - training_embed.py - training - Epoch: [7][1100/2022]	Time 0.004 (0.006)	Data 0.002 (0.003)	Loss 493.9109 (496.0331)	
2019-01-08 12:02:07,782 - 10 - training_embed.py - training - Epoch: [7][1200/2022]	Time 0.002 (0.006)	Data 0.000 (0.003)	Loss 501.3441 (496.0719)	
2019-01-08 12:02:08,111 - 10 - training_embed.py - training - Epoch: [7][1300/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 490.3035 (496.0109)	
2019-01-08 12:02:08,491 - 10 - training_embed.py - training - Epoch: [7][1400/2022]	Time 0.002 (0.006)	Data 0.000 (0.003)	Loss 491.5073 (496.0076)	
2019-01-08 12:02:08,784 - 10 - training_embed.py - training - Epoch: [7][1500/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 500.7948 (495.9782)	
2019-01-08 12:02:09,085 - 10 - training_embed.py - training - Epoch: [7][1600/2022]	Time 0.007 (0.006)	Data 0.002 (0.003)	Loss 492.4094 (495.9764)	
2019-01-08 12:02:09,465 - 10 - training_embed.py - training - Epoch: [7][1700/2022]	Time 0.009 (0.006)	Data 0.000 (0.003)	Loss 492.1279 (495.9522)	
2019-01-08 12:02:09,847 - 10 - training_embed.py - training - Epoch: [7][1800/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 496.5479 (495.9509)	
2019-01-08 12:02:10,155 - 10 - training_embed.py - training - Epoch: [7][1900/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 495.0320 (495.9100)	
2019-01-08 12:02:10,427 - 10 - training_embed.py - training - Epoch: [7][2000/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 496.2471 (495.8924)	
2019-01-08 12:02:10,479 - 10 - training_embed.py - training - loss: 495.839541
2019-01-08 12:02:10,480 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 12:02:10,922 - 10 - training_embed.py - training - Epoch: [8][100/2022]	Time 0.003 (0.006)	Data 0.002 (0.003)	Loss 496.6037 (495.5356)	
2019-01-08 12:02:11,180 - 10 - training_embed.py - training - Epoch: [8][200/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 504.0172 (495.1927)	
2019-01-08 12:02:11,432 - 10 - training_embed.py - training - Epoch: [8][300/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 494.7298 (495.1834)	
2019-01-08 12:02:11,686 - 10 - training_embed.py - training - Epoch: [8][400/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 500.9222 (495.1247)	
2019-01-08 12:02:11,952 - 10 - training_embed.py - training - Epoch: [8][500/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 495.9470 (495.0609)	
2019-01-08 12:02:12,264 - 10 - training_embed.py - training - Epoch: [8][600/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 491.4465 (495.1722)	
2019-01-08 12:02:12,584 - 10 - training_embed.py - training - Epoch: [8][700/2022]	Time 0.003 (0.006)	Data 0.002 (0.003)	Loss 501.9612 (495.2145)	
2019-01-08 12:02:12,887 - 10 - training_embed.py - training - Epoch: [8][800/2022]	Time 0.004 (0.006)	Data 0.002 (0.003)	Loss 501.8637 (495.1621)	
2019-01-08 12:02:13,256 - 10 - training_embed.py - training - Epoch: [8][900/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 493.5411 (495.0823)	
2019-01-08 12:02:13,590 - 10 - training_embed.py - training - Epoch: [8][1000/2022]	Time 0.006 (0.006)	Data 0.001 (0.003)	Loss 494.6193 (494.9464)	
2019-01-08 12:02:13,910 - 10 - training_embed.py - training - Epoch: [8][1100/2022]	Time 0.004 (0.006)	Data 0.002 (0.003)	Loss 497.7923 (494.9855)	
2019-01-08 12:02:14,283 - 10 - training_embed.py - training - Epoch: [8][1200/2022]	Time 0.001 (0.006)	Data 0.000 (0.003)	Loss 500.7766 (494.9467)	
2019-01-08 12:02:14,585 - 10 - training_embed.py - training - Epoch: [8][1300/2022]	Time 0.002 (0.006)	Data 0.000 (0.003)	Loss 495.5450 (494.8912)	
2019-01-08 12:02:14,956 - 10 - training_embed.py - training - Epoch: [8][1400/2022]	Time 0.007 (0.006)	Data 0.005 (0.003)	Loss 493.9003 (494.8894)	
2019-01-08 12:02:15,326 - 10 - training_embed.py - training - Epoch: [8][1500/2022]	Time 0.006 (0.006)	Data 0.004 (0.003)	Loss 491.0223 (494.8460)	
2019-01-08 12:02:15,585 - 10 - training_embed.py - training - Epoch: [8][1600/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 495.7946 (494.8285)	
2019-01-08 12:02:15,920 - 10 - training_embed.py - training - Epoch: [8][1700/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 495.2877 (494.8033)	
2019-01-08 12:02:16,223 - 10 - training_embed.py - training - Epoch: [8][1800/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 479.9246 (494.7817)	
2019-01-08 12:02:16,629 - 10 - training_embed.py - training - Epoch: [8][1900/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 497.6968 (494.7784)	
2019-01-08 12:02:16,999 - 10 - training_embed.py - training - Epoch: [8][2000/2022]	Time 0.006 (0.006)	Data 0.004 (0.003)	Loss 489.9622 (494.7438)	
2019-01-08 12:02:17,056 - 10 - training_embed.py - training - loss: 494.671336
2019-01-08 12:02:17,057 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 12:02:17,518 - 10 - training_embed.py - training - Epoch: [9][100/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 501.8944 (493.8743)	
2019-01-08 12:02:17,801 - 10 - training_embed.py - training - Epoch: [9][200/2022]	Time 0.004 (0.006)	Data 0.002 (0.003)	Loss 501.9573 (493.7930)	
2019-01-08 12:02:18,087 - 10 - training_embed.py - training - Epoch: [9][300/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 501.8893 (493.9470)	
2019-01-08 12:02:18,350 - 10 - training_embed.py - training - Epoch: [9][400/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 495.8959 (494.0557)	
2019-01-08 12:02:18,630 - 10 - training_embed.py - training - Epoch: [9][500/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 493.6894 (493.9982)	
2019-01-08 12:02:18,903 - 10 - training_embed.py - training - Epoch: [9][600/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 499.5958 (493.9169)	
2019-01-08 12:02:19,187 - 10 - training_embed.py - training - Epoch: [9][700/2022]	Time 0.003 (0.006)	Data 0.002 (0.003)	Loss 493.1873 (493.8703)	
2019-01-08 12:02:19,473 - 10 - training_embed.py - training - Epoch: [9][800/2022]	Time 0.003 (0.006)	Data 0.002 (0.003)	Loss 490.6579 (493.7896)	
2019-01-08 12:02:19,766 - 10 - training_embed.py - training - Epoch: [9][900/2022]	Time 0.003 (0.006)	Data 0.002 (0.003)	Loss 492.4551 (493.8054)	
2019-01-08 12:02:20,022 - 10 - training_embed.py - training - Epoch: [9][1000/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 495.9376 (493.7924)	
2019-01-08 12:02:20,305 - 10 - training_embed.py - training - Epoch: [9][1100/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 489.5330 (493.7745)	
2019-01-08 12:02:20,590 - 10 - training_embed.py - training - Epoch: [9][1200/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 492.5944 (493.7306)	
2019-01-08 12:02:20,894 - 10 - training_embed.py - training - Epoch: [9][1300/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 497.2383 (493.7016)	
2019-01-08 12:02:21,156 - 10 - training_embed.py - training - Epoch: [9][1400/2022]	Time 0.001 (0.006)	Data 0.000 (0.003)	Loss 491.0925 (493.6545)	
2019-01-08 12:02:21,421 - 10 - training_embed.py - training - Epoch: [9][1500/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 493.3766 (493.6749)	
2019-01-08 12:02:21,701 - 10 - training_embed.py - training - Epoch: [9][1600/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 498.3861 (493.6709)	
2019-01-08 12:02:21,962 - 10 - training_embed.py - training - Epoch: [9][1700/2022]	Time 0.003 (0.006)	Data 0.002 (0.003)	Loss 490.8913 (493.5951)	
2019-01-08 12:02:22,239 - 10 - training_embed.py - training - Epoch: [9][1800/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 497.8378 (493.5628)	
2019-01-08 12:02:22,510 - 10 - training_embed.py - training - Epoch: [9][1900/2022]	Time 0.006 (0.006)	Data 0.001 (0.003)	Loss 495.0518 (493.5192)	
2019-01-08 12:02:22,767 - 10 - training_embed.py - training - Epoch: [9][2000/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 501.7363 (493.5433)	
2019-01-08 12:02:22,814 - 10 - training_embed.py - training - loss: 493.488547
2019-01-08 12:02:22,814 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 12:02:23,230 - 10 - training_embed.py - training - Epoch: [10][100/2022]	Time 0.003 (0.006)	Data 0.002 (0.003)	Loss 488.9590 (492.3966)	
2019-01-08 12:02:23,499 - 10 - training_embed.py - training - Epoch: [10][200/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 493.3011 (492.1451)	
2019-01-08 12:02:23,779 - 10 - training_embed.py - training - Epoch: [10][300/2022]	Time 0.004 (0.006)	Data 0.003 (0.003)	Loss 491.8862 (492.4212)	
2019-01-08 12:02:24,069 - 10 - training_embed.py - training - Epoch: [10][400/2022]	Time 0.004 (0.006)	Data 0.002 (0.003)	Loss 486.7382 (492.5309)	
2019-01-08 12:02:24,348 - 10 - training_embed.py - training - Epoch: [10][500/2022]	Time 0.003 (0.006)	Data 0.002 (0.003)	Loss 492.9933 (492.4826)	
2019-01-08 12:02:24,636 - 10 - training_embed.py - training - Epoch: [10][600/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 498.8701 (492.5410)	
2019-01-08 12:02:24,918 - 10 - training_embed.py - training - Epoch: [10][700/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 492.2106 (492.5094)	
2019-01-08 12:02:25,196 - 10 - training_embed.py - training - Epoch: [10][800/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 498.1586 (492.5081)	
2019-01-08 12:02:25,485 - 10 - training_embed.py - training - Epoch: [10][900/2022]	Time 0.003 (0.006)	Data 0.001 (0.003)	Loss 491.6046 (492.4546)	
2019-01-08 12:02:25,758 - 10 - training_embed.py - training - Epoch: [10][1000/2022]	Time 0.002 (0.006)	Data 0.001 (0.003)	Loss 489.9442 (492.4103)	
2019-01-08 12:02:26,046 - 10 - training_embed.py - training - Epoch: [10][1100/2022]	Time 0.002 (0.005)	Data 0.001 (0.003)	Loss 484.0344 (492.4164)	
2019-01-08 12:02:26,327 - 10 - training_embed.py - training - Epoch: [10][1200/2022]	Time 0.003 (0.005)	Data 0.001 (0.003)	Loss 491.7614 (492.3871)	
2019-01-08 12:02:26,594 - 10 - training_embed.py - training - Epoch: [10][1300/2022]	Time 0.003 (0.005)	Data 0.001 (0.003)	Loss 495.5560 (492.3495)	
2019-01-08 12:02:26,884 - 10 - training_embed.py - training - Epoch: [10][1400/2022]	Time 0.003 (0.005)	Data 0.001 (0.003)	Loss 493.0851 (492.3694)	
2019-01-08 12:02:27,170 - 10 - training_embed.py - training - Epoch: [10][1500/2022]	Time 0.003 (0.005)	Data 0.002 (0.003)	Loss 489.4313 (492.3460)	
2019-01-08 12:02:27,443 - 10 - training_embed.py - training - Epoch: [10][1600/2022]	Time 0.001 (0.005)	Data 0.000 (0.003)	Loss 491.6932 (492.3494)	
2019-01-08 12:02:27,727 - 10 - training_embed.py - training - Epoch: [10][1700/2022]	Time 0.003 (0.005)	Data 0.001 (0.003)	Loss 492.7495 (492.3537)	
2019-01-08 12:02:27,999 - 10 - training_embed.py - training - Epoch: [10][1800/2022]	Time 0.002 (0.005)	Data 0.000 (0.003)	Loss 490.7288 (492.3944)	
2019-01-08 12:02:28,275 - 10 - training_embed.py - training - Epoch: [10][1900/2022]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 502.1610 (492.3854)	
2019-01-08 12:02:28,536 - 10 - training_embed.py - training - Epoch: [10][2000/2022]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 486.5558 (492.3709)	
2019-01-08 12:02:28,584 - 10 - training_embed.py - training - loss: 492.287105
2019-01-08 12:02:28,585 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 12:02:29,054 - 10 - training_embed.py - training - Epoch: [11][100/2022]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 495.2873 (491.4917)	
2019-01-08 12:02:29,346 - 10 - training_embed.py - training - Epoch: [11][200/2022]	Time 0.006 (0.005)	Data 0.001 (0.002)	Loss 496.7669 (491.7426)	
2019-01-08 12:02:29,626 - 10 - training_embed.py - training - Epoch: [11][300/2022]	Time 0.004 (0.005)	Data 0.001 (0.002)	Loss 488.7493 (491.6984)	
2019-01-08 12:02:29,927 - 10 - training_embed.py - training - Epoch: [11][400/2022]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 498.6279 (491.7439)	
2019-01-08 12:02:30,211 - 10 - training_embed.py - training - Epoch: [11][500/2022]	Time 0.005 (0.005)	Data 0.003 (0.002)	Loss 494.2650 (491.6028)	
2019-01-08 12:02:30,481 - 10 - training_embed.py - training - Epoch: [11][600/2022]	Time 0.005 (0.005)	Data 0.004 (0.002)	Loss 495.9416 (491.5721)	
2019-01-08 12:02:30,770 - 10 - training_embed.py - training - Epoch: [11][700/2022]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 491.9383 (491.5439)	
2019-01-08 12:02:31,070 - 10 - training_embed.py - training - Epoch: [11][800/2022]	Time 0.004 (0.005)	Data 0.002 (0.002)	Loss 486.1301 (491.4696)	
2019-01-08 12:02:31,369 - 10 - training_embed.py - training - Epoch: [11][900/2022]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 481.0225 (491.4471)	
2019-01-08 12:02:31,653 - 10 - training_embed.py - training - Epoch: [11][1000/2022]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 488.2674 (491.5433)	
2019-01-08 12:02:31,933 - 10 - training_embed.py - training - Epoch: [11][1100/2022]	Time 0.003 (0.005)	Data 0.001 (0.002)	Loss 489.9524 (491.4704)	
2019-01-08 12:02:32,218 - 10 - training_embed.py - training - Epoch: [11][1200/2022]	Time 0.004 (0.005)	Data 0.000 (0.002)	Loss 484.8929 (491.3928)	
2019-01-08 12:02:32,506 - 10 - training_embed.py - training - Epoch: [11][1300/2022]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 493.0031 (491.3454)	
2019-01-08 12:02:32,794 - 10 - training_embed.py - training - Epoch: [11][1400/2022]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 482.5066 (491.3051)	
2019-01-08 12:02:33,070 - 10 - training_embed.py - training - Epoch: [11][1500/2022]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 483.2682 (491.3013)	
2019-01-08 12:02:33,332 - 10 - training_embed.py - training - Epoch: [11][1600/2022]	Time 0.001 (0.005)	Data 0.000 (0.002)	Loss 490.8170 (491.2686)	
2019-01-08 12:02:33,610 - 10 - training_embed.py - training - Epoch: [11][1700/2022]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 490.4599 (491.2441)	
2019-01-08 12:02:33,882 - 10 - training_embed.py - training - Epoch: [11][1800/2022]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 489.4266 (491.2093)	
2019-01-08 12:02:34,160 - 10 - training_embed.py - training - Epoch: [11][1900/2022]	Time 0.003 (0.005)	Data 0.002 (0.002)	Loss 490.0839 (491.1607)	
2019-01-08 12:02:34,420 - 10 - training_embed.py - training - Epoch: [11][2000/2022]	Time 0.002 (0.005)	Data 0.001 (0.002)	Loss 487.7743 (491.1434)	
2019-01-08 12:02:34,477 - 10 - training_embed.py - training - loss: 491.066925
2019-01-08 12:02:34,550 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 12:02:35,667 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1117.125 ms ~ 0.019 min ~ 1.117 sec
2019-01-08 12:02:36,889 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 2339.514 ms ~ 0.039 min ~ 2.340 sec
2019-01-08 12:02:36,890 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 12:02:36,890 - 10 - corpus.py - subactivity_sampler - 0 / 166
2019-01-08 12:02:36,891 - 10 - corpus.py - subactivity_sampler - [63932. 32654. 30369. 58514. 26425. 88903. 12530. 88426. 18436. 47881.
 49408.]
2019-01-08 12:04:02,144 - 10 - corpus.py - subactivity_sampler - 20 / 166
2019-01-08 12:04:02,145 - 10 - corpus.py - subactivity_sampler - [64746. 31654. 29653. 59083. 25200. 91187. 11344. 90607. 16614. 48117.
 49273.]
2019-01-08 12:05:12,619 - 10 - corpus.py - subactivity_sampler - 40 / 166
2019-01-08 12:05:12,619 - 10 - corpus.py - subactivity_sampler - [65104. 31322. 29089. 59344. 23285. 93646. 10171. 92709. 15183. 48398.
 49227.]
2019-01-08 12:06:41,458 - 10 - corpus.py - subactivity_sampler - 60 / 166
2019-01-08 12:06:41,458 - 10 - corpus.py - subactivity_sampler - [65218. 31234. 28681. 60086. 22144. 94755.  9633. 93946. 14349. 48385.
 49047.]
2019-01-08 12:08:07,317 - 10 - corpus.py - subactivity_sampler - 80 / 166
2019-01-08 12:08:07,317 - 10 - corpus.py - subactivity_sampler - [65914. 30473. 28078. 60635. 20783. 96297.  8838. 96032. 13135. 48300.
 48993.]
2019-01-08 12:09:20,150 - 10 - corpus.py - subactivity_sampler - 100 / 166
2019-01-08 12:09:20,150 - 10 - corpus.py - subactivity_sampler - [66507. 29779. 27248. 61311. 19487. 97614.  8399. 97541. 12471. 48281.
 48840.]
2019-01-08 12:10:46,027 - 10 - corpus.py - subactivity_sampler - 120 / 166
2019-01-08 12:10:46,028 - 10 - corpus.py - subactivity_sampler - [66980. 29389. 26583. 61825. 18302. 98632.  7925. 98424. 11771. 48580.
 49067.]
2019-01-08 12:12:12,141 - 10 - corpus.py - subactivity_sampler - 140 / 166
2019-01-08 12:12:12,141 - 10 - corpus.py - subactivity_sampler - [67365. 29071. 25554. 63015. 17201. 99617.  7557. 98796. 11059. 49070.
 49173.]
2019-01-08 12:13:35,672 - 10 - corpus.py - subactivity_sampler - 160 / 166
2019-01-08 12:13:35,672 - 10 - corpus.py - subactivity_sampler - [ 67320.  28735.  25144.  63444.  16222. 100510.   7371.  99852.  10427.
  49111.  49342.]
2019-01-08 12:13:54,801 - 10 - corpus.py - subactivity_sampler - [ 67391.  28639.  24805.  63759.  15981. 100798.   7286. 100022.  10402.
  49003.  49392.]
2019-01-08 12:13:54,801 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 677911.548 ms ~ 11.299 min ~ 677.912 sec
2019-01-08 12:13:54,801 - 10 - corpus.py - ordering_sampler - .
2019-01-08 12:13:59,495 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 12:13:59,496 - 10 - corpus.py - ordering_sampler - inv_count_vec: [13.  6.  0.  4. 72. 39. 47. 26. 20.  3.]
2019-01-08 12:13:59,496 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 12:13:59,515 - 10 - corpus.py - rho_sampling - ['53.2532', '10.6038', '928.4017', '73.0209', '0.5780', '2.0664', '505.2272', '26.8903', '33.7846', '3.9117']
2019-01-08 12:13:59,515 - 10 - pipeline.py - baseline - Iteration 3
2019-01-08 12:13:59,695 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 12:13:59,707 - 10 - accuracy_class.py - mof - # gt_labels: 12   # pr_labels: 11
2019-01-08 12:13:59,707 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 11', '1: 10', '2: 42', '3: 17', '4: 43', '5: 12', '6: 16', '7: 44', '8: 4', '9: 14', '10: 15']
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_val - frames true: 195508	frames overall : 517478
2019-01-08 12:13:59,731 - 10 - corpus.py - accuracy_corpus - Action: scrambledegg
2019-01-08 12:13:59,731 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3778092981730624
2019-01-08 12:13:59,731 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.37564495495460676
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 29280
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1218
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_classes - label 10: 0.063923  699 / 10935
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_classes - label 11: 0.387283  21768 / 56207
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_classes - label 12: 0.330318  13290 / 40234
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_classes - label 14: 0.457214  8052 / 17611
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_classes - label 15: 0.715209  31088 / 43467
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 1003
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_classes - label 17: 0.290470  14335 / 49351
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_classes - label 42: 0.367892  11820 / 32129
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_classes - label 43: 0.096401  2215 / 22977
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_classes - label 44: 0.432922  92241 / 213066
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - mof_classes - average class mof: 0.261803
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 29280
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 11620
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - iou_classes - label 10: 0.017981  699 / 38875
2019-01-08 12:13:59,731 - 10 - accuracy_class.py - iou_classes - label 11: 0.213768  21768 / 101830
2019-01-08 12:13:59,732 - 10 - accuracy_class.py - iou_classes - label 12: 0.104038  13290 / 127742
2019-01-08 12:13:59,732 - 10 - accuracy_class.py - iou_classes - label 14: 0.137495  8052 / 58562
2019-01-08 12:13:59,732 - 10 - accuracy_class.py - iou_classes - label 15: 0.503278  31088 / 61771
2019-01-08 12:13:59,732 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 8289
2019-01-08 12:13:59,732 - 10 - accuracy_class.py - iou_classes - label 17: 0.145128  14335 / 98775
2019-01-08 12:13:59,732 - 10 - accuracy_class.py - iou_classes - label 42: 0.262003  11820 / 45114
2019-01-08 12:13:59,732 - 10 - accuracy_class.py - iou_classes - label 43: 0.060284  2215 / 36743
2019-01-08 12:13:59,732 - 10 - accuracy_class.py - iou_classes - label 44: 0.417669  92241 / 220847
2019-01-08 12:13:59,732 - 10 - accuracy_class.py - iou_classes - average IoU: 0.169240
2019-01-08 12:13:59,732 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.155137
2019-01-08 12:14:07,385 - 10 - f1_score.py - f1 - f1 score: 0.304642
2019-01-08 12:14:07,405 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 7889.319 ms ~ 0.131 min ~ 7.889 sec
2019-01-08 12:14:07,405 - 10 - corpus.py - embedding_training - .
2019-01-08 12:14:07,405 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 12:14:07,405 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 12:14:07,405 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 12:14:10,665 - 10 - training_embed.py - training - create model
2019-01-08 12:14:10,666 - 10 - training_embed.py - training - epochs: 12
2019-01-08 12:14:10,666 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 12:14:11,025 - 10 - training_embed.py - training - Epoch: [0][100/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 497.5266 (502.9525)	
2019-01-08 12:14:11,252 - 10 - training_embed.py - training - Epoch: [0][200/2022]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 502.1590 (502.5418)	
2019-01-08 12:14:11,472 - 10 - training_embed.py - training - Epoch: [0][300/2022]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 502.4008 (502.2235)	
2019-01-08 12:14:11,701 - 10 - training_embed.py - training - Epoch: [0][400/2022]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 502.7642 (502.1883)	
2019-01-08 12:14:11,922 - 10 - training_embed.py - training - Epoch: [0][500/2022]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 495.0972 (502.1486)	
2019-01-08 12:14:12,151 - 10 - training_embed.py - training - Epoch: [0][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.6262 (502.0660)	
2019-01-08 12:14:12,374 - 10 - training_embed.py - training - Epoch: [0][700/2022]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 505.8912 (501.9915)	
2019-01-08 12:14:12,595 - 10 - training_embed.py - training - Epoch: [0][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.9056 (501.9671)	
2019-01-08 12:14:12,824 - 10 - training_embed.py - training - Epoch: [0][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 508.5723 (501.8534)	
2019-01-08 12:14:13,066 - 10 - training_embed.py - training - Epoch: [0][1000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 502.1185 (501.8494)	
2019-01-08 12:14:13,299 - 10 - training_embed.py - training - Epoch: [0][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 505.4225 (501.8696)	
2019-01-08 12:14:13,523 - 10 - training_embed.py - training - Epoch: [0][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.7181 (501.8682)	
2019-01-08 12:14:13,750 - 10 - training_embed.py - training - Epoch: [0][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.9988 (501.7507)	
2019-01-08 12:14:13,963 - 10 - training_embed.py - training - Epoch: [0][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.0366 (501.7496)	
2019-01-08 12:14:14,197 - 10 - training_embed.py - training - Epoch: [0][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 504.9968 (501.7334)	
2019-01-08 12:14:14,428 - 10 - training_embed.py - training - Epoch: [0][1600/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 505.0333 (501.7044)	
2019-01-08 12:14:14,654 - 10 - training_embed.py - training - Epoch: [0][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.9499 (501.6397)	
2019-01-08 12:14:14,868 - 10 - training_embed.py - training - Epoch: [0][1800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 497.3732 (501.6146)	
2019-01-08 12:14:15,092 - 10 - training_embed.py - training - Epoch: [0][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.4993 (501.5721)	
2019-01-08 12:14:15,315 - 10 - training_embed.py - training - Epoch: [0][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.8146 (501.5396)	
2019-01-08 12:14:15,358 - 10 - training_embed.py - training - loss: 501.462792
2019-01-08 12:14:15,358 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 12:14:15,780 - 10 - training_embed.py - training - Epoch: [1][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.5194 (501.0628)	
2019-01-08 12:14:15,994 - 10 - training_embed.py - training - Epoch: [1][200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 508.1246 (500.9110)	
2019-01-08 12:14:16,217 - 10 - training_embed.py - training - Epoch: [1][300/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 504.2684 (500.7177)	
2019-01-08 12:14:16,441 - 10 - training_embed.py - training - Epoch: [1][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.5005 (500.4561)	
2019-01-08 12:14:16,658 - 10 - training_embed.py - training - Epoch: [1][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.0335 (500.3822)	
2019-01-08 12:14:16,886 - 10 - training_embed.py - training - Epoch: [1][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 504.9102 (500.4037)	
2019-01-08 12:14:17,113 - 10 - training_embed.py - training - Epoch: [1][700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 497.3655 (500.4150)	
2019-01-08 12:14:17,333 - 10 - training_embed.py - training - Epoch: [1][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.4196 (500.4000)	
2019-01-08 12:14:17,566 - 10 - training_embed.py - training - Epoch: [1][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.2208 (500.3548)	
2019-01-08 12:14:17,800 - 10 - training_embed.py - training - Epoch: [1][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.6476 (500.3206)	
2019-01-08 12:14:18,015 - 10 - training_embed.py - training - Epoch: [1][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.2242 (500.2680)	
2019-01-08 12:14:18,235 - 10 - training_embed.py - training - Epoch: [1][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 506.6483 (500.2136)	
2019-01-08 12:14:18,467 - 10 - training_embed.py - training - Epoch: [1][1300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 504.9396 (500.1307)	
2019-01-08 12:14:18,692 - 10 - training_embed.py - training - Epoch: [1][1400/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 504.2299 (500.1099)	
2019-01-08 12:14:18,917 - 10 - training_embed.py - training - Epoch: [1][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 503.1259 (500.0696)	
2019-01-08 12:14:19,139 - 10 - training_embed.py - training - Epoch: [1][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.8904 (500.0504)	
2019-01-08 12:14:19,390 - 10 - training_embed.py - training - Epoch: [1][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.0707 (500.0273)	
2019-01-08 12:14:19,603 - 10 - training_embed.py - training - Epoch: [1][1800/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 503.1324 (500.0003)	
2019-01-08 12:14:19,828 - 10 - training_embed.py - training - Epoch: [1][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.5522 (499.9695)	
2019-01-08 12:14:20,053 - 10 - training_embed.py - training - Epoch: [1][2000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 504.4493 (499.9367)	
2019-01-08 12:14:20,106 - 10 - training_embed.py - training - loss: 499.871326
2019-01-08 12:14:20,106 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 12:14:20,551 - 10 - training_embed.py - training - Epoch: [2][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.7200 (499.0856)	
2019-01-08 12:14:20,768 - 10 - training_embed.py - training - Epoch: [2][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.3942 (498.7577)	
2019-01-08 12:14:20,988 - 10 - training_embed.py - training - Epoch: [2][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.4332 (498.9634)	
2019-01-08 12:14:21,202 - 10 - training_embed.py - training - Epoch: [2][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.1197 (498.8872)	
2019-01-08 12:14:21,427 - 10 - training_embed.py - training - Epoch: [2][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.9050 (498.7974)	
2019-01-08 12:14:21,641 - 10 - training_embed.py - training - Epoch: [2][600/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 499.9693 (498.8709)	
2019-01-08 12:14:21,855 - 10 - training_embed.py - training - Epoch: [2][700/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 499.9331 (498.8181)	
2019-01-08 12:14:22,066 - 10 - training_embed.py - training - Epoch: [2][800/2022]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 497.1993 (498.7675)	
2019-01-08 12:14:22,291 - 10 - training_embed.py - training - Epoch: [2][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.9757 (498.7455)	
2019-01-08 12:14:22,516 - 10 - training_embed.py - training - Epoch: [2][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.4136 (498.6215)	
2019-01-08 12:14:22,751 - 10 - training_embed.py - training - Epoch: [2][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.1562 (498.6176)	
2019-01-08 12:14:22,968 - 10 - training_embed.py - training - Epoch: [2][1200/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 504.3074 (498.6196)	
2019-01-08 12:14:23,204 - 10 - training_embed.py - training - Epoch: [2][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.6633 (498.5958)	
2019-01-08 12:14:23,426 - 10 - training_embed.py - training - Epoch: [2][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.1278 (498.6091)	
2019-01-08 12:14:23,642 - 10 - training_embed.py - training - Epoch: [2][1500/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 502.1714 (498.5807)	
2019-01-08 12:14:23,861 - 10 - training_embed.py - training - Epoch: [2][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.9350 (498.4782)	
2019-01-08 12:14:24,101 - 10 - training_embed.py - training - Epoch: [2][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.0124 (498.4420)	
2019-01-08 12:14:24,333 - 10 - training_embed.py - training - Epoch: [2][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.2971 (498.4306)	
2019-01-08 12:14:24,553 - 10 - training_embed.py - training - Epoch: [2][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.6715 (498.3717)	
2019-01-08 12:14:24,791 - 10 - training_embed.py - training - Epoch: [2][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.0873 (498.3299)	
2019-01-08 12:14:24,841 - 10 - training_embed.py - training - loss: 498.266238
2019-01-08 12:14:24,842 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 12:14:25,264 - 10 - training_embed.py - training - Epoch: [3][100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 494.5741 (497.7885)	
2019-01-08 12:14:25,475 - 10 - training_embed.py - training - Epoch: [3][200/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 494.4869 (497.4218)	
2019-01-08 12:14:25,699 - 10 - training_embed.py - training - Epoch: [3][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.9017 (497.3529)	
2019-01-08 12:14:25,926 - 10 - training_embed.py - training - Epoch: [3][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.0344 (497.3740)	
2019-01-08 12:14:26,151 - 10 - training_embed.py - training - Epoch: [3][500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 499.2755 (497.3529)	
2019-01-08 12:14:26,387 - 10 - training_embed.py - training - Epoch: [3][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.2642 (497.3595)	
2019-01-08 12:14:26,600 - 10 - training_embed.py - training - Epoch: [3][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.5521 (497.2000)	
2019-01-08 12:14:26,813 - 10 - training_embed.py - training - Epoch: [3][800/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 494.9638 (497.1655)	
2019-01-08 12:14:27,036 - 10 - training_embed.py - training - Epoch: [3][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.2988 (497.1446)	
2019-01-08 12:14:27,263 - 10 - training_embed.py - training - Epoch: [3][1000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 498.9323 (497.0813)	
2019-01-08 12:14:27,494 - 10 - training_embed.py - training - Epoch: [3][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 491.6414 (497.0337)	
2019-01-08 12:14:27,714 - 10 - training_embed.py - training - Epoch: [3][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.7869 (496.9991)	
2019-01-08 12:14:27,947 - 10 - training_embed.py - training - Epoch: [3][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.9995 (496.9665)	
2019-01-08 12:14:28,164 - 10 - training_embed.py - training - Epoch: [3][1400/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 495.2342 (496.9215)	
2019-01-08 12:14:28,384 - 10 - training_embed.py - training - Epoch: [3][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.0339 (496.9070)	
2019-01-08 12:14:28,604 - 10 - training_embed.py - training - Epoch: [3][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.9151 (496.8918)	
2019-01-08 12:14:28,828 - 10 - training_embed.py - training - Epoch: [3][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.6378 (496.8507)	
2019-01-08 12:14:29,054 - 10 - training_embed.py - training - Epoch: [3][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.6661 (496.8288)	
2019-01-08 12:14:29,272 - 10 - training_embed.py - training - Epoch: [3][1900/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 498.4472 (496.7651)	
2019-01-08 12:14:29,501 - 10 - training_embed.py - training - Epoch: [3][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.4532 (496.7152)	
2019-01-08 12:14:29,547 - 10 - training_embed.py - training - loss: 496.639537
2019-01-08 12:14:29,547 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 12:14:29,969 - 10 - training_embed.py - training - Epoch: [4][100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 493.7335 (495.7215)	
2019-01-08 12:14:30,185 - 10 - training_embed.py - training - Epoch: [4][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.1700 (495.8838)	
2019-01-08 12:14:30,406 - 10 - training_embed.py - training - Epoch: [4][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.2039 (495.8555)	
2019-01-08 12:14:30,618 - 10 - training_embed.py - training - Epoch: [4][400/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 492.5250 (495.7986)	
2019-01-08 12:14:30,849 - 10 - training_embed.py - training - Epoch: [4][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.3618 (495.8082)	
2019-01-08 12:14:31,077 - 10 - training_embed.py - training - Epoch: [4][600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 497.8997 (495.7099)	
2019-01-08 12:14:31,298 - 10 - training_embed.py - training - Epoch: [4][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.5912 (495.6103)	
2019-01-08 12:14:31,546 - 10 - training_embed.py - training - Epoch: [4][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.2869 (495.5279)	
2019-01-08 12:14:31,765 - 10 - training_embed.py - training - Epoch: [4][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.7901 (495.5552)	
2019-01-08 12:14:32,007 - 10 - training_embed.py - training - Epoch: [4][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.7471 (495.5191)	
2019-01-08 12:14:32,228 - 10 - training_embed.py - training - Epoch: [4][1100/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 493.9604 (495.4418)	
2019-01-08 12:14:32,470 - 10 - training_embed.py - training - Epoch: [4][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.6075 (495.3960)	
2019-01-08 12:14:32,695 - 10 - training_embed.py - training - Epoch: [4][1300/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 502.3089 (495.3398)	
2019-01-08 12:14:32,914 - 10 - training_embed.py - training - Epoch: [4][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.4515 (495.2746)	
2019-01-08 12:14:33,127 - 10 - training_embed.py - training - Epoch: [4][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.0192 (495.2122)	
2019-01-08 12:14:33,351 - 10 - training_embed.py - training - Epoch: [4][1600/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 493.9660 (495.1519)	
2019-01-08 12:14:33,574 - 10 - training_embed.py - training - Epoch: [4][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.5392 (495.1334)	
2019-01-08 12:14:33,803 - 10 - training_embed.py - training - Epoch: [4][1800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 495.8546 (495.1106)	
2019-01-08 12:14:34,036 - 10 - training_embed.py - training - Epoch: [4][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.9166 (495.0790)	
2019-01-08 12:14:34,268 - 10 - training_embed.py - training - Epoch: [4][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.5845 (495.0613)	
2019-01-08 12:14:34,317 - 10 - training_embed.py - training - loss: 494.994580
2019-01-08 12:14:34,317 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 12:14:34,755 - 10 - training_embed.py - training - Epoch: [5][100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 496.4825 (494.0234)	
2019-01-08 12:14:34,994 - 10 - training_embed.py - training - Epoch: [5][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.6575 (494.0730)	
2019-01-08 12:14:35,226 - 10 - training_embed.py - training - Epoch: [5][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.4507 (494.0515)	
2019-01-08 12:14:35,469 - 10 - training_embed.py - training - Epoch: [5][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.4312 (494.1945)	
2019-01-08 12:14:35,695 - 10 - training_embed.py - training - Epoch: [5][500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 488.4327 (494.0207)	
2019-01-08 12:14:35,937 - 10 - training_embed.py - training - Epoch: [5][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.6670 (493.9566)	
2019-01-08 12:14:36,185 - 10 - training_embed.py - training - Epoch: [5][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.9584 (494.0123)	
2019-01-08 12:14:36,430 - 10 - training_embed.py - training - Epoch: [5][800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 496.3239 (493.9920)	
2019-01-08 12:14:36,679 - 10 - training_embed.py - training - Epoch: [5][900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 499.8712 (493.8920)	
2019-01-08 12:14:36,922 - 10 - training_embed.py - training - Epoch: [5][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.2390 (493.8869)	
2019-01-08 12:14:37,150 - 10 - training_embed.py - training - Epoch: [5][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.4731 (493.8196)	
2019-01-08 12:14:37,382 - 10 - training_embed.py - training - Epoch: [5][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.7556 (493.7504)	
2019-01-08 12:14:37,596 - 10 - training_embed.py - training - Epoch: [5][1300/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 488.6915 (493.7033)	
2019-01-08 12:14:37,812 - 10 - training_embed.py - training - Epoch: [5][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.1411 (493.6290)	
2019-01-08 12:14:38,039 - 10 - training_embed.py - training - Epoch: [5][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 487.2333 (493.5626)	
2019-01-08 12:14:38,270 - 10 - training_embed.py - training - Epoch: [5][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.0239 (493.5362)	
2019-01-08 12:14:38,492 - 10 - training_embed.py - training - Epoch: [5][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.0147 (493.5176)	
2019-01-08 12:14:38,718 - 10 - training_embed.py - training - Epoch: [5][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.9582 (493.4805)	
2019-01-08 12:14:38,946 - 10 - training_embed.py - training - Epoch: [5][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.3000 (493.4508)	
2019-01-08 12:14:39,168 - 10 - training_embed.py - training - Epoch: [5][2000/2022]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 496.4886 (493.4050)	
2019-01-08 12:14:39,219 - 10 - training_embed.py - training - loss: 493.324582
2019-01-08 12:14:39,220 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 12:14:39,632 - 10 - training_embed.py - training - Epoch: [6][100/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 490.0430 (491.8802)	
2019-01-08 12:14:39,857 - 10 - training_embed.py - training - Epoch: [6][200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 485.6357 (492.1365)	
2019-01-08 12:14:40,102 - 10 - training_embed.py - training - Epoch: [6][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.4716 (492.1537)	
2019-01-08 12:14:40,320 - 10 - training_embed.py - training - Epoch: [6][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.0724 (492.0450)	
2019-01-08 12:14:40,541 - 10 - training_embed.py - training - Epoch: [6][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.5649 (491.9388)	
2019-01-08 12:14:40,777 - 10 - training_embed.py - training - Epoch: [6][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.4509 (492.0014)	
2019-01-08 12:14:40,997 - 10 - training_embed.py - training - Epoch: [6][700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 495.1349 (491.9768)	
2019-01-08 12:14:41,234 - 10 - training_embed.py - training - Epoch: [6][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.3450 (491.9918)	
2019-01-08 12:14:41,451 - 10 - training_embed.py - training - Epoch: [6][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.2312 (491.9435)	
2019-01-08 12:14:41,673 - 10 - training_embed.py - training - Epoch: [6][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.9233 (491.9998)	
2019-01-08 12:14:41,902 - 10 - training_embed.py - training - Epoch: [6][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 491.1507 (491.9584)	
2019-01-08 12:14:42,127 - 10 - training_embed.py - training - Epoch: [6][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.0520 (491.9626)	
2019-01-08 12:14:42,361 - 10 - training_embed.py - training - Epoch: [6][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.7177 (491.9659)	
2019-01-08 12:14:42,593 - 10 - training_embed.py - training - Epoch: [6][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.1971 (491.9095)	
2019-01-08 12:14:42,820 - 10 - training_embed.py - training - Epoch: [6][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.9235 (491.8987)	
2019-01-08 12:14:43,056 - 10 - training_embed.py - training - Epoch: [6][1600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 495.4122 (491.8648)	
2019-01-08 12:14:43,281 - 10 - training_embed.py - training - Epoch: [6][1700/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 488.8734 (491.7923)	
2019-01-08 12:14:43,497 - 10 - training_embed.py - training - Epoch: [6][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.4305 (491.7420)	
2019-01-08 12:14:43,724 - 10 - training_embed.py - training - Epoch: [6][1900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 497.7015 (491.7125)	
2019-01-08 12:14:43,947 - 10 - training_embed.py - training - Epoch: [6][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.0031 (491.6699)	
2019-01-08 12:14:43,995 - 10 - training_embed.py - training - loss: 491.629420
2019-01-08 12:14:43,996 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 12:14:44,400 - 10 - training_embed.py - training - Epoch: [7][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.3979 (490.4059)	
2019-01-08 12:14:44,632 - 10 - training_embed.py - training - Epoch: [7][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.2266 (490.4442)	
2019-01-08 12:14:44,859 - 10 - training_embed.py - training - Epoch: [7][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.2557 (490.5195)	
2019-01-08 12:14:45,078 - 10 - training_embed.py - training - Epoch: [7][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.8964 (490.4587)	
2019-01-08 12:14:45,291 - 10 - training_embed.py - training - Epoch: [7][500/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 485.7761 (490.5270)	
2019-01-08 12:14:45,508 - 10 - training_embed.py - training - Epoch: [7][600/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 487.4937 (490.4756)	
2019-01-08 12:14:45,734 - 10 - training_embed.py - training - Epoch: [7][700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 487.3364 (490.4881)	
2019-01-08 12:14:45,949 - 10 - training_embed.py - training - Epoch: [7][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.3870 (490.4137)	
2019-01-08 12:14:46,164 - 10 - training_embed.py - training - Epoch: [7][900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 499.0172 (490.3438)	
2019-01-08 12:14:46,386 - 10 - training_embed.py - training - Epoch: [7][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.6082 (490.3337)	
2019-01-08 12:14:46,610 - 10 - training_embed.py - training - Epoch: [7][1100/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 487.7000 (490.2628)	
2019-01-08 12:14:46,835 - 10 - training_embed.py - training - Epoch: [7][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.9839 (490.2766)	
2019-01-08 12:14:47,054 - 10 - training_embed.py - training - Epoch: [7][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.8848 (490.2040)	
2019-01-08 12:14:47,284 - 10 - training_embed.py - training - Epoch: [7][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.9149 (490.1867)	
2019-01-08 12:14:47,522 - 10 - training_embed.py - training - Epoch: [7][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.9062 (490.1602)	
2019-01-08 12:14:47,738 - 10 - training_embed.py - training - Epoch: [7][1600/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 488.8723 (490.1302)	
2019-01-08 12:14:47,947 - 10 - training_embed.py - training - Epoch: [7][1700/2022]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 485.4284 (490.1101)	
2019-01-08 12:14:48,176 - 10 - training_embed.py - training - Epoch: [7][1800/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 489.2775 (490.0889)	
2019-01-08 12:14:48,393 - 10 - training_embed.py - training - Epoch: [7][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.6119 (490.0250)	
2019-01-08 12:14:48,618 - 10 - training_embed.py - training - Epoch: [7][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.2512 (489.9758)	
2019-01-08 12:14:48,663 - 10 - training_embed.py - training - loss: 489.907214
2019-01-08 12:14:48,663 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 12:14:49,098 - 10 - training_embed.py - training - Epoch: [8][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.1029 (489.3275)	
2019-01-08 12:14:49,328 - 10 - training_embed.py - training - Epoch: [8][200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 496.5400 (488.8994)	
2019-01-08 12:14:49,550 - 10 - training_embed.py - training - Epoch: [8][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.9832 (488.7863)	
2019-01-08 12:14:49,769 - 10 - training_embed.py - training - Epoch: [8][400/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 491.7585 (488.8388)	
2019-01-08 12:14:49,993 - 10 - training_embed.py - training - Epoch: [8][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.8245 (488.7735)	
2019-01-08 12:14:50,210 - 10 - training_embed.py - training - Epoch: [8][600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 487.8759 (488.8738)	
2019-01-08 12:14:50,448 - 10 - training_embed.py - training - Epoch: [8][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.3423 (488.9243)	
2019-01-08 12:14:50,680 - 10 - training_embed.py - training - Epoch: [8][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.5046 (488.8745)	
2019-01-08 12:14:50,902 - 10 - training_embed.py - training - Epoch: [8][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.5576 (488.7455)	
2019-01-08 12:14:51,138 - 10 - training_embed.py - training - Epoch: [8][1000/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 489.9881 (488.5791)	
2019-01-08 12:14:51,369 - 10 - training_embed.py - training - Epoch: [8][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.3210 (488.6070)	
2019-01-08 12:14:51,589 - 10 - training_embed.py - training - Epoch: [8][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.3919 (488.5454)	
2019-01-08 12:14:51,821 - 10 - training_embed.py - training - Epoch: [8][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.1597 (488.4728)	
2019-01-08 12:14:52,070 - 10 - training_embed.py - training - Epoch: [8][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.2274 (488.4441)	
2019-01-08 12:14:52,297 - 10 - training_embed.py - training - Epoch: [8][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 485.8589 (488.3764)	
2019-01-08 12:14:52,519 - 10 - training_embed.py - training - Epoch: [8][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.9306 (488.3409)	
2019-01-08 12:14:52,736 - 10 - training_embed.py - training - Epoch: [8][1700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 493.1860 (488.3156)	
2019-01-08 12:14:52,977 - 10 - training_embed.py - training - Epoch: [8][1800/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 470.1411 (488.2909)	
2019-01-08 12:14:53,199 - 10 - training_embed.py - training - Epoch: [8][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.7017 (488.2687)	
2019-01-08 12:14:53,422 - 10 - training_embed.py - training - Epoch: [8][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.2197 (488.2270)	
2019-01-08 12:14:53,469 - 10 - training_embed.py - training - loss: 488.155970
2019-01-08 12:14:53,470 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 12:14:53,900 - 10 - training_embed.py - training - Epoch: [9][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.3210 (486.7794)	
2019-01-08 12:14:54,126 - 10 - training_embed.py - training - Epoch: [9][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.5146 (486.7976)	
2019-01-08 12:14:54,355 - 10 - training_embed.py - training - Epoch: [9][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.0200 (487.0362)	
2019-01-08 12:14:54,578 - 10 - training_embed.py - training - Epoch: [9][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.1508 (487.1044)	
2019-01-08 12:14:54,799 - 10 - training_embed.py - training - Epoch: [9][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.7385 (487.0551)	
2019-01-08 12:14:55,016 - 10 - training_embed.py - training - Epoch: [9][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.2847 (486.9481)	
2019-01-08 12:14:55,238 - 10 - training_embed.py - training - Epoch: [9][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.3604 (486.8619)	
2019-01-08 12:14:55,482 - 10 - training_embed.py - training - Epoch: [9][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.6419 (486.7635)	
2019-01-08 12:14:55,702 - 10 - training_embed.py - training - Epoch: [9][900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 483.6522 (486.8028)	
2019-01-08 12:14:55,922 - 10 - training_embed.py - training - Epoch: [9][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.5081 (486.7709)	
2019-01-08 12:14:56,139 - 10 - training_embed.py - training - Epoch: [9][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.7766 (486.7695)	
2019-01-08 12:14:56,357 - 10 - training_embed.py - training - Epoch: [9][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.8609 (486.7323)	
2019-01-08 12:14:56,581 - 10 - training_embed.py - training - Epoch: [9][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.9241 (486.7088)	
2019-01-08 12:14:56,805 - 10 - training_embed.py - training - Epoch: [9][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.1430 (486.5926)	
2019-01-08 12:14:57,022 - 10 - training_embed.py - training - Epoch: [9][1500/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 487.8517 (486.6373)	
2019-01-08 12:14:57,233 - 10 - training_embed.py - training - Epoch: [9][1600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 489.9124 (486.6408)	
2019-01-08 12:14:57,467 - 10 - training_embed.py - training - Epoch: [9][1700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 479.7022 (486.5417)	
2019-01-08 12:14:57,693 - 10 - training_embed.py - training - Epoch: [9][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.2676 (486.4983)	
2019-01-08 12:14:57,914 - 10 - training_embed.py - training - Epoch: [9][1900/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 487.9362 (486.4354)	
2019-01-08 12:14:58,154 - 10 - training_embed.py - training - Epoch: [9][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.9570 (486.4336)	
2019-01-08 12:14:58,200 - 10 - training_embed.py - training - loss: 486.371638
2019-01-08 12:14:58,200 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 12:14:58,621 - 10 - training_embed.py - training - Epoch: [10][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.4845 (485.1165)	
2019-01-08 12:14:58,861 - 10 - training_embed.py - training - Epoch: [10][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.9217 (484.8164)	
2019-01-08 12:14:59,101 - 10 - training_embed.py - training - Epoch: [10][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.8211 (484.8995)	
2019-01-08 12:14:59,336 - 10 - training_embed.py - training - Epoch: [10][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.2050 (485.0290)	
2019-01-08 12:14:59,545 - 10 - training_embed.py - training - Epoch: [10][500/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 483.2473 (484.9782)	
2019-01-08 12:14:59,775 - 10 - training_embed.py - training - Epoch: [10][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.8730 (485.0921)	
2019-01-08 12:15:00,000 - 10 - training_embed.py - training - Epoch: [10][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.3615 (485.0546)	
2019-01-08 12:15:00,230 - 10 - training_embed.py - training - Epoch: [10][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.3549 (485.0600)	
2019-01-08 12:15:00,451 - 10 - training_embed.py - training - Epoch: [10][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.3023 (484.9362)	
2019-01-08 12:15:00,670 - 10 - training_embed.py - training - Epoch: [10][1000/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 481.5704 (484.8767)	
2019-01-08 12:15:00,914 - 10 - training_embed.py - training - Epoch: [10][1100/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 476.7598 (484.9207)	
2019-01-08 12:15:01,136 - 10 - training_embed.py - training - Epoch: [10][1200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 479.6045 (484.8325)	
2019-01-08 12:15:01,355 - 10 - training_embed.py - training - Epoch: [10][1300/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 485.7709 (484.7873)	
2019-01-08 12:15:01,583 - 10 - training_embed.py - training - Epoch: [10][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.2637 (484.7760)	
2019-01-08 12:15:01,804 - 10 - training_embed.py - training - Epoch: [10][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.0595 (484.7545)	
2019-01-08 12:15:02,034 - 10 - training_embed.py - training - Epoch: [10][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.0311 (484.7164)	
2019-01-08 12:15:02,252 - 10 - training_embed.py - training - Epoch: [10][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.7501 (484.7153)	
2019-01-08 12:15:02,488 - 10 - training_embed.py - training - Epoch: [10][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.5065 (484.7472)	
2019-01-08 12:15:02,718 - 10 - training_embed.py - training - Epoch: [10][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.9202 (484.6829)	
2019-01-08 12:15:02,935 - 10 - training_embed.py - training - Epoch: [10][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 478.9648 (484.6371)	
2019-01-08 12:15:02,982 - 10 - training_embed.py - training - loss: 484.551741
2019-01-08 12:15:02,982 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 12:15:03,434 - 10 - training_embed.py - training - Epoch: [11][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.4142 (483.8057)	
2019-01-08 12:15:03,653 - 10 - training_embed.py - training - Epoch: [11][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.3190 (483.9340)	
2019-01-08 12:15:03,879 - 10 - training_embed.py - training - Epoch: [11][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.4458 (483.7357)	
2019-01-08 12:15:04,102 - 10 - training_embed.py - training - Epoch: [11][400/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 490.5647 (483.7466)	
2019-01-08 12:15:04,334 - 10 - training_embed.py - training - Epoch: [11][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.1399 (483.6090)	
2019-01-08 12:15:04,555 - 10 - training_embed.py - training - Epoch: [11][600/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 489.1113 (483.5920)	
2019-01-08 12:15:04,775 - 10 - training_embed.py - training - Epoch: [11][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.6373 (483.5051)	
2019-01-08 12:15:05,001 - 10 - training_embed.py - training - Epoch: [11][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 479.1918 (483.4711)	
2019-01-08 12:15:05,227 - 10 - training_embed.py - training - Epoch: [11][900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 467.8685 (483.4126)	
2019-01-08 12:15:05,462 - 10 - training_embed.py - training - Epoch: [11][1000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 474.6400 (483.4265)	
2019-01-08 12:15:05,685 - 10 - training_embed.py - training - Epoch: [11][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 477.6127 (483.3576)	
2019-01-08 12:15:05,911 - 10 - training_embed.py - training - Epoch: [11][1200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 472.3879 (483.2617)	
2019-01-08 12:15:06,149 - 10 - training_embed.py - training - Epoch: [11][1300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 481.1509 (483.1600)	
2019-01-08 12:15:06,375 - 10 - training_embed.py - training - Epoch: [11][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 471.5085 (483.1150)	
2019-01-08 12:15:06,602 - 10 - training_embed.py - training - Epoch: [11][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 478.3334 (483.0784)	
2019-01-08 12:15:06,824 - 10 - training_embed.py - training - Epoch: [11][1600/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 482.3102 (483.0320)	
2019-01-08 12:15:07,038 - 10 - training_embed.py - training - Epoch: [11][1700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 484.2253 (482.9488)	
2019-01-08 12:15:07,270 - 10 - training_embed.py - training - Epoch: [11][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 478.7211 (482.8906)	
2019-01-08 12:15:07,488 - 10 - training_embed.py - training - Epoch: [11][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 477.9630 (482.8045)	
2019-01-08 12:15:07,715 - 10 - training_embed.py - training - Epoch: [11][2000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 480.1830 (482.7760)	
2019-01-08 12:15:07,769 - 10 - training_embed.py - training - loss: 482.694838
2019-01-08 12:15:07,836 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 12:15:08,830 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 993.992 ms ~ 0.017 min ~ 0.994 sec
2019-01-08 12:15:09,848 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 2012.380 ms ~ 0.034 min ~ 2.012 sec
2019-01-08 12:15:09,849 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 12:15:09,849 - 10 - corpus.py - subactivity_sampler - 0 / 166
2019-01-08 12:15:09,849 - 10 - corpus.py - subactivity_sampler - [ 67391.  28639.  24805.  63759.  15981. 100798.   7286. 100022.  10402.
  49003.  49392.]
2019-01-08 12:16:32,945 - 10 - corpus.py - subactivity_sampler - 20 / 166
2019-01-08 12:16:32,945 - 10 - corpus.py - subactivity_sampler - [ 67733.  28102.  24372.  63837.  15395. 102511.   6942. 100479.   9767.
  48921.  49419.]
2019-01-08 12:17:41,986 - 10 - corpus.py - subactivity_sampler - 40 / 166
2019-01-08 12:17:41,987 - 10 - corpus.py - subactivity_sampler - [ 68267.  27358.  24120.  63918.  14901. 103589.   6495. 101580.   8787.
  49003.  49460.]
2019-01-08 12:19:11,783 - 10 - corpus.py - subactivity_sampler - 60 / 166
2019-01-08 12:19:11,783 - 10 - corpus.py - subactivity_sampler - [ 68292.  27301.  23716.  64226.  14746. 103891.   6209. 102095.   8567.
  48902.  49533.]
2019-01-08 12:20:40,174 - 10 - corpus.py - subactivity_sampler - 80 / 166
2019-01-08 12:20:40,174 - 10 - corpus.py - subactivity_sampler - [ 68380.  27115.  23370.  64444.  14337. 104253.   5623. 102311.   9102.
  48939.  49604.]
2019-01-08 12:21:53,107 - 10 - corpus.py - subactivity_sampler - 100 / 166
2019-01-08 12:21:53,108 - 10 - corpus.py - subactivity_sampler - [ 68314.  26791.  23196.  64110.  14109. 105512.   5440. 103316.   8567.
  48643.  49480.]
2019-01-08 12:23:19,134 - 10 - corpus.py - subactivity_sampler - 120 / 166
2019-01-08 12:23:19,135 - 10 - corpus.py - subactivity_sampler - [ 68374.  26733.  22854.  64405.  13768. 105669.   5170. 104366.   8293.
  48295.  49551.]
2019-01-08 12:24:45,627 - 10 - corpus.py - subactivity_sampler - 140 / 166
2019-01-08 12:24:45,627 - 10 - corpus.py - subactivity_sampler - [ 68423.  26245.  22489.  64738.  13215. 106941.   5229. 104781.   8192.
  47608.  49617.]
2019-01-08 12:26:09,939 - 10 - corpus.py - subactivity_sampler - 160 / 166
2019-01-08 12:26:09,940 - 10 - corpus.py - subactivity_sampler - [ 68516.  26142.  22290.  64564.  12966. 107806.   5171. 105174.   8108.
  47293.  49448.]
2019-01-08 12:26:29,156 - 10 - corpus.py - subactivity_sampler - [ 68524.  26123.  22183.  64714.  12904. 107949.   5134. 105190.   8102.
  47332.  49323.]
2019-01-08 12:26:29,156 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 679307.879 ms ~ 11.322 min ~ 679.308 sec
2019-01-08 12:26:29,156 - 10 - corpus.py - ordering_sampler - .
2019-01-08 12:26:33,932 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 12:26:33,932 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 13.  10.   0.   8. 115.  77.   2.  25.  18.  14.]
2019-01-08 12:26:33,932 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 12:26:33,959 - 10 - corpus.py - rho_sampling - ['52.2900', '21.2739', '928.2654', '80.2616', '9.3966', '0.1782', '6.1874', '47.1110', '4.5060', '362.3455']
2019-01-08 12:26:33,959 - 10 - pipeline.py - baseline - Iteration 4
2019-01-08 12:26:34,141 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 12:26:34,153 - 10 - accuracy_class.py - mof - # gt_labels: 12   # pr_labels: 11
2019-01-08 12:26:34,153 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 11', '1: 12', '2: 42', '3: 17', '4: 16', '5: 43', '6: 10', '7: 44', '8: 4', '9: 14', '10: 15']
2019-01-08 12:26:34,182 - 10 - accuracy_class.py - mof_val - frames true: 199428	frames overall : 517478
2019-01-08 12:26:34,182 - 10 - corpus.py - accuracy_corpus - Action: scrambledegg
2019-01-08 12:26:34,182 - 10 - corpus.py - accuracy_corpus - MoF val: 0.38538449943765724
2019-01-08 12:26:34,182 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3834501176861625
2019-01-08 12:26:34,182 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 29280
2019-01-08 12:26:34,182 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1218
2019-01-08 12:26:34,182 - 10 - accuracy_class.py - mof_classes - label 10: 0.021674  237 / 10935
2019-01-08 12:26:34,182 - 10 - accuracy_class.py - mof_classes - label 11: 0.393350  22109 / 56207
2019-01-08 12:26:34,182 - 10 - accuracy_class.py - mof_classes - label 12: 0.076875  3093 / 40234
2019-01-08 12:26:34,182 - 10 - accuracy_class.py - mof_classes - label 14: 0.415479  7317 / 17611
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - mof_classes - label 15: 0.707203  30740 / 43467
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 1003
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - mof_classes - label 17: 0.307917  15196 / 49351
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - mof_classes - label 42: 0.353637  11362 / 32129
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - mof_classes - label 43: 0.578840  13300 / 22977
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - mof_classes - label 44: 0.450912  96074 / 213066
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - mof_classes - average class mof: 0.275490
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 29280
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 9320
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - label 10: 0.014970  237 / 15832
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - label 11: 0.215441  22109 / 102622
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - label 12: 0.048890  3093 / 63264
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - label 14: 0.126974  7317 / 57626
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - label 15: 0.495407  30740 / 62050
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 13907
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - label 17: 0.153698  15196 / 98869
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - label 42: 0.264540  11362 / 42950
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - label 43: 0.113070  13300 / 117626
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - label 44: 0.432411  96074 / 222182
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - average IoU: 0.169582
2019-01-08 12:26:34,183 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.155450
2019-01-08 12:26:42,230 - 10 - f1_score.py - f1 - f1 score: 0.289934
2019-01-08 12:26:42,251 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 8291.489 ms ~ 0.138 min ~ 8.291 sec
2019-01-08 12:26:42,251 - 10 - corpus.py - embedding_training - .
2019-01-08 12:26:42,251 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 12:26:42,251 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 12:26:42,251 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 12:26:45,948 - 10 - training_embed.py - training - create model
2019-01-08 12:26:45,949 - 10 - training_embed.py - training - epochs: 12
2019-01-08 12:26:45,949 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 12:26:46,330 - 10 - training_embed.py - training - Epoch: [0][100/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 497.2679 (502.2549)	
2019-01-08 12:26:46,550 - 10 - training_embed.py - training - Epoch: [0][200/2022]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 502.6555 (501.8806)	
2019-01-08 12:26:46,789 - 10 - training_embed.py - training - Epoch: [0][300/2022]	Time 0.002 (0.003)	Data 0.000 (0.002)	Loss 501.0983 (501.5649)	
2019-01-08 12:26:47,056 - 10 - training_embed.py - training - Epoch: [0][400/2022]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 501.8345 (501.4784)	
2019-01-08 12:26:47,332 - 10 - training_embed.py - training - Epoch: [0][500/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 497.1217 (501.4473)	
2019-01-08 12:26:47,566 - 10 - training_embed.py - training - Epoch: [0][600/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 502.3904 (501.3442)	
2019-01-08 12:26:47,807 - 10 - training_embed.py - training - Epoch: [0][700/2022]	Time 0.008 (0.003)	Data 0.004 (0.001)	Loss 504.0139 (501.3034)	
2019-01-08 12:26:48,049 - 10 - training_embed.py - training - Epoch: [0][800/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 505.4475 (501.2808)	
2019-01-08 12:26:48,280 - 10 - training_embed.py - training - Epoch: [0][900/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 509.2029 (501.1865)	
2019-01-08 12:26:48,505 - 10 - training_embed.py - training - Epoch: [0][1000/2022]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 501.8210 (501.1632)	
2019-01-08 12:26:48,768 - 10 - training_embed.py - training - Epoch: [0][1100/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 502.8524 (501.1544)	
2019-01-08 12:26:49,070 - 10 - training_embed.py - training - Epoch: [0][1200/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 500.5056 (501.1670)	
2019-01-08 12:26:49,381 - 10 - training_embed.py - training - Epoch: [0][1300/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 501.2036 (501.0350)	
2019-01-08 12:26:49,637 - 10 - training_embed.py - training - Epoch: [0][1400/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 501.2756 (501.0260)	
2019-01-08 12:26:49,896 - 10 - training_embed.py - training - Epoch: [0][1500/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 505.0425 (501.0137)	
2019-01-08 12:26:50,126 - 10 - training_embed.py - training - Epoch: [0][1600/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 505.3298 (500.9858)	
2019-01-08 12:26:50,349 - 10 - training_embed.py - training - Epoch: [0][1700/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 497.6551 (500.9282)	
2019-01-08 12:26:50,583 - 10 - training_embed.py - training - Epoch: [0][1800/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 498.6671 (500.8978)	
2019-01-08 12:26:50,800 - 10 - training_embed.py - training - Epoch: [0][1900/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 500.0967 (500.8432)	
2019-01-08 12:26:51,008 - 10 - training_embed.py - training - Epoch: [0][2000/2022]	Time 0.005 (0.003)	Data 0.004 (0.001)	Loss 499.8764 (500.8086)	
2019-01-08 12:26:51,055 - 10 - training_embed.py - training - loss: 500.728951
2019-01-08 12:26:51,055 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 12:26:51,494 - 10 - training_embed.py - training - Epoch: [1][100/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 496.7689 (500.2845)	
2019-01-08 12:26:51,735 - 10 - training_embed.py - training - Epoch: [1][200/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 505.5727 (500.1182)	
2019-01-08 12:26:51,962 - 10 - training_embed.py - training - Epoch: [1][300/2022]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 501.1489 (499.9051)	
2019-01-08 12:26:52,182 - 10 - training_embed.py - training - Epoch: [1][400/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 493.2138 (499.6998)	
2019-01-08 12:26:52,400 - 10 - training_embed.py - training - Epoch: [1][500/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 498.4321 (499.5997)	
2019-01-08 12:26:52,651 - 10 - training_embed.py - training - Epoch: [1][600/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 506.0350 (499.5863)	
2019-01-08 12:26:52,886 - 10 - training_embed.py - training - Epoch: [1][700/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 499.0004 (499.5598)	
2019-01-08 12:26:53,113 - 10 - training_embed.py - training - Epoch: [1][800/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 502.4162 (499.4989)	
2019-01-08 12:26:53,350 - 10 - training_embed.py - training - Epoch: [1][900/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 495.3864 (499.4579)	
2019-01-08 12:26:53,580 - 10 - training_embed.py - training - Epoch: [1][1000/2022]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 498.8453 (499.4028)	
2019-01-08 12:26:53,804 - 10 - training_embed.py - training - Epoch: [1][1100/2022]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 500.1520 (499.3455)	
2019-01-08 12:26:54,043 - 10 - training_embed.py - training - Epoch: [1][1200/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 506.8533 (499.2892)	
2019-01-08 12:26:54,339 - 10 - training_embed.py - training - Epoch: [1][1300/2022]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 504.2596 (499.2050)	
2019-01-08 12:26:54,587 - 10 - training_embed.py - training - Epoch: [1][1400/2022]	Time 0.005 (0.003)	Data 0.004 (0.001)	Loss 503.3834 (499.1674)	
2019-01-08 12:26:54,820 - 10 - training_embed.py - training - Epoch: [1][1500/2022]	Time 0.005 (0.003)	Data 0.004 (0.001)	Loss 498.4152 (499.1204)	
2019-01-08 12:26:55,056 - 10 - training_embed.py - training - Epoch: [1][1600/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 498.7585 (499.1054)	
2019-01-08 12:26:55,297 - 10 - training_embed.py - training - Epoch: [1][1700/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 492.9130 (499.0834)	
2019-01-08 12:26:55,519 - 10 - training_embed.py - training - Epoch: [1][1800/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 503.2673 (499.0368)	
2019-01-08 12:26:55,765 - 10 - training_embed.py - training - Epoch: [1][1900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 496.3445 (499.0028)	
2019-01-08 12:26:56,004 - 10 - training_embed.py - training - Epoch: [1][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.8084 (498.9662)	
2019-01-08 12:26:56,051 - 10 - training_embed.py - training - loss: 498.897297
2019-01-08 12:26:56,051 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 12:26:56,493 - 10 - training_embed.py - training - Epoch: [2][100/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 501.9318 (498.0315)	
2019-01-08 12:26:56,751 - 10 - training_embed.py - training - Epoch: [2][200/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 502.7460 (497.6517)	
2019-01-08 12:26:57,000 - 10 - training_embed.py - training - Epoch: [2][300/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 497.3812 (497.8938)	
2019-01-08 12:26:57,231 - 10 - training_embed.py - training - Epoch: [2][400/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 493.1963 (497.7154)	
2019-01-08 12:26:57,469 - 10 - training_embed.py - training - Epoch: [2][500/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 492.1638 (497.6752)	
2019-01-08 12:26:57,697 - 10 - training_embed.py - training - Epoch: [2][600/2022]	Time 0.004 (0.003)	Data 0.001 (0.001)	Loss 497.6918 (497.7343)	
2019-01-08 12:26:57,948 - 10 - training_embed.py - training - Epoch: [2][700/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 498.7888 (497.6662)	
2019-01-08 12:26:58,173 - 10 - training_embed.py - training - Epoch: [2][800/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 494.0465 (497.5880)	
2019-01-08 12:26:58,407 - 10 - training_embed.py - training - Epoch: [2][900/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 500.4232 (497.5915)	
2019-01-08 12:26:58,622 - 10 - training_embed.py - training - Epoch: [2][1000/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 488.5291 (497.4712)	
2019-01-08 12:26:58,879 - 10 - training_embed.py - training - Epoch: [2][1100/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 498.4758 (497.4452)	
2019-01-08 12:26:59,184 - 10 - training_embed.py - training - Epoch: [2][1200/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 500.6720 (497.4416)	
2019-01-08 12:26:59,485 - 10 - training_embed.py - training - Epoch: [2][1300/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 502.2740 (497.4316)	
2019-01-08 12:26:59,774 - 10 - training_embed.py - training - Epoch: [2][1400/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 495.3173 (497.4425)	
2019-01-08 12:27:00,032 - 10 - training_embed.py - training - Epoch: [2][1500/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 501.2144 (497.3976)	
2019-01-08 12:27:00,277 - 10 - training_embed.py - training - Epoch: [2][1600/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 491.9246 (497.2766)	
2019-01-08 12:27:00,514 - 10 - training_embed.py - training - Epoch: [2][1700/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 495.3063 (497.2355)	
2019-01-08 12:27:00,755 - 10 - training_embed.py - training - Epoch: [2][1800/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 496.8694 (497.2254)	
2019-01-08 12:27:00,987 - 10 - training_embed.py - training - Epoch: [2][1900/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 497.7701 (497.1567)	
2019-01-08 12:27:01,228 - 10 - training_embed.py - training - Epoch: [2][2000/2022]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 502.8724 (497.1162)	
2019-01-08 12:27:01,279 - 10 - training_embed.py - training - loss: 497.049161
2019-01-08 12:27:01,279 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 12:27:01,729 - 10 - training_embed.py - training - Epoch: [3][100/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 492.7202 (496.3351)	
2019-01-08 12:27:01,961 - 10 - training_embed.py - training - Epoch: [3][200/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 492.9960 (495.9222)	
2019-01-08 12:27:02,204 - 10 - training_embed.py - training - Epoch: [3][300/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 492.3455 (495.8952)	
2019-01-08 12:27:02,426 - 10 - training_embed.py - training - Epoch: [3][400/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 492.8577 (495.9560)	
2019-01-08 12:27:02,666 - 10 - training_embed.py - training - Epoch: [3][500/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 497.2301 (495.9469)	
2019-01-08 12:27:02,901 - 10 - training_embed.py - training - Epoch: [3][600/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 497.4471 (495.9834)	
2019-01-08 12:27:03,154 - 10 - training_embed.py - training - Epoch: [3][700/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 491.3661 (495.8207)	
2019-01-08 12:27:03,388 - 10 - training_embed.py - training - Epoch: [3][800/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 493.8848 (495.7403)	
2019-01-08 12:27:03,621 - 10 - training_embed.py - training - Epoch: [3][900/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 494.4627 (495.7208)	
2019-01-08 12:27:03,876 - 10 - training_embed.py - training - Epoch: [3][1000/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 497.5192 (495.6594)	
2019-01-08 12:27:04,117 - 10 - training_embed.py - training - Epoch: [3][1100/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 488.4338 (495.6004)	
2019-01-08 12:27:04,373 - 10 - training_embed.py - training - Epoch: [3][1200/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 496.3694 (495.5736)	
2019-01-08 12:27:04,626 - 10 - training_embed.py - training - Epoch: [3][1300/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 494.5056 (495.5289)	
2019-01-08 12:27:04,852 - 10 - training_embed.py - training - Epoch: [3][1400/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 492.9656 (495.4821)	
2019-01-08 12:27:05,094 - 10 - training_embed.py - training - Epoch: [3][1500/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 490.9808 (495.4655)	
2019-01-08 12:27:05,314 - 10 - training_embed.py - training - Epoch: [3][1600/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 497.0485 (495.4544)	
2019-01-08 12:27:05,531 - 10 - training_embed.py - training - Epoch: [3][1700/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 493.7535 (495.4088)	
2019-01-08 12:27:05,769 - 10 - training_embed.py - training - Epoch: [3][1800/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 496.9157 (495.3693)	
2019-01-08 12:27:06,008 - 10 - training_embed.py - training - Epoch: [3][1900/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 493.4146 (495.3052)	
2019-01-08 12:27:06,240 - 10 - training_embed.py - training - Epoch: [3][2000/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 490.0406 (495.2506)	
2019-01-08 12:27:06,287 - 10 - training_embed.py - training - loss: 495.174142
2019-01-08 12:27:06,287 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 12:27:06,730 - 10 - training_embed.py - training - Epoch: [4][100/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 495.0786 (494.3183)	
2019-01-08 12:27:06,968 - 10 - training_embed.py - training - Epoch: [4][200/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 486.7681 (494.3075)	
2019-01-08 12:27:07,188 - 10 - training_embed.py - training - Epoch: [4][300/2022]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 497.8583 (494.3284)	
2019-01-08 12:27:07,410 - 10 - training_embed.py - training - Epoch: [4][400/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 491.5721 (494.2822)	
2019-01-08 12:27:07,634 - 10 - training_embed.py - training - Epoch: [4][500/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 490.7718 (494.2874)	
2019-01-08 12:27:07,861 - 10 - training_embed.py - training - Epoch: [4][600/2022]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 495.8752 (494.1712)	
2019-01-08 12:27:08,104 - 10 - training_embed.py - training - Epoch: [4][700/2022]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 491.5025 (494.0566)	
2019-01-08 12:27:08,329 - 10 - training_embed.py - training - Epoch: [4][800/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 486.8583 (493.9491)	
2019-01-08 12:27:08,552 - 10 - training_embed.py - training - Epoch: [4][900/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 489.6971 (493.9547)	
2019-01-08 12:27:08,787 - 10 - training_embed.py - training - Epoch: [4][1000/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 489.4809 (493.8624)	
2019-01-08 12:27:09,002 - 10 - training_embed.py - training - Epoch: [4][1100/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 493.6920 (493.7958)	
2019-01-08 12:27:09,225 - 10 - training_embed.py - training - Epoch: [4][1200/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 494.1187 (493.7427)	
2019-01-08 12:27:09,457 - 10 - training_embed.py - training - Epoch: [4][1300/2022]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 501.7334 (493.6757)	
2019-01-08 12:27:09,698 - 10 - training_embed.py - training - Epoch: [4][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.5822 (493.6159)	
2019-01-08 12:27:09,928 - 10 - training_embed.py - training - Epoch: [4][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.7645 (493.5410)	
2019-01-08 12:27:10,157 - 10 - training_embed.py - training - Epoch: [4][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.6037 (493.4796)	
2019-01-08 12:27:10,397 - 10 - training_embed.py - training - Epoch: [4][1700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 493.5489 (493.4478)	
2019-01-08 12:27:10,642 - 10 - training_embed.py - training - Epoch: [4][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.0597 (493.4224)	
2019-01-08 12:27:10,855 - 10 - training_embed.py - training - Epoch: [4][1900/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 489.5309 (493.3849)	
2019-01-08 12:27:11,075 - 10 - training_embed.py - training - Epoch: [4][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.6215 (493.3516)	
2019-01-08 12:27:11,123 - 10 - training_embed.py - training - loss: 493.276669
2019-01-08 12:27:11,124 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 12:27:11,552 - 10 - training_embed.py - training - Epoch: [5][100/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 493.6717 (492.2076)	
2019-01-08 12:27:11,791 - 10 - training_embed.py - training - Epoch: [5][200/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 489.1789 (492.3397)	
2019-01-08 12:27:12,022 - 10 - training_embed.py - training - Epoch: [5][300/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 491.0643 (492.2177)	
2019-01-08 12:27:12,250 - 10 - training_embed.py - training - Epoch: [5][400/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 492.7328 (492.2889)	
2019-01-08 12:27:12,489 - 10 - training_embed.py - training - Epoch: [5][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.2878 (492.1302)	
2019-01-08 12:27:12,721 - 10 - training_embed.py - training - Epoch: [5][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.3824 (492.0714)	
2019-01-08 12:27:12,959 - 10 - training_embed.py - training - Epoch: [5][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.4500 (492.1068)	
2019-01-08 12:27:13,199 - 10 - training_embed.py - training - Epoch: [5][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.4987 (492.0499)	
2019-01-08 12:27:13,438 - 10 - training_embed.py - training - Epoch: [5][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.9058 (491.9257)	
2019-01-08 12:27:13,659 - 10 - training_embed.py - training - Epoch: [5][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.6210 (491.9265)	
2019-01-08 12:27:13,895 - 10 - training_embed.py - training - Epoch: [5][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 495.5984 (491.8501)	
2019-01-08 12:27:14,141 - 10 - training_embed.py - training - Epoch: [5][1200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 493.1216 (491.8032)	
2019-01-08 12:27:14,374 - 10 - training_embed.py - training - Epoch: [5][1300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 484.8257 (491.7397)	
2019-01-08 12:27:14,610 - 10 - training_embed.py - training - Epoch: [5][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.6561 (491.6786)	
2019-01-08 12:27:14,866 - 10 - training_embed.py - training - Epoch: [5][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.0780 (491.6182)	
2019-01-08 12:27:15,093 - 10 - training_embed.py - training - Epoch: [5][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.0841 (491.5981)	
2019-01-08 12:27:15,337 - 10 - training_embed.py - training - Epoch: [5][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.6531 (491.5693)	
2019-01-08 12:27:15,571 - 10 - training_embed.py - training - Epoch: [5][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.6223 (491.5454)	
2019-01-08 12:27:15,797 - 10 - training_embed.py - training - Epoch: [5][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.8751 (491.4903)	
2019-01-08 12:27:16,022 - 10 - training_embed.py - training - Epoch: [5][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.3347 (491.4345)	
2019-01-08 12:27:16,070 - 10 - training_embed.py - training - loss: 491.347874
2019-01-08 12:27:16,070 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 12:27:16,494 - 10 - training_embed.py - training - Epoch: [6][100/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 486.9686 (489.6392)	
2019-01-08 12:27:16,710 - 10 - training_embed.py - training - Epoch: [6][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.5570 (489.9411)	
2019-01-08 12:27:16,952 - 10 - training_embed.py - training - Epoch: [6][300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 493.6979 (490.0696)	
2019-01-08 12:27:17,179 - 10 - training_embed.py - training - Epoch: [6][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.0904 (489.9483)	
2019-01-08 12:27:17,413 - 10 - training_embed.py - training - Epoch: [6][500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 482.7490 (489.7839)	
2019-01-08 12:27:17,651 - 10 - training_embed.py - training - Epoch: [6][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.2468 (489.8557)	
2019-01-08 12:27:17,868 - 10 - training_embed.py - training - Epoch: [6][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.9682 (489.8113)	
2019-01-08 12:27:18,085 - 10 - training_embed.py - training - Epoch: [6][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.9005 (489.7944)	
2019-01-08 12:27:18,303 - 10 - training_embed.py - training - Epoch: [6][900/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 495.9365 (489.7473)	
2019-01-08 12:27:18,526 - 10 - training_embed.py - training - Epoch: [6][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.0833 (489.7724)	
2019-01-08 12:27:18,755 - 10 - training_embed.py - training - Epoch: [6][1100/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 490.7160 (489.7253)	
2019-01-08 12:27:18,965 - 10 - training_embed.py - training - Epoch: [6][1200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 499.9714 (489.7239)	
2019-01-08 12:27:19,184 - 10 - training_embed.py - training - Epoch: [6][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.2956 (489.7461)	
2019-01-08 12:27:19,403 - 10 - training_embed.py - training - Epoch: [6][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.9712 (489.6896)	
2019-01-08 12:27:19,630 - 10 - training_embed.py - training - Epoch: [6][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 490.3248 (489.6595)	
2019-01-08 12:27:19,859 - 10 - training_embed.py - training - Epoch: [6][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.8441 (489.6170)	
2019-01-08 12:27:20,101 - 10 - training_embed.py - training - Epoch: [6][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.6528 (489.5260)	
2019-01-08 12:27:20,323 - 10 - training_embed.py - training - Epoch: [6][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.4494 (489.4983)	
2019-01-08 12:27:20,538 - 10 - training_embed.py - training - Epoch: [6][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.9127 (489.4766)	
2019-01-08 12:27:20,754 - 10 - training_embed.py - training - Epoch: [6][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.1793 (489.4352)	
2019-01-08 12:27:20,802 - 10 - training_embed.py - training - loss: 489.387445
2019-01-08 12:27:20,802 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 12:27:21,238 - 10 - training_embed.py - training - Epoch: [7][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.8985 (488.2285)	
2019-01-08 12:27:21,458 - 10 - training_embed.py - training - Epoch: [7][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.8111 (488.0812)	
2019-01-08 12:27:21,686 - 10 - training_embed.py - training - Epoch: [7][300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 497.6730 (488.1639)	
2019-01-08 12:27:21,924 - 10 - training_embed.py - training - Epoch: [7][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.0570 (488.1034)	
2019-01-08 12:27:22,149 - 10 - training_embed.py - training - Epoch: [7][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.7786 (488.1474)	
2019-01-08 12:27:22,375 - 10 - training_embed.py - training - Epoch: [7][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.6533 (488.0992)	
2019-01-08 12:27:22,597 - 10 - training_embed.py - training - Epoch: [7][700/2022]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 487.8698 (488.0949)	
2019-01-08 12:27:22,832 - 10 - training_embed.py - training - Epoch: [7][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.2733 (487.9874)	
2019-01-08 12:27:23,076 - 10 - training_embed.py - training - Epoch: [7][900/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 493.8189 (487.9671)	
2019-01-08 12:27:23,327 - 10 - training_embed.py - training - Epoch: [7][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.8212 (487.9468)	
2019-01-08 12:27:23,553 - 10 - training_embed.py - training - Epoch: [7][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.8022 (487.8364)	
2019-01-08 12:27:23,789 - 10 - training_embed.py - training - Epoch: [7][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.9620 (487.8511)	
2019-01-08 12:27:24,030 - 10 - training_embed.py - training - Epoch: [7][1300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 480.9019 (487.7723)	
2019-01-08 12:27:24,277 - 10 - training_embed.py - training - Epoch: [7][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.3493 (487.7437)	
2019-01-08 12:27:24,506 - 10 - training_embed.py - training - Epoch: [7][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.9340 (487.6980)	
2019-01-08 12:27:24,772 - 10 - training_embed.py - training - Epoch: [7][1600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 487.7469 (487.6494)	
2019-01-08 12:27:25,017 - 10 - training_embed.py - training - Epoch: [7][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.8981 (487.6085)	
2019-01-08 12:27:25,248 - 10 - training_embed.py - training - Epoch: [7][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.8976 (487.5827)	
2019-01-08 12:27:25,467 - 10 - training_embed.py - training - Epoch: [7][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.4106 (487.5119)	
2019-01-08 12:27:25,689 - 10 - training_embed.py - training - Epoch: [7][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.0018 (487.4606)	
2019-01-08 12:27:25,738 - 10 - training_embed.py - training - loss: 487.391179
2019-01-08 12:27:25,738 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 12:27:26,164 - 10 - training_embed.py - training - Epoch: [8][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.5347 (486.6346)	
2019-01-08 12:27:26,385 - 10 - training_embed.py - training - Epoch: [8][200/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 497.1106 (486.3100)	
2019-01-08 12:27:26,602 - 10 - training_embed.py - training - Epoch: [8][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.2881 (486.2054)	
2019-01-08 12:27:26,833 - 10 - training_embed.py - training - Epoch: [8][400/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 487.1423 (486.2971)	
2019-01-08 12:27:27,072 - 10 - training_embed.py - training - Epoch: [8][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.0140 (486.1698)	
2019-01-08 12:27:27,294 - 10 - training_embed.py - training - Epoch: [8][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.1655 (486.2037)	
2019-01-08 12:27:27,517 - 10 - training_embed.py - training - Epoch: [8][700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 489.3391 (486.1926)	
2019-01-08 12:27:27,753 - 10 - training_embed.py - training - Epoch: [8][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.9395 (486.1765)	
2019-01-08 12:27:27,992 - 10 - training_embed.py - training - Epoch: [8][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.9753 (486.0646)	
2019-01-08 12:27:28,213 - 10 - training_embed.py - training - Epoch: [8][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.9962 (485.9166)	
2019-01-08 12:27:28,431 - 10 - training_embed.py - training - Epoch: [8][1100/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 486.5778 (485.9056)	
2019-01-08 12:27:28,646 - 10 - training_embed.py - training - Epoch: [8][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.7484 (485.8502)	
2019-01-08 12:27:28,864 - 10 - training_embed.py - training - Epoch: [8][1300/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 475.4842 (485.7589)	
2019-01-08 12:27:29,079 - 10 - training_embed.py - training - Epoch: [8][1400/2022]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 483.8144 (485.7266)	
2019-01-08 12:27:29,306 - 10 - training_embed.py - training - Epoch: [8][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.4307 (485.6158)	
2019-01-08 12:27:29,518 - 10 - training_embed.py - training - Epoch: [8][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.1942 (485.5621)	
2019-01-08 12:27:29,743 - 10 - training_embed.py - training - Epoch: [8][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.9991 (485.5213)	
2019-01-08 12:27:29,977 - 10 - training_embed.py - training - Epoch: [8][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 468.4911 (485.4929)	
2019-01-08 12:27:30,204 - 10 - training_embed.py - training - Epoch: [8][1900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 482.9943 (485.4616)	
2019-01-08 12:27:30,434 - 10 - training_embed.py - training - Epoch: [8][2000/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 478.0992 (485.4262)	
2019-01-08 12:27:30,481 - 10 - training_embed.py - training - loss: 485.359426
2019-01-08 12:27:30,481 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 12:27:30,924 - 10 - training_embed.py - training - Epoch: [9][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.0962 (483.7160)	
2019-01-08 12:27:31,159 - 10 - training_embed.py - training - Epoch: [9][200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 492.1408 (483.8807)	
2019-01-08 12:27:31,385 - 10 - training_embed.py - training - Epoch: [9][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.8216 (484.0342)	
2019-01-08 12:27:31,607 - 10 - training_embed.py - training - Epoch: [9][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.6393 (484.0790)	
2019-01-08 12:27:31,846 - 10 - training_embed.py - training - Epoch: [9][500/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 483.1410 (484.0557)	
2019-01-08 12:27:32,073 - 10 - training_embed.py - training - Epoch: [9][600/2022]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 490.8868 (484.0125)	
2019-01-08 12:27:32,301 - 10 - training_embed.py - training - Epoch: [9][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.7288 (483.9009)	
2019-01-08 12:27:32,530 - 10 - training_embed.py - training - Epoch: [9][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 477.8078 (483.8458)	
2019-01-08 12:27:32,759 - 10 - training_embed.py - training - Epoch: [9][900/2022]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 479.1097 (483.8226)	
2019-01-08 12:27:32,986 - 10 - training_embed.py - training - Epoch: [9][1000/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 485.2790 (483.7753)	
2019-01-08 12:27:33,227 - 10 - training_embed.py - training - Epoch: [9][1100/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 477.2872 (483.7411)	
2019-01-08 12:27:33,446 - 10 - training_embed.py - training - Epoch: [9][1200/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 487.8127 (483.6861)	
2019-01-08 12:27:33,669 - 10 - training_embed.py - training - Epoch: [9][1300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 487.4112 (483.6195)	
2019-01-08 12:27:33,918 - 10 - training_embed.py - training - Epoch: [9][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 475.9621 (483.5400)	
2019-01-08 12:27:34,187 - 10 - training_embed.py - training - Epoch: [9][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.2643 (483.5846)	
2019-01-08 12:27:34,431 - 10 - training_embed.py - training - Epoch: [9][1600/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 486.2988 (483.5709)	
2019-01-08 12:27:34,665 - 10 - training_embed.py - training - Epoch: [9][1700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 475.0922 (483.4857)	
2019-01-08 12:27:34,901 - 10 - training_embed.py - training - Epoch: [9][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.8262 (483.4380)	
2019-01-08 12:27:35,163 - 10 - training_embed.py - training - Epoch: [9][1900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 486.5195 (483.3661)	
2019-01-08 12:27:35,458 - 10 - training_embed.py - training - Epoch: [9][2000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 483.7183 (483.3480)	
2019-01-08 12:27:35,515 - 10 - training_embed.py - training - loss: 483.285303
2019-01-08 12:27:35,515 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 12:27:35,961 - 10 - training_embed.py - training - Epoch: [10][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 477.6943 (481.8708)	
2019-01-08 12:27:36,211 - 10 - training_embed.py - training - Epoch: [10][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.6566 (481.7387)	
2019-01-08 12:27:36,447 - 10 - training_embed.py - training - Epoch: [10][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 476.7097 (481.7314)	
2019-01-08 12:27:36,673 - 10 - training_embed.py - training - Epoch: [10][400/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 478.0159 (481.7915)	
2019-01-08 12:27:36,963 - 10 - training_embed.py - training - Epoch: [10][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 479.0695 (481.7284)	
2019-01-08 12:27:37,276 - 10 - training_embed.py - training - Epoch: [10][600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 483.9510 (481.8270)	
2019-01-08 12:27:37,553 - 10 - training_embed.py - training - Epoch: [10][700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 481.0837 (481.7277)	
2019-01-08 12:27:37,806 - 10 - training_embed.py - training - Epoch: [10][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.3002 (481.7766)	
2019-01-08 12:27:38,082 - 10 - training_embed.py - training - Epoch: [10][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 471.9048 (481.6464)	
2019-01-08 12:27:38,352 - 10 - training_embed.py - training - Epoch: [10][1000/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 477.7001 (481.6076)	
2019-01-08 12:27:38,652 - 10 - training_embed.py - training - Epoch: [10][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 474.1889 (481.6251)	
2019-01-08 12:27:38,962 - 10 - training_embed.py - training - Epoch: [10][1200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 482.7552 (481.5378)	
2019-01-08 12:27:39,218 - 10 - training_embed.py - training - Epoch: [10][1300/2022]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 484.4349 (481.4943)	
2019-01-08 12:27:39,457 - 10 - training_embed.py - training - Epoch: [10][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 479.9345 (481.4743)	
2019-01-08 12:27:39,677 - 10 - training_embed.py - training - Epoch: [10][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 480.3564 (481.4343)	
2019-01-08 12:27:39,939 - 10 - training_embed.py - training - Epoch: [10][1600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 480.0175 (481.3820)	
2019-01-08 12:27:40,250 - 10 - training_embed.py - training - Epoch: [10][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.5958 (481.3753)	
2019-01-08 12:27:40,556 - 10 - training_embed.py - training - Epoch: [10][1800/2022]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 476.7144 (481.3924)	
2019-01-08 12:27:40,813 - 10 - training_embed.py - training - Epoch: [10][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.3687 (481.3077)	
2019-01-08 12:27:41,055 - 10 - training_embed.py - training - Epoch: [10][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 476.0212 (481.2505)	
2019-01-08 12:27:41,101 - 10 - training_embed.py - training - loss: 481.165407
2019-01-08 12:27:41,101 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 12:27:41,522 - 10 - training_embed.py - training - Epoch: [11][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.6270 (480.1742)	
2019-01-08 12:27:41,746 - 10 - training_embed.py - training - Epoch: [11][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.9552 (480.3388)	
2019-01-08 12:27:41,955 - 10 - training_embed.py - training - Epoch: [11][300/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 482.5906 (480.1801)	
2019-01-08 12:27:42,164 - 10 - training_embed.py - training - Epoch: [11][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.2165 (480.1613)	
2019-01-08 12:27:42,405 - 10 - training_embed.py - training - Epoch: [11][500/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 485.3161 (480.0426)	
2019-01-08 12:27:42,634 - 10 - training_embed.py - training - Epoch: [11][600/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 485.4915 (479.9854)	
2019-01-08 12:27:42,875 - 10 - training_embed.py - training - Epoch: [11][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 476.6484 (479.8774)	
2019-01-08 12:27:43,102 - 10 - training_embed.py - training - Epoch: [11][800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 474.7275 (479.7992)	
2019-01-08 12:27:43,332 - 10 - training_embed.py - training - Epoch: [11][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 464.2599 (479.7951)	
2019-01-08 12:27:43,548 - 10 - training_embed.py - training - Epoch: [11][1000/2022]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 470.8614 (479.8088)	
2019-01-08 12:27:43,783 - 10 - training_embed.py - training - Epoch: [11][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 476.3141 (479.7486)	
2019-01-08 12:27:44,001 - 10 - training_embed.py - training - Epoch: [11][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 469.2920 (479.6291)	
2019-01-08 12:27:44,233 - 10 - training_embed.py - training - Epoch: [11][1300/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 477.7081 (479.5140)	
2019-01-08 12:27:44,459 - 10 - training_embed.py - training - Epoch: [11][1400/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 471.4564 (479.4692)	
2019-01-08 12:27:44,682 - 10 - training_embed.py - training - Epoch: [11][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 473.5482 (479.4157)	
2019-01-08 12:27:44,918 - 10 - training_embed.py - training - Epoch: [11][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 478.8965 (479.3578)	
2019-01-08 12:27:45,186 - 10 - training_embed.py - training - Epoch: [11][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.3887 (479.2601)	
2019-01-08 12:27:45,473 - 10 - training_embed.py - training - Epoch: [11][1800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 475.9570 (479.1972)	
2019-01-08 12:27:45,726 - 10 - training_embed.py - training - Epoch: [11][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 477.8887 (479.1137)	
2019-01-08 12:27:46,036 - 10 - training_embed.py - training - Epoch: [11][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 478.3690 (479.0784)	
2019-01-08 12:27:46,085 - 10 - training_embed.py - training - loss: 478.998321
2019-01-08 12:27:46,162 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 12:27:47,350 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1188.179 ms ~ 0.020 min ~ 1.188 sec
2019-01-08 12:27:48,485 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 2323.258 ms ~ 0.039 min ~ 2.323 sec
2019-01-08 12:27:48,485 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 12:27:48,486 - 10 - corpus.py - subactivity_sampler - 0 / 166
2019-01-08 12:27:48,486 - 10 - corpus.py - subactivity_sampler - [ 68524.  26123.  22183.  64714.  12904. 107949.   5134. 105190.   8102.
  47332.  49323.]
2019-01-08 12:29:13,433 - 10 - corpus.py - subactivity_sampler - 20 / 166
2019-01-08 12:29:13,433 - 10 - corpus.py - subactivity_sampler - [ 69066.  25653.  21940.  64335.  12782. 109710.   4933. 105982.   7657.
  46281.  49139.]
2019-01-08 12:30:21,591 - 10 - corpus.py - subactivity_sampler - 40 / 166
2019-01-08 12:30:21,592 - 10 - corpus.py - subactivity_sampler - [ 69082.  25380.  21791.  64357.  12535. 110264.   4697. 106942.   7293.
  46111.  49026.]
2019-01-08 12:31:51,829 - 10 - corpus.py - subactivity_sampler - 60 / 166
2019-01-08 12:31:51,830 - 10 - corpus.py - subactivity_sampler - [ 68993.  25272.  21626.  64071.  12380. 110773.   5084. 107482.   7206.
  45625.  48966.]
2019-01-08 12:33:17,039 - 10 - corpus.py - subactivity_sampler - 80 / 166
2019-01-08 12:33:17,040 - 10 - corpus.py - subactivity_sampler - [ 69016.  25206.  21469.  63872.  12321. 111356.   4853. 107682.   7744.
  44998.  48961.]
2019-01-08 12:34:27,902 - 10 - corpus.py - subactivity_sampler - 100 / 166
2019-01-08 12:34:27,902 - 10 - corpus.py - subactivity_sampler - [ 69064.  25103.  21372.  63925.  12245. 111507.   4824. 108570.   7714.
  44298.  48856.]
2019-01-08 12:35:53,817 - 10 - corpus.py - subactivity_sampler - 120 / 166
2019-01-08 12:35:53,817 - 10 - corpus.py - subactivity_sampler - [ 69052.  25086.  21175.  63612.  12085. 112253.   4694. 109049.   7264.
  44327.  48881.]
2019-01-08 12:37:20,079 - 10 - corpus.py - subactivity_sampler - 140 / 166
2019-01-08 12:37:20,080 - 10 - corpus.py - subactivity_sampler - [ 69310.  24725.  21062.  63664.  11909. 112614.   4681. 109085.   7221.
  44318.  48889.]
2019-01-08 12:38:43,822 - 10 - corpus.py - subactivity_sampler - 160 / 166
2019-01-08 12:38:43,822 - 10 - corpus.py - subactivity_sampler - [ 69495.  24562.  20878.  63372.  11827. 114051.   4664. 109102.   7180.
  43482.  48865.]
2019-01-08 12:39:02,816 - 10 - corpus.py - subactivity_sampler - [ 69647.  24581.  20656.  63423.  11803. 114093.   4655. 109113.   7180.
  43477.  48850.]
2019-01-08 12:39:02,816 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 674331.464 ms ~ 11.239 min ~ 674.331 sec
2019-01-08 12:39:02,816 - 10 - corpus.py - ordering_sampler - .
2019-01-08 12:39:07,677 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 12:39:07,677 - 10 - corpus.py - ordering_sampler - inv_count_vec: [11. 12.  0.  8. 61. 93.  5. 21. 20.  1.]
2019-01-08 12:39:07,677 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 12:39:07,700 - 10 - corpus.py - rho_sampling - ['51.8671', '10.1537', '928.1292', '5.2484', '2.4980', '6.3409', '26.5173', '63.7759', '86.9938', '21.2612']
2019-01-08 12:39:07,700 - 10 - pipeline.py - baseline - Iteration 5
2019-01-08 12:39:07,912 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 12:39:07,922 - 10 - accuracy_class.py - mof - # gt_labels: 12   # pr_labels: 11
2019-01-08 12:39:07,922 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 11', '1: 12', '2: 42', '3: 17', '4: 16', '5: 43', '6: 10', '7: 44', '8: 4', '9: 14', '10: 15']
2019-01-08 12:39:07,948 - 10 - accuracy_class.py - mof_val - frames true: 202586	frames overall : 517478
2019-01-08 12:39:07,948 - 10 - corpus.py - accuracy_corpus - Action: scrambledegg
2019-01-08 12:39:07,948 - 10 - corpus.py - accuracy_corpus - MoF val: 0.39148717433398134
2019-01-08 12:39:07,948 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.39148717433398134
2019-01-08 12:39:07,948 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 29280
2019-01-08 12:39:07,948 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1218
2019-01-08 12:39:07,948 - 10 - accuracy_class.py - mof_classes - label 10: 0.030270  331 / 10935
2019-01-08 12:39:07,948 - 10 - accuracy_class.py - mof_classes - label 11: 0.394844  22193 / 56207
2019-01-08 12:39:07,948 - 10 - accuracy_class.py - mof_classes - label 12: 0.069891  2812 / 40234
2019-01-08 12:39:07,948 - 10 - accuracy_class.py - mof_classes - label 14: 0.419851  7394 / 17611
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - mof_classes - label 15: 0.702487  30535 / 43467
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 1003
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - mof_classes - label 17: 0.303601  14983 / 49351
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - mof_classes - label 42: 0.341280  10965 / 32129
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - mof_classes - label 43: 0.588066  13512 / 22977
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - mof_classes - label 44: 0.468686  99861 / 213066
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - mof_classes - average class mof: 0.276581
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 29280
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 8398
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - label 10: 0.021692  331 / 15259
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - label 11: 0.214092  22193 / 103661
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - label 12: 0.045353  2812 / 62003
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - label 14: 0.137706  7394 / 53694
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - label 15: 0.494238  30535 / 61782
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 12806
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - label 17: 0.153215  14983 / 97791
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - label 42: 0.262195  10965 / 41820
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - label 43: 0.109358  13512 / 123558
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - label 44: 0.449181  99861 / 222318
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - average IoU: 0.171548
2019-01-08 12:39:07,949 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.157252
2019-01-08 12:39:15,813 - 10 - f1_score.py - f1 - f1 score: 0.291506
2019-01-08 12:39:15,833 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 8133.135 ms ~ 0.136 min ~ 8.133 sec
2019-01-08 12:39:15,833 - 10 - corpus.py - embedding_training - .
2019-01-08 12:39:15,834 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 12:39:15,834 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 12:39:15,834 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 12:39:19,610 - 10 - training_embed.py - training - create model
2019-01-08 12:39:19,611 - 10 - training_embed.py - training - epochs: 12
2019-01-08 12:39:19,611 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 12:39:20,003 - 10 - training_embed.py - training - Epoch: [0][100/2022]	Time 0.003 (0.004)	Data 0.001 (0.003)	Loss 496.3756 (501.7336)	
2019-01-08 12:39:20,246 - 10 - training_embed.py - training - Epoch: [0][200/2022]	Time 0.003 (0.003)	Data 0.002 (0.002)	Loss 500.3109 (501.3935)	
2019-01-08 12:39:20,487 - 10 - training_embed.py - training - Epoch: [0][300/2022]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 502.9913 (501.1330)	
2019-01-08 12:39:20,719 - 10 - training_embed.py - training - Epoch: [0][400/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 501.0125 (501.0730)	
2019-01-08 12:39:20,955 - 10 - training_embed.py - training - Epoch: [0][500/2022]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 495.6637 (501.0580)	
2019-01-08 12:39:21,195 - 10 - training_embed.py - training - Epoch: [0][600/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 499.4154 (500.9520)	
2019-01-08 12:39:21,426 - 10 - training_embed.py - training - Epoch: [0][700/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 503.7296 (500.8969)	
2019-01-08 12:39:21,646 - 10 - training_embed.py - training - Epoch: [0][800/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 506.7373 (500.8814)	
2019-01-08 12:39:21,954 - 10 - training_embed.py - training - Epoch: [0][900/2022]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 508.3345 (500.7597)	
2019-01-08 12:39:22,276 - 10 - training_embed.py - training - Epoch: [0][1000/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 502.2426 (500.7268)	
2019-01-08 12:39:22,597 - 10 - training_embed.py - training - Epoch: [0][1100/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 501.9039 (500.7090)	
2019-01-08 12:39:22,874 - 10 - training_embed.py - training - Epoch: [0][1200/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 499.8906 (500.7165)	
2019-01-08 12:39:23,131 - 10 - training_embed.py - training - Epoch: [0][1300/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 501.9561 (500.6113)	
2019-01-08 12:39:23,443 - 10 - training_embed.py - training - Epoch: [0][1400/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 499.4149 (500.5933)	
2019-01-08 12:39:23,697 - 10 - training_embed.py - training - Epoch: [0][1500/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 503.4957 (500.5822)	
2019-01-08 12:39:23,957 - 10 - training_embed.py - training - Epoch: [0][1600/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 503.3640 (500.5421)	
2019-01-08 12:39:24,185 - 10 - training_embed.py - training - Epoch: [0][1700/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 496.7728 (500.4709)	
2019-01-08 12:39:24,425 - 10 - training_embed.py - training - Epoch: [0][1800/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 498.3889 (500.4297)	
2019-01-08 12:39:24,665 - 10 - training_embed.py - training - Epoch: [0][1900/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 499.1492 (500.3680)	
2019-01-08 12:39:24,895 - 10 - training_embed.py - training - Epoch: [0][2000/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 500.2816 (500.3155)	
2019-01-08 12:39:24,942 - 10 - training_embed.py - training - loss: 500.237371
2019-01-08 12:39:24,942 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 12:39:25,445 - 10 - training_embed.py - training - Epoch: [1][100/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 495.3508 (499.5300)	
2019-01-08 12:39:25,731 - 10 - training_embed.py - training - Epoch: [1][200/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 504.1956 (499.4475)	
2019-01-08 12:39:25,959 - 10 - training_embed.py - training - Epoch: [1][300/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 502.0038 (499.2317)	
2019-01-08 12:39:26,193 - 10 - training_embed.py - training - Epoch: [1][400/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 490.9536 (499.0304)	
2019-01-08 12:39:26,430 - 10 - training_embed.py - training - Epoch: [1][500/2022]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 498.4597 (498.9480)	
2019-01-08 12:39:26,654 - 10 - training_embed.py - training - Epoch: [1][600/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 504.0706 (498.9528)	
2019-01-08 12:39:26,872 - 10 - training_embed.py - training - Epoch: [1][700/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 496.5155 (498.9043)	
2019-01-08 12:39:27,119 - 10 - training_embed.py - training - Epoch: [1][800/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 501.0219 (498.8463)	
2019-01-08 12:39:27,358 - 10 - training_embed.py - training - Epoch: [1][900/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 494.0989 (498.7889)	
2019-01-08 12:39:27,595 - 10 - training_embed.py - training - Epoch: [1][1000/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 498.7584 (498.7559)	
2019-01-08 12:39:27,823 - 10 - training_embed.py - training - Epoch: [1][1100/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 499.8251 (498.6931)	
2019-01-08 12:39:28,063 - 10 - training_embed.py - training - Epoch: [1][1200/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 505.8604 (498.6281)	
2019-01-08 12:39:28,301 - 10 - training_embed.py - training - Epoch: [1][1300/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 503.3832 (498.5531)	
2019-01-08 12:39:28,541 - 10 - training_embed.py - training - Epoch: [1][1400/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 500.9886 (498.4963)	
2019-01-08 12:39:28,767 - 10 - training_embed.py - training - Epoch: [1][1500/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 498.3936 (498.4463)	
2019-01-08 12:39:28,989 - 10 - training_embed.py - training - Epoch: [1][1600/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 498.5613 (498.4295)	
2019-01-08 12:39:29,255 - 10 - training_embed.py - training - Epoch: [1][1700/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 494.2987 (498.4105)	
2019-01-08 12:39:29,513 - 10 - training_embed.py - training - Epoch: [1][1800/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 501.3970 (498.3640)	
2019-01-08 12:39:29,740 - 10 - training_embed.py - training - Epoch: [1][1900/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 496.0519 (498.3278)	
2019-01-08 12:39:29,967 - 10 - training_embed.py - training - Epoch: [1][2000/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 502.5071 (498.2828)	
2019-01-08 12:39:30,018 - 10 - training_embed.py - training - loss: 498.211013
2019-01-08 12:39:30,019 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 12:39:30,471 - 10 - training_embed.py - training - Epoch: [2][100/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 501.0768 (497.2776)	
2019-01-08 12:39:30,697 - 10 - training_embed.py - training - Epoch: [2][200/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 504.7629 (496.9870)	
2019-01-08 12:39:30,916 - 10 - training_embed.py - training - Epoch: [2][300/2022]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 496.7408 (497.1081)	
2019-01-08 12:39:31,149 - 10 - training_embed.py - training - Epoch: [2][400/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 492.9904 (496.9628)	
2019-01-08 12:39:31,373 - 10 - training_embed.py - training - Epoch: [2][500/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 492.0988 (496.8440)	
2019-01-08 12:39:31,608 - 10 - training_embed.py - training - Epoch: [2][600/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 498.8308 (496.9437)	
2019-01-08 12:39:31,830 - 10 - training_embed.py - training - Epoch: [2][700/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 497.2593 (496.8923)	
2019-01-08 12:39:32,052 - 10 - training_embed.py - training - Epoch: [2][800/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 492.5381 (496.8130)	
2019-01-08 12:39:32,285 - 10 - training_embed.py - training - Epoch: [2][900/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 501.3521 (496.7975)	
2019-01-08 12:39:32,502 - 10 - training_embed.py - training - Epoch: [2][1000/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 487.9606 (496.6525)	
2019-01-08 12:39:32,728 - 10 - training_embed.py - training - Epoch: [2][1100/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 497.5649 (496.6145)	
2019-01-08 12:39:32,959 - 10 - training_embed.py - training - Epoch: [2][1200/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 501.2849 (496.6191)	
2019-01-08 12:39:33,195 - 10 - training_embed.py - training - Epoch: [2][1300/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 501.5640 (496.5915)	
2019-01-08 12:39:33,425 - 10 - training_embed.py - training - Epoch: [2][1400/2022]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 494.4595 (496.5834)	
2019-01-08 12:39:33,657 - 10 - training_embed.py - training - Epoch: [2][1500/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 499.4802 (496.5414)	
2019-01-08 12:39:33,882 - 10 - training_embed.py - training - Epoch: [2][1600/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 494.1291 (496.4295)	
2019-01-08 12:39:34,119 - 10 - training_embed.py - training - Epoch: [2][1700/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 494.8817 (496.3893)	
2019-01-08 12:39:34,352 - 10 - training_embed.py - training - Epoch: [2][1800/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 495.6185 (496.3689)	
2019-01-08 12:39:34,592 - 10 - training_embed.py - training - Epoch: [2][1900/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 497.0975 (496.2897)	
2019-01-08 12:39:34,834 - 10 - training_embed.py - training - Epoch: [2][2000/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 498.1158 (496.2327)	
2019-01-08 12:39:34,884 - 10 - training_embed.py - training - loss: 496.164238
2019-01-08 12:39:34,884 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 12:39:35,327 - 10 - training_embed.py - training - Epoch: [3][100/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 493.3727 (495.4580)	
2019-01-08 12:39:35,550 - 10 - training_embed.py - training - Epoch: [3][200/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 490.8821 (494.9200)	
2019-01-08 12:39:35,780 - 10 - training_embed.py - training - Epoch: [3][300/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 491.7032 (494.8289)	
2019-01-08 12:39:36,016 - 10 - training_embed.py - training - Epoch: [3][400/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 490.3618 (494.8553)	
2019-01-08 12:39:36,250 - 10 - training_embed.py - training - Epoch: [3][500/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 495.1686 (494.8740)	
2019-01-08 12:39:36,466 - 10 - training_embed.py - training - Epoch: [3][600/2022]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 494.6578 (494.8952)	
2019-01-08 12:39:36,698 - 10 - training_embed.py - training - Epoch: [3][700/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 491.9205 (494.7493)	
2019-01-08 12:39:36,917 - 10 - training_embed.py - training - Epoch: [3][800/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 492.0867 (494.6687)	
2019-01-08 12:39:37,154 - 10 - training_embed.py - training - Epoch: [3][900/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 491.2288 (494.6451)	
2019-01-08 12:39:37,386 - 10 - training_embed.py - training - Epoch: [3][1000/2022]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 498.1454 (494.5594)	
2019-01-08 12:39:37,629 - 10 - training_embed.py - training - Epoch: [3][1100/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 489.9925 (494.5229)	
2019-01-08 12:39:37,860 - 10 - training_embed.py - training - Epoch: [3][1200/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 492.7345 (494.4941)	
2019-01-08 12:39:38,097 - 10 - training_embed.py - training - Epoch: [3][1300/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 493.1784 (494.4310)	
2019-01-08 12:39:38,358 - 10 - training_embed.py - training - Epoch: [3][1400/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 490.6033 (494.3947)	
2019-01-08 12:39:38,583 - 10 - training_embed.py - training - Epoch: [3][1500/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 488.1283 (494.3814)	
2019-01-08 12:39:38,803 - 10 - training_embed.py - training - Epoch: [3][1600/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 495.7474 (494.3626)	
2019-01-08 12:39:39,061 - 10 - training_embed.py - training - Epoch: [3][1700/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 493.1006 (494.3268)	
2019-01-08 12:39:39,303 - 10 - training_embed.py - training - Epoch: [3][1800/2022]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 495.1838 (494.2833)	
2019-01-08 12:39:39,551 - 10 - training_embed.py - training - Epoch: [3][1900/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 490.5284 (494.2166)	
2019-01-08 12:39:39,778 - 10 - training_embed.py - training - Epoch: [3][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.3054 (494.1608)	
2019-01-08 12:39:39,826 - 10 - training_embed.py - training - loss: 494.086071
2019-01-08 12:39:39,826 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 12:39:40,268 - 10 - training_embed.py - training - Epoch: [4][100/2022]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 494.8409 (493.1357)	
2019-01-08 12:39:40,487 - 10 - training_embed.py - training - Epoch: [4][200/2022]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 483.5521 (493.2026)	
2019-01-08 12:39:40,728 - 10 - training_embed.py - training - Epoch: [4][300/2022]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 495.8584 (493.1402)	
2019-01-08 12:39:40,944 - 10 - training_embed.py - training - Epoch: [4][400/2022]	Time 0.005 (0.003)	Data 0.003 (0.001)	Loss 489.0676 (493.0424)	
2019-01-08 12:39:41,181 - 10 - training_embed.py - training - Epoch: [4][500/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 490.8120 (493.0086)	
2019-01-08 12:39:41,416 - 10 - training_embed.py - training - Epoch: [4][600/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 496.2850 (492.9091)	
2019-01-08 12:39:41,634 - 10 - training_embed.py - training - Epoch: [4][700/2022]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 492.7277 (492.8490)	
2019-01-08 12:39:41,897 - 10 - training_embed.py - training - Epoch: [4][800/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 485.8852 (492.7330)	
2019-01-08 12:39:42,130 - 10 - training_embed.py - training - Epoch: [4][900/2022]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 489.9493 (492.7288)	
2019-01-08 12:39:42,349 - 10 - training_embed.py - training - Epoch: [4][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.2863 (492.6136)	
2019-01-08 12:39:42,572 - 10 - training_embed.py - training - Epoch: [4][1100/2022]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 490.9056 (492.5415)	
2019-01-08 12:39:42,820 - 10 - training_embed.py - training - Epoch: [4][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.6926 (492.4797)	
2019-01-08 12:39:43,053 - 10 - training_embed.py - training - Epoch: [4][1300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 498.7437 (492.3970)	
2019-01-08 12:39:43,280 - 10 - training_embed.py - training - Epoch: [4][1400/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 487.8957 (492.3360)	
2019-01-08 12:39:43,514 - 10 - training_embed.py - training - Epoch: [4][1500/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 498.0021 (492.2523)	
2019-01-08 12:39:43,746 - 10 - training_embed.py - training - Epoch: [4][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.2398 (492.1904)	
2019-01-08 12:39:43,977 - 10 - training_embed.py - training - Epoch: [4][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.2187 (492.1515)	
2019-01-08 12:39:44,201 - 10 - training_embed.py - training - Epoch: [4][1800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 491.3997 (492.1182)	
2019-01-08 12:39:44,436 - 10 - training_embed.py - training - Epoch: [4][1900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 488.7526 (492.0897)	
2019-01-08 12:39:44,663 - 10 - training_embed.py - training - Epoch: [4][2000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 492.4172 (492.0535)	
2019-01-08 12:39:44,713 - 10 - training_embed.py - training - loss: 491.981041
2019-01-08 12:39:44,717 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 12:39:45,175 - 10 - training_embed.py - training - Epoch: [5][100/2022]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 491.1318 (490.7819)	
2019-01-08 12:39:45,405 - 10 - training_embed.py - training - Epoch: [5][200/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 487.0793 (490.8979)	
2019-01-08 12:39:45,637 - 10 - training_embed.py - training - Epoch: [5][300/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 492.8148 (490.8762)	
2019-01-08 12:39:45,859 - 10 - training_embed.py - training - Epoch: [5][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.1340 (490.8781)	
2019-01-08 12:39:46,086 - 10 - training_embed.py - training - Epoch: [5][500/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 488.1445 (490.6893)	
2019-01-08 12:39:46,310 - 10 - training_embed.py - training - Epoch: [5][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.8384 (490.6445)	
2019-01-08 12:39:46,534 - 10 - training_embed.py - training - Epoch: [5][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.2634 (490.6893)	
2019-01-08 12:39:46,769 - 10 - training_embed.py - training - Epoch: [5][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.5593 (490.6415)	
2019-01-08 12:39:46,987 - 10 - training_embed.py - training - Epoch: [5][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.6765 (490.5163)	
2019-01-08 12:39:47,222 - 10 - training_embed.py - training - Epoch: [5][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.6391 (490.4864)	
2019-01-08 12:39:47,446 - 10 - training_embed.py - training - Epoch: [5][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 493.6967 (490.4241)	
2019-01-08 12:39:47,679 - 10 - training_embed.py - training - Epoch: [5][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.0199 (490.3550)	
2019-01-08 12:39:47,909 - 10 - training_embed.py - training - Epoch: [5][1300/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 484.2159 (490.2895)	
2019-01-08 12:39:48,155 - 10 - training_embed.py - training - Epoch: [5][1400/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 480.3358 (490.2303)	
2019-01-08 12:39:48,377 - 10 - training_embed.py - training - Epoch: [5][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.3871 (490.1623)	
2019-01-08 12:39:48,618 - 10 - training_embed.py - training - Epoch: [5][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.5405 (490.1269)	
2019-01-08 12:39:48,833 - 10 - training_embed.py - training - Epoch: [5][1700/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 496.8863 (490.0873)	
2019-01-08 12:39:49,053 - 10 - training_embed.py - training - Epoch: [5][1800/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 497.6636 (490.0401)	
2019-01-08 12:39:49,290 - 10 - training_embed.py - training - Epoch: [5][1900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 488.4295 (489.9843)	
2019-01-08 12:39:49,510 - 10 - training_embed.py - training - Epoch: [5][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.3127 (489.9220)	
2019-01-08 12:39:49,556 - 10 - training_embed.py - training - loss: 489.838374
2019-01-08 12:39:49,556 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 12:39:50,001 - 10 - training_embed.py - training - Epoch: [6][100/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 484.9682 (488.0275)	
2019-01-08 12:39:50,240 - 10 - training_embed.py - training - Epoch: [6][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.3819 (488.3826)	
2019-01-08 12:39:50,467 - 10 - training_embed.py - training - Epoch: [6][300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 492.3846 (488.3933)	
2019-01-08 12:39:50,698 - 10 - training_embed.py - training - Epoch: [6][400/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 494.0211 (488.2456)	
2019-01-08 12:39:50,918 - 10 - training_embed.py - training - Epoch: [6][500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 480.8585 (488.1460)	
2019-01-08 12:39:51,144 - 10 - training_embed.py - training - Epoch: [6][600/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 499.6853 (488.1984)	
2019-01-08 12:39:51,366 - 10 - training_embed.py - training - Epoch: [6][700/2022]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 490.7141 (488.1583)	
2019-01-08 12:39:51,594 - 10 - training_embed.py - training - Epoch: [6][800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 491.3094 (488.1474)	
2019-01-08 12:39:51,838 - 10 - training_embed.py - training - Epoch: [6][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.6134 (488.0887)	
2019-01-08 12:39:52,057 - 10 - training_embed.py - training - Epoch: [6][1000/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 486.6091 (488.1180)	
2019-01-08 12:39:52,280 - 10 - training_embed.py - training - Epoch: [6][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.5605 (488.0660)	
2019-01-08 12:39:52,496 - 10 - training_embed.py - training - Epoch: [6][1200/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 498.6678 (488.0419)	
2019-01-08 12:39:52,732 - 10 - training_embed.py - training - Epoch: [6][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.3939 (488.0616)	
2019-01-08 12:39:52,956 - 10 - training_embed.py - training - Epoch: [6][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.9065 (488.0079)	
2019-01-08 12:39:53,184 - 10 - training_embed.py - training - Epoch: [6][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.3212 (487.9805)	
2019-01-08 12:39:53,414 - 10 - training_embed.py - training - Epoch: [6][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.2792 (487.9445)	
2019-01-08 12:39:53,639 - 10 - training_embed.py - training - Epoch: [6][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.1861 (487.8351)	
2019-01-08 12:39:53,859 - 10 - training_embed.py - training - Epoch: [6][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.1364 (487.7980)	
2019-01-08 12:39:54,084 - 10 - training_embed.py - training - Epoch: [6][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.2620 (487.7613)	
2019-01-08 12:39:54,306 - 10 - training_embed.py - training - Epoch: [6][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.5267 (487.7074)	
2019-01-08 12:39:54,354 - 10 - training_embed.py - training - loss: 487.658076
2019-01-08 12:39:54,355 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 12:39:54,746 - 10 - training_embed.py - training - Epoch: [7][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.5292 (486.3413)	
2019-01-08 12:39:54,959 - 10 - training_embed.py - training - Epoch: [7][200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 484.8472 (486.2148)	
2019-01-08 12:39:55,194 - 10 - training_embed.py - training - Epoch: [7][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.8377 (486.2529)	
2019-01-08 12:39:55,415 - 10 - training_embed.py - training - Epoch: [7][400/2022]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 482.1710 (486.2642)	
2019-01-08 12:39:55,634 - 10 - training_embed.py - training - Epoch: [7][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.8909 (486.2225)	
2019-01-08 12:39:55,881 - 10 - training_embed.py - training - Epoch: [7][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.7570 (486.1779)	
2019-01-08 12:39:56,113 - 10 - training_embed.py - training - Epoch: [7][700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 487.1706 (486.1575)	
2019-01-08 12:39:56,336 - 10 - training_embed.py - training - Epoch: [7][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.1259 (486.0365)	
2019-01-08 12:39:56,559 - 10 - training_embed.py - training - Epoch: [7][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.6979 (486.0146)	
2019-01-08 12:39:56,782 - 10 - training_embed.py - training - Epoch: [7][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.5961 (486.0086)	
2019-01-08 12:39:57,005 - 10 - training_embed.py - training - Epoch: [7][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.8274 (485.9143)	
2019-01-08 12:39:57,236 - 10 - training_embed.py - training - Epoch: [7][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.0380 (485.9126)	
2019-01-08 12:39:57,461 - 10 - training_embed.py - training - Epoch: [7][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.5259 (485.8455)	
2019-01-08 12:39:57,703 - 10 - training_embed.py - training - Epoch: [7][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 479.8571 (485.8011)	
2019-01-08 12:39:57,935 - 10 - training_embed.py - training - Epoch: [7][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.6634 (485.7392)	
2019-01-08 12:39:58,163 - 10 - training_embed.py - training - Epoch: [7][1600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 487.6592 (485.6978)	
2019-01-08 12:39:58,391 - 10 - training_embed.py - training - Epoch: [7][1700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 484.5096 (485.6558)	
2019-01-08 12:39:58,624 - 10 - training_embed.py - training - Epoch: [7][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.2924 (485.6236)	
2019-01-08 12:39:58,871 - 10 - training_embed.py - training - Epoch: [7][1900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 485.9352 (485.5514)	
2019-01-08 12:39:59,100 - 10 - training_embed.py - training - Epoch: [7][2000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 491.2483 (485.5058)	
2019-01-08 12:39:59,150 - 10 - training_embed.py - training - loss: 485.434633
2019-01-08 12:39:59,151 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 12:39:59,571 - 10 - training_embed.py - training - Epoch: [8][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 479.9086 (484.7592)	
2019-01-08 12:39:59,793 - 10 - training_embed.py - training - Epoch: [8][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.8907 (484.2450)	
2019-01-08 12:40:00,018 - 10 - training_embed.py - training - Epoch: [8][300/2022]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 483.3030 (484.0171)	
2019-01-08 12:40:00,245 - 10 - training_embed.py - training - Epoch: [8][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.0401 (484.1072)	
2019-01-08 12:40:00,476 - 10 - training_embed.py - training - Epoch: [8][500/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 485.6246 (483.9904)	
2019-01-08 12:40:00,711 - 10 - training_embed.py - training - Epoch: [8][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.2430 (484.0588)	
2019-01-08 12:40:00,936 - 10 - training_embed.py - training - Epoch: [8][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.4248 (484.0320)	
2019-01-08 12:40:01,162 - 10 - training_embed.py - training - Epoch: [8][800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 489.7327 (484.0412)	
2019-01-08 12:40:01,390 - 10 - training_embed.py - training - Epoch: [8][900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 487.1893 (483.9271)	
2019-01-08 12:40:01,616 - 10 - training_embed.py - training - Epoch: [8][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.4913 (483.7803)	
2019-01-08 12:40:01,845 - 10 - training_embed.py - training - Epoch: [8][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.6033 (483.7814)	
2019-01-08 12:40:02,078 - 10 - training_embed.py - training - Epoch: [8][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.0547 (483.7097)	
2019-01-08 12:40:02,302 - 10 - training_embed.py - training - Epoch: [8][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 473.5716 (483.6026)	
2019-01-08 12:40:02,524 - 10 - training_embed.py - training - Epoch: [8][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.1198 (483.5606)	
2019-01-08 12:40:02,744 - 10 - training_embed.py - training - Epoch: [8][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 478.6635 (483.4410)	
2019-01-08 12:40:02,973 - 10 - training_embed.py - training - Epoch: [8][1600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 481.3214 (483.3782)	
2019-01-08 12:40:03,209 - 10 - training_embed.py - training - Epoch: [8][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.9124 (483.3400)	
2019-01-08 12:40:03,458 - 10 - training_embed.py - training - Epoch: [8][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 468.6787 (483.3147)	
2019-01-08 12:40:03,679 - 10 - training_embed.py - training - Epoch: [8][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.1165 (483.2763)	
2019-01-08 12:40:03,894 - 10 - training_embed.py - training - Epoch: [8][2000/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 475.9533 (483.2341)	
2019-01-08 12:40:03,945 - 10 - training_embed.py - training - loss: 483.167024
2019-01-08 12:40:03,945 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 12:40:04,409 - 10 - training_embed.py - training - Epoch: [9][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.6740 (481.0524)	
2019-01-08 12:40:04,631 - 10 - training_embed.py - training - Epoch: [9][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.8148 (481.3502)	
2019-01-08 12:40:04,861 - 10 - training_embed.py - training - Epoch: [9][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.0713 (481.5565)	
2019-01-08 12:40:05,090 - 10 - training_embed.py - training - Epoch: [9][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.5676 (481.6013)	
2019-01-08 12:40:05,308 - 10 - training_embed.py - training - Epoch: [9][500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 479.5430 (481.6164)	
2019-01-08 12:40:05,528 - 10 - training_embed.py - training - Epoch: [9][600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 488.4818 (481.5336)	
2019-01-08 12:40:05,762 - 10 - training_embed.py - training - Epoch: [9][700/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 481.3115 (481.4488)	
2019-01-08 12:40:06,015 - 10 - training_embed.py - training - Epoch: [9][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 476.6802 (481.4022)	
2019-01-08 12:40:06,236 - 10 - training_embed.py - training - Epoch: [9][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 475.5182 (481.4045)	
2019-01-08 12:40:06,462 - 10 - training_embed.py - training - Epoch: [9][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.8638 (481.3557)	
2019-01-08 12:40:06,689 - 10 - training_embed.py - training - Epoch: [9][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 478.9627 (481.3414)	
2019-01-08 12:40:06,908 - 10 - training_embed.py - training - Epoch: [9][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.9470 (481.2860)	
2019-01-08 12:40:07,137 - 10 - training_embed.py - training - Epoch: [9][1300/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 486.9365 (481.2332)	
2019-01-08 12:40:07,367 - 10 - training_embed.py - training - Epoch: [9][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 470.5732 (481.1475)	
2019-01-08 12:40:07,603 - 10 - training_embed.py - training - Epoch: [9][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 486.0578 (481.1800)	
2019-01-08 12:40:07,845 - 10 - training_embed.py - training - Epoch: [9][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.6051 (481.1701)	
2019-01-08 12:40:08,084 - 10 - training_embed.py - training - Epoch: [9][1700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 472.1322 (481.0651)	
2019-01-08 12:40:08,316 - 10 - training_embed.py - training - Epoch: [9][1800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 483.4800 (481.0008)	
2019-01-08 12:40:08,532 - 10 - training_embed.py - training - Epoch: [9][1900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 486.1230 (480.9372)	
2019-01-08 12:40:08,768 - 10 - training_embed.py - training - Epoch: [9][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 476.3527 (480.9132)	
2019-01-08 12:40:08,817 - 10 - training_embed.py - training - loss: 480.849677
2019-01-08 12:40:08,817 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 12:40:09,259 - 10 - training_embed.py - training - Epoch: [10][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 474.5838 (479.4202)	
2019-01-08 12:40:09,504 - 10 - training_embed.py - training - Epoch: [10][200/2022]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 480.9679 (479.1439)	
2019-01-08 12:40:09,731 - 10 - training_embed.py - training - Epoch: [10][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 473.0978 (479.1876)	
2019-01-08 12:40:09,970 - 10 - training_embed.py - training - Epoch: [10][400/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 474.3963 (479.2778)	
2019-01-08 12:40:10,208 - 10 - training_embed.py - training - Epoch: [10][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 476.8403 (479.1867)	
2019-01-08 12:40:10,436 - 10 - training_embed.py - training - Epoch: [10][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.4388 (479.2356)	
2019-01-08 12:40:10,663 - 10 - training_embed.py - training - Epoch: [10][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 479.7982 (479.1195)	
2019-01-08 12:40:10,897 - 10 - training_embed.py - training - Epoch: [10][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.4157 (479.1652)	
2019-01-08 12:40:11,122 - 10 - training_embed.py - training - Epoch: [10][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 470.7543 (479.0265)	
2019-01-08 12:40:11,342 - 10 - training_embed.py - training - Epoch: [10][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 474.8597 (478.9610)	
2019-01-08 12:40:11,582 - 10 - training_embed.py - training - Epoch: [10][1100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 472.3858 (478.9520)	
2019-01-08 12:40:11,802 - 10 - training_embed.py - training - Epoch: [10][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.1104 (478.8516)	
2019-01-08 12:40:12,030 - 10 - training_embed.py - training - Epoch: [10][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.2652 (478.8227)	
2019-01-08 12:40:12,255 - 10 - training_embed.py - training - Epoch: [10][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 477.8923 (478.7984)	
2019-01-08 12:40:12,490 - 10 - training_embed.py - training - Epoch: [10][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 476.9636 (478.7590)	
2019-01-08 12:40:12,720 - 10 - training_embed.py - training - Epoch: [10][1600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 478.5113 (478.7008)	
2019-01-08 12:40:12,938 - 10 - training_embed.py - training - Epoch: [10][1700/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 480.2619 (478.6754)	
2019-01-08 12:40:13,158 - 10 - training_embed.py - training - Epoch: [10][1800/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 470.3699 (478.6956)	
2019-01-08 12:40:13,381 - 10 - training_embed.py - training - Epoch: [10][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.2985 (478.6271)	
2019-01-08 12:40:13,606 - 10 - training_embed.py - training - Epoch: [10][2000/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 474.4346 (478.5708)	
2019-01-08 12:40:13,656 - 10 - training_embed.py - training - loss: 478.476431
2019-01-08 12:40:13,657 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 12:40:14,107 - 10 - training_embed.py - training - Epoch: [11][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.6635 (477.4631)	
2019-01-08 12:40:14,338 - 10 - training_embed.py - training - Epoch: [11][200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 482.5690 (477.5530)	
2019-01-08 12:40:14,556 - 10 - training_embed.py - training - Epoch: [11][300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 476.7526 (477.4137)	
2019-01-08 12:40:14,787 - 10 - training_embed.py - training - Epoch: [11][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.4525 (477.3042)	
2019-01-08 12:40:15,005 - 10 - training_embed.py - training - Epoch: [11][500/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 482.1433 (477.2385)	
2019-01-08 12:40:15,225 - 10 - training_embed.py - training - Epoch: [11][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.6676 (477.1750)	
2019-01-08 12:40:15,462 - 10 - training_embed.py - training - Epoch: [11][700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 473.9293 (477.0865)	
2019-01-08 12:40:15,701 - 10 - training_embed.py - training - Epoch: [11][800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 472.0394 (476.9483)	
2019-01-08 12:40:15,930 - 10 - training_embed.py - training - Epoch: [11][900/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 460.9528 (476.9286)	
2019-01-08 12:40:16,170 - 10 - training_embed.py - training - Epoch: [11][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 467.4308 (476.9318)	
2019-01-08 12:40:16,397 - 10 - training_embed.py - training - Epoch: [11][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 474.7868 (476.8501)	
2019-01-08 12:40:16,631 - 10 - training_embed.py - training - Epoch: [11][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 465.2016 (476.7473)	
2019-01-08 12:40:16,852 - 10 - training_embed.py - training - Epoch: [11][1300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 472.7675 (476.6336)	
2019-01-08 12:40:17,089 - 10 - training_embed.py - training - Epoch: [11][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 467.8729 (476.5384)	
2019-01-08 12:40:17,317 - 10 - training_embed.py - training - Epoch: [11][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 473.5250 (476.4730)	
2019-01-08 12:40:17,553 - 10 - training_embed.py - training - Epoch: [11][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 477.4377 (476.4221)	
2019-01-08 12:40:17,785 - 10 - training_embed.py - training - Epoch: [11][1700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 479.4053 (476.3325)	
2019-01-08 12:40:18,015 - 10 - training_embed.py - training - Epoch: [11][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 470.2098 (476.2665)	
2019-01-08 12:40:18,246 - 10 - training_embed.py - training - Epoch: [11][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 473.1229 (476.1709)	
2019-01-08 12:40:18,475 - 10 - training_embed.py - training - Epoch: [11][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 478.3030 (476.1257)	
2019-01-08 12:40:18,524 - 10 - training_embed.py - training - loss: 476.045576
2019-01-08 12:40:18,610 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 12:40:19,653 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1042.708 ms ~ 0.017 min ~ 1.043 sec
2019-01-08 12:40:20,699 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 2089.000 ms ~ 0.035 min ~ 2.089 sec
2019-01-08 12:40:20,699 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 12:40:20,699 - 10 - corpus.py - subactivity_sampler - 0 / 166
2019-01-08 12:40:20,700 - 10 - corpus.py - subactivity_sampler - [ 69647.  24581.  20656.  63423.  11803. 114093.   4655. 109113.   7180.
  43477.  48850.]
2019-01-08 12:41:46,205 - 10 - corpus.py - subactivity_sampler - 20 / 166
2019-01-08 12:41:46,205 - 10 - corpus.py - subactivity_sampler - [ 69794.  24385.  20627.  63196.  11670. 114249.   4579. 109756.   7007.
  43393.  48822.]
2019-01-08 12:42:52,698 - 10 - corpus.py - subactivity_sampler - 40 / 166
2019-01-08 12:42:52,698 - 10 - corpus.py - subactivity_sampler - [ 69799.  24265.  20607.  63252.  11545. 114402.   4495. 110433.   6820.
  43191.  48669.]
2019-01-08 12:44:17,748 - 10 - corpus.py - subactivity_sampler - 60 / 166
2019-01-08 12:44:17,749 - 10 - corpus.py - subactivity_sampler - [ 69826.  24263.  20516.  63232.  11496. 114589.   4494. 110641.   6741.
  43032.  48648.]
2019-01-08 12:45:40,747 - 10 - corpus.py - subactivity_sampler - 80 / 166
2019-01-08 12:45:40,747 - 10 - corpus.py - subactivity_sampler - [ 69824.  24187.  20430.  63334.  11475. 114476.   4365. 110740.   7214.
  42838.  48595.]
2019-01-08 12:46:50,094 - 10 - corpus.py - subactivity_sampler - 100 / 166
2019-01-08 12:46:50,095 - 10 - corpus.py - subactivity_sampler - [ 69815.  24102.  20424.  63423.  11444. 114574.   4562. 110495.   7210.
  42819.  48610.]
2019-01-08 12:48:13,219 - 10 - corpus.py - subactivity_sampler - 120 / 166
2019-01-08 12:48:13,219 - 10 - corpus.py - subactivity_sampler - [ 69839.  23962.  20363.  63391.  11325. 114914.   4520. 110727.   7148.
  42698.  48591.]
2019-01-08 12:49:36,785 - 10 - corpus.py - subactivity_sampler - 140 / 166
2019-01-08 12:49:36,785 - 10 - corpus.py - subactivity_sampler - [ 70024.  23710.  20323.  63369.  11164. 115198.   4587. 110766.   7128.
  42618.  48591.]
2019-01-08 12:51:00,061 - 10 - corpus.py - subactivity_sampler - 160 / 166
2019-01-08 12:51:00,061 - 10 - corpus.py - subactivity_sampler - [ 70117.  23552.  20303.  63306.  11136. 115572.   4581. 110824.   7116.
  42402.  48569.]
2019-01-08 12:51:18,610 - 10 - corpus.py - subactivity_sampler - [ 70118.  23530.  20294.  63337.  11126. 115585.   4579. 110815.   7116.
  42447.  48531.]
2019-01-08 12:51:18,611 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 657911.778 ms ~ 10.965 min ~ 657.912 sec
2019-01-08 12:51:18,611 - 10 - corpus.py - ordering_sampler - .
2019-01-08 12:51:24,437 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 12:51:24,438 - 10 - corpus.py - ordering_sampler - inv_count_vec: [11. 18.  0. 33. 63. 81.  5. 17. 14.  7.]
2019-01-08 12:51:24,438 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 12:51:24,466 - 10 - corpus.py - rho_sampling - ['52.5416', '11.7552', '928.5602', '180.2552', '31.6307', '9.5077', '29.8102', '0.2274', '86.5067', '2.5282']
2019-01-08 12:51:24,466 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 12:51:24,649 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 12:51:24,660 - 10 - accuracy_class.py - mof - # gt_labels: 12   # pr_labels: 11
2019-01-08 12:51:24,661 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 11', '1: 12', '2: 42', '3: 17', '4: 16', '5: 43', '6: 10', '7: 44', '8: 4', '9: 14', '10: 15']
2019-01-08 12:51:24,685 - 10 - accuracy_class.py - mof_val - frames true: 204276	frames overall : 517478
2019-01-08 12:51:24,685 - 10 - corpus.py - accuracy_corpus - Action: scrambledegg
2019-01-08 12:51:24,685 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3947530136546868
2019-01-08 12:51:24,685 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3947530136546868
2019-01-08 12:51:24,685 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 29280
2019-01-08 12:51:24,685 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1218
2019-01-08 12:51:24,685 - 10 - accuracy_class.py - mof_classes - label 10: 0.021674  237 / 10935
2019-01-08 12:51:24,685 - 10 - accuracy_class.py - mof_classes - label 11: 0.398100  22376 / 56207
2019-01-08 12:51:24,685 - 10 - accuracy_class.py - mof_classes - label 12: 0.072899  2933 / 40234
2019-01-08 12:51:24,685 - 10 - accuracy_class.py - mof_classes - label 14: 0.403157  7100 / 17611
2019-01-08 12:51:24,685 - 10 - accuracy_class.py - mof_classes - label 15: 0.707502  30753 / 43467
2019-01-08 12:51:24,685 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 1003
2019-01-08 12:51:24,685 - 10 - accuracy_class.py - mof_classes - label 17: 0.305505  15077 / 49351
2019-01-08 12:51:24,685 - 10 - accuracy_class.py - mof_classes - label 42: 0.339506  10908 / 32129
2019-01-08 12:51:24,685 - 10 - accuracy_class.py - mof_classes - label 43: 0.592375  13611 / 22977
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - mof_classes - label 44: 0.475350  101281 / 213066
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - mof_classes - average class mof: 0.276339
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 29280
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 8334
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - label 10: 0.015514  237 / 15277
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - label 11: 0.215259  22376 / 103949
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - label 12: 0.048216  2933 / 60831
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - label 14: 0.134069  7100 / 52958
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - label 15: 0.502131  30753 / 61245
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 12129
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - label 17: 0.154460  15077 / 97611
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - label 42: 0.262748  10908 / 41515
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - label 43: 0.108931  13611 / 124951
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - label 44: 0.454991  101281 / 222600
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - average IoU: 0.172393
2019-01-08 12:51:24,686 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.158026
2019-01-08 12:51:32,926 - 10 - f1_score.py - f1 - f1 score: 0.291776
2019-01-08 12:51:32,945 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 8479.461 ms ~ 0.141 min ~ 8.479 sec
2019-01-08 12:51:32,945 - 10 - corpus.py - embedding_training - .
2019-01-08 12:51:32,945 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 12:51:32,946 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 12:51:32,946 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 12:51:36,442 - 10 - training_embed.py - training - create model
2019-01-08 12:51:36,443 - 10 - training_embed.py - training - epochs: 12
2019-01-08 12:51:36,443 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 12:51:36,825 - 10 - training_embed.py - training - Epoch: [0][100/2022]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 495.9185 (501.5498)	
2019-01-08 12:51:37,058 - 10 - training_embed.py - training - Epoch: [0][200/2022]	Time 0.003 (0.003)	Data 0.002 (0.002)	Loss 499.7471 (501.2672)	
2019-01-08 12:51:37,287 - 10 - training_embed.py - training - Epoch: [0][300/2022]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 503.5306 (501.0422)	
2019-01-08 12:51:37,500 - 10 - training_embed.py - training - Epoch: [0][400/2022]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 501.0667 (501.0030)	
2019-01-08 12:51:37,716 - 10 - training_embed.py - training - Epoch: [0][500/2022]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 495.1843 (500.9886)	
2019-01-08 12:51:37,933 - 10 - training_embed.py - training - Epoch: [0][600/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 499.4634 (500.8485)	
2019-01-08 12:51:38,142 - 10 - training_embed.py - training - Epoch: [0][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.4254 (500.8176)	
2019-01-08 12:51:38,368 - 10 - training_embed.py - training - Epoch: [0][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 507.1729 (500.8077)	
2019-01-08 12:51:38,600 - 10 - training_embed.py - training - Epoch: [0][900/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 507.5963 (500.6786)	
2019-01-08 12:51:38,825 - 10 - training_embed.py - training - Epoch: [0][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.2938 (500.6479)	
2019-01-08 12:51:39,059 - 10 - training_embed.py - training - Epoch: [0][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.0535 (500.6456)	
2019-01-08 12:51:39,280 - 10 - training_embed.py - training - Epoch: [0][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.7513 (500.6477)	
2019-01-08 12:51:39,516 - 10 - training_embed.py - training - Epoch: [0][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.1078 (500.5452)	
2019-01-08 12:51:39,733 - 10 - training_embed.py - training - Epoch: [0][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.2308 (500.5195)	
2019-01-08 12:51:39,956 - 10 - training_embed.py - training - Epoch: [0][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.4456 (500.5010)	
2019-01-08 12:51:40,184 - 10 - training_embed.py - training - Epoch: [0][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.4666 (500.4627)	
2019-01-08 12:51:40,407 - 10 - training_embed.py - training - Epoch: [0][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.4757 (500.3843)	
2019-01-08 12:51:40,619 - 10 - training_embed.py - training - Epoch: [0][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.2429 (500.3467)	
2019-01-08 12:51:40,846 - 10 - training_embed.py - training - Epoch: [0][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.7449 (500.2803)	
2019-01-08 12:51:41,066 - 10 - training_embed.py - training - Epoch: [0][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.6537 (500.2327)	
2019-01-08 12:51:41,110 - 10 - training_embed.py - training - loss: 500.152948
2019-01-08 12:51:41,110 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 12:51:41,550 - 10 - training_embed.py - training - Epoch: [1][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.4837 (499.2893)	
2019-01-08 12:51:41,765 - 10 - training_embed.py - training - Epoch: [1][200/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 503.2794 (499.2513)	
2019-01-08 12:51:41,998 - 10 - training_embed.py - training - Epoch: [1][300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 501.7159 (499.0679)	
2019-01-08 12:51:42,216 - 10 - training_embed.py - training - Epoch: [1][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.4122 (498.8445)	
2019-01-08 12:51:42,443 - 10 - training_embed.py - training - Epoch: [1][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.3531 (498.7868)	
2019-01-08 12:51:42,674 - 10 - training_embed.py - training - Epoch: [1][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 504.1313 (498.7731)	
2019-01-08 12:51:42,898 - 10 - training_embed.py - training - Epoch: [1][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.7158 (498.7326)	
2019-01-08 12:51:43,109 - 10 - training_embed.py - training - Epoch: [1][800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 500.3487 (498.6793)	
2019-01-08 12:51:43,330 - 10 - training_embed.py - training - Epoch: [1][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.2873 (498.6392)	
2019-01-08 12:51:43,541 - 10 - training_embed.py - training - Epoch: [1][1000/2022]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 500.3637 (498.6113)	
2019-01-08 12:51:43,755 - 10 - training_embed.py - training - Epoch: [1][1100/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 499.2910 (498.5427)	
2019-01-08 12:51:43,992 - 10 - training_embed.py - training - Epoch: [1][1200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 505.5046 (498.4881)	
2019-01-08 12:51:44,221 - 10 - training_embed.py - training - Epoch: [1][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 503.5135 (498.4074)	
2019-01-08 12:51:44,443 - 10 - training_embed.py - training - Epoch: [1][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.2630 (498.3486)	
2019-01-08 12:51:44,650 - 10 - training_embed.py - training - Epoch: [1][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 498.6700 (498.3002)	
2019-01-08 12:51:44,870 - 10 - training_embed.py - training - Epoch: [1][1600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 499.2487 (498.2770)	
2019-01-08 12:51:45,095 - 10 - training_embed.py - training - Epoch: [1][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.6859 (498.2550)	
2019-01-08 12:51:45,318 - 10 - training_embed.py - training - Epoch: [1][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.1816 (498.2114)	
2019-01-08 12:51:45,544 - 10 - training_embed.py - training - Epoch: [1][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.9167 (498.1725)	
2019-01-08 12:51:45,759 - 10 - training_embed.py - training - Epoch: [1][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 502.8564 (498.1255)	
2019-01-08 12:51:45,809 - 10 - training_embed.py - training - loss: 498.053423
2019-01-08 12:51:45,809 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 12:51:46,225 - 10 - training_embed.py - training - Epoch: [2][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.5923 (497.0329)	
2019-01-08 12:51:46,440 - 10 - training_embed.py - training - Epoch: [2][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 504.3837 (496.6688)	
2019-01-08 12:51:46,664 - 10 - training_embed.py - training - Epoch: [2][300/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 496.1165 (496.8820)	
2019-01-08 12:51:46,890 - 10 - training_embed.py - training - Epoch: [2][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.7820 (496.7469)	
2019-01-08 12:51:47,113 - 10 - training_embed.py - training - Epoch: [2][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.5405 (496.6097)	
2019-01-08 12:51:47,340 - 10 - training_embed.py - training - Epoch: [2][600/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 498.4266 (496.6955)	
2019-01-08 12:51:47,570 - 10 - training_embed.py - training - Epoch: [2][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.5518 (496.6570)	
2019-01-08 12:51:47,793 - 10 - training_embed.py - training - Epoch: [2][800/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 492.7495 (496.5743)	
2019-01-08 12:51:48,021 - 10 - training_embed.py - training - Epoch: [2][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 500.2192 (496.5465)	
2019-01-08 12:51:48,231 - 10 - training_embed.py - training - Epoch: [2][1000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 485.7802 (496.4070)	
2019-01-08 12:51:48,442 - 10 - training_embed.py - training - Epoch: [2][1100/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 499.5787 (496.3680)	
2019-01-08 12:51:48,658 - 10 - training_embed.py - training - Epoch: [2][1200/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 499.7095 (496.3695)	
2019-01-08 12:51:48,875 - 10 - training_embed.py - training - Epoch: [2][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 501.3773 (496.3561)	
2019-01-08 12:51:49,095 - 10 - training_embed.py - training - Epoch: [2][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.1420 (496.3488)	
2019-01-08 12:51:49,310 - 10 - training_embed.py - training - Epoch: [2][1500/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 499.4634 (496.2990)	
2019-01-08 12:51:49,541 - 10 - training_embed.py - training - Epoch: [2][1600/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 493.3544 (496.1905)	
2019-01-08 12:51:49,771 - 10 - training_embed.py - training - Epoch: [2][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.5065 (496.1522)	
2019-01-08 12:51:50,000 - 10 - training_embed.py - training - Epoch: [2][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.4252 (496.1334)	
2019-01-08 12:51:50,212 - 10 - training_embed.py - training - Epoch: [2][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.8669 (496.0602)	
2019-01-08 12:51:50,435 - 10 - training_embed.py - training - Epoch: [2][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.8849 (496.0013)	
2019-01-08 12:51:50,481 - 10 - training_embed.py - training - loss: 495.931951
2019-01-08 12:51:50,482 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 12:51:50,924 - 10 - training_embed.py - training - Epoch: [3][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.0916 (495.1658)	
2019-01-08 12:51:51,147 - 10 - training_embed.py - training - Epoch: [3][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.0169 (494.6738)	
2019-01-08 12:51:51,358 - 10 - training_embed.py - training - Epoch: [3][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.7814 (494.5938)	
2019-01-08 12:51:51,565 - 10 - training_embed.py - training - Epoch: [3][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.7011 (494.6142)	
2019-01-08 12:51:51,795 - 10 - training_embed.py - training - Epoch: [3][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.5434 (494.6104)	
2019-01-08 12:51:52,019 - 10 - training_embed.py - training - Epoch: [3][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.9659 (494.5914)	
2019-01-08 12:51:52,235 - 10 - training_embed.py - training - Epoch: [3][700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 490.4401 (494.4600)	
2019-01-08 12:51:52,447 - 10 - training_embed.py - training - Epoch: [3][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.8531 (494.3736)	
2019-01-08 12:51:52,655 - 10 - training_embed.py - training - Epoch: [3][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.4966 (494.3464)	
2019-01-08 12:51:52,868 - 10 - training_embed.py - training - Epoch: [3][1000/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 497.1008 (494.2821)	
2019-01-08 12:51:53,078 - 10 - training_embed.py - training - Epoch: [3][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 489.8241 (494.2387)	
2019-01-08 12:51:53,301 - 10 - training_embed.py - training - Epoch: [3][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.0735 (494.2126)	
2019-01-08 12:51:53,521 - 10 - training_embed.py - training - Epoch: [3][1300/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 492.7567 (494.1437)	
2019-01-08 12:51:53,737 - 10 - training_embed.py - training - Epoch: [3][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.9290 (494.1036)	
2019-01-08 12:51:53,948 - 10 - training_embed.py - training - Epoch: [3][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 489.5220 (494.0687)	
2019-01-08 12:51:54,166 - 10 - training_embed.py - training - Epoch: [3][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.8380 (494.0531)	
2019-01-08 12:51:54,389 - 10 - training_embed.py - training - Epoch: [3][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.2933 (494.0287)	
2019-01-08 12:51:54,615 - 10 - training_embed.py - training - Epoch: [3][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.3032 (493.9800)	
2019-01-08 12:51:54,840 - 10 - training_embed.py - training - Epoch: [3][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.0918 (493.9102)	
2019-01-08 12:51:55,047 - 10 - training_embed.py - training - Epoch: [3][2000/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 487.6765 (493.8559)	
2019-01-08 12:51:55,093 - 10 - training_embed.py - training - loss: 493.778178
2019-01-08 12:51:55,093 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 12:51:55,522 - 10 - training_embed.py - training - Epoch: [4][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.5250 (493.0028)	
2019-01-08 12:51:55,731 - 10 - training_embed.py - training - Epoch: [4][200/2022]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 482.9164 (492.9201)	
2019-01-08 12:51:55,951 - 10 - training_embed.py - training - Epoch: [4][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.9103 (492.8025)	
2019-01-08 12:51:56,158 - 10 - training_embed.py - training - Epoch: [4][400/2022]	Time 0.003 (0.002)	Data 0.000 (0.001)	Loss 488.9118 (492.7233)	
2019-01-08 12:51:56,367 - 10 - training_embed.py - training - Epoch: [4][500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 490.2068 (492.6843)	
2019-01-08 12:51:56,578 - 10 - training_embed.py - training - Epoch: [4][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.2875 (492.5615)	
2019-01-08 12:51:56,793 - 10 - training_embed.py - training - Epoch: [4][700/2022]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 491.2640 (492.4931)	
2019-01-08 12:51:57,029 - 10 - training_embed.py - training - Epoch: [4][800/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 485.3400 (492.3840)	
2019-01-08 12:51:57,241 - 10 - training_embed.py - training - Epoch: [4][900/2022]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 491.2276 (492.3730)	
2019-01-08 12:51:57,456 - 10 - training_embed.py - training - Epoch: [4][1000/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 487.1807 (492.2541)	
2019-01-08 12:51:57,676 - 10 - training_embed.py - training - Epoch: [4][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.4626 (492.1765)	
2019-01-08 12:51:57,899 - 10 - training_embed.py - training - Epoch: [4][1200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 493.7864 (492.1104)	
2019-01-08 12:51:58,120 - 10 - training_embed.py - training - Epoch: [4][1300/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 497.7911 (492.0260)	
2019-01-08 12:51:58,343 - 10 - training_embed.py - training - Epoch: [4][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.8561 (491.9622)	
2019-01-08 12:51:58,550 - 10 - training_embed.py - training - Epoch: [4][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 498.1022 (491.8710)	
2019-01-08 12:51:58,772 - 10 - training_embed.py - training - Epoch: [4][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.7548 (491.8014)	
2019-01-08 12:51:58,992 - 10 - training_embed.py - training - Epoch: [4][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.8696 (491.7711)	
2019-01-08 12:51:59,205 - 10 - training_embed.py - training - Epoch: [4][1800/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 491.5717 (491.7367)	
2019-01-08 12:51:59,432 - 10 - training_embed.py - training - Epoch: [4][1900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 487.5149 (491.7061)	
2019-01-08 12:51:59,656 - 10 - training_embed.py - training - Epoch: [4][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.4781 (491.6684)	
2019-01-08 12:51:59,705 - 10 - training_embed.py - training - loss: 491.595393
2019-01-08 12:51:59,706 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 12:52:00,136 - 10 - training_embed.py - training - Epoch: [5][100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 490.0981 (490.2849)	
2019-01-08 12:52:00,364 - 10 - training_embed.py - training - Epoch: [5][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.5027 (490.4374)	
2019-01-08 12:52:00,594 - 10 - training_embed.py - training - Epoch: [5][300/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 493.5240 (490.4387)	
2019-01-08 12:52:00,810 - 10 - training_embed.py - training - Epoch: [5][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.2236 (490.4509)	
2019-01-08 12:52:01,033 - 10 - training_embed.py - training - Epoch: [5][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.3622 (490.2993)	
2019-01-08 12:52:01,246 - 10 - training_embed.py - training - Epoch: [5][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.7160 (490.2369)	
2019-01-08 12:52:01,475 - 10 - training_embed.py - training - Epoch: [5][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.8533 (490.2761)	
2019-01-08 12:52:01,703 - 10 - training_embed.py - training - Epoch: [5][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.6193 (490.2179)	
2019-01-08 12:52:01,922 - 10 - training_embed.py - training - Epoch: [5][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 495.2788 (490.0834)	
2019-01-08 12:52:02,142 - 10 - training_embed.py - training - Epoch: [5][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.0806 (490.0689)	
2019-01-08 12:52:02,368 - 10 - training_embed.py - training - Epoch: [5][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.2162 (490.0047)	
2019-01-08 12:52:02,588 - 10 - training_embed.py - training - Epoch: [5][1200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.2912 (489.9262)	
2019-01-08 12:52:02,811 - 10 - training_embed.py - training - Epoch: [5][1300/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 483.3846 (489.8563)	
2019-01-08 12:52:03,043 - 10 - training_embed.py - training - Epoch: [5][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.0933 (489.7817)	
2019-01-08 12:52:03,256 - 10 - training_embed.py - training - Epoch: [5][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.3758 (489.7096)	
2019-01-08 12:52:03,477 - 10 - training_embed.py - training - Epoch: [5][1600/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 490.0231 (489.6742)	
2019-01-08 12:52:03,704 - 10 - training_embed.py - training - Epoch: [5][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 497.4268 (489.6382)	
2019-01-08 12:52:03,924 - 10 - training_embed.py - training - Epoch: [5][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 496.8838 (489.5921)	
2019-01-08 12:52:04,150 - 10 - training_embed.py - training - Epoch: [5][1900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 488.5024 (489.5338)	
2019-01-08 12:52:04,378 - 10 - training_embed.py - training - Epoch: [5][2000/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 494.5945 (489.4620)	
2019-01-08 12:52:04,423 - 10 - training_embed.py - training - loss: 489.373293
2019-01-08 12:52:04,423 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 12:52:04,846 - 10 - training_embed.py - training - Epoch: [6][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.0737 (487.4600)	
2019-01-08 12:52:05,061 - 10 - training_embed.py - training - Epoch: [6][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.0074 (487.8792)	
2019-01-08 12:52:05,275 - 10 - training_embed.py - training - Epoch: [6][300/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 491.5983 (487.9214)	
2019-01-08 12:52:05,506 - 10 - training_embed.py - training - Epoch: [6][400/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 492.5154 (487.7486)	
2019-01-08 12:52:05,722 - 10 - training_embed.py - training - Epoch: [6][500/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 480.7524 (487.6056)	
2019-01-08 12:52:05,935 - 10 - training_embed.py - training - Epoch: [6][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 499.1495 (487.6625)	
2019-01-08 12:52:06,164 - 10 - training_embed.py - training - Epoch: [6][700/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 490.1259 (487.6157)	
2019-01-08 12:52:06,378 - 10 - training_embed.py - training - Epoch: [6][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.4355 (487.6021)	
2019-01-08 12:52:06,592 - 10 - training_embed.py - training - Epoch: [6][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 493.5510 (487.5504)	
2019-01-08 12:52:06,809 - 10 - training_embed.py - training - Epoch: [6][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.9919 (487.5706)	
2019-01-08 12:52:07,027 - 10 - training_embed.py - training - Epoch: [6][1100/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 486.8669 (487.5113)	
2019-01-08 12:52:07,244 - 10 - training_embed.py - training - Epoch: [6][1200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 497.5181 (487.4982)	
2019-01-08 12:52:07,457 - 10 - training_embed.py - training - Epoch: [6][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.1523 (487.5091)	
2019-01-08 12:52:07,679 - 10 - training_embed.py - training - Epoch: [6][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.8887 (487.4659)	
2019-01-08 12:52:07,898 - 10 - training_embed.py - training - Epoch: [6][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.0659 (487.4398)	
2019-01-08 12:52:08,110 - 10 - training_embed.py - training - Epoch: [6][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.5088 (487.3971)	
2019-01-08 12:52:08,325 - 10 - training_embed.py - training - Epoch: [6][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.5303 (487.3044)	
2019-01-08 12:52:08,542 - 10 - training_embed.py - training - Epoch: [6][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.8748 (487.2586)	
2019-01-08 12:52:08,749 - 10 - training_embed.py - training - Epoch: [6][1900/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 492.2147 (487.2173)	
2019-01-08 12:52:08,958 - 10 - training_embed.py - training - Epoch: [6][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.0942 (487.1645)	
2019-01-08 12:52:09,008 - 10 - training_embed.py - training - loss: 487.111398
2019-01-08 12:52:09,008 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 12:52:09,476 - 10 - training_embed.py - training - Epoch: [7][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.4221 (485.6843)	
2019-01-08 12:52:09,689 - 10 - training_embed.py - training - Epoch: [7][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.0617 (485.6410)	
2019-01-08 12:52:09,905 - 10 - training_embed.py - training - Epoch: [7][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 492.3615 (485.6680)	
2019-01-08 12:52:10,123 - 10 - training_embed.py - training - Epoch: [7][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.9579 (485.6358)	
2019-01-08 12:52:10,333 - 10 - training_embed.py - training - Epoch: [7][500/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 482.2723 (485.5884)	
2019-01-08 12:52:10,569 - 10 - training_embed.py - training - Epoch: [7][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.5662 (485.5776)	
2019-01-08 12:52:10,795 - 10 - training_embed.py - training - Epoch: [7][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.4328 (485.5610)	
2019-01-08 12:52:11,014 - 10 - training_embed.py - training - Epoch: [7][800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 487.7064 (485.4487)	
2019-01-08 12:52:11,241 - 10 - training_embed.py - training - Epoch: [7][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.2444 (485.4075)	
2019-01-08 12:52:11,468 - 10 - training_embed.py - training - Epoch: [7][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.2027 (485.3995)	
2019-01-08 12:52:11,700 - 10 - training_embed.py - training - Epoch: [7][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.8612 (485.3104)	
2019-01-08 12:52:11,914 - 10 - training_embed.py - training - Epoch: [7][1200/2022]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 487.8525 (485.3072)	
2019-01-08 12:52:12,139 - 10 - training_embed.py - training - Epoch: [7][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.9074 (485.2490)	
2019-01-08 12:52:12,364 - 10 - training_embed.py - training - Epoch: [7][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 478.6417 (485.1989)	
2019-01-08 12:52:12,577 - 10 - training_embed.py - training - Epoch: [7][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.8853 (485.1274)	
2019-01-08 12:52:12,796 - 10 - training_embed.py - training - Epoch: [7][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.7850 (485.0765)	
2019-01-08 12:52:13,010 - 10 - training_embed.py - training - Epoch: [7][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.2744 (485.0248)	
2019-01-08 12:52:13,226 - 10 - training_embed.py - training - Epoch: [7][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.9127 (485.0005)	
2019-01-08 12:52:13,460 - 10 - training_embed.py - training - Epoch: [7][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.4879 (484.9275)	
2019-01-08 12:52:13,683 - 10 - training_embed.py - training - Epoch: [7][2000/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 489.7497 (484.8716)	
2019-01-08 12:52:13,741 - 10 - training_embed.py - training - loss: 484.803717
2019-01-08 12:52:13,741 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 12:52:14,183 - 10 - training_embed.py - training - Epoch: [8][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 478.9407 (484.0935)	
2019-01-08 12:52:14,411 - 10 - training_embed.py - training - Epoch: [8][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 491.4144 (483.6176)	
2019-01-08 12:52:14,633 - 10 - training_embed.py - training - Epoch: [8][300/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 482.7425 (483.3671)	
2019-01-08 12:52:14,858 - 10 - training_embed.py - training - Epoch: [8][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.9678 (483.4655)	
2019-01-08 12:52:15,080 - 10 - training_embed.py - training - Epoch: [8][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.7561 (483.3205)	
2019-01-08 12:52:15,306 - 10 - training_embed.py - training - Epoch: [8][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 479.7798 (483.4011)	
2019-01-08 12:52:15,524 - 10 - training_embed.py - training - Epoch: [8][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.6771 (483.3635)	
2019-01-08 12:52:15,745 - 10 - training_embed.py - training - Epoch: [8][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 485.9852 (483.3574)	
2019-01-08 12:52:15,951 - 10 - training_embed.py - training - Epoch: [8][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.0460 (483.2216)	
2019-01-08 12:52:16,176 - 10 - training_embed.py - training - Epoch: [8][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.3860 (483.0900)	
2019-01-08 12:52:16,385 - 10 - training_embed.py - training - Epoch: [8][1100/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 485.3707 (483.0893)	
2019-01-08 12:52:16,598 - 10 - training_embed.py - training - Epoch: [8][1200/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 488.3622 (483.0153)	
2019-01-08 12:52:16,809 - 10 - training_embed.py - training - Epoch: [8][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 472.7457 (482.9036)	
2019-01-08 12:52:17,028 - 10 - training_embed.py - training - Epoch: [8][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.2847 (482.8567)	
2019-01-08 12:52:17,243 - 10 - training_embed.py - training - Epoch: [8][1500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 479.6665 (482.7272)	
2019-01-08 12:52:17,455 - 10 - training_embed.py - training - Epoch: [8][1600/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 482.4180 (482.6709)	
2019-01-08 12:52:17,674 - 10 - training_embed.py - training - Epoch: [8][1700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 488.7927 (482.6245)	
2019-01-08 12:52:17,889 - 10 - training_embed.py - training - Epoch: [8][1800/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 468.9330 (482.5989)	
2019-01-08 12:52:18,119 - 10 - training_embed.py - training - Epoch: [8][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 479.9266 (482.5615)	
2019-01-08 12:52:18,341 - 10 - training_embed.py - training - Epoch: [8][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 475.2183 (482.5188)	
2019-01-08 12:52:18,387 - 10 - training_embed.py - training - loss: 482.449510
2019-01-08 12:52:18,388 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 12:52:18,832 - 10 - training_embed.py - training - Epoch: [9][100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 490.8384 (480.2499)	
2019-01-08 12:52:19,054 - 10 - training_embed.py - training - Epoch: [9][200/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 489.4479 (480.5989)	
2019-01-08 12:52:19,275 - 10 - training_embed.py - training - Epoch: [9][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 494.4407 (480.8014)	
2019-01-08 12:52:19,499 - 10 - training_embed.py - training - Epoch: [9][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.6163 (480.8256)	
2019-01-08 12:52:19,739 - 10 - training_embed.py - training - Epoch: [9][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 477.0696 (480.8098)	
2019-01-08 12:52:19,955 - 10 - training_embed.py - training - Epoch: [9][600/2022]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 487.9396 (480.7250)	
2019-01-08 12:52:20,171 - 10 - training_embed.py - training - Epoch: [9][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.0036 (480.6519)	
2019-01-08 12:52:20,391 - 10 - training_embed.py - training - Epoch: [9][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 476.5161 (480.5942)	
2019-01-08 12:52:20,611 - 10 - training_embed.py - training - Epoch: [9][900/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 473.5739 (480.5919)	
2019-01-08 12:52:20,838 - 10 - training_embed.py - training - Epoch: [9][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.3174 (480.5413)	
2019-01-08 12:52:21,072 - 10 - training_embed.py - training - Epoch: [9][1100/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 477.9919 (480.5406)	
2019-01-08 12:52:21,284 - 10 - training_embed.py - training - Epoch: [9][1200/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 480.4852 (480.4822)	
2019-01-08 12:52:21,498 - 10 - training_embed.py - training - Epoch: [9][1300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 483.9123 (480.4235)	
2019-01-08 12:52:21,720 - 10 - training_embed.py - training - Epoch: [9][1400/2022]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 468.7133 (480.3496)	
2019-01-08 12:52:21,952 - 10 - training_embed.py - training - Epoch: [9][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 486.1187 (480.3747)	
2019-01-08 12:52:22,171 - 10 - training_embed.py - training - Epoch: [9][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.3346 (480.3613)	
2019-01-08 12:52:22,384 - 10 - training_embed.py - training - Epoch: [9][1700/2022]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 471.5789 (480.2580)	
2019-01-08 12:52:22,605 - 10 - training_embed.py - training - Epoch: [9][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 484.9880 (480.1868)	
2019-01-08 12:52:22,818 - 10 - training_embed.py - training - Epoch: [9][1900/2022]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 486.3725 (480.1285)	
2019-01-08 12:52:23,050 - 10 - training_embed.py - training - Epoch: [9][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 476.7205 (480.1032)	
2019-01-08 12:52:23,094 - 10 - training_embed.py - training - loss: 480.041946
2019-01-08 12:52:23,094 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 12:52:23,529 - 10 - training_embed.py - training - Epoch: [10][100/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 473.7301 (478.7184)	
2019-01-08 12:52:23,750 - 10 - training_embed.py - training - Epoch: [10][200/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 477.5118 (478.3561)	
2019-01-08 12:52:23,966 - 10 - training_embed.py - training - Epoch: [10][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 472.2498 (478.3208)	
2019-01-08 12:52:24,179 - 10 - training_embed.py - training - Epoch: [10][400/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 473.5727 (478.4393)	
2019-01-08 12:52:24,409 - 10 - training_embed.py - training - Epoch: [10][500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 476.5961 (478.3152)	
2019-01-08 12:52:24,633 - 10 - training_embed.py - training - Epoch: [10][600/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 480.3961 (478.3743)	
2019-01-08 12:52:24,859 - 10 - training_embed.py - training - Epoch: [10][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 478.0878 (478.2609)	
2019-01-08 12:52:25,080 - 10 - training_embed.py - training - Epoch: [10][800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 482.3801 (478.2899)	
2019-01-08 12:52:25,307 - 10 - training_embed.py - training - Epoch: [10][900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 469.2886 (478.1565)	
2019-01-08 12:52:25,534 - 10 - training_embed.py - training - Epoch: [10][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 474.7334 (478.0848)	
2019-01-08 12:52:25,747 - 10 - training_embed.py - training - Epoch: [10][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 472.2220 (478.0831)	
2019-01-08 12:52:25,982 - 10 - training_embed.py - training - Epoch: [10][1200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 478.8911 (477.9790)	
2019-01-08 12:52:26,222 - 10 - training_embed.py - training - Epoch: [10][1300/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 481.0306 (477.9445)	
2019-01-08 12:52:26,455 - 10 - training_embed.py - training - Epoch: [10][1400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 475.5623 (477.9150)	
2019-01-08 12:52:26,684 - 10 - training_embed.py - training - Epoch: [10][1500/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 477.0059 (477.8672)	
2019-01-08 12:52:26,902 - 10 - training_embed.py - training - Epoch: [10][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 477.8039 (477.8090)	
2019-01-08 12:52:27,137 - 10 - training_embed.py - training - Epoch: [10][1700/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 477.8382 (477.7900)	
2019-01-08 12:52:27,356 - 10 - training_embed.py - training - Epoch: [10][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 471.0234 (477.8092)	
2019-01-08 12:52:27,587 - 10 - training_embed.py - training - Epoch: [10][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.9333 (477.7293)	
2019-01-08 12:52:27,818 - 10 - training_embed.py - training - Epoch: [10][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 474.1612 (477.6689)	
2019-01-08 12:52:27,864 - 10 - training_embed.py - training - loss: 477.576070
2019-01-08 12:52:27,865 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 12:52:28,294 - 10 - training_embed.py - training - Epoch: [11][100/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 479.7938 (476.3640)	
2019-01-08 12:52:28,509 - 10 - training_embed.py - training - Epoch: [11][200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 481.8566 (476.4851)	
2019-01-08 12:52:28,734 - 10 - training_embed.py - training - Epoch: [11][300/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 475.7556 (476.3292)	
2019-01-08 12:52:28,962 - 10 - training_embed.py - training - Epoch: [11][400/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 487.5632 (476.2194)	
2019-01-08 12:52:29,187 - 10 - training_embed.py - training - Epoch: [11][500/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 479.6744 (476.1649)	
2019-01-08 12:52:29,412 - 10 - training_embed.py - training - Epoch: [11][600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 481.7191 (476.1165)	
2019-01-08 12:52:29,635 - 10 - training_embed.py - training - Epoch: [11][700/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 473.2326 (476.0453)	
2019-01-08 12:52:29,851 - 10 - training_embed.py - training - Epoch: [11][800/2022]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 472.4708 (475.9189)	
2019-01-08 12:52:30,065 - 10 - training_embed.py - training - Epoch: [11][900/2022]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 459.2825 (475.9099)	
2019-01-08 12:52:30,286 - 10 - training_embed.py - training - Epoch: [11][1000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 467.1971 (475.9028)	
2019-01-08 12:52:30,502 - 10 - training_embed.py - training - Epoch: [11][1100/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 474.9717 (475.8262)	
2019-01-08 12:52:30,722 - 10 - training_embed.py - training - Epoch: [11][1200/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 464.6765 (475.7173)	
2019-01-08 12:52:30,943 - 10 - training_embed.py - training - Epoch: [11][1300/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 471.6737 (475.5983)	
2019-01-08 12:52:31,155 - 10 - training_embed.py - training - Epoch: [11][1400/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 466.0846 (475.5099)	
2019-01-08 12:52:31,399 - 10 - training_embed.py - training - Epoch: [11][1500/2022]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 473.3612 (475.4544)	
2019-01-08 12:52:31,628 - 10 - training_embed.py - training - Epoch: [11][1600/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 475.0815 (475.4066)	
2019-01-08 12:52:31,865 - 10 - training_embed.py - training - Epoch: [11][1700/2022]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 478.9464 (475.3224)	
2019-01-08 12:52:32,089 - 10 - training_embed.py - training - Epoch: [11][1800/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 470.7551 (475.2585)	
2019-01-08 12:52:32,313 - 10 - training_embed.py - training - Epoch: [11][1900/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 474.6597 (475.1655)	
2019-01-08 12:52:32,548 - 10 - training_embed.py - training - Epoch: [11][2000/2022]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 480.0937 (475.1274)	
2019-01-08 12:52:32,593 - 10 - training_embed.py - training - loss: 475.048324
2019-01-08 12:52:32,669 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 12:52:33,664 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 994.058 ms ~ 0.017 min ~ 0.994 sec
2019-01-08 12:52:34,695 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 2025.451 ms ~ 0.034 min ~ 2.025 sec
2019-01-08 12:52:34,695 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 12:52:34,696 - 10 - corpus.py - subactivity_sampler - 0 / 166
2019-01-08 12:52:34,696 - 10 - corpus.py - subactivity_sampler - [ 70118.  23530.  20294.  63337.  11126. 115585.   4579. 110815.   7116.
  42447.  48531.]
2019-01-08 12:53:56,671 - 10 - corpus.py - subactivity_sampler - 20 / 166
2019-01-08 12:53:56,671 - 10 - corpus.py - subactivity_sampler - [ 70255.  23415.  20118.  63395.  11077. 115742.   4536. 110999.   7106.
  42338.  48497.]
2019-01-08 12:55:04,015 - 10 - corpus.py - subactivity_sampler - 40 / 166
2019-01-08 12:55:04,016 - 10 - corpus.py - subactivity_sampler - [ 70359.  23130.  20098.  63296.  11072. 115923.   4499. 111339.   6961.
  42240.  48561.]
2019-01-08 12:56:29,472 - 10 - corpus.py - subactivity_sampler - 60 / 166
2019-01-08 12:56:29,472 - 10 - corpus.py - subactivity_sampler - [ 70828.  22748.  19946.  63315.  11060. 115969.   4475. 111399.   6994.
  42316.  48428.]
2019-01-08 12:57:53,004 - 10 - corpus.py - subactivity_sampler - 80 / 166
2019-01-08 12:57:53,004 - 10 - corpus.py - subactivity_sampler - [ 71335.  22347.  19378.  62864.  11075. 116321.   4418. 111431.   7496.
  42291.  48522.]
2019-01-08 12:59:02,540 - 10 - corpus.py - subactivity_sampler - 100 / 166
2019-01-08 12:59:02,540 - 10 - corpus.py - subactivity_sampler - [ 71350.  22145.  19372.  62859.  11067. 116530.   4418. 111445.   7496.
  42271.  48525.]
2019-01-08 13:00:25,598 - 10 - corpus.py - subactivity_sampler - 120 / 166
2019-01-08 13:00:25,599 - 10 - corpus.py - subactivity_sampler - [ 71422.  22010.  19322.  62795.  10965. 116774.   4361. 111497.   7481.
  42285.  48566.]
2019-01-08 13:01:48,948 - 10 - corpus.py - subactivity_sampler - 140 / 166
2019-01-08 13:01:48,948 - 10 - corpus.py - subactivity_sampler - [ 71767.  21852.  19306.  62021.  10865. 117182.   4626. 111592.   7374.
  42656.  48237.]
2019-01-08 13:03:10,552 - 10 - corpus.py - subactivity_sampler - 160 / 166
2019-01-08 13:03:10,552 - 10 - corpus.py - subactivity_sampler - [ 71872.  21628.  19273.  61960.  10857. 117403.   4623. 111589.   7280.
  42764.  48229.]
2019-01-08 13:03:29,097 - 10 - corpus.py - subactivity_sampler - [ 71877.  21612.  19268.  61973.  10850. 117407.   4622. 111596.   7280.
  42765.  48228.]
2019-01-08 13:03:29,097 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 654402.468 ms ~ 10.907 min ~ 654.402 sec
2019-01-08 13:03:29,097 - 10 - corpus.py - ordering_sampler - .
2019-01-08 13:03:34,684 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 13:03:34,685 - 10 - corpus.py - ordering_sampler - inv_count_vec: [11. 24.  0. 11. 40. 76.  4. 22. 14. 12.]
2019-01-08 13:03:34,685 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 13:03:34,714 - 10 - corpus.py - rho_sampling - ['55.3216', '13.2333', '928.4240', '66.1676', '8.6876', '111.1833', '175.5018', '3.8461', '35.9268', '3.4623']
2019-01-08 13:03:34,714 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 13:03:34,918 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 13:03:34,928 - 10 - accuracy_class.py - mof - # gt_labels: 12   # pr_labels: 11
2019-01-08 13:03:34,928 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 11', '1: 12', '2: 42', '3: 17', '4: 16', '5: 43', '6: 10', '7: 44', '8: 4', '9: 14', '10: 15']
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_val - frames true: 204579	frames overall : 517478
2019-01-08 13:03:34,952 - 10 - corpus.py - accuracy_corpus - Action: scrambledegg
2019-01-08 13:03:34,952 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3953385457932511
2019-01-08 13:03:34,952 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3953385457932511
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 29280
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1218
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_classes - label 10: 0.021674  237 / 10935
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_classes - label 11: 0.410162  23054 / 56207
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_classes - label 12: 0.069618  2801 / 40234
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_classes - label 14: 0.402987  7097 / 17611
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_classes - label 15: 0.705133  30650 / 43467
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 1003
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_classes - label 17: 0.297279  14671 / 49351
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_classes - label 42: 0.322357  10357 / 32129
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_classes - label 43: 0.612656  14077 / 22977
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_classes - label 44: 0.477012  101635 / 213066
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - mof_classes - average class mof: 0.276573
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 29280
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 8498
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - iou_classes - label 10: 0.015470  237 / 15320
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - iou_classes - label 11: 0.219499  23054 / 105030
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - iou_classes - label 12: 0.047438  2801 / 59045
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - iou_classes - label 14: 0.133204  7097 / 53279
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - iou_classes - label 15: 0.502089  30650 / 61045
2019-01-08 13:03:34,952 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 11853
2019-01-08 13:03:34,953 - 10 - accuracy_class.py - iou_classes - label 17: 0.151790  14671 / 96653
2019-01-08 13:03:34,953 - 10 - accuracy_class.py - iou_classes - label 42: 0.252364  10357 / 41040
2019-01-08 13:03:34,953 - 10 - accuracy_class.py - iou_classes - label 43: 0.111451  14077 / 126307
2019-01-08 13:03:34,953 - 10 - accuracy_class.py - iou_classes - label 44: 0.455707  101635 / 223027
2019-01-08 13:03:34,953 - 10 - accuracy_class.py - iou_classes - average IoU: 0.171728
2019-01-08 13:03:34,953 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.157418
2019-01-08 13:03:42,563 - 10 - f1_score.py - f1 - f1 score: 0.290917
2019-01-08 13:03:42,582 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 7867.907 ms ~ 0.131 min ~ 7.868 sec
2019-01-08 13:03:42,602 - 10 - utils.py - wrap - <function baseline at 0x7f3991d24e18> took 5629185.302 ms ~ 93.820 min ~ 5629.185 sec
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - batch_size: 256
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - bg: False
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - bg_trh: 85
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - data: /media/data/kukleva/lab/Breakfast/feat/s1
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - data_type: 2
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - dataset: bf
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - dataset_root: /media/data/kukleva/lab/Breakfast
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - embed_dim: 30
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - epochs: 12
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - ext: txt
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - feature_dim: 64
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - full: True
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - gmm: 1
2019-01-08 13:03:42,603 - 10 - utils.py - update_opt_str - gmms: one
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - gt: /media/data/kukleva/lab/Breakfast/feat/s1/groundTruth
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - gt_training: False
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - log: DEBUG
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - log_str: slim.mallow._friedegg_!bg_data2_bf_embed30_epochs12_full_gmm1_one_!gt_lr1e-10_!lr_ordering_reg0.1_!viterbi_!zeros_
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - lr: 1e-10
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - lr_adj: False
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - momentum: 0.9
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - num_workers: 4
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - ordering: True
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - prefix: slim.mallow.
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - reg_cov: 0.1
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - resume: False
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - resume_str: 
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - save_embed_feat: False
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - save_likelihood: False
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - save_model: False
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - seed: 0
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - subaction: friedegg
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - vis: False
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - viterbi: False
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - weight_decay: 0.0001
2019-01-08 13:03:42,604 - 10 - utils.py - update_opt_str - zeros: False
2019-01-08 13:03:42,623 - 10 - utils.py - wrap - <function GroundTruth.load_gt at 0x7f39322b36a8> took 18.135 ms ~ 0.000 min ~ 0.018 sec
2019-01-08 13:03:42,739 - 10 - corpus.py - __init__ - friedegg  subactions: 8
2019-01-08 13:03:42,740 - 10 - corpus.py - _init_videos - .
2019-01-08 13:04:02,103 - 10 - corpus.py - _init_videos - gt statistic: Counter({13: 318572, 11: 71061, 15: 40912, 12: 33508, 17: 33455, 14: 25722, 10: 14212, 16: 2291})
2019-01-08 13:04:02,103 - 10 - corpus.py - _update_fg_mask - .
2019-01-08 13:04:02,145 - 10 - corpus.py - __init__ - min: -45.965328  max: 37.383629  avg: 0.047181
2019-01-08 13:04:02,145 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 13:04:02,185 - 10 - corpus.py - rho_sampling - ['64.1837', '100.9221', '929.0535', '90.9104', '17.1809', '163.7961', '57.9843']
2019-01-08 13:04:02,185 - 10 - pipeline.py - baseline - Iteration 0
2019-01-08 13:04:02,375 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 13:04:02,385 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 13:04:02,385 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 17', '1: 11', '2: 16', '3: 12', '4: 10', '5: 13', '6: 14', '7: 15']
2019-01-08 13:04:02,404 - 10 - accuracy_class.py - mof_val - frames true: 149108	frames overall : 539733
2019-01-08 13:04:02,404 - 10 - corpus.py - accuracy_corpus - Action: friedegg
2019-01-08 13:04:02,405 - 10 - corpus.py - accuracy_corpus - MoF val: 0.2762625223953325
2019-01-08 13:04:02,405 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.0
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 17933
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - mof_classes - label 10: 0.006927  76 / 10971
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - mof_classes - label 11: 0.373135  25756 / 69026
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - mof_classes - label 12: 0.196043  6569 / 33508
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - mof_classes - label 13: 0.198949  63326 / 318302
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - mof_classes - label 14: 0.272367  7002 / 25708
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - mof_classes - label 15: 0.836272  26749 / 31986
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - mof_classes - label 16: 0.043617  96 / 2201
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - mof_classes - label 17: 0.649013  19534 / 30098
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - mof_classes - average class mof: 0.286258
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 17933
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - iou_classes - label 10: 0.000970  76 / 78351
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - iou_classes - label 11: 0.232465  25756 / 110795
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - iou_classes - label 12: 0.069573  6569 / 94419
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - iou_classes - label 13: 0.196415  63326 / 322409
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - iou_classes - label 14: 0.081310  7002 / 86115
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - iou_classes - label 15: 0.368342  26749 / 72620
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - iou_classes - label 16: 0.001379  96 / 69614
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - iou_classes - label 17: 0.250109  19534 / 78102
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - iou_classes - average IoU: 0.150070
2019-01-08 13:04:02,405 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.133396
2019-01-08 13:04:08,865 - 10 - f1_score.py - f1 - f1 score: 0.285893
2019-01-08 13:04:08,886 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 6700.802 ms ~ 0.112 min ~ 6.701 sec
2019-01-08 13:04:08,886 - 10 - corpus.py - embedding_training - .
2019-01-08 13:04:08,886 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 13:04:08,886 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 13:04:08,886 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 13:04:12,710 - 10 - training_embed.py - training - create model
2019-01-08 13:04:12,711 - 10 - training_embed.py - training - epochs: 12
2019-01-08 13:04:12,711 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 13:04:13,076 - 10 - training_embed.py - training - Epoch: [0][100/2109]	Time 0.001 (0.004)	Data 0.000 (0.002)	Loss 358.7228 (356.6279)	
2019-01-08 13:04:13,281 - 10 - training_embed.py - training - Epoch: [0][200/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 360.3121 (356.6291)	
2019-01-08 13:04:13,499 - 10 - training_embed.py - training - Epoch: [0][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 356.6658 (356.4577)	
2019-01-08 13:04:13,719 - 10 - training_embed.py - training - Epoch: [0][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.6233 (356.4524)	
2019-01-08 13:04:13,924 - 10 - training_embed.py - training - Epoch: [0][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.0194 (356.3870)	
2019-01-08 13:04:14,135 - 10 - training_embed.py - training - Epoch: [0][600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.6816 (356.3998)	
2019-01-08 13:04:14,343 - 10 - training_embed.py - training - Epoch: [0][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.1662 (356.3705)	
2019-01-08 13:04:14,562 - 10 - training_embed.py - training - Epoch: [0][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.3234 (356.3461)	
2019-01-08 13:04:14,766 - 10 - training_embed.py - training - Epoch: [0][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.0268 (356.3071)	
2019-01-08 13:04:14,975 - 10 - training_embed.py - training - Epoch: [0][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.9958 (356.2862)	
2019-01-08 13:04:15,181 - 10 - training_embed.py - training - Epoch: [0][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.4972 (356.2622)	
2019-01-08 13:04:15,391 - 10 - training_embed.py - training - Epoch: [0][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 354.4074 (356.2532)	
2019-01-08 13:04:15,601 - 10 - training_embed.py - training - Epoch: [0][1300/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 355.7986 (356.2701)	
2019-01-08 13:04:15,804 - 10 - training_embed.py - training - Epoch: [0][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.2712 (356.3062)	
2019-01-08 13:04:16,014 - 10 - training_embed.py - training - Epoch: [0][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 360.5869 (356.2945)	
2019-01-08 13:04:16,238 - 10 - training_embed.py - training - Epoch: [0][1600/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 357.6612 (356.2837)	
2019-01-08 13:04:16,464 - 10 - training_embed.py - training - Epoch: [0][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.7591 (356.2709)	
2019-01-08 13:04:16,684 - 10 - training_embed.py - training - Epoch: [0][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.3444 (356.2511)	
2019-01-08 13:04:16,898 - 10 - training_embed.py - training - Epoch: [0][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.4573 (356.2304)	
2019-01-08 13:04:17,126 - 10 - training_embed.py - training - Epoch: [0][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.2939 (356.2181)	
2019-01-08 13:04:17,352 - 10 - training_embed.py - training - Epoch: [0][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 354.3394 (356.2286)	
2019-01-08 13:04:17,366 - 10 - training_embed.py - training - loss: 356.187558
2019-01-08 13:04:17,366 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 13:04:17,774 - 10 - training_embed.py - training - Epoch: [1][100/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.8228 (356.1198)	
2019-01-08 13:04:17,995 - 10 - training_embed.py - training - Epoch: [1][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.7154 (355.9700)	
2019-01-08 13:04:18,204 - 10 - training_embed.py - training - Epoch: [1][300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.3582 (355.9963)	
2019-01-08 13:04:18,422 - 10 - training_embed.py - training - Epoch: [1][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.4609 (356.1520)	
2019-01-08 13:04:18,622 - 10 - training_embed.py - training - Epoch: [1][500/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 355.5275 (356.1381)	
2019-01-08 13:04:18,830 - 10 - training_embed.py - training - Epoch: [1][600/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 352.2292 (356.1336)	
2019-01-08 13:04:19,048 - 10 - training_embed.py - training - Epoch: [1][700/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.5814 (356.1455)	
2019-01-08 13:04:19,274 - 10 - training_embed.py - training - Epoch: [1][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0139 (356.1502)	
2019-01-08 13:04:19,504 - 10 - training_embed.py - training - Epoch: [1][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0874 (356.1654)	
2019-01-08 13:04:19,706 - 10 - training_embed.py - training - Epoch: [1][1000/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.5084 (356.1408)	
2019-01-08 13:04:19,924 - 10 - training_embed.py - training - Epoch: [1][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.4542 (356.1858)	
2019-01-08 13:04:20,142 - 10 - training_embed.py - training - Epoch: [1][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.1280 (356.1829)	
2019-01-08 13:04:20,342 - 10 - training_embed.py - training - Epoch: [1][1300/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.5317 (356.1463)	
2019-01-08 13:04:20,551 - 10 - training_embed.py - training - Epoch: [1][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.2354 (356.1518)	
2019-01-08 13:04:20,761 - 10 - training_embed.py - training - Epoch: [1][1500/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 357.4894 (356.1435)	
2019-01-08 13:04:20,960 - 10 - training_embed.py - training - Epoch: [1][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.7189 (356.1615)	
2019-01-08 13:04:21,167 - 10 - training_embed.py - training - Epoch: [1][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.1227 (356.1586)	
2019-01-08 13:04:21,383 - 10 - training_embed.py - training - Epoch: [1][1800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.3569 (356.1583)	
2019-01-08 13:04:21,585 - 10 - training_embed.py - training - Epoch: [1][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.1841 (356.1681)	
2019-01-08 13:04:21,796 - 10 - training_embed.py - training - Epoch: [1][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.4245 (356.1855)	
2019-01-08 13:04:22,005 - 10 - training_embed.py - training - Epoch: [1][2100/2109]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 357.3905 (356.1879)	
2019-01-08 13:04:22,019 - 10 - training_embed.py - training - loss: 356.147641
2019-01-08 13:04:22,019 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 13:04:22,439 - 10 - training_embed.py - training - Epoch: [2][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.3681 (355.8644)	
2019-01-08 13:04:22,646 - 10 - training_embed.py - training - Epoch: [2][200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 355.0728 (356.1239)	
2019-01-08 13:04:22,867 - 10 - training_embed.py - training - Epoch: [2][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.9289 (356.0567)	
2019-01-08 13:04:23,076 - 10 - training_embed.py - training - Epoch: [2][400/2109]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 355.6454 (356.0512)	
2019-01-08 13:04:23,286 - 10 - training_embed.py - training - Epoch: [2][500/2109]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 352.2342 (356.0374)	
2019-01-08 13:04:23,501 - 10 - training_embed.py - training - Epoch: [2][600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 358.2545 (356.0322)	
2019-01-08 13:04:23,706 - 10 - training_embed.py - training - Epoch: [2][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.1563 (356.0867)	
2019-01-08 13:04:23,916 - 10 - training_embed.py - training - Epoch: [2][800/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.8909 (356.1112)	
2019-01-08 13:04:24,129 - 10 - training_embed.py - training - Epoch: [2][900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 357.5482 (356.0571)	
2019-01-08 13:04:24,349 - 10 - training_embed.py - training - Epoch: [2][1000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.8897 (356.0152)	
2019-01-08 13:04:24,569 - 10 - training_embed.py - training - Epoch: [2][1100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.7684 (356.0352)	
2019-01-08 13:04:24,775 - 10 - training_embed.py - training - Epoch: [2][1200/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.9007 (356.0344)	
2019-01-08 13:04:24,982 - 10 - training_embed.py - training - Epoch: [2][1300/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 358.8731 (356.0599)	
2019-01-08 13:04:25,194 - 10 - training_embed.py - training - Epoch: [2][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.6739 (356.1038)	
2019-01-08 13:04:25,412 - 10 - training_embed.py - training - Epoch: [2][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.3937 (356.1055)	
2019-01-08 13:04:25,615 - 10 - training_embed.py - training - Epoch: [2][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.1562 (356.1139)	
2019-01-08 13:04:25,822 - 10 - training_embed.py - training - Epoch: [2][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.7721 (356.1364)	
2019-01-08 13:04:26,041 - 10 - training_embed.py - training - Epoch: [2][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.2079 (356.1723)	
2019-01-08 13:04:26,256 - 10 - training_embed.py - training - Epoch: [2][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.3882 (356.1682)	
2019-01-08 13:04:26,473 - 10 - training_embed.py - training - Epoch: [2][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.0000 (356.1701)	
2019-01-08 13:04:26,691 - 10 - training_embed.py - training - Epoch: [2][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 358.1468 (356.1491)	
2019-01-08 13:04:26,704 - 10 - training_embed.py - training - loss: 356.110080
2019-01-08 13:04:26,704 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 13:04:27,112 - 10 - training_embed.py - training - Epoch: [3][100/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.0419 (356.0336)	
2019-01-08 13:04:27,315 - 10 - training_embed.py - training - Epoch: [3][200/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 355.2491 (355.9193)	
2019-01-08 13:04:27,535 - 10 - training_embed.py - training - Epoch: [3][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.4995 (355.8129)	
2019-01-08 13:04:27,741 - 10 - training_embed.py - training - Epoch: [3][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.5395 (355.9044)	
2019-01-08 13:04:27,958 - 10 - training_embed.py - training - Epoch: [3][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.5639 (356.0136)	
2019-01-08 13:04:28,176 - 10 - training_embed.py - training - Epoch: [3][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.0252 (356.0541)	
2019-01-08 13:04:28,388 - 10 - training_embed.py - training - Epoch: [3][700/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 350.8789 (356.0475)	
2019-01-08 13:04:28,593 - 10 - training_embed.py - training - Epoch: [3][800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 357.3581 (356.0356)	
2019-01-08 13:04:28,823 - 10 - training_embed.py - training - Epoch: [3][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 360.9335 (356.0775)	
2019-01-08 13:04:29,048 - 10 - training_embed.py - training - Epoch: [3][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.7161 (356.0764)	
2019-01-08 13:04:29,262 - 10 - training_embed.py - training - Epoch: [3][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.0805 (356.0980)	
2019-01-08 13:04:29,495 - 10 - training_embed.py - training - Epoch: [3][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 358.4178 (356.0955)	
2019-01-08 13:04:29,722 - 10 - training_embed.py - training - Epoch: [3][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.2061 (356.0942)	
2019-01-08 13:04:29,937 - 10 - training_embed.py - training - Epoch: [3][1400/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.2743 (356.1281)	
2019-01-08 13:04:30,151 - 10 - training_embed.py - training - Epoch: [3][1500/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 357.5023 (356.1049)	
2019-01-08 13:04:30,362 - 10 - training_embed.py - training - Epoch: [3][1600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.4125 (356.0762)	
2019-01-08 13:04:30,580 - 10 - training_embed.py - training - Epoch: [3][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.4606 (356.0856)	
2019-01-08 13:04:30,788 - 10 - training_embed.py - training - Epoch: [3][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.6203 (356.0821)	
2019-01-08 13:04:31,013 - 10 - training_embed.py - training - Epoch: [3][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.9455 (356.0973)	
2019-01-08 13:04:31,239 - 10 - training_embed.py - training - Epoch: [3][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 361.5333 (356.1170)	
2019-01-08 13:04:31,472 - 10 - training_embed.py - training - Epoch: [3][2100/2109]	Time 0.007 (0.002)	Data 0.006 (0.001)	Loss 354.5759 (356.1142)	
2019-01-08 13:04:31,486 - 10 - training_embed.py - training - loss: 356.070788
2019-01-08 13:04:31,487 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 13:04:31,910 - 10 - training_embed.py - training - Epoch: [4][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.7619 (356.2700)	
2019-01-08 13:04:32,116 - 10 - training_embed.py - training - Epoch: [4][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.0174 (356.0699)	
2019-01-08 13:04:32,348 - 10 - training_embed.py - training - Epoch: [4][300/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 360.6978 (356.0390)	
2019-01-08 13:04:32,569 - 10 - training_embed.py - training - Epoch: [4][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.5869 (356.1029)	
2019-01-08 13:04:32,785 - 10 - training_embed.py - training - Epoch: [4][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.6518 (356.1235)	
2019-01-08 13:04:33,000 - 10 - training_embed.py - training - Epoch: [4][600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.7656 (356.1073)	
2019-01-08 13:04:33,201 - 10 - training_embed.py - training - Epoch: [4][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.5776 (356.1029)	
2019-01-08 13:04:33,419 - 10 - training_embed.py - training - Epoch: [4][800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 349.6622 (356.1030)	
2019-01-08 13:04:33,621 - 10 - training_embed.py - training - Epoch: [4][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.9916 (356.0848)	
2019-01-08 13:04:33,829 - 10 - training_embed.py - training - Epoch: [4][1000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 357.8149 (356.0706)	
2019-01-08 13:04:34,041 - 10 - training_embed.py - training - Epoch: [4][1100/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 358.4053 (356.0443)	
2019-01-08 13:04:34,253 - 10 - training_embed.py - training - Epoch: [4][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.6638 (356.0625)	
2019-01-08 13:04:34,473 - 10 - training_embed.py - training - Epoch: [4][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.4656 (356.0685)	
2019-01-08 13:04:34,701 - 10 - training_embed.py - training - Epoch: [4][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.1957 (356.0785)	
2019-01-08 13:04:34,909 - 10 - training_embed.py - training - Epoch: [4][1500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 354.6643 (356.0836)	
2019-01-08 13:04:35,115 - 10 - training_embed.py - training - Epoch: [4][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.3643 (356.0708)	
2019-01-08 13:04:35,338 - 10 - training_embed.py - training - Epoch: [4][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.3003 (356.0606)	
2019-01-08 13:04:35,546 - 10 - training_embed.py - training - Epoch: [4][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.8060 (356.0623)	
2019-01-08 13:04:35,756 - 10 - training_embed.py - training - Epoch: [4][1900/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 357.3878 (356.0600)	
2019-01-08 13:04:35,962 - 10 - training_embed.py - training - Epoch: [4][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.4335 (356.0663)	
2019-01-08 13:04:36,181 - 10 - training_embed.py - training - Epoch: [4][2100/2109]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 359.7818 (356.0658)	
2019-01-08 13:04:36,195 - 10 - training_embed.py - training - loss: 356.031555
2019-01-08 13:04:36,195 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 13:04:36,598 - 10 - training_embed.py - training - Epoch: [5][100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 357.9809 (356.0910)	
2019-01-08 13:04:36,806 - 10 - training_embed.py - training - Epoch: [5][200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.1438 (356.0614)	
2019-01-08 13:04:37,030 - 10 - training_embed.py - training - Epoch: [5][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.7263 (356.0398)	
2019-01-08 13:04:37,234 - 10 - training_embed.py - training - Epoch: [5][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 360.9454 (356.0243)	
2019-01-08 13:04:37,441 - 10 - training_embed.py - training - Epoch: [5][500/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 353.3379 (356.0107)	
2019-01-08 13:04:37,649 - 10 - training_embed.py - training - Epoch: [5][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.9030 (355.9799)	
2019-01-08 13:04:37,864 - 10 - training_embed.py - training - Epoch: [5][700/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 357.8091 (355.9625)	
2019-01-08 13:04:38,072 - 10 - training_embed.py - training - Epoch: [5][800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 357.3690 (355.9498)	
2019-01-08 13:04:38,288 - 10 - training_embed.py - training - Epoch: [5][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.0540 (355.9716)	
2019-01-08 13:04:38,497 - 10 - training_embed.py - training - Epoch: [5][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0710 (355.9924)	
2019-01-08 13:04:38,713 - 10 - training_embed.py - training - Epoch: [5][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.3916 (355.9588)	
2019-01-08 13:04:38,927 - 10 - training_embed.py - training - Epoch: [5][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 358.4985 (355.9737)	
2019-01-08 13:04:39,150 - 10 - training_embed.py - training - Epoch: [5][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.6898 (355.9869)	
2019-01-08 13:04:39,362 - 10 - training_embed.py - training - Epoch: [5][1400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 358.0806 (355.9735)	
2019-01-08 13:04:39,572 - 10 - training_embed.py - training - Epoch: [5][1500/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 356.3734 (355.9838)	
2019-01-08 13:04:39,780 - 10 - training_embed.py - training - Epoch: [5][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 360.1143 (355.9815)	
2019-01-08 13:04:40,003 - 10 - training_embed.py - training - Epoch: [5][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.9294 (355.9847)	
2019-01-08 13:04:40,208 - 10 - training_embed.py - training - Epoch: [5][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.7191 (356.0061)	
2019-01-08 13:04:40,431 - 10 - training_embed.py - training - Epoch: [5][1900/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 355.5782 (356.0168)	
2019-01-08 13:04:40,661 - 10 - training_embed.py - training - Epoch: [5][2000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.3848 (356.0178)	
2019-01-08 13:04:40,885 - 10 - training_embed.py - training - Epoch: [5][2100/2109]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 358.9996 (356.0284)	
2019-01-08 13:04:40,898 - 10 - training_embed.py - training - loss: 355.991936
2019-01-08 13:04:40,899 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 13:04:41,276 - 10 - training_embed.py - training - Epoch: [6][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.9406 (355.5742)	
2019-01-08 13:04:41,482 - 10 - training_embed.py - training - Epoch: [6][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.7856 (355.7591)	
2019-01-08 13:04:41,696 - 10 - training_embed.py - training - Epoch: [6][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.3348 (355.7735)	
2019-01-08 13:04:41,900 - 10 - training_embed.py - training - Epoch: [6][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.8971 (355.8571)	
2019-01-08 13:04:42,115 - 10 - training_embed.py - training - Epoch: [6][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.4469 (355.9271)	
2019-01-08 13:04:42,337 - 10 - training_embed.py - training - Epoch: [6][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.3946 (355.9388)	
2019-01-08 13:04:42,555 - 10 - training_embed.py - training - Epoch: [6][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.0136 (355.9490)	
2019-01-08 13:04:42,778 - 10 - training_embed.py - training - Epoch: [6][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0876 (355.9027)	
2019-01-08 13:04:42,985 - 10 - training_embed.py - training - Epoch: [6][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.8313 (355.9031)	
2019-01-08 13:04:43,204 - 10 - training_embed.py - training - Epoch: [6][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.8937 (355.8988)	
2019-01-08 13:04:43,410 - 10 - training_embed.py - training - Epoch: [6][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.3969 (355.9042)	
2019-01-08 13:04:43,624 - 10 - training_embed.py - training - Epoch: [6][1200/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 353.2430 (355.9224)	
2019-01-08 13:04:43,828 - 10 - training_embed.py - training - Epoch: [6][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.9104 (355.9443)	
2019-01-08 13:04:44,037 - 10 - training_embed.py - training - Epoch: [6][1400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 353.6074 (355.9499)	
2019-01-08 13:04:44,251 - 10 - training_embed.py - training - Epoch: [6][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.1547 (355.9786)	
2019-01-08 13:04:44,455 - 10 - training_embed.py - training - Epoch: [6][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.0832 (355.9857)	
2019-01-08 13:04:44,677 - 10 - training_embed.py - training - Epoch: [6][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.4825 (355.9928)	
2019-01-08 13:04:44,882 - 10 - training_embed.py - training - Epoch: [6][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.3802 (356.0018)	
2019-01-08 13:04:45,100 - 10 - training_embed.py - training - Epoch: [6][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.5132 (356.0011)	
2019-01-08 13:04:45,328 - 10 - training_embed.py - training - Epoch: [6][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.0531 (356.0071)	
2019-01-08 13:04:45,548 - 10 - training_embed.py - training - Epoch: [6][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 352.3125 (355.9889)	
2019-01-08 13:04:45,562 - 10 - training_embed.py - training - loss: 355.953945
2019-01-08 13:04:45,562 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 13:04:45,968 - 10 - training_embed.py - training - Epoch: [7][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.8054 (356.0136)	
2019-01-08 13:04:46,174 - 10 - training_embed.py - training - Epoch: [7][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0945 (356.0357)	
2019-01-08 13:04:46,389 - 10 - training_embed.py - training - Epoch: [7][300/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.8482 (355.9534)	
2019-01-08 13:04:46,606 - 10 - training_embed.py - training - Epoch: [7][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.7859 (356.1176)	
2019-01-08 13:04:46,820 - 10 - training_embed.py - training - Epoch: [7][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.2061 (356.1616)	
2019-01-08 13:04:47,031 - 10 - training_embed.py - training - Epoch: [7][600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 355.7829 (356.1418)	
2019-01-08 13:04:47,242 - 10 - training_embed.py - training - Epoch: [7][700/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 360.7657 (356.1189)	
2019-01-08 13:04:47,456 - 10 - training_embed.py - training - Epoch: [7][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.9206 (356.0781)	
2019-01-08 13:04:47,671 - 10 - training_embed.py - training - Epoch: [7][900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.1753 (356.0516)	
2019-01-08 13:04:47,879 - 10 - training_embed.py - training - Epoch: [7][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0652 (356.0366)	
2019-01-08 13:04:48,096 - 10 - training_embed.py - training - Epoch: [7][1100/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 357.8284 (356.0279)	
2019-01-08 13:04:48,317 - 10 - training_embed.py - training - Epoch: [7][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.3445 (356.0183)	
2019-01-08 13:04:48,541 - 10 - training_embed.py - training - Epoch: [7][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.6704 (355.9930)	
2019-01-08 13:04:48,769 - 10 - training_embed.py - training - Epoch: [7][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.2587 (355.9821)	
2019-01-08 13:04:48,974 - 10 - training_embed.py - training - Epoch: [7][1500/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 348.2840 (355.9895)	
2019-01-08 13:04:49,200 - 10 - training_embed.py - training - Epoch: [7][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 360.0241 (355.9870)	
2019-01-08 13:04:49,421 - 10 - training_embed.py - training - Epoch: [7][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.6468 (355.9789)	
2019-01-08 13:04:49,631 - 10 - training_embed.py - training - Epoch: [7][1800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 352.8596 (355.9885)	
2019-01-08 13:04:49,838 - 10 - training_embed.py - training - Epoch: [7][1900/2109]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 352.6974 (355.9745)	
2019-01-08 13:04:50,048 - 10 - training_embed.py - training - Epoch: [7][2000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 358.4193 (355.9599)	
2019-01-08 13:04:50,265 - 10 - training_embed.py - training - Epoch: [7][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 354.2509 (355.9537)	
2019-01-08 13:04:50,278 - 10 - training_embed.py - training - loss: 355.914005
2019-01-08 13:04:50,279 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 13:04:50,653 - 10 - training_embed.py - training - Epoch: [8][100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.9117 (356.0201)	
2019-01-08 13:04:50,875 - 10 - training_embed.py - training - Epoch: [8][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.0861 (355.9790)	
2019-01-08 13:04:51,088 - 10 - training_embed.py - training - Epoch: [8][300/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.8705 (356.1456)	
2019-01-08 13:04:51,308 - 10 - training_embed.py - training - Epoch: [8][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.7992 (356.0592)	
2019-01-08 13:04:51,532 - 10 - training_embed.py - training - Epoch: [8][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0895 (355.9629)	
2019-01-08 13:04:51,752 - 10 - training_embed.py - training - Epoch: [8][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.8063 (355.9691)	
2019-01-08 13:04:51,961 - 10 - training_embed.py - training - Epoch: [8][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.0315 (355.9808)	
2019-01-08 13:04:52,175 - 10 - training_embed.py - training - Epoch: [8][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.0022 (355.9922)	
2019-01-08 13:04:52,401 - 10 - training_embed.py - training - Epoch: [8][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.5463 (355.9632)	
2019-01-08 13:04:52,613 - 10 - training_embed.py - training - Epoch: [8][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.0306 (355.9156)	
2019-01-08 13:04:52,819 - 10 - training_embed.py - training - Epoch: [8][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.6873 (355.9436)	
2019-01-08 13:04:53,035 - 10 - training_embed.py - training - Epoch: [8][1200/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 358.9916 (355.9033)	
2019-01-08 13:04:53,256 - 10 - training_embed.py - training - Epoch: [8][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.8970 (355.9093)	
2019-01-08 13:04:53,459 - 10 - training_embed.py - training - Epoch: [8][1400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 354.9037 (355.8834)	
2019-01-08 13:04:53,683 - 10 - training_embed.py - training - Epoch: [8][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.4719 (355.9044)	
2019-01-08 13:04:53,900 - 10 - training_embed.py - training - Epoch: [8][1600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 355.3519 (355.8956)	
2019-01-08 13:04:54,122 - 10 - training_embed.py - training - Epoch: [8][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.7404 (355.8958)	
2019-01-08 13:04:54,341 - 10 - training_embed.py - training - Epoch: [8][1800/2109]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 351.1342 (355.9172)	
2019-01-08 13:04:54,562 - 10 - training_embed.py - training - Epoch: [8][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.4551 (355.9002)	
2019-01-08 13:04:54,775 - 10 - training_embed.py - training - Epoch: [8][2000/2109]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 356.5578 (355.9033)	
2019-01-08 13:04:54,997 - 10 - training_embed.py - training - Epoch: [8][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 356.8655 (355.9089)	
2019-01-08 13:04:55,010 - 10 - training_embed.py - training - loss: 355.874969
2019-01-08 13:04:55,011 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 13:04:55,444 - 10 - training_embed.py - training - Epoch: [9][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.7853 (355.6233)	
2019-01-08 13:04:55,658 - 10 - training_embed.py - training - Epoch: [9][200/2109]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 357.5258 (355.8646)	
2019-01-08 13:04:55,885 - 10 - training_embed.py - training - Epoch: [9][300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 365.9102 (355.9913)	
2019-01-08 13:04:56,106 - 10 - training_embed.py - training - Epoch: [9][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.3325 (355.9441)	
2019-01-08 13:04:56,309 - 10 - training_embed.py - training - Epoch: [9][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.2633 (355.9382)	
2019-01-08 13:04:56,523 - 10 - training_embed.py - training - Epoch: [9][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.5292 (355.9023)	
2019-01-08 13:04:56,755 - 10 - training_embed.py - training - Epoch: [9][700/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.6755 (355.8808)	
2019-01-08 13:04:56,967 - 10 - training_embed.py - training - Epoch: [9][800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.1506 (355.8502)	
2019-01-08 13:04:57,174 - 10 - training_embed.py - training - Epoch: [9][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.3529 (355.8579)	
2019-01-08 13:04:57,381 - 10 - training_embed.py - training - Epoch: [9][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.7196 (355.8606)	
2019-01-08 13:04:57,595 - 10 - training_embed.py - training - Epoch: [9][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.2531 (355.8970)	
2019-01-08 13:04:57,804 - 10 - training_embed.py - training - Epoch: [9][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.2841 (355.8811)	
2019-01-08 13:04:58,009 - 10 - training_embed.py - training - Epoch: [9][1300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 355.0031 (355.8740)	
2019-01-08 13:04:58,231 - 10 - training_embed.py - training - Epoch: [9][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.0204 (355.8678)	
2019-01-08 13:04:58,443 - 10 - training_embed.py - training - Epoch: [9][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.3679 (355.8617)	
2019-01-08 13:04:58,656 - 10 - training_embed.py - training - Epoch: [9][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.3195 (355.8771)	
2019-01-08 13:04:58,864 - 10 - training_embed.py - training - Epoch: [9][1700/2109]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 355.8901 (355.8824)	
2019-01-08 13:04:59,079 - 10 - training_embed.py - training - Epoch: [9][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.3341 (355.8846)	
2019-01-08 13:04:59,299 - 10 - training_embed.py - training - Epoch: [9][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.7011 (355.8792)	
2019-01-08 13:04:59,509 - 10 - training_embed.py - training - Epoch: [9][2000/2109]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 354.8778 (355.8742)	
2019-01-08 13:04:59,735 - 10 - training_embed.py - training - Epoch: [9][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 356.3626 (355.8717)	
2019-01-08 13:04:59,749 - 10 - training_embed.py - training - loss: 355.836156
2019-01-08 13:04:59,749 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 13:05:00,216 - 10 - training_embed.py - training - Epoch: [10][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.4051 (355.6464)	
2019-01-08 13:05:00,428 - 10 - training_embed.py - training - Epoch: [10][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.8649 (355.8496)	
2019-01-08 13:05:00,646 - 10 - training_embed.py - training - Epoch: [10][300/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 351.1392 (355.9235)	
2019-01-08 13:05:00,860 - 10 - training_embed.py - training - Epoch: [10][400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.5850 (355.9138)	
2019-01-08 13:05:01,089 - 10 - training_embed.py - training - Epoch: [10][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.6115 (355.8347)	
2019-01-08 13:05:01,297 - 10 - training_embed.py - training - Epoch: [10][600/2109]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 357.3586 (355.8605)	
2019-01-08 13:05:01,516 - 10 - training_embed.py - training - Epoch: [10][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.9679 (355.8445)	
2019-01-08 13:05:01,733 - 10 - training_embed.py - training - Epoch: [10][800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 352.9577 (355.7818)	
2019-01-08 13:05:01,946 - 10 - training_embed.py - training - Epoch: [10][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.3200 (355.8106)	
2019-01-08 13:05:02,155 - 10 - training_embed.py - training - Epoch: [10][1000/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 357.1976 (355.8103)	
2019-01-08 13:05:02,376 - 10 - training_embed.py - training - Epoch: [10][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.7927 (355.7771)	
2019-01-08 13:05:02,587 - 10 - training_embed.py - training - Epoch: [10][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.8972 (355.7897)	
2019-01-08 13:05:02,800 - 10 - training_embed.py - training - Epoch: [10][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.6097 (355.7935)	
2019-01-08 13:05:03,014 - 10 - training_embed.py - training - Epoch: [10][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.2886 (355.8300)	
2019-01-08 13:05:03,226 - 10 - training_embed.py - training - Epoch: [10][1500/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 353.4301 (355.8280)	
2019-01-08 13:05:03,443 - 10 - training_embed.py - training - Epoch: [10][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.5329 (355.8528)	
2019-01-08 13:05:03,652 - 10 - training_embed.py - training - Epoch: [10][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.2099 (355.8622)	
2019-01-08 13:05:03,868 - 10 - training_embed.py - training - Epoch: [10][1800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.5312 (355.8461)	
2019-01-08 13:05:04,080 - 10 - training_embed.py - training - Epoch: [10][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.6390 (355.8543)	
2019-01-08 13:05:04,293 - 10 - training_embed.py - training - Epoch: [10][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.9695 (355.8499)	
2019-01-08 13:05:04,520 - 10 - training_embed.py - training - Epoch: [10][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 355.8262 (355.8379)	
2019-01-08 13:05:04,534 - 10 - training_embed.py - training - loss: 355.797111
2019-01-08 13:05:04,534 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 13:05:04,922 - 10 - training_embed.py - training - Epoch: [11][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.4901 (355.9258)	
2019-01-08 13:05:05,130 - 10 - training_embed.py - training - Epoch: [11][200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.6734 (356.0624)	
2019-01-08 13:05:05,346 - 10 - training_embed.py - training - Epoch: [11][300/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 356.6100 (356.0266)	
2019-01-08 13:05:05,561 - 10 - training_embed.py - training - Epoch: [11][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.4666 (356.0372)	
2019-01-08 13:05:05,763 - 10 - training_embed.py - training - Epoch: [11][500/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.2640 (356.0490)	
2019-01-08 13:05:05,975 - 10 - training_embed.py - training - Epoch: [11][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.6773 (356.0047)	
2019-01-08 13:05:06,185 - 10 - training_embed.py - training - Epoch: [11][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.4993 (355.9602)	
2019-01-08 13:05:06,410 - 10 - training_embed.py - training - Epoch: [11][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.3210 (355.9045)	
2019-01-08 13:05:06,618 - 10 - training_embed.py - training - Epoch: [11][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.8863 (355.9031)	
2019-01-08 13:05:06,823 - 10 - training_embed.py - training - Epoch: [11][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.8451 (355.8484)	
2019-01-08 13:05:07,043 - 10 - training_embed.py - training - Epoch: [11][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.8998 (355.8135)	
2019-01-08 13:05:07,252 - 10 - training_embed.py - training - Epoch: [11][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 355.9485 (355.8187)	
2019-01-08 13:05:07,466 - 10 - training_embed.py - training - Epoch: [11][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.0975 (355.8089)	
2019-01-08 13:05:07,679 - 10 - training_embed.py - training - Epoch: [11][1400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 351.9101 (355.7986)	
2019-01-08 13:05:07,885 - 10 - training_embed.py - training - Epoch: [11][1500/2109]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 360.9739 (355.7950)	
2019-01-08 13:05:08,102 - 10 - training_embed.py - training - Epoch: [11][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.4488 (355.7882)	
2019-01-08 13:05:08,320 - 10 - training_embed.py - training - Epoch: [11][1700/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.1248 (355.7918)	
2019-01-08 13:05:08,539 - 10 - training_embed.py - training - Epoch: [11][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.8438 (355.7862)	
2019-01-08 13:05:08,770 - 10 - training_embed.py - training - Epoch: [11][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.5808 (355.8060)	
2019-01-08 13:05:08,979 - 10 - training_embed.py - training - Epoch: [11][2000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.2452 (355.7994)	
2019-01-08 13:05:09,209 - 10 - training_embed.py - training - Epoch: [11][2100/2109]	Time 0.007 (0.002)	Data 0.006 (0.001)	Loss 355.1084 (355.7979)	
2019-01-08 13:05:09,222 - 10 - training_embed.py - training - loss: 355.756932
2019-01-08 13:05:09,312 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 13:05:10,112 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 799.457 ms ~ 0.013 min ~ 0.799 sec
2019-01-08 13:05:11,001 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1688.230 ms ~ 0.028 min ~ 1.688 sec
2019-01-08 13:05:11,001 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 13:05:11,001 - 10 - corpus.py - subactivity_sampler - 0 / 173
2019-01-08 13:05:11,002 - 10 - corpus.py - subactivity_sampler - [67538. 67525. 67509. 67480. 67456. 67433. 67409. 67383.]
2019-01-08 13:06:34,181 - 10 - corpus.py - subactivity_sampler - 20 / 173
2019-01-08 13:06:34,182 - 10 - corpus.py - subactivity_sampler - [68288. 66644. 67944. 66339. 69054. 66497. 68266. 66701.]
2019-01-08 13:07:29,144 - 10 - corpus.py - subactivity_sampler - 40 / 173
2019-01-08 13:07:29,145 - 10 - corpus.py - subactivity_sampler - [69423. 64894. 68717. 65352. 70350. 66098. 68976. 65923.]
2019-01-08 13:08:39,636 - 10 - corpus.py - subactivity_sampler - 60 / 173
2019-01-08 13:08:39,636 - 10 - corpus.py - subactivity_sampler - [70033. 63637. 69375. 64432. 71735. 65016. 70219. 65286.]
2019-01-08 13:09:51,248 - 10 - corpus.py - subactivity_sampler - 80 / 173
2019-01-08 13:09:51,249 - 10 - corpus.py - subactivity_sampler - [70546. 62506. 69803. 63393. 73923. 63074. 71945. 64543.]
2019-01-08 13:11:07,583 - 10 - corpus.py - subactivity_sampler - 100 / 173
2019-01-08 13:11:07,583 - 10 - corpus.py - subactivity_sampler - [71742. 60886. 70794. 61110. 77372. 60074. 73648. 64107.]
2019-01-08 13:11:51,229 - 10 - corpus.py - subactivity_sampler - 120 / 173
2019-01-08 13:11:51,230 - 10 - corpus.py - subactivity_sampler - [72529. 59863. 71929. 58667. 80286. 57319. 75626. 63514.]
2019-01-08 13:12:42,513 - 10 - corpus.py - subactivity_sampler - 140 / 173
2019-01-08 13:12:42,513 - 10 - corpus.py - subactivity_sampler - [74549. 57266. 73328. 55151. 84972. 53533. 77966. 62968.]
2019-01-08 13:13:47,304 - 10 - corpus.py - subactivity_sampler - 160 / 173
2019-01-08 13:13:47,304 - 10 - corpus.py - subactivity_sampler - [76092. 54039. 76040. 50999. 91571. 48517. 80921. 61554.]
2019-01-08 13:14:23,175 - 10 - corpus.py - subactivity_sampler - [77306. 51567. 78169. 48237. 95321. 45270. 83233. 60630.]
2019-01-08 13:14:23,175 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 552173.964 ms ~ 9.203 min ~ 552.174 sec
2019-01-08 13:14:23,175 - 10 - corpus.py - ordering_sampler - .
2019-01-08 13:14:26,164 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 13:14:26,165 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 3.  6.  0.  0. 14.  0.  2.]
2019-01-08 13:14:26,165 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 13:14:26,183 - 10 - corpus.py - rho_sampling - ['59.5468', '40.1907', '928.1069', '7.5322', '1.4172', '22.0165', '6.7191']
2019-01-08 13:14:26,183 - 10 - pipeline.py - baseline - Iteration 1
2019-01-08 13:14:26,351 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 13:14:26,361 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 13:14:26,361 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 17', '1: 11', '2: 12', '3: 10', '4: 13', '5: 16', '6: 14', '7: 15']
2019-01-08 13:14:26,384 - 10 - accuracy_class.py - mof_val - frames true: 169730	frames overall : 539733
2019-01-08 13:14:26,385 - 10 - corpus.py - accuracy_corpus - Action: friedegg
2019-01-08 13:14:26,385 - 10 - corpus.py - accuracy_corpus - MoF val: 0.31447030290903094
2019-01-08 13:14:26,385 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.2280294145438578
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 17933
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - mof_classes - label 10: 0.000000  0 / 10971
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - mof_classes - label 11: 0.289210  19963 / 69026
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - mof_classes - label 12: 0.267608  8967 / 33508
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - mof_classes - label 13: 0.265697  84572 / 318302
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - mof_classes - label 14: 0.306792  7887 / 25708
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - mof_classes - label 15: 0.801851  25648 / 31986
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 2201
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - mof_classes - label 17: 0.753970  22693 / 30098
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - mof_classes - average class mof: 0.298348
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 17933
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - iou_classes - label 10: 0.000000  0 / 59208
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - iou_classes - label 11: 0.198380  19963 / 100630
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - iou_classes - label 12: 0.087304  8967 / 102710
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - iou_classes - label 13: 0.257018  84572 / 329051
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - iou_classes - label 14: 0.078047  7887 / 101054
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - iou_classes - label 15: 0.382989  25648 / 66968
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 47471
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - iou_classes - label 17: 0.267887  22693 / 84711
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - iou_classes - average IoU: 0.158953
2019-01-08 13:14:26,385 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.141292
2019-01-08 13:14:32,624 - 10 - f1_score.py - f1 - f1 score: 0.296557
2019-01-08 13:14:32,642 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 6458.968 ms ~ 0.108 min ~ 6.459 sec
2019-01-08 13:14:32,642 - 10 - corpus.py - embedding_training - .
2019-01-08 13:14:32,642 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 13:14:32,642 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 13:14:32,643 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 13:14:36,572 - 10 - training_embed.py - training - create model
2019-01-08 13:14:36,573 - 10 - training_embed.py - training - epochs: 12
2019-01-08 13:14:36,573 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 13:14:36,949 - 10 - training_embed.py - training - Epoch: [0][100/2109]	Time 0.003 (0.004)	Data 0.002 (0.003)	Loss 358.0314 (355.9886)	
2019-01-08 13:14:37,168 - 10 - training_embed.py - training - Epoch: [0][200/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 359.2589 (355.7642)	
2019-01-08 13:14:37,387 - 10 - training_embed.py - training - Epoch: [0][300/2109]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 355.9729 (355.6059)	
2019-01-08 13:14:37,624 - 10 - training_embed.py - training - Epoch: [0][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.7823 (355.5398)	
2019-01-08 13:14:37,838 - 10 - training_embed.py - training - Epoch: [0][500/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 349.4691 (355.5383)	
2019-01-08 13:14:38,062 - 10 - training_embed.py - training - Epoch: [0][600/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 351.7150 (355.5614)	
2019-01-08 13:14:38,271 - 10 - training_embed.py - training - Epoch: [0][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.6387 (355.5677)	
2019-01-08 13:14:38,484 - 10 - training_embed.py - training - Epoch: [0][800/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 355.5396 (355.5541)	
2019-01-08 13:14:38,702 - 10 - training_embed.py - training - Epoch: [0][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 361.6561 (355.5416)	
2019-01-08 13:14:38,926 - 10 - training_embed.py - training - Epoch: [0][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.5162 (355.5433)	
2019-01-08 13:14:39,160 - 10 - training_embed.py - training - Epoch: [0][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.8929 (355.5367)	
2019-01-08 13:14:39,399 - 10 - training_embed.py - training - Epoch: [0][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.0416 (355.5275)	
2019-01-08 13:14:39,603 - 10 - training_embed.py - training - Epoch: [0][1300/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 353.8105 (355.5287)	
2019-01-08 13:14:39,827 - 10 - training_embed.py - training - Epoch: [0][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.1359 (355.5472)	
2019-01-08 13:14:40,050 - 10 - training_embed.py - training - Epoch: [0][1500/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 360.0960 (355.5135)	
2019-01-08 13:14:40,274 - 10 - training_embed.py - training - Epoch: [0][1600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.8821 (355.5052)	
2019-01-08 13:14:40,490 - 10 - training_embed.py - training - Epoch: [0][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.7339 (355.4995)	
2019-01-08 13:14:40,722 - 10 - training_embed.py - training - Epoch: [0][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.7831 (355.4810)	
2019-01-08 13:14:40,944 - 10 - training_embed.py - training - Epoch: [0][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.5984 (355.4543)	
2019-01-08 13:14:41,167 - 10 - training_embed.py - training - Epoch: [0][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.2833 (355.4451)	
2019-01-08 13:14:41,377 - 10 - training_embed.py - training - Epoch: [0][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 354.8365 (355.4603)	
2019-01-08 13:14:41,392 - 10 - training_embed.py - training - loss: 355.420415
2019-01-08 13:14:41,392 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 13:14:41,832 - 10 - training_embed.py - training - Epoch: [1][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.1747 (355.1934)	
2019-01-08 13:14:42,041 - 10 - training_embed.py - training - Epoch: [1][200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.2636 (355.0466)	
2019-01-08 13:14:42,257 - 10 - training_embed.py - training - Epoch: [1][300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 354.0338 (355.1138)	
2019-01-08 13:14:42,487 - 10 - training_embed.py - training - Epoch: [1][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.9691 (355.2261)	
2019-01-08 13:14:42,707 - 10 - training_embed.py - training - Epoch: [1][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.7973 (355.2147)	
2019-01-08 13:14:42,930 - 10 - training_embed.py - training - Epoch: [1][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.6595 (355.2506)	
2019-01-08 13:14:43,145 - 10 - training_embed.py - training - Epoch: [1][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.3848 (355.2783)	
2019-01-08 13:14:43,360 - 10 - training_embed.py - training - Epoch: [1][800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 354.4539 (355.3353)	
2019-01-08 13:14:43,570 - 10 - training_embed.py - training - Epoch: [1][900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.6587 (355.3459)	
2019-01-08 13:14:43,799 - 10 - training_embed.py - training - Epoch: [1][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.6629 (355.3069)	
2019-01-08 13:14:44,018 - 10 - training_embed.py - training - Epoch: [1][1100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 352.7493 (355.3535)	
2019-01-08 13:14:44,233 - 10 - training_embed.py - training - Epoch: [1][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.1888 (355.3420)	
2019-01-08 13:14:44,439 - 10 - training_embed.py - training - Epoch: [1][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.3976 (355.3293)	
2019-01-08 13:14:44,670 - 10 - training_embed.py - training - Epoch: [1][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.2730 (355.3404)	
2019-01-08 13:14:44,891 - 10 - training_embed.py - training - Epoch: [1][1500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.1392 (355.3231)	
2019-01-08 13:14:45,124 - 10 - training_embed.py - training - Epoch: [1][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.0139 (355.3406)	
2019-01-08 13:14:45,359 - 10 - training_embed.py - training - Epoch: [1][1700/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.3237 (355.3313)	
2019-01-08 13:14:45,580 - 10 - training_embed.py - training - Epoch: [1][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.0640 (355.3314)	
2019-01-08 13:14:45,790 - 10 - training_embed.py - training - Epoch: [1][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.1227 (355.3366)	
2019-01-08 13:14:46,021 - 10 - training_embed.py - training - Epoch: [1][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.5032 (355.3581)	
2019-01-08 13:14:46,241 - 10 - training_embed.py - training - Epoch: [1][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 356.8233 (355.3576)	
2019-01-08 13:14:46,255 - 10 - training_embed.py - training - loss: 355.319160
2019-01-08 13:14:46,256 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 13:14:46,688 - 10 - training_embed.py - training - Epoch: [2][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.6554 (355.0421)	
2019-01-08 13:14:46,897 - 10 - training_embed.py - training - Epoch: [2][200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.4082 (355.2147)	
2019-01-08 13:14:47,118 - 10 - training_embed.py - training - Epoch: [2][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0082 (355.1458)	
2019-01-08 13:14:47,335 - 10 - training_embed.py - training - Epoch: [2][400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.7634 (355.1346)	
2019-01-08 13:14:47,549 - 10 - training_embed.py - training - Epoch: [2][500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 351.5463 (355.1471)	
2019-01-08 13:14:47,779 - 10 - training_embed.py - training - Epoch: [2][600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 357.3123 (355.1470)	
2019-01-08 13:14:48,003 - 10 - training_embed.py - training - Epoch: [2][700/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 357.8661 (355.2026)	
2019-01-08 13:14:48,214 - 10 - training_embed.py - training - Epoch: [2][800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.7656 (355.1941)	
2019-01-08 13:14:48,429 - 10 - training_embed.py - training - Epoch: [2][900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 357.9401 (355.1538)	
2019-01-08 13:14:48,640 - 10 - training_embed.py - training - Epoch: [2][1000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.7064 (355.1060)	
2019-01-08 13:14:48,870 - 10 - training_embed.py - training - Epoch: [2][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.2754 (355.1417)	
2019-01-08 13:14:49,085 - 10 - training_embed.py - training - Epoch: [2][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 360.6085 (355.1746)	
2019-01-08 13:14:49,294 - 10 - training_embed.py - training - Epoch: [2][1300/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 356.3567 (355.1882)	
2019-01-08 13:14:49,508 - 10 - training_embed.py - training - Epoch: [2][1400/2109]	Time 0.004 (0.002)	Data 0.000 (0.001)	Loss 355.0663 (355.2197)	
2019-01-08 13:14:49,723 - 10 - training_embed.py - training - Epoch: [2][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.3865 (355.2039)	
2019-01-08 13:14:49,938 - 10 - training_embed.py - training - Epoch: [2][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.2024 (355.2106)	
2019-01-08 13:14:50,153 - 10 - training_embed.py - training - Epoch: [2][1700/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 357.0857 (355.2502)	
2019-01-08 13:14:50,376 - 10 - training_embed.py - training - Epoch: [2][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.7187 (355.2778)	
2019-01-08 13:14:50,593 - 10 - training_embed.py - training - Epoch: [2][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.4044 (355.2852)	
2019-01-08 13:14:50,815 - 10 - training_embed.py - training - Epoch: [2][2000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 354.6593 (355.2833)	
2019-01-08 13:14:51,044 - 10 - training_embed.py - training - Epoch: [2][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 356.4623 (355.2592)	
2019-01-08 13:14:51,058 - 10 - training_embed.py - training - loss: 355.219887
2019-01-08 13:14:51,058 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 13:14:51,497 - 10 - training_embed.py - training - Epoch: [3][100/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.9160 (355.1230)	
2019-01-08 13:14:51,714 - 10 - training_embed.py - training - Epoch: [3][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.6645 (355.0480)	
2019-01-08 13:14:51,935 - 10 - training_embed.py - training - Epoch: [3][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.5132 (354.9992)	
2019-01-08 13:14:52,149 - 10 - training_embed.py - training - Epoch: [3][400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 353.1081 (355.0465)	
2019-01-08 13:14:52,379 - 10 - training_embed.py - training - Epoch: [3][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.5686 (355.0929)	
2019-01-08 13:14:52,590 - 10 - training_embed.py - training - Epoch: [3][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.0910 (355.1313)	
2019-01-08 13:14:52,797 - 10 - training_embed.py - training - Epoch: [3][700/2109]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 350.5209 (355.1411)	
2019-01-08 13:14:53,013 - 10 - training_embed.py - training - Epoch: [3][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.8748 (355.1440)	
2019-01-08 13:14:53,229 - 10 - training_embed.py - training - Epoch: [3][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.9209 (355.1425)	
2019-01-08 13:14:53,445 - 10 - training_embed.py - training - Epoch: [3][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.3964 (355.1389)	
2019-01-08 13:14:53,660 - 10 - training_embed.py - training - Epoch: [3][1100/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 356.0041 (355.1492)	
2019-01-08 13:14:53,866 - 10 - training_embed.py - training - Epoch: [3][1200/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 359.2249 (355.1598)	
2019-01-08 13:14:54,089 - 10 - training_embed.py - training - Epoch: [3][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.2508 (355.1730)	
2019-01-08 13:14:54,309 - 10 - training_embed.py - training - Epoch: [3][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.4645 (355.2144)	
2019-01-08 13:14:54,537 - 10 - training_embed.py - training - Epoch: [3][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.0393 (355.1889)	
2019-01-08 13:14:54,746 - 10 - training_embed.py - training - Epoch: [3][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.9058 (355.1634)	
2019-01-08 13:14:54,963 - 10 - training_embed.py - training - Epoch: [3][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.5313 (355.1643)	
2019-01-08 13:14:55,181 - 10 - training_embed.py - training - Epoch: [3][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.5125 (355.1466)	
2019-01-08 13:14:55,409 - 10 - training_embed.py - training - Epoch: [3][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.0352 (355.1607)	
2019-01-08 13:14:55,629 - 10 - training_embed.py - training - Epoch: [3][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 360.2005 (355.1635)	
2019-01-08 13:14:55,847 - 10 - training_embed.py - training - Epoch: [3][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 354.7562 (355.1583)	
2019-01-08 13:14:55,861 - 10 - training_embed.py - training - loss: 355.118804
2019-01-08 13:14:55,862 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 13:14:56,309 - 10 - training_embed.py - training - Epoch: [4][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.8372 (355.1678)	
2019-01-08 13:14:56,526 - 10 - training_embed.py - training - Epoch: [4][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.5216 (355.0326)	
2019-01-08 13:14:56,755 - 10 - training_embed.py - training - Epoch: [4][300/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.6129 (355.0194)	
2019-01-08 13:14:56,969 - 10 - training_embed.py - training - Epoch: [4][400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.4698 (355.0668)	
2019-01-08 13:14:57,196 - 10 - training_embed.py - training - Epoch: [4][500/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.0054 (355.1024)	
2019-01-08 13:14:57,417 - 10 - training_embed.py - training - Epoch: [4][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.7252 (355.0957)	
2019-01-08 13:14:57,651 - 10 - training_embed.py - training - Epoch: [4][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.1274 (355.1088)	
2019-01-08 13:14:57,858 - 10 - training_embed.py - training - Epoch: [4][800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 352.9349 (355.1304)	
2019-01-08 13:14:58,083 - 10 - training_embed.py - training - Epoch: [4][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.8279 (355.1070)	
2019-01-08 13:14:58,305 - 10 - training_embed.py - training - Epoch: [4][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.2286 (355.0921)	
2019-01-08 13:14:58,525 - 10 - training_embed.py - training - Epoch: [4][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.8125 (355.0471)	
2019-01-08 13:14:58,731 - 10 - training_embed.py - training - Epoch: [4][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.1941 (355.0654)	
2019-01-08 13:14:58,942 - 10 - training_embed.py - training - Epoch: [4][1300/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 356.1425 (355.0575)	
2019-01-08 13:14:59,169 - 10 - training_embed.py - training - Epoch: [4][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.8115 (355.0589)	
2019-01-08 13:14:59,400 - 10 - training_embed.py - training - Epoch: [4][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.6263 (355.0504)	
2019-01-08 13:14:59,615 - 10 - training_embed.py - training - Epoch: [4][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.8869 (355.0370)	
2019-01-08 13:14:59,833 - 10 - training_embed.py - training - Epoch: [4][1700/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.2999 (355.0211)	
2019-01-08 13:15:00,068 - 10 - training_embed.py - training - Epoch: [4][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.9371 (355.0291)	
2019-01-08 13:15:00,304 - 10 - training_embed.py - training - Epoch: [4][1900/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 356.1546 (355.0398)	
2019-01-08 13:15:00,519 - 10 - training_embed.py - training - Epoch: [4][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.9749 (355.0501)	
2019-01-08 13:15:00,755 - 10 - training_embed.py - training - Epoch: [4][2100/2109]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 358.1627 (355.0522)	
2019-01-08 13:15:00,769 - 10 - training_embed.py - training - loss: 355.017542
2019-01-08 13:15:00,769 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 13:15:01,196 - 10 - training_embed.py - training - Epoch: [5][100/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 354.4594 (354.9472)	
2019-01-08 13:15:01,414 - 10 - training_embed.py - training - Epoch: [5][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.0619 (354.9111)	
2019-01-08 13:15:01,641 - 10 - training_embed.py - training - Epoch: [5][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.7899 (354.8878)	
2019-01-08 13:15:01,867 - 10 - training_embed.py - training - Epoch: [5][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.1187 (354.8778)	
2019-01-08 13:15:02,086 - 10 - training_embed.py - training - Epoch: [5][500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.6388 (354.8968)	
2019-01-08 13:15:02,306 - 10 - training_embed.py - training - Epoch: [5][600/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 355.6880 (354.8763)	
2019-01-08 13:15:02,503 - 10 - training_embed.py - training - Epoch: [5][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.2772 (354.8741)	
2019-01-08 13:15:02,740 - 10 - training_embed.py - training - Epoch: [5][800/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 355.2418 (354.8633)	
2019-01-08 13:15:02,970 - 10 - training_embed.py - training - Epoch: [5][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.4308 (354.8578)	
2019-01-08 13:15:03,181 - 10 - training_embed.py - training - Epoch: [5][1000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.8260 (354.8887)	
2019-01-08 13:15:03,391 - 10 - training_embed.py - training - Epoch: [5][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.0025 (354.8688)	
2019-01-08 13:15:03,620 - 10 - training_embed.py - training - Epoch: [5][1200/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 356.6950 (354.8776)	
2019-01-08 13:15:03,838 - 10 - training_embed.py - training - Epoch: [5][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.4691 (354.9120)	
2019-01-08 13:15:04,056 - 10 - training_embed.py - training - Epoch: [5][1400/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 355.6703 (354.9152)	
2019-01-08 13:15:04,273 - 10 - training_embed.py - training - Epoch: [5][1500/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 354.8615 (354.9232)	
2019-01-08 13:15:04,491 - 10 - training_embed.py - training - Epoch: [5][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.2722 (354.9215)	
2019-01-08 13:15:04,714 - 10 - training_embed.py - training - Epoch: [5][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.5211 (354.9433)	
2019-01-08 13:15:04,918 - 10 - training_embed.py - training - Epoch: [5][1800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.2858 (354.9492)	
2019-01-08 13:15:05,154 - 10 - training_embed.py - training - Epoch: [5][1900/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 357.7876 (354.9551)	
2019-01-08 13:15:05,375 - 10 - training_embed.py - training - Epoch: [5][2000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 349.4092 (354.9550)	
2019-01-08 13:15:05,606 - 10 - training_embed.py - training - Epoch: [5][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 355.7829 (354.9531)	
2019-01-08 13:15:05,621 - 10 - training_embed.py - training - loss: 354.915889
2019-01-08 13:15:05,621 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 13:15:06,055 - 10 - training_embed.py - training - Epoch: [6][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.6067 (354.6837)	
2019-01-08 13:15:06,264 - 10 - training_embed.py - training - Epoch: [6][200/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.8456 (354.7765)	
2019-01-08 13:15:06,471 - 10 - training_embed.py - training - Epoch: [6][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.3302 (354.8078)	
2019-01-08 13:15:06,687 - 10 - training_embed.py - training - Epoch: [6][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.8244 (354.8865)	
2019-01-08 13:15:06,916 - 10 - training_embed.py - training - Epoch: [6][500/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 354.8422 (354.8957)	
2019-01-08 13:15:07,130 - 10 - training_embed.py - training - Epoch: [6][600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 354.6253 (354.8837)	
2019-01-08 13:15:07,354 - 10 - training_embed.py - training - Epoch: [6][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.8102 (354.8855)	
2019-01-08 13:15:07,571 - 10 - training_embed.py - training - Epoch: [6][800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.2351 (354.8067)	
2019-01-08 13:15:07,791 - 10 - training_embed.py - training - Epoch: [6][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.5294 (354.7821)	
2019-01-08 13:15:07,995 - 10 - training_embed.py - training - Epoch: [6][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.1242 (354.7880)	
2019-01-08 13:15:08,218 - 10 - training_embed.py - training - Epoch: [6][1100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 351.3051 (354.7916)	
2019-01-08 13:15:08,435 - 10 - training_embed.py - training - Epoch: [6][1200/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.4726 (354.8040)	
2019-01-08 13:15:08,647 - 10 - training_embed.py - training - Epoch: [6][1300/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 350.1786 (354.8215)	
2019-01-08 13:15:08,871 - 10 - training_embed.py - training - Epoch: [6][1400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 353.0548 (354.8359)	
2019-01-08 13:15:09,077 - 10 - training_embed.py - training - Epoch: [6][1500/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.9494 (354.8609)	
2019-01-08 13:15:09,295 - 10 - training_embed.py - training - Epoch: [6][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.6086 (354.8712)	
2019-01-08 13:15:09,503 - 10 - training_embed.py - training - Epoch: [6][1700/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.3875 (354.8712)	
2019-01-08 13:15:09,733 - 10 - training_embed.py - training - Epoch: [6][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.6176 (354.8755)	
2019-01-08 13:15:09,964 - 10 - training_embed.py - training - Epoch: [6][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.0156 (354.8650)	
2019-01-08 13:15:10,188 - 10 - training_embed.py - training - Epoch: [6][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0568 (354.8737)	
2019-01-08 13:15:10,409 - 10 - training_embed.py - training - Epoch: [6][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 349.9118 (354.8518)	
2019-01-08 13:15:10,423 - 10 - training_embed.py - training - loss: 354.815838
2019-01-08 13:15:10,423 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 13:15:10,856 - 10 - training_embed.py - training - Epoch: [7][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.2591 (354.8448)	
2019-01-08 13:15:11,071 - 10 - training_embed.py - training - Epoch: [7][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.2666 (354.9465)	
2019-01-08 13:15:11,291 - 10 - training_embed.py - training - Epoch: [7][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.8889 (354.7567)	
2019-01-08 13:15:11,519 - 10 - training_embed.py - training - Epoch: [7][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.0645 (354.8574)	
2019-01-08 13:15:11,729 - 10 - training_embed.py - training - Epoch: [7][500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.9989 (354.8744)	
2019-01-08 13:15:11,937 - 10 - training_embed.py - training - Epoch: [7][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.6681 (354.8549)	
2019-01-08 13:15:12,154 - 10 - training_embed.py - training - Epoch: [7][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.0982 (354.8855)	
2019-01-08 13:15:12,366 - 10 - training_embed.py - training - Epoch: [7][800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.5803 (354.8498)	
2019-01-08 13:15:12,593 - 10 - training_embed.py - training - Epoch: [7][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.5906 (354.8415)	
2019-01-08 13:15:12,824 - 10 - training_embed.py - training - Epoch: [7][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.1838 (354.8388)	
2019-01-08 13:15:13,050 - 10 - training_embed.py - training - Epoch: [7][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 358.4502 (354.8178)	
2019-01-08 13:15:13,276 - 10 - training_embed.py - training - Epoch: [7][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.8458 (354.8035)	
2019-01-08 13:15:13,487 - 10 - training_embed.py - training - Epoch: [7][1300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.5739 (354.7889)	
2019-01-08 13:15:13,711 - 10 - training_embed.py - training - Epoch: [7][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.4317 (354.7755)	
2019-01-08 13:15:13,925 - 10 - training_embed.py - training - Epoch: [7][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.3790 (354.7799)	
2019-01-08 13:15:14,145 - 10 - training_embed.py - training - Epoch: [7][1600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 358.5022 (354.7689)	
2019-01-08 13:15:14,359 - 10 - training_embed.py - training - Epoch: [7][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.5656 (354.7580)	
2019-01-08 13:15:14,577 - 10 - training_embed.py - training - Epoch: [7][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.9514 (354.7628)	
2019-01-08 13:15:14,800 - 10 - training_embed.py - training - Epoch: [7][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.6556 (354.7580)	
2019-01-08 13:15:15,010 - 10 - training_embed.py - training - Epoch: [7][2000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.2163 (354.7649)	
2019-01-08 13:15:15,223 - 10 - training_embed.py - training - Epoch: [7][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 350.2892 (354.7542)	
2019-01-08 13:15:15,238 - 10 - training_embed.py - training - loss: 354.713912
2019-01-08 13:15:15,239 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 13:15:15,675 - 10 - training_embed.py - training - Epoch: [8][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.5770 (354.7828)	
2019-01-08 13:15:15,886 - 10 - training_embed.py - training - Epoch: [8][200/2109]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 351.6408 (354.8293)	
2019-01-08 13:15:16,097 - 10 - training_embed.py - training - Epoch: [8][300/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.3615 (354.9174)	
2019-01-08 13:15:16,309 - 10 - training_embed.py - training - Epoch: [8][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.6531 (354.8453)	
2019-01-08 13:15:16,523 - 10 - training_embed.py - training - Epoch: [8][500/2109]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 352.5094 (354.7225)	
2019-01-08 13:15:16,743 - 10 - training_embed.py - training - Epoch: [8][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.9571 (354.7013)	
2019-01-08 13:15:16,967 - 10 - training_embed.py - training - Epoch: [8][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.2246 (354.6808)	
2019-01-08 13:15:17,194 - 10 - training_embed.py - training - Epoch: [8][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.4016 (354.6969)	
2019-01-08 13:15:17,418 - 10 - training_embed.py - training - Epoch: [8][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.7089 (354.6544)	
2019-01-08 13:15:17,649 - 10 - training_embed.py - training - Epoch: [8][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.3005 (354.6380)	
2019-01-08 13:15:17,858 - 10 - training_embed.py - training - Epoch: [8][1100/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 354.0950 (354.6682)	
2019-01-08 13:15:18,074 - 10 - training_embed.py - training - Epoch: [8][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 359.5978 (354.6315)	
2019-01-08 13:15:18,291 - 10 - training_embed.py - training - Epoch: [8][1300/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 349.1336 (354.6459)	
2019-01-08 13:15:18,522 - 10 - training_embed.py - training - Epoch: [8][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.5592 (354.6200)	
2019-01-08 13:15:18,736 - 10 - training_embed.py - training - Epoch: [8][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.1747 (354.6468)	
2019-01-08 13:15:18,945 - 10 - training_embed.py - training - Epoch: [8][1600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 354.8095 (354.6400)	
2019-01-08 13:15:19,155 - 10 - training_embed.py - training - Epoch: [8][1700/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 355.0223 (354.6255)	
2019-01-08 13:15:19,357 - 10 - training_embed.py - training - Epoch: [8][1800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 350.4793 (354.6437)	
2019-01-08 13:15:19,570 - 10 - training_embed.py - training - Epoch: [8][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.4696 (354.6291)	
2019-01-08 13:15:19,787 - 10 - training_embed.py - training - Epoch: [8][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.2454 (354.6389)	
2019-01-08 13:15:20,016 - 10 - training_embed.py - training - Epoch: [8][2100/2109]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 356.8642 (354.6462)	
2019-01-08 13:15:20,030 - 10 - training_embed.py - training - loss: 354.612504
2019-01-08 13:15:20,031 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 13:15:20,470 - 10 - training_embed.py - training - Epoch: [9][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.9048 (354.4411)	
2019-01-08 13:15:20,701 - 10 - training_embed.py - training - Epoch: [9][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.0105 (354.4997)	
2019-01-08 13:15:20,928 - 10 - training_embed.py - training - Epoch: [9][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 362.3222 (354.5686)	
2019-01-08 13:15:21,142 - 10 - training_embed.py - training - Epoch: [9][400/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 353.9361 (354.6003)	
2019-01-08 13:15:21,351 - 10 - training_embed.py - training - Epoch: [9][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.1361 (354.6292)	
2019-01-08 13:15:21,567 - 10 - training_embed.py - training - Epoch: [9][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.6111 (354.5790)	
2019-01-08 13:15:21,784 - 10 - training_embed.py - training - Epoch: [9][700/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.0483 (354.5895)	
2019-01-08 13:15:21,991 - 10 - training_embed.py - training - Epoch: [9][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.8343 (354.5703)	
2019-01-08 13:15:22,204 - 10 - training_embed.py - training - Epoch: [9][900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 351.8463 (354.5778)	
2019-01-08 13:15:22,428 - 10 - training_embed.py - training - Epoch: [9][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.3013 (354.5953)	
2019-01-08 13:15:22,659 - 10 - training_embed.py - training - Epoch: [9][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.3218 (354.6211)	
2019-01-08 13:15:22,871 - 10 - training_embed.py - training - Epoch: [9][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 357.4071 (354.5924)	
2019-01-08 13:15:23,090 - 10 - training_embed.py - training - Epoch: [9][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.3087 (354.5729)	
2019-01-08 13:15:23,321 - 10 - training_embed.py - training - Epoch: [9][1400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.5686 (354.5703)	
2019-01-08 13:15:23,553 - 10 - training_embed.py - training - Epoch: [9][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.5460 (354.5549)	
2019-01-08 13:15:23,769 - 10 - training_embed.py - training - Epoch: [9][1600/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 351.3111 (354.5715)	
2019-01-08 13:15:23,978 - 10 - training_embed.py - training - Epoch: [9][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.1349 (354.5726)	
2019-01-08 13:15:24,198 - 10 - training_embed.py - training - Epoch: [9][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.6347 (354.5648)	
2019-01-08 13:15:24,428 - 10 - training_embed.py - training - Epoch: [9][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.3030 (354.5684)	
2019-01-08 13:15:24,652 - 10 - training_embed.py - training - Epoch: [9][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0232 (354.5528)	
2019-01-08 13:15:24,885 - 10 - training_embed.py - training - Epoch: [9][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 358.7453 (354.5492)	
2019-01-08 13:15:24,898 - 10 - training_embed.py - training - loss: 354.510791
2019-01-08 13:15:24,898 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 13:15:25,324 - 10 - training_embed.py - training - Epoch: [10][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.9836 (354.1551)	
2019-01-08 13:15:25,539 - 10 - training_embed.py - training - Epoch: [10][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.1242 (354.2927)	
2019-01-08 13:15:25,750 - 10 - training_embed.py - training - Epoch: [10][300/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 351.8776 (354.4010)	
2019-01-08 13:15:25,975 - 10 - training_embed.py - training - Epoch: [10][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.7837 (354.4863)	
2019-01-08 13:15:26,193 - 10 - training_embed.py - training - Epoch: [10][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.6965 (354.4233)	
2019-01-08 13:15:26,419 - 10 - training_embed.py - training - Epoch: [10][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.7346 (354.4305)	
2019-01-08 13:15:26,638 - 10 - training_embed.py - training - Epoch: [10][700/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 357.3375 (354.4173)	
2019-01-08 13:15:26,862 - 10 - training_embed.py - training - Epoch: [10][800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 350.3815 (354.4065)	
2019-01-08 13:15:27,082 - 10 - training_embed.py - training - Epoch: [10][900/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 358.9083 (354.4511)	
2019-01-08 13:15:27,294 - 10 - training_embed.py - training - Epoch: [10][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.9624 (354.4454)	
2019-01-08 13:15:27,504 - 10 - training_embed.py - training - Epoch: [10][1100/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.5048 (354.4117)	
2019-01-08 13:15:27,723 - 10 - training_embed.py - training - Epoch: [10][1200/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 351.5332 (354.4422)	
2019-01-08 13:15:27,942 - 10 - training_embed.py - training - Epoch: [10][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.6103 (354.4584)	
2019-01-08 13:15:28,155 - 10 - training_embed.py - training - Epoch: [10][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.6574 (354.4711)	
2019-01-08 13:15:28,371 - 10 - training_embed.py - training - Epoch: [10][1500/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.3992 (354.4585)	
2019-01-08 13:15:28,595 - 10 - training_embed.py - training - Epoch: [10][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.4724 (354.4715)	
2019-01-08 13:15:28,812 - 10 - training_embed.py - training - Epoch: [10][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.1656 (354.4799)	
2019-01-08 13:15:29,035 - 10 - training_embed.py - training - Epoch: [10][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.2724 (354.4648)	
2019-01-08 13:15:29,261 - 10 - training_embed.py - training - Epoch: [10][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.2911 (354.4735)	
2019-01-08 13:15:29,467 - 10 - training_embed.py - training - Epoch: [10][2000/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 355.1277 (354.4654)	
2019-01-08 13:15:29,688 - 10 - training_embed.py - training - Epoch: [10][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 353.4001 (354.4537)	
2019-01-08 13:15:29,702 - 10 - training_embed.py - training - loss: 354.409515
2019-01-08 13:15:29,703 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 13:15:30,159 - 10 - training_embed.py - training - Epoch: [11][100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 356.4353 (354.2553)	
2019-01-08 13:15:30,378 - 10 - training_embed.py - training - Epoch: [11][200/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.3324 (354.4946)	
2019-01-08 13:15:30,590 - 10 - training_embed.py - training - Epoch: [11][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.2365 (354.4744)	
2019-01-08 13:15:30,807 - 10 - training_embed.py - training - Epoch: [11][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.3932 (354.4990)	
2019-01-08 13:15:31,039 - 10 - training_embed.py - training - Epoch: [11][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.1492 (354.5679)	
2019-01-08 13:15:31,264 - 10 - training_embed.py - training - Epoch: [11][600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 354.8385 (354.5316)	
2019-01-08 13:15:31,471 - 10 - training_embed.py - training - Epoch: [11][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.1414 (354.5176)	
2019-01-08 13:15:31,693 - 10 - training_embed.py - training - Epoch: [11][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.5764 (354.4518)	
2019-01-08 13:15:31,905 - 10 - training_embed.py - training - Epoch: [11][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.9318 (354.4521)	
2019-01-08 13:15:32,112 - 10 - training_embed.py - training - Epoch: [11][1000/2109]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 352.7007 (354.4039)	
2019-01-08 13:15:32,332 - 10 - training_embed.py - training - Epoch: [11][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.9630 (354.3680)	
2019-01-08 13:15:32,551 - 10 - training_embed.py - training - Epoch: [11][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.9784 (354.3744)	
2019-01-08 13:15:32,763 - 10 - training_embed.py - training - Epoch: [11][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.0465 (354.3622)	
2019-01-08 13:15:33,003 - 10 - training_embed.py - training - Epoch: [11][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.9692 (354.3600)	
2019-01-08 13:15:33,226 - 10 - training_embed.py - training - Epoch: [11][1500/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 359.8194 (354.3489)	
2019-01-08 13:15:33,434 - 10 - training_embed.py - training - Epoch: [11][1600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 355.5420 (354.3500)	
2019-01-08 13:15:33,652 - 10 - training_embed.py - training - Epoch: [11][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.8929 (354.3406)	
2019-01-08 13:15:33,866 - 10 - training_embed.py - training - Epoch: [11][1800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 356.0744 (354.3365)	
2019-01-08 13:15:34,087 - 10 - training_embed.py - training - Epoch: [11][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.4108 (354.3583)	
2019-01-08 13:15:34,313 - 10 - training_embed.py - training - Epoch: [11][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.9761 (354.3503)	
2019-01-08 13:15:34,540 - 10 - training_embed.py - training - Epoch: [11][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 351.6987 (354.3467)	
2019-01-08 13:15:34,553 - 10 - training_embed.py - training - loss: 354.306562
2019-01-08 13:15:34,633 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 13:15:35,587 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 953.447 ms ~ 0.016 min ~ 0.953 sec
2019-01-08 13:15:36,414 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1780.269 ms ~ 0.030 min ~ 1.780 sec
2019-01-08 13:15:36,414 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 13:15:36,415 - 10 - corpus.py - subactivity_sampler - 0 / 173
2019-01-08 13:15:36,415 - 10 - corpus.py - subactivity_sampler - [77306. 51567. 78169. 48237. 95321. 45270. 83233. 60630.]
2019-01-08 13:17:02,723 - 10 - corpus.py - subactivity_sampler - 20 / 173
2019-01-08 13:17:02,723 - 10 - corpus.py - subactivity_sampler - [78211. 49902. 80474. 45115. 97402. 42413. 86773. 59443.]
2019-01-08 13:18:00,777 - 10 - corpus.py - subactivity_sampler - 40 / 173
2019-01-08 13:18:00,777 - 10 - corpus.py - subactivity_sampler - [79735. 47099. 84024. 42082. 99602. 39207. 89128. 58856.]
2019-01-08 13:19:12,965 - 10 - corpus.py - subactivity_sampler - 60 / 173
2019-01-08 13:19:12,965 - 10 - corpus.py - subactivity_sampler - [ 80369.  44802.  86983.  38515. 104169.  35510.  91074.  58311.]
2019-01-08 13:20:25,731 - 10 - corpus.py - subactivity_sampler - 80 / 173
2019-01-08 13:20:25,731 - 10 - corpus.py - subactivity_sampler - [ 81306.  41800.  91682.  34119. 107332.  31749.  94223.  57522.]
2019-01-08 13:21:46,469 - 10 - corpus.py - subactivity_sampler - 100 / 173
2019-01-08 13:21:46,469 - 10 - corpus.py - subactivity_sampler - [ 81494.  39048.  96969.  30222. 110092.  28657.  96476.  56775.]
2019-01-08 13:22:31,691 - 10 - corpus.py - subactivity_sampler - 120 / 173
2019-01-08 13:22:31,691 - 10 - corpus.py - subactivity_sampler - [ 81753.  36142. 100428.  27891. 113788.  26168.  97818.  55745.]
2019-01-08 13:23:24,525 - 10 - corpus.py - subactivity_sampler - 140 / 173
2019-01-08 13:23:24,525 - 10 - corpus.py - subactivity_sampler - [ 82261.  33845. 102806.  25898. 115240.  24447. 100237.  54999.]
2019-01-08 13:24:32,596 - 10 - corpus.py - subactivity_sampler - 160 / 173
2019-01-08 13:24:32,596 - 10 - corpus.py - subactivity_sampler - [ 82106.  32023. 106237.  23227. 116931.  22697. 102712.  53800.]
2019-01-08 13:25:09,718 - 10 - corpus.py - subactivity_sampler - [ 82201.  30820. 107858.  21911. 118008.  21755. 104296.  52884.]
2019-01-08 13:25:09,718 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 573304.101 ms ~ 9.555 min ~ 573.304 sec
2019-01-08 13:25:09,718 - 10 - corpus.py - ordering_sampler - .
2019-01-08 13:25:12,786 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 13:25:12,786 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 9.  7.  0. 42. 75. 38.  8.]
2019-01-08 13:25:12,786 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 13:25:12,812 - 10 - corpus.py - rho_sampling - ['51.9233', '10.8653', '928.5380', '232.7775', '-0.8929', '10.6009', '601.5953']
2019-01-08 13:25:12,812 - 10 - pipeline.py - baseline - Iteration 2
2019-01-08 13:25:12,979 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 13:25:12,989 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 13:25:12,989 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 17', '1: 12', '2: 11', '3: 10', '4: 13', '5: 16', '6: 14', '7: 15']
2019-01-08 13:25:13,015 - 10 - accuracy_class.py - mof_val - frames true: 182058	frames overall : 539733
2019-01-08 13:25:13,016 - 10 - corpus.py - accuracy_corpus - Action: friedegg
2019-01-08 13:25:13,016 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3373112261062414
2019-01-08 13:25:13,016 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3349193026922571
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 17933
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - mof_classes - label 10: 0.000000  0 / 10971
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - mof_classes - label 11: 0.324660  22410 / 69026
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - mof_classes - label 12: 0.104423  3499 / 33508
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - mof_classes - label 13: 0.322722  102723 / 318302
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - mof_classes - label 14: 0.309126  7947 / 25708
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - mof_classes - label 15: 0.679391  21731 / 31986
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 2201
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - mof_classes - label 17: 0.789023  23748 / 30098
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - mof_classes - average class mof: 0.281038
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 17933
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - iou_classes - label 10: 0.000000  0 / 32882
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - iou_classes - label 11: 0.145073  22410 / 154474
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - iou_classes - label 12: 0.057522  3499 / 60829
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - iou_classes - label 13: 0.307935  102723 / 333587
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - iou_classes - label 14: 0.065109  7947 / 122057
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - iou_classes - label 15: 0.344177  21731 / 63139
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 23956
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - iou_classes - label 17: 0.268184  23748 / 88551
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - iou_classes - average IoU: 0.148500
2019-01-08 13:25:13,016 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.132000
2019-01-08 13:25:19,392 - 10 - f1_score.py - f1 - f1 score: 0.278330
2019-01-08 13:25:19,410 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 6598.624 ms ~ 0.110 min ~ 6.599 sec
2019-01-08 13:25:19,411 - 10 - corpus.py - embedding_training - .
2019-01-08 13:25:19,411 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 13:25:19,411 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 13:25:19,411 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 13:25:23,811 - 10 - training_embed.py - training - create model
2019-01-08 13:25:23,813 - 10 - training_embed.py - training - epochs: 12
2019-01-08 13:25:23,813 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 13:25:24,208 - 10 - training_embed.py - training - Epoch: [0][100/2109]	Time 0.003 (0.004)	Data 0.002 (0.003)	Loss 354.3109 (355.0274)	
2019-01-08 13:25:24,454 - 10 - training_embed.py - training - Epoch: [0][200/2109]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 356.6905 (354.7893)	
2019-01-08 13:25:24,779 - 10 - training_embed.py - training - Epoch: [0][300/2109]	Time 0.002 (0.003)	Data 0.000 (0.002)	Loss 352.2386 (354.7522)	
2019-01-08 13:25:25,107 - 10 - training_embed.py - training - Epoch: [0][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 351.0855 (354.6252)	
2019-01-08 13:25:25,342 - 10 - training_embed.py - training - Epoch: [0][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 350.9977 (354.6244)	
2019-01-08 13:25:25,575 - 10 - training_embed.py - training - Epoch: [0][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 346.8038 (354.6402)	
2019-01-08 13:25:25,816 - 10 - training_embed.py - training - Epoch: [0][700/2109]	Time 0.004 (0.003)	Data 0.002 (0.002)	Loss 354.9282 (354.6142)	
2019-01-08 13:25:26,051 - 10 - training_embed.py - training - Epoch: [0][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.2375 (354.6007)	
2019-01-08 13:25:26,272 - 10 - training_embed.py - training - Epoch: [0][900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 362.1689 (354.5769)	
2019-01-08 13:25:26,509 - 10 - training_embed.py - training - Epoch: [0][1000/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 349.4771 (354.5850)	
2019-01-08 13:25:26,750 - 10 - training_embed.py - training - Epoch: [0][1100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.3260 (354.5833)	
2019-01-08 13:25:26,963 - 10 - training_embed.py - training - Epoch: [0][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.4021 (354.5490)	
2019-01-08 13:25:27,179 - 10 - training_embed.py - training - Epoch: [0][1300/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 354.2113 (354.5374)	
2019-01-08 13:25:27,410 - 10 - training_embed.py - training - Epoch: [0][1400/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 353.0382 (354.5526)	
2019-01-08 13:25:27,657 - 10 - training_embed.py - training - Epoch: [0][1500/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 358.6027 (354.4998)	
2019-01-08 13:25:27,891 - 10 - training_embed.py - training - Epoch: [0][1600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.5179 (354.4886)	
2019-01-08 13:25:28,110 - 10 - training_embed.py - training - Epoch: [0][1700/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 356.3350 (354.4780)	
2019-01-08 13:25:28,333 - 10 - training_embed.py - training - Epoch: [0][1800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 356.0175 (354.4700)	
2019-01-08 13:25:28,568 - 10 - training_embed.py - training - Epoch: [0][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.7095 (354.4359)	
2019-01-08 13:25:28,786 - 10 - training_embed.py - training - Epoch: [0][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.6879 (354.4226)	
2019-01-08 13:25:29,011 - 10 - training_embed.py - training - Epoch: [0][2100/2109]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 358.4164 (354.4302)	
2019-01-08 13:25:29,026 - 10 - training_embed.py - training - loss: 354.391727
2019-01-08 13:25:29,026 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 13:25:29,460 - 10 - training_embed.py - training - Epoch: [1][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.6490 (354.3033)	
2019-01-08 13:25:29,716 - 10 - training_embed.py - training - Epoch: [1][200/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 357.4367 (353.9791)	
2019-01-08 13:25:29,969 - 10 - training_embed.py - training - Epoch: [1][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.5260 (354.0458)	
2019-01-08 13:25:30,191 - 10 - training_embed.py - training - Epoch: [1][400/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 356.2755 (354.0799)	
2019-01-08 13:25:30,425 - 10 - training_embed.py - training - Epoch: [1][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.0758 (354.1304)	
2019-01-08 13:25:30,653 - 10 - training_embed.py - training - Epoch: [1][600/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 356.1336 (354.1923)	
2019-01-08 13:25:30,889 - 10 - training_embed.py - training - Epoch: [1][700/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 357.6880 (354.2054)	
2019-01-08 13:25:31,118 - 10 - training_embed.py - training - Epoch: [1][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.2795 (354.2445)	
2019-01-08 13:25:31,351 - 10 - training_embed.py - training - Epoch: [1][900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.6661 (354.2468)	
2019-01-08 13:25:31,585 - 10 - training_embed.py - training - Epoch: [1][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.0444 (354.2287)	
2019-01-08 13:25:31,812 - 10 - training_embed.py - training - Epoch: [1][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.7191 (354.2405)	
2019-01-08 13:25:32,038 - 10 - training_embed.py - training - Epoch: [1][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.2722 (354.2077)	
2019-01-08 13:25:32,272 - 10 - training_embed.py - training - Epoch: [1][1300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.1899 (354.2041)	
2019-01-08 13:25:32,493 - 10 - training_embed.py - training - Epoch: [1][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.1771 (354.1912)	
2019-01-08 13:25:32,719 - 10 - training_embed.py - training - Epoch: [1][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.7609 (354.1835)	
2019-01-08 13:25:32,965 - 10 - training_embed.py - training - Epoch: [1][1600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 350.0261 (354.2007)	
2019-01-08 13:25:33,192 - 10 - training_embed.py - training - Epoch: [1][1700/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.1909 (354.1815)	
2019-01-08 13:25:33,428 - 10 - training_embed.py - training - Epoch: [1][1800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 350.6596 (354.1764)	
2019-01-08 13:25:33,647 - 10 - training_embed.py - training - Epoch: [1][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.4976 (354.1577)	
2019-01-08 13:25:33,885 - 10 - training_embed.py - training - Epoch: [1][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.2127 (354.1632)	
2019-01-08 13:25:34,236 - 10 - training_embed.py - training - Epoch: [1][2100/2109]	Time 0.008 (0.002)	Data 0.005 (0.001)	Loss 356.3873 (354.1587)	
2019-01-08 13:25:34,278 - 10 - training_embed.py - training - loss: 354.119692
2019-01-08 13:25:34,280 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 13:25:35,095 - 10 - training_embed.py - training - Epoch: [2][100/2109]	Time 0.005 (0.003)	Data 0.003 (0.001)	Loss 353.9225 (353.6206)	
2019-01-08 13:25:35,536 - 10 - training_embed.py - training - Epoch: [2][200/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 354.6926 (353.8092)	
2019-01-08 13:25:35,834 - 10 - training_embed.py - training - Epoch: [2][300/2109]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 357.5362 (353.8122)	
2019-01-08 13:25:36,125 - 10 - training_embed.py - training - Epoch: [2][400/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 351.6961 (353.7786)	
2019-01-08 13:25:36,436 - 10 - training_embed.py - training - Epoch: [2][500/2109]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 351.8407 (353.7555)	
2019-01-08 13:25:36,827 - 10 - training_embed.py - training - Epoch: [2][600/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 355.1460 (353.7727)	
2019-01-08 13:25:37,149 - 10 - training_embed.py - training - Epoch: [2][700/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 355.8617 (353.8524)	
2019-01-08 13:25:37,457 - 10 - training_embed.py - training - Epoch: [2][800/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 354.3484 (353.8609)	
2019-01-08 13:25:37,895 - 10 - training_embed.py - training - Epoch: [2][900/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 359.8109 (353.8288)	
2019-01-08 13:25:38,179 - 10 - training_embed.py - training - Epoch: [2][1000/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 354.3119 (353.8022)	
2019-01-08 13:25:38,473 - 10 - training_embed.py - training - Epoch: [2][1100/2109]	Time 0.004 (0.003)	Data 0.001 (0.001)	Loss 352.5696 (353.8319)	
2019-01-08 13:25:38,771 - 10 - training_embed.py - training - Epoch: [2][1200/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 357.2353 (353.8533)	
2019-01-08 13:25:39,010 - 10 - training_embed.py - training - Epoch: [2][1300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 356.6239 (353.8670)	
2019-01-08 13:25:39,229 - 10 - training_embed.py - training - Epoch: [2][1400/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 356.6885 (353.8878)	
2019-01-08 13:25:39,439 - 10 - training_embed.py - training - Epoch: [2][1500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 357.9089 (353.8923)	
2019-01-08 13:25:39,670 - 10 - training_embed.py - training - Epoch: [2][1600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.4165 (353.8960)	
2019-01-08 13:25:39,949 - 10 - training_embed.py - training - Epoch: [2][1700/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 354.0257 (353.9244)	
2019-01-08 13:25:40,192 - 10 - training_embed.py - training - Epoch: [2][1800/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 354.7151 (353.9316)	
2019-01-08 13:25:40,423 - 10 - training_embed.py - training - Epoch: [2][1900/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 351.4835 (353.9265)	
2019-01-08 13:25:40,671 - 10 - training_embed.py - training - Epoch: [2][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.9391 (353.9214)	
2019-01-08 13:25:40,904 - 10 - training_embed.py - training - Epoch: [2][2100/2109]	Time 0.009 (0.003)	Data 0.008 (0.001)	Loss 353.8726 (353.8858)	
2019-01-08 13:25:40,920 - 10 - training_embed.py - training - loss: 353.848777
2019-01-08 13:25:40,920 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 13:25:41,373 - 10 - training_embed.py - training - Epoch: [3][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.0538 (353.4524)	
2019-01-08 13:25:41,592 - 10 - training_embed.py - training - Epoch: [3][200/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 351.2131 (353.4942)	
2019-01-08 13:25:41,816 - 10 - training_embed.py - training - Epoch: [3][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.2757 (353.4808)	
2019-01-08 13:25:42,037 - 10 - training_embed.py - training - Epoch: [3][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.9730 (353.5505)	
2019-01-08 13:25:42,251 - 10 - training_embed.py - training - Epoch: [3][500/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 350.4649 (353.5590)	
2019-01-08 13:25:42,472 - 10 - training_embed.py - training - Epoch: [3][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.1134 (353.6019)	
2019-01-08 13:25:42,685 - 10 - training_embed.py - training - Epoch: [3][700/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 350.3329 (353.6168)	
2019-01-08 13:25:42,916 - 10 - training_embed.py - training - Epoch: [3][800/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 358.0168 (353.6411)	
2019-01-08 13:25:43,139 - 10 - training_embed.py - training - Epoch: [3][900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.8995 (353.6280)	
2019-01-08 13:25:43,356 - 10 - training_embed.py - training - Epoch: [3][1000/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 351.3825 (353.6368)	
2019-01-08 13:25:43,570 - 10 - training_embed.py - training - Epoch: [3][1100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.6912 (353.6383)	
2019-01-08 13:25:43,797 - 10 - training_embed.py - training - Epoch: [3][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.2382 (353.6455)	
2019-01-08 13:25:44,042 - 10 - training_embed.py - training - Epoch: [3][1300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.3373 (353.6630)	
2019-01-08 13:25:44,345 - 10 - training_embed.py - training - Epoch: [3][1400/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 348.7049 (353.6691)	
2019-01-08 13:25:44,605 - 10 - training_embed.py - training - Epoch: [3][1500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.6296 (353.6399)	
2019-01-08 13:25:44,874 - 10 - training_embed.py - training - Epoch: [3][1600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.1096 (353.6185)	
2019-01-08 13:25:45,123 - 10 - training_embed.py - training - Epoch: [3][1700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.6945 (353.6208)	
2019-01-08 13:25:45,349 - 10 - training_embed.py - training - Epoch: [3][1800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.5090 (353.6084)	
2019-01-08 13:25:45,590 - 10 - training_embed.py - training - Epoch: [3][1900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 356.0779 (353.6164)	
2019-01-08 13:25:45,840 - 10 - training_embed.py - training - Epoch: [3][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 359.8333 (353.6177)	
2019-01-08 13:25:46,095 - 10 - training_embed.py - training - Epoch: [3][2100/2109]	Time 0.005 (0.003)	Data 0.004 (0.001)	Loss 351.2807 (353.6112)	
2019-01-08 13:25:46,109 - 10 - training_embed.py - training - loss: 353.575330
2019-01-08 13:25:46,109 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 13:25:46,570 - 10 - training_embed.py - training - Epoch: [4][100/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 352.1857 (353.6719)	
2019-01-08 13:25:46,806 - 10 - training_embed.py - training - Epoch: [4][200/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 354.8001 (353.4299)	
2019-01-08 13:25:47,041 - 10 - training_embed.py - training - Epoch: [4][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.7210 (353.3400)	
2019-01-08 13:25:47,281 - 10 - training_embed.py - training - Epoch: [4][400/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 356.0090 (353.3237)	
2019-01-08 13:25:47,507 - 10 - training_embed.py - training - Epoch: [4][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.1543 (353.3776)	
2019-01-08 13:25:47,739 - 10 - training_embed.py - training - Epoch: [4][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.2793 (353.3778)	
2019-01-08 13:25:47,964 - 10 - training_embed.py - training - Epoch: [4][700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.7808 (353.4246)	
2019-01-08 13:25:48,197 - 10 - training_embed.py - training - Epoch: [4][800/2109]	Time 0.005 (0.003)	Data 0.004 (0.001)	Loss 350.2118 (353.4400)	
2019-01-08 13:25:48,424 - 10 - training_embed.py - training - Epoch: [4][900/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 354.7151 (353.4044)	
2019-01-08 13:25:48,638 - 10 - training_embed.py - training - Epoch: [4][1000/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 358.5182 (353.4059)	
2019-01-08 13:25:48,865 - 10 - training_embed.py - training - Epoch: [4][1100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 357.1586 (353.3486)	
2019-01-08 13:25:49,092 - 10 - training_embed.py - training - Epoch: [4][1200/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 350.7941 (353.3755)	
2019-01-08 13:25:49,318 - 10 - training_embed.py - training - Epoch: [4][1300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.2903 (353.3391)	
2019-01-08 13:25:49,555 - 10 - training_embed.py - training - Epoch: [4][1400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.2201 (353.3223)	
2019-01-08 13:25:49,780 - 10 - training_embed.py - training - Epoch: [4][1500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.0022 (353.3276)	
2019-01-08 13:25:50,008 - 10 - training_embed.py - training - Epoch: [4][1600/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 352.5685 (353.3224)	
2019-01-08 13:25:50,231 - 10 - training_embed.py - training - Epoch: [4][1700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 356.4005 (353.3093)	
2019-01-08 13:25:50,461 - 10 - training_embed.py - training - Epoch: [4][1800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.3477 (353.3170)	
2019-01-08 13:25:50,690 - 10 - training_embed.py - training - Epoch: [4][1900/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 353.2252 (353.3358)	
2019-01-08 13:25:50,921 - 10 - training_embed.py - training - Epoch: [4][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.8411 (353.3503)	
2019-01-08 13:25:51,146 - 10 - training_embed.py - training - Epoch: [4][2100/2109]	Time 0.006 (0.003)	Data 0.004 (0.001)	Loss 359.3739 (353.3389)	
2019-01-08 13:25:51,160 - 10 - training_embed.py - training - loss: 353.303103
2019-01-08 13:25:51,161 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 13:25:51,609 - 10 - training_embed.py - training - Epoch: [5][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.4621 (353.1789)	
2019-01-08 13:25:51,835 - 10 - training_embed.py - training - Epoch: [5][200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.2479 (353.1877)	
2019-01-08 13:25:52,059 - 10 - training_embed.py - training - Epoch: [5][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.9949 (353.1121)	
2019-01-08 13:25:52,272 - 10 - training_embed.py - training - Epoch: [5][400/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 357.1194 (353.0922)	
2019-01-08 13:25:52,489 - 10 - training_embed.py - training - Epoch: [5][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.1649 (353.1000)	
2019-01-08 13:25:52,710 - 10 - training_embed.py - training - Epoch: [5][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.8063 (353.0843)	
2019-01-08 13:25:52,940 - 10 - training_embed.py - training - Epoch: [5][700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.0616 (353.0857)	
2019-01-08 13:25:53,164 - 10 - training_embed.py - training - Epoch: [5][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.5082 (353.1205)	
2019-01-08 13:25:53,384 - 10 - training_embed.py - training - Epoch: [5][900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.5782 (353.1133)	
2019-01-08 13:25:53,618 - 10 - training_embed.py - training - Epoch: [5][1000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 356.8793 (353.1014)	
2019-01-08 13:25:53,850 - 10 - training_embed.py - training - Epoch: [5][1100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.6677 (353.0653)	
2019-01-08 13:25:54,070 - 10 - training_embed.py - training - Epoch: [5][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.9413 (353.0908)	
2019-01-08 13:25:54,298 - 10 - training_embed.py - training - Epoch: [5][1300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.1230 (353.1091)	
2019-01-08 13:25:54,520 - 10 - training_embed.py - training - Epoch: [5][1400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 356.7850 (353.1205)	
2019-01-08 13:25:54,741 - 10 - training_embed.py - training - Epoch: [5][1500/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 350.6198 (353.1005)	
2019-01-08 13:25:54,961 - 10 - training_embed.py - training - Epoch: [5][1600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.8475 (353.0721)	
2019-01-08 13:25:55,199 - 10 - training_embed.py - training - Epoch: [5][1700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 357.1341 (353.0852)	
2019-01-08 13:25:55,439 - 10 - training_embed.py - training - Epoch: [5][1800/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 350.2444 (353.0735)	
2019-01-08 13:25:55,672 - 10 - training_embed.py - training - Epoch: [5][1900/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 354.1773 (353.0654)	
2019-01-08 13:25:55,901 - 10 - training_embed.py - training - Epoch: [5][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.9047 (353.0694)	
2019-01-08 13:25:56,129 - 10 - training_embed.py - training - Epoch: [5][2100/2109]	Time 0.008 (0.003)	Data 0.006 (0.001)	Loss 350.9639 (353.0678)	
2019-01-08 13:25:56,144 - 10 - training_embed.py - training - loss: 353.028837
2019-01-08 13:25:56,144 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 13:25:56,598 - 10 - training_embed.py - training - Epoch: [6][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.7344 (352.9706)	
2019-01-08 13:25:56,835 - 10 - training_embed.py - training - Epoch: [6][200/2109]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 354.5852 (352.9196)	
2019-01-08 13:25:57,060 - 10 - training_embed.py - training - Epoch: [6][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 358.9690 (353.0069)	
2019-01-08 13:25:57,277 - 10 - training_embed.py - training - Epoch: [6][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.8881 (353.0707)	
2019-01-08 13:25:57,502 - 10 - training_embed.py - training - Epoch: [6][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.9622 (353.0265)	
2019-01-08 13:25:57,729 - 10 - training_embed.py - training - Epoch: [6][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.3121 (352.9628)	
2019-01-08 13:25:57,962 - 10 - training_embed.py - training - Epoch: [6][700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.6455 (352.9370)	
2019-01-08 13:25:58,188 - 10 - training_embed.py - training - Epoch: [6][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.8708 (352.9009)	
2019-01-08 13:25:58,408 - 10 - training_embed.py - training - Epoch: [6][900/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 352.8321 (352.8768)	
2019-01-08 13:25:58,634 - 10 - training_embed.py - training - Epoch: [6][1000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.4521 (352.8476)	
2019-01-08 13:25:58,863 - 10 - training_embed.py - training - Epoch: [6][1100/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 348.3633 (352.8468)	
2019-01-08 13:25:59,077 - 10 - training_embed.py - training - Epoch: [6][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.2937 (352.8318)	
2019-01-08 13:25:59,305 - 10 - training_embed.py - training - Epoch: [6][1300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.6542 (352.8596)	
2019-01-08 13:25:59,526 - 10 - training_embed.py - training - Epoch: [6][1400/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 354.7281 (352.8592)	
2019-01-08 13:25:59,753 - 10 - training_embed.py - training - Epoch: [6][1500/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 353.6503 (352.8559)	
2019-01-08 13:25:59,978 - 10 - training_embed.py - training - Epoch: [6][1600/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 357.6579 (352.8583)	
2019-01-08 13:26:00,196 - 10 - training_embed.py - training - Epoch: [6][1700/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 351.8599 (352.8579)	
2019-01-08 13:26:00,417 - 10 - training_embed.py - training - Epoch: [6][1800/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 351.9678 (352.8421)	
2019-01-08 13:26:00,634 - 10 - training_embed.py - training - Epoch: [6][1900/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 351.9716 (352.8206)	
2019-01-08 13:26:00,868 - 10 - training_embed.py - training - Epoch: [6][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.2360 (352.8157)	
2019-01-08 13:26:01,094 - 10 - training_embed.py - training - Epoch: [6][2100/2109]	Time 0.005 (0.003)	Data 0.004 (0.001)	Loss 347.7764 (352.7926)	
2019-01-08 13:26:01,113 - 10 - training_embed.py - training - loss: 352.755640
2019-01-08 13:26:01,113 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 13:26:01,579 - 10 - training_embed.py - training - Epoch: [7][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.6488 (352.4628)	
2019-01-08 13:26:01,798 - 10 - training_embed.py - training - Epoch: [7][200/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 350.4756 (352.5648)	
2019-01-08 13:26:02,036 - 10 - training_embed.py - training - Epoch: [7][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.5760 (352.4292)	
2019-01-08 13:26:02,277 - 10 - training_embed.py - training - Epoch: [7][400/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 353.5218 (352.4602)	
2019-01-08 13:26:02,513 - 10 - training_embed.py - training - Epoch: [7][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.8117 (352.4912)	
2019-01-08 13:26:02,743 - 10 - training_embed.py - training - Epoch: [7][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.1997 (352.5122)	
2019-01-08 13:26:02,986 - 10 - training_embed.py - training - Epoch: [7][700/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 353.7574 (352.5381)	
2019-01-08 13:26:03,221 - 10 - training_embed.py - training - Epoch: [7][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.4299 (352.5680)	
2019-01-08 13:26:03,455 - 10 - training_embed.py - training - Epoch: [7][900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.8716 (352.5599)	
2019-01-08 13:26:03,701 - 10 - training_embed.py - training - Epoch: [7][1000/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 350.6901 (352.5732)	
2019-01-08 13:26:03,937 - 10 - training_embed.py - training - Epoch: [7][1100/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 353.9235 (352.5521)	
2019-01-08 13:26:04,168 - 10 - training_embed.py - training - Epoch: [7][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.4395 (352.5411)	
2019-01-08 13:26:04,393 - 10 - training_embed.py - training - Epoch: [7][1300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.8903 (352.5370)	
2019-01-08 13:26:04,634 - 10 - training_embed.py - training - Epoch: [7][1400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.1718 (352.5412)	
2019-01-08 13:26:04,855 - 10 - training_embed.py - training - Epoch: [7][1500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.2621 (352.5622)	
2019-01-08 13:26:05,079 - 10 - training_embed.py - training - Epoch: [7][1600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.7081 (352.5615)	
2019-01-08 13:26:05,322 - 10 - training_embed.py - training - Epoch: [7][1700/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 352.0435 (352.5342)	
2019-01-08 13:26:05,552 - 10 - training_embed.py - training - Epoch: [7][1800/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 352.2452 (352.5357)	
2019-01-08 13:26:05,775 - 10 - training_embed.py - training - Epoch: [7][1900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.9872 (352.5414)	
2019-01-08 13:26:05,998 - 10 - training_embed.py - training - Epoch: [7][2000/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 352.9113 (352.5395)	
2019-01-08 13:26:06,228 - 10 - training_embed.py - training - Epoch: [7][2100/2109]	Time 0.007 (0.003)	Data 0.005 (0.001)	Loss 349.4610 (352.5212)	
2019-01-08 13:26:06,243 - 10 - training_embed.py - training - loss: 352.480097
2019-01-08 13:26:06,244 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 13:26:06,689 - 10 - training_embed.py - training - Epoch: [8][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.5994 (352.3063)	
2019-01-08 13:26:06,903 - 10 - training_embed.py - training - Epoch: [8][200/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 349.5455 (352.4839)	
2019-01-08 13:26:07,145 - 10 - training_embed.py - training - Epoch: [8][300/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 355.6894 (352.5516)	
2019-01-08 13:26:07,370 - 10 - training_embed.py - training - Epoch: [8][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.1336 (352.4435)	
2019-01-08 13:26:07,605 - 10 - training_embed.py - training - Epoch: [8][500/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 349.7184 (352.3913)	
2019-01-08 13:26:07,827 - 10 - training_embed.py - training - Epoch: [8][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.9159 (352.3387)	
2019-01-08 13:26:08,054 - 10 - training_embed.py - training - Epoch: [8][700/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 350.8357 (352.2915)	
2019-01-08 13:26:08,278 - 10 - training_embed.py - training - Epoch: [8][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.3423 (352.3005)	
2019-01-08 13:26:08,500 - 10 - training_embed.py - training - Epoch: [8][900/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 352.4957 (352.2966)	
2019-01-08 13:26:08,716 - 10 - training_embed.py - training - Epoch: [8][1000/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 349.9440 (352.3020)	
2019-01-08 13:26:08,944 - 10 - training_embed.py - training - Epoch: [8][1100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.3077 (352.3023)	
2019-01-08 13:26:09,166 - 10 - training_embed.py - training - Epoch: [8][1200/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 355.4237 (352.2912)	
2019-01-08 13:26:09,382 - 10 - training_embed.py - training - Epoch: [8][1300/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 348.4704 (352.2945)	
2019-01-08 13:26:09,604 - 10 - training_embed.py - training - Epoch: [8][1400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.5005 (352.2656)	
2019-01-08 13:26:09,837 - 10 - training_embed.py - training - Epoch: [8][1500/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 349.8377 (352.2748)	
2019-01-08 13:26:10,059 - 10 - training_embed.py - training - Epoch: [8][1600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.8030 (352.2557)	
2019-01-08 13:26:10,288 - 10 - training_embed.py - training - Epoch: [8][1700/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.4928 (352.2533)	
2019-01-08 13:26:10,512 - 10 - training_embed.py - training - Epoch: [8][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.8338 (352.2570)	
2019-01-08 13:26:10,735 - 10 - training_embed.py - training - Epoch: [8][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 351.1299 (352.2388)	
2019-01-08 13:26:10,972 - 10 - training_embed.py - training - Epoch: [8][2000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 354.7731 (352.2384)	
2019-01-08 13:26:11,243 - 10 - training_embed.py - training - Epoch: [8][2100/2109]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 355.2354 (352.2410)	
2019-01-08 13:26:11,259 - 10 - training_embed.py - training - loss: 352.205319
2019-01-08 13:26:11,260 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 13:26:11,660 - 10 - training_embed.py - training - Epoch: [9][100/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 350.4079 (351.8483)	
2019-01-08 13:26:11,890 - 10 - training_embed.py - training - Epoch: [9][200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.9578 (351.9532)	
2019-01-08 13:26:12,121 - 10 - training_embed.py - training - Epoch: [9][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 358.8108 (352.0317)	
2019-01-08 13:26:12,359 - 10 - training_embed.py - training - Epoch: [9][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.3530 (352.0856)	
2019-01-08 13:26:12,582 - 10 - training_embed.py - training - Epoch: [9][500/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 355.1410 (352.1153)	
2019-01-08 13:26:12,820 - 10 - training_embed.py - training - Epoch: [9][600/2109]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 355.4359 (352.1229)	
2019-01-08 13:26:13,067 - 10 - training_embed.py - training - Epoch: [9][700/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.2301 (352.1231)	
2019-01-08 13:26:13,290 - 10 - training_embed.py - training - Epoch: [9][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.9668 (352.1202)	
2019-01-08 13:26:13,502 - 10 - training_embed.py - training - Epoch: [9][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.1088 (352.1285)	
2019-01-08 13:26:13,724 - 10 - training_embed.py - training - Epoch: [9][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.7693 (352.1052)	
2019-01-08 13:26:13,942 - 10 - training_embed.py - training - Epoch: [9][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.6062 (352.1162)	
2019-01-08 13:26:14,173 - 10 - training_embed.py - training - Epoch: [9][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.4761 (352.0635)	
2019-01-08 13:26:14,406 - 10 - training_embed.py - training - Epoch: [9][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.4107 (352.0410)	
2019-01-08 13:26:14,620 - 10 - training_embed.py - training - Epoch: [9][1400/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 356.8839 (352.0297)	
2019-01-08 13:26:14,849 - 10 - training_embed.py - training - Epoch: [9][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.5279 (352.0064)	
2019-01-08 13:26:15,076 - 10 - training_embed.py - training - Epoch: [9][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.0455 (352.0097)	
2019-01-08 13:26:15,282 - 10 - training_embed.py - training - Epoch: [9][1700/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 351.5046 (351.9956)	
2019-01-08 13:26:15,528 - 10 - training_embed.py - training - Epoch: [9][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.8810 (351.9803)	
2019-01-08 13:26:15,749 - 10 - training_embed.py - training - Epoch: [9][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.6225 (351.9964)	
2019-01-08 13:26:15,980 - 10 - training_embed.py - training - Epoch: [9][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.6935 (351.9815)	
2019-01-08 13:26:16,209 - 10 - training_embed.py - training - Epoch: [9][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 356.2261 (351.9661)	
2019-01-08 13:26:16,223 - 10 - training_embed.py - training - loss: 351.928919
2019-01-08 13:26:16,223 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 13:26:16,660 - 10 - training_embed.py - training - Epoch: [10][100/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 351.4720 (351.5968)	
2019-01-08 13:26:16,910 - 10 - training_embed.py - training - Epoch: [10][200/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 353.6957 (351.7064)	
2019-01-08 13:26:17,127 - 10 - training_embed.py - training - Epoch: [10][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.3795 (351.8834)	
2019-01-08 13:26:17,341 - 10 - training_embed.py - training - Epoch: [10][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.0878 (351.9618)	
2019-01-08 13:26:17,562 - 10 - training_embed.py - training - Epoch: [10][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.7279 (351.8922)	
2019-01-08 13:26:17,798 - 10 - training_embed.py - training - Epoch: [10][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.7603 (351.8557)	
2019-01-08 13:26:18,019 - 10 - training_embed.py - training - Epoch: [10][700/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.6158 (351.7790)	
2019-01-08 13:26:18,241 - 10 - training_embed.py - training - Epoch: [10][800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.5257 (351.7574)	
2019-01-08 13:26:18,467 - 10 - training_embed.py - training - Epoch: [10][900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 355.4281 (351.7908)	
2019-01-08 13:26:18,693 - 10 - training_embed.py - training - Epoch: [10][1000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.7140 (351.7605)	
2019-01-08 13:26:18,925 - 10 - training_embed.py - training - Epoch: [10][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.0944 (351.7162)	
2019-01-08 13:26:19,167 - 10 - training_embed.py - training - Epoch: [10][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.4973 (351.7230)	
2019-01-08 13:26:19,403 - 10 - training_embed.py - training - Epoch: [10][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.1858 (351.7307)	
2019-01-08 13:26:19,634 - 10 - training_embed.py - training - Epoch: [10][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.8177 (351.7221)	
2019-01-08 13:26:19,870 - 10 - training_embed.py - training - Epoch: [10][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.0662 (351.7098)	
2019-01-08 13:26:20,101 - 10 - training_embed.py - training - Epoch: [10][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.9972 (351.7174)	
2019-01-08 13:26:20,342 - 10 - training_embed.py - training - Epoch: [10][1700/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.5259 (351.7264)	
2019-01-08 13:26:20,593 - 10 - training_embed.py - training - Epoch: [10][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.9946 (351.7152)	
2019-01-08 13:26:20,820 - 10 - training_embed.py - training - Epoch: [10][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.2037 (351.7128)	
2019-01-08 13:26:21,065 - 10 - training_embed.py - training - Epoch: [10][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.0289 (351.7023)	
2019-01-08 13:26:21,300 - 10 - training_embed.py - training - Epoch: [10][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 348.8902 (351.6902)	
2019-01-08 13:26:21,315 - 10 - training_embed.py - training - loss: 351.651146
2019-01-08 13:26:21,315 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 13:26:21,765 - 10 - training_embed.py - training - Epoch: [11][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.4696 (351.4370)	
2019-01-08 13:26:21,986 - 10 - training_embed.py - training - Epoch: [11][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.5714 (351.5482)	
2019-01-08 13:26:22,217 - 10 - training_embed.py - training - Epoch: [11][300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.2394 (351.6475)	
2019-01-08 13:26:22,441 - 10 - training_embed.py - training - Epoch: [11][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.7396 (351.6052)	
2019-01-08 13:26:22,679 - 10 - training_embed.py - training - Epoch: [11][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.2191 (351.6983)	
2019-01-08 13:26:22,906 - 10 - training_embed.py - training - Epoch: [11][600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 353.5364 (351.6464)	
2019-01-08 13:26:23,128 - 10 - training_embed.py - training - Epoch: [11][700/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 350.0099 (351.6156)	
2019-01-08 13:26:23,360 - 10 - training_embed.py - training - Epoch: [11][800/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.9643 (351.5663)	
2019-01-08 13:26:23,582 - 10 - training_embed.py - training - Epoch: [11][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.3239 (351.5712)	
2019-01-08 13:26:23,823 - 10 - training_embed.py - training - Epoch: [11][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.8634 (351.5428)	
2019-01-08 13:26:24,050 - 10 - training_embed.py - training - Epoch: [11][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.1865 (351.5044)	
2019-01-08 13:26:24,285 - 10 - training_embed.py - training - Epoch: [11][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.9404 (351.4939)	
2019-01-08 13:26:24,530 - 10 - training_embed.py - training - Epoch: [11][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.6423 (351.4787)	
2019-01-08 13:26:24,758 - 10 - training_embed.py - training - Epoch: [11][1400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 349.1728 (351.4647)	
2019-01-08 13:26:24,983 - 10 - training_embed.py - training - Epoch: [11][1500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.8556 (351.4511)	
2019-01-08 13:26:25,227 - 10 - training_embed.py - training - Epoch: [11][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.7509 (351.4287)	
2019-01-08 13:26:25,461 - 10 - training_embed.py - training - Epoch: [11][1700/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 348.9352 (351.4200)	
2019-01-08 13:26:25,710 - 10 - training_embed.py - training - Epoch: [11][1800/2109]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 354.3051 (351.4187)	
2019-01-08 13:26:25,948 - 10 - training_embed.py - training - Epoch: [11][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.7064 (351.4268)	
2019-01-08 13:26:26,171 - 10 - training_embed.py - training - Epoch: [11][2000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.2428 (351.4181)	
2019-01-08 13:26:26,398 - 10 - training_embed.py - training - Epoch: [11][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 349.2240 (351.4134)	
2019-01-08 13:26:26,412 - 10 - training_embed.py - training - loss: 351.373285
2019-01-08 13:26:26,504 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 13:26:27,684 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1179.350 ms ~ 0.020 min ~ 1.179 sec
2019-01-08 13:26:28,680 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 2175.478 ms ~ 0.036 min ~ 2.175 sec
2019-01-08 13:26:28,680 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 13:26:28,680 - 10 - corpus.py - subactivity_sampler - 0 / 173
2019-01-08 13:26:28,681 - 10 - corpus.py - subactivity_sampler - [ 82201.  30820. 107858.  21911. 118008.  21755. 104296.  52884.]
2019-01-08 13:27:56,351 - 10 - corpus.py - subactivity_sampler - 20 / 173
2019-01-08 13:27:56,351 - 10 - corpus.py - subactivity_sampler - [ 82738.  28947. 110267.  18865. 120360.  18137. 108185.  52234.]
2019-01-08 13:28:53,960 - 10 - corpus.py - subactivity_sampler - 40 / 173
2019-01-08 13:28:53,960 - 10 - corpus.py - subactivity_sampler - [ 83318.  27872. 111475.  17604. 121411.  16308. 110214.  51531.]
2019-01-08 13:30:08,204 - 10 - corpus.py - subactivity_sampler - 60 / 173
2019-01-08 13:30:08,204 - 10 - corpus.py - subactivity_sampler - [ 83316.  26602. 113538.  15647. 123920.  14451. 111436.  50823.]
2019-01-08 13:31:20,355 - 10 - corpus.py - subactivity_sampler - 80 / 173
2019-01-08 13:31:20,356 - 10 - corpus.py - subactivity_sampler - [ 83437.  24838. 116670.  13454. 124269.  12582. 114493.  49990.]
2019-01-08 13:32:37,258 - 10 - corpus.py - subactivity_sampler - 100 / 173
2019-01-08 13:32:37,258 - 10 - corpus.py - subactivity_sampler - [ 83599.  23451. 118431.  13001. 125447.  11001. 115169.  49634.]
2019-01-08 13:33:21,428 - 10 - corpus.py - subactivity_sampler - 120 / 173
2019-01-08 13:33:21,429 - 10 - corpus.py - subactivity_sampler - [ 83629.  22096. 120071.  11983. 126346.  10353. 116412.  48843.]
2019-01-08 13:34:12,766 - 10 - corpus.py - subactivity_sampler - 140 / 173
2019-01-08 13:34:12,766 - 10 - corpus.py - subactivity_sampler - [ 83656.  21148. 121242.  11313. 126996.   9269. 119438.  46671.]
2019-01-08 13:35:17,993 - 10 - corpus.py - subactivity_sampler - 160 / 173
2019-01-08 13:35:17,993 - 10 - corpus.py - subactivity_sampler - [ 83501.  20392. 122736.  10521. 128262.   8465. 121330.  44526.]
2019-01-08 13:35:54,257 - 10 - corpus.py - subactivity_sampler - [ 83467.  19967. 123196.  10103. 128747.   7990. 122541.  43722.]
2019-01-08 13:35:54,258 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 565577.830 ms ~ 9.426 min ~ 565.578 sec
2019-01-08 13:35:54,258 - 10 - corpus.py - ordering_sampler - .
2019-01-08 13:35:57,334 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 13:35:57,334 - 10 - corpus.py - ordering_sampler - inv_count_vec: [  9.  27.   0.   0. 122.  23.   0.]
2019-01-08 13:35:57,334 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 13:35:57,358 - 10 - corpus.py - rho_sampling - ['52.5978', '27.1904', '51.5713', '36.6652', '1.3712', '48.0570', '41.5771']
2019-01-08 13:35:57,358 - 10 - pipeline.py - baseline - Iteration 3
2019-01-08 13:35:57,532 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 13:35:57,542 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 13:35:57,543 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 17', '1: 12', '2: 11', '3: 10', '4: 13', '5: 16', '6: 14', '7: 15']
2019-01-08 13:35:57,567 - 10 - accuracy_class.py - mof_val - frames true: 192500	frames overall : 539733
2019-01-08 13:35:57,568 - 10 - corpus.py - accuracy_corpus - Action: friedegg
2019-01-08 13:35:57,568 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3566578289635801
2019-01-08 13:35:57,568 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3566578289635801
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 17933
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - mof_classes - label 10: 0.000000  0 / 10971
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - mof_classes - label 11: 0.398343  27496 / 69026
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - mof_classes - label 12: 0.064343  2156 / 33508
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - mof_classes - label 13: 0.347403  110579 / 318302
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - mof_classes - label 14: 0.375564  9655 / 25708
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - mof_classes - label 15: 0.577628  18476 / 31986
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 2201
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - mof_classes - label 17: 0.801980  24138 / 30098
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - mof_classes - average class mof: 0.285029
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 17933
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - iou_classes - label 10: 0.000000  0 / 21074
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - iou_classes - label 11: 0.166920  27496 / 164726
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - iou_classes - label 12: 0.042012  2156 / 51319
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - iou_classes - label 13: 0.328644  110579 / 336470
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - iou_classes - label 14: 0.069664  9655 / 138594
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - iou_classes - label 15: 0.322826  18476 / 57232
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 10191
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - iou_classes - label 17: 0.269918  24138 / 89427
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - iou_classes - average IoU: 0.149998
2019-01-08 13:35:57,568 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.133332
2019-01-08 13:36:04,180 - 10 - f1_score.py - f1 - f1 score: 0.278279
2019-01-08 13:36:04,198 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 6840.235 ms ~ 0.114 min ~ 6.840 sec
2019-01-08 13:36:04,198 - 10 - corpus.py - embedding_training - .
2019-01-08 13:36:04,198 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 13:36:04,198 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 13:36:04,198 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 13:36:08,592 - 10 - training_embed.py - training - create model
2019-01-08 13:36:08,600 - 10 - training_embed.py - training - epochs: 12
2019-01-08 13:36:08,600 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 13:36:09,075 - 10 - training_embed.py - training - Epoch: [0][100/2109]	Time 0.003 (0.005)	Data 0.001 (0.003)	Loss 354.6004 (354.0731)	
2019-01-08 13:36:09,341 - 10 - training_embed.py - training - Epoch: [0][200/2109]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 356.5124 (353.8294)	
2019-01-08 13:36:09,586 - 10 - training_embed.py - training - Epoch: [0][300/2109]	Time 0.003 (0.003)	Data 0.002 (0.002)	Loss 352.8935 (353.7956)	
2019-01-08 13:36:09,856 - 10 - training_embed.py - training - Epoch: [0][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 349.6332 (353.7258)	
2019-01-08 13:36:10,139 - 10 - training_embed.py - training - Epoch: [0][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 352.2632 (353.7557)	
2019-01-08 13:36:10,382 - 10 - training_embed.py - training - Epoch: [0][600/2109]	Time 0.014 (0.003)	Data 0.001 (0.002)	Loss 347.6069 (353.8018)	
2019-01-08 13:36:10,681 - 10 - training_embed.py - training - Epoch: [0][700/2109]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 355.4137 (353.7698)	
2019-01-08 13:36:10,919 - 10 - training_embed.py - training - Epoch: [0][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 355.2548 (353.7611)	
2019-01-08 13:36:11,188 - 10 - training_embed.py - training - Epoch: [0][900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 362.7115 (353.7338)	
2019-01-08 13:36:11,467 - 10 - training_embed.py - training - Epoch: [0][1000/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 350.2905 (353.7339)	
2019-01-08 13:36:11,699 - 10 - training_embed.py - training - Epoch: [0][1100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.3597 (353.7320)	
2019-01-08 13:36:12,010 - 10 - training_embed.py - training - Epoch: [0][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.2227 (353.6949)	
2019-01-08 13:36:12,304 - 10 - training_embed.py - training - Epoch: [0][1300/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 354.0553 (353.6494)	
2019-01-08 13:36:12,550 - 10 - training_embed.py - training - Epoch: [0][1400/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 353.9604 (353.6557)	
2019-01-08 13:36:12,832 - 10 - training_embed.py - training - Epoch: [0][1500/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 356.3510 (353.6033)	
2019-01-08 13:36:13,125 - 10 - training_embed.py - training - Epoch: [0][1600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.3982 (353.5926)	
2019-01-08 13:36:13,350 - 10 - training_embed.py - training - Epoch: [0][1700/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 355.2657 (353.5695)	
2019-01-08 13:36:13,614 - 10 - training_embed.py - training - Epoch: [0][1800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.4816 (353.5649)	
2019-01-08 13:36:13,915 - 10 - training_embed.py - training - Epoch: [0][1900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.9882 (353.5362)	
2019-01-08 13:36:14,182 - 10 - training_embed.py - training - Epoch: [0][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.1284 (353.5232)	
2019-01-08 13:36:14,449 - 10 - training_embed.py - training - Epoch: [0][2100/2109]	Time 0.008 (0.003)	Data 0.006 (0.001)	Loss 356.6659 (353.5300)	
2019-01-08 13:36:14,471 - 10 - training_embed.py - training - loss: 353.492165
2019-01-08 13:36:14,471 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 13:36:14,938 - 10 - training_embed.py - training - Epoch: [1][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 351.0562 (353.0953)	
2019-01-08 13:36:15,235 - 10 - training_embed.py - training - Epoch: [1][200/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 355.2957 (352.8984)	
2019-01-08 13:36:15,457 - 10 - training_embed.py - training - Epoch: [1][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.8907 (353.0916)	
2019-01-08 13:36:15,743 - 10 - training_embed.py - training - Epoch: [1][400/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 355.0196 (353.0581)	
2019-01-08 13:36:16,031 - 10 - training_embed.py - training - Epoch: [1][500/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 351.4602 (353.0726)	
2019-01-08 13:36:16,285 - 10 - training_embed.py - training - Epoch: [1][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.0999 (353.1195)	
2019-01-08 13:36:16,526 - 10 - training_embed.py - training - Epoch: [1][700/2109]	Time 0.009 (0.003)	Data 0.001 (0.001)	Loss 354.3546 (353.1349)	
2019-01-08 13:36:16,797 - 10 - training_embed.py - training - Epoch: [1][800/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 351.5847 (353.1994)	
2019-01-08 13:36:17,078 - 10 - training_embed.py - training - Epoch: [1][900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.0158 (353.2085)	
2019-01-08 13:36:17,313 - 10 - training_embed.py - training - Epoch: [1][1000/2109]	Time 0.010 (0.003)	Data 0.001 (0.001)	Loss 352.3501 (353.2017)	
2019-01-08 13:36:17,601 - 10 - training_embed.py - training - Epoch: [1][1100/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 351.6193 (353.1957)	
2019-01-08 13:36:17,894 - 10 - training_embed.py - training - Epoch: [1][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.0877 (353.1659)	
2019-01-08 13:36:18,134 - 10 - training_embed.py - training - Epoch: [1][1300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.5961 (353.1633)	
2019-01-08 13:36:18,357 - 10 - training_embed.py - training - Epoch: [1][1400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.0319 (353.1449)	
2019-01-08 13:36:18,649 - 10 - training_embed.py - training - Epoch: [1][1500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.7200 (353.1433)	
2019-01-08 13:36:18,884 - 10 - training_embed.py - training - Epoch: [1][1600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.7164 (353.1493)	
2019-01-08 13:36:19,148 - 10 - training_embed.py - training - Epoch: [1][1700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.8043 (353.1262)	
2019-01-08 13:36:19,428 - 10 - training_embed.py - training - Epoch: [1][1800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.9588 (353.1277)	
2019-01-08 13:36:19,681 - 10 - training_embed.py - training - Epoch: [1][1900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.0877 (353.1119)	
2019-01-08 13:36:19,925 - 10 - training_embed.py - training - Epoch: [1][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.1759 (353.1058)	
2019-01-08 13:36:20,181 - 10 - training_embed.py - training - Epoch: [1][2100/2109]	Time 0.007 (0.003)	Data 0.005 (0.001)	Loss 355.3869 (353.0945)	
2019-01-08 13:36:20,196 - 10 - training_embed.py - training - loss: 353.055937
2019-01-08 13:36:20,196 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 13:36:20,757 - 10 - training_embed.py - training - Epoch: [2][100/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 352.2214 (352.6043)	
2019-01-08 13:36:21,059 - 10 - training_embed.py - training - Epoch: [2][200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.1659 (352.7445)	
2019-01-08 13:36:21,284 - 10 - training_embed.py - training - Epoch: [2][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.7138 (352.7449)	
2019-01-08 13:36:21,508 - 10 - training_embed.py - training - Epoch: [2][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.9764 (352.6841)	
2019-01-08 13:36:21,749 - 10 - training_embed.py - training - Epoch: [2][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.9599 (352.6262)	
2019-01-08 13:36:22,052 - 10 - training_embed.py - training - Epoch: [2][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.3267 (352.6370)	
2019-01-08 13:36:22,353 - 10 - training_embed.py - training - Epoch: [2][700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.5688 (352.6680)	
2019-01-08 13:36:22,595 - 10 - training_embed.py - training - Epoch: [2][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.3163 (352.6974)	
2019-01-08 13:36:22,826 - 10 - training_embed.py - training - Epoch: [2][900/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 356.5732 (352.6918)	
2019-01-08 13:36:23,046 - 10 - training_embed.py - training - Epoch: [2][1000/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 354.6269 (352.6588)	
2019-01-08 13:36:23,279 - 10 - training_embed.py - training - Epoch: [2][1100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.1776 (352.6906)	
2019-01-08 13:36:23,515 - 10 - training_embed.py - training - Epoch: [2][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.1815 (352.7116)	
2019-01-08 13:36:23,788 - 10 - training_embed.py - training - Epoch: [2][1300/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 353.9426 (352.7246)	
2019-01-08 13:36:24,021 - 10 - training_embed.py - training - Epoch: [2][1400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.3095 (352.7357)	
2019-01-08 13:36:24,318 - 10 - training_embed.py - training - Epoch: [2][1500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 356.0929 (352.7127)	
2019-01-08 13:36:24,620 - 10 - training_embed.py - training - Epoch: [2][1600/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 348.5695 (352.7067)	
2019-01-08 13:36:24,869 - 10 - training_embed.py - training - Epoch: [2][1700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.0179 (352.7207)	
2019-01-08 13:36:25,115 - 10 - training_embed.py - training - Epoch: [2][1800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.4590 (352.7195)	
2019-01-08 13:36:25,351 - 10 - training_embed.py - training - Epoch: [2][1900/2109]	Time 0.004 (0.003)	Data 0.001 (0.001)	Loss 352.6560 (352.7102)	
2019-01-08 13:36:25,577 - 10 - training_embed.py - training - Epoch: [2][2000/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 352.7717 (352.7035)	
2019-01-08 13:36:25,815 - 10 - training_embed.py - training - Epoch: [2][2100/2109]	Time 0.006 (0.003)	Data 0.005 (0.001)	Loss 351.0478 (352.6585)	
2019-01-08 13:36:25,830 - 10 - training_embed.py - training - loss: 352.619578
2019-01-08 13:36:25,830 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 13:36:26,252 - 10 - training_embed.py - training - Epoch: [3][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.1189 (352.3424)	
2019-01-08 13:36:26,532 - 10 - training_embed.py - training - Epoch: [3][200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.9031 (352.2396)	
2019-01-08 13:36:26,816 - 10 - training_embed.py - training - Epoch: [3][300/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 350.2253 (352.2744)	
2019-01-08 13:36:27,043 - 10 - training_embed.py - training - Epoch: [3][400/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 352.6188 (352.2691)	
2019-01-08 13:36:27,324 - 10 - training_embed.py - training - Epoch: [3][500/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 351.6390 (352.3234)	
2019-01-08 13:36:27,592 - 10 - training_embed.py - training - Epoch: [3][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.9128 (352.3410)	
2019-01-08 13:36:27,831 - 10 - training_embed.py - training - Epoch: [3][700/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 349.5347 (352.3177)	
2019-01-08 13:36:28,126 - 10 - training_embed.py - training - Epoch: [3][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.6469 (352.3338)	
2019-01-08 13:36:28,437 - 10 - training_embed.py - training - Epoch: [3][900/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 351.0060 (352.3080)	
2019-01-08 13:36:28,708 - 10 - training_embed.py - training - Epoch: [3][1000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.5525 (352.3095)	
2019-01-08 13:36:28,961 - 10 - training_embed.py - training - Epoch: [3][1100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.3036 (352.3125)	
2019-01-08 13:36:29,215 - 10 - training_embed.py - training - Epoch: [3][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.1862 (352.3169)	
2019-01-08 13:36:29,459 - 10 - training_embed.py - training - Epoch: [3][1300/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 350.4955 (352.3120)	
2019-01-08 13:36:29,714 - 10 - training_embed.py - training - Epoch: [3][1400/2109]	Time 0.006 (0.003)	Data 0.002 (0.001)	Loss 347.9788 (352.3208)	
2019-01-08 13:36:29,932 - 10 - training_embed.py - training - Epoch: [3][1500/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 355.9953 (352.2853)	
2019-01-08 13:36:30,215 - 10 - training_embed.py - training - Epoch: [3][1600/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 349.9181 (352.2603)	
2019-01-08 13:36:30,443 - 10 - training_embed.py - training - Epoch: [3][1700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.5922 (352.2548)	
2019-01-08 13:36:30,728 - 10 - training_embed.py - training - Epoch: [3][1800/2109]	Time 0.005 (0.003)	Data 0.002 (0.001)	Loss 354.2001 (352.2416)	
2019-01-08 13:36:31,033 - 10 - training_embed.py - training - Epoch: [3][1900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.6781 (352.2330)	
2019-01-08 13:36:31,290 - 10 - training_embed.py - training - Epoch: [3][2000/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 357.3539 (352.2256)	
2019-01-08 13:36:31,515 - 10 - training_embed.py - training - Epoch: [3][2100/2109]	Time 0.006 (0.003)	Data 0.004 (0.001)	Loss 351.2179 (352.2158)	
2019-01-08 13:36:31,531 - 10 - training_embed.py - training - loss: 352.180481
2019-01-08 13:36:31,531 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 13:36:32,009 - 10 - training_embed.py - training - Epoch: [4][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.7076 (352.2357)	
2019-01-08 13:36:32,265 - 10 - training_embed.py - training - Epoch: [4][200/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 352.5179 (351.9629)	
2019-01-08 13:36:32,520 - 10 - training_embed.py - training - Epoch: [4][300/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 350.1406 (351.8626)	
2019-01-08 13:36:32,799 - 10 - training_embed.py - training - Epoch: [4][400/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 352.4062 (351.8794)	
2019-01-08 13:36:33,026 - 10 - training_embed.py - training - Epoch: [4][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.0208 (351.8663)	
2019-01-08 13:36:33,234 - 10 - training_embed.py - training - Epoch: [4][600/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 351.7367 (351.9042)	
2019-01-08 13:36:33,461 - 10 - training_embed.py - training - Epoch: [4][700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.2022 (351.9298)	
2019-01-08 13:36:33,676 - 10 - training_embed.py - training - Epoch: [4][800/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 347.1130 (351.9539)	
2019-01-08 13:36:33,925 - 10 - training_embed.py - training - Epoch: [4][900/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 350.8278 (351.8906)	
2019-01-08 13:36:34,252 - 10 - training_embed.py - training - Epoch: [4][1000/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 354.7931 (351.9073)	
2019-01-08 13:36:34,524 - 10 - training_embed.py - training - Epoch: [4][1100/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 355.5314 (351.8477)	
2019-01-08 13:36:34,829 - 10 - training_embed.py - training - Epoch: [4][1200/2109]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 350.4019 (351.8662)	
2019-01-08 13:36:35,094 - 10 - training_embed.py - training - Epoch: [4][1300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.9546 (351.8244)	
2019-01-08 13:36:35,428 - 10 - training_embed.py - training - Epoch: [4][1400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.6526 (351.7915)	
2019-01-08 13:36:35,710 - 10 - training_embed.py - training - Epoch: [4][1500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.6343 (351.7907)	
2019-01-08 13:36:35,930 - 10 - training_embed.py - training - Epoch: [4][1600/2109]	Time 0.005 (0.003)	Data 0.003 (0.001)	Loss 351.1479 (351.7716)	
2019-01-08 13:36:36,148 - 10 - training_embed.py - training - Epoch: [4][1700/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 351.2872 (351.7574)	
2019-01-08 13:36:36,373 - 10 - training_embed.py - training - Epoch: [4][1800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.5000 (351.7544)	
2019-01-08 13:36:36,591 - 10 - training_embed.py - training - Epoch: [4][1900/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 351.8801 (351.7687)	
2019-01-08 13:36:36,809 - 10 - training_embed.py - training - Epoch: [4][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 356.5455 (351.7844)	
2019-01-08 13:36:37,033 - 10 - training_embed.py - training - Epoch: [4][2100/2109]	Time 0.006 (0.003)	Data 0.005 (0.001)	Loss 356.7036 (351.7748)	
2019-01-08 13:36:37,048 - 10 - training_embed.py - training - loss: 351.741336
2019-01-08 13:36:37,049 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 13:36:37,494 - 10 - training_embed.py - training - Epoch: [5][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.5947 (351.6332)	
2019-01-08 13:36:37,716 - 10 - training_embed.py - training - Epoch: [5][200/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 356.2476 (351.4971)	
2019-01-08 13:36:37,936 - 10 - training_embed.py - training - Epoch: [5][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.0826 (351.3824)	
2019-01-08 13:36:38,179 - 10 - training_embed.py - training - Epoch: [5][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.8869 (351.3574)	
2019-01-08 13:36:38,401 - 10 - training_embed.py - training - Epoch: [5][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.6906 (351.3724)	
2019-01-08 13:36:38,616 - 10 - training_embed.py - training - Epoch: [5][600/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 350.6163 (351.3720)	
2019-01-08 13:36:38,845 - 10 - training_embed.py - training - Epoch: [5][700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.5251 (351.3877)	
2019-01-08 13:36:39,067 - 10 - training_embed.py - training - Epoch: [5][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.4453 (351.4161)	
2019-01-08 13:36:39,283 - 10 - training_embed.py - training - Epoch: [5][900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.3789 (351.4100)	
2019-01-08 13:36:39,501 - 10 - training_embed.py - training - Epoch: [5][1000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.4871 (351.3730)	
2019-01-08 13:36:39,727 - 10 - training_embed.py - training - Epoch: [5][1100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.4379 (351.3510)	
2019-01-08 13:36:39,960 - 10 - training_embed.py - training - Epoch: [5][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.8580 (351.3781)	
2019-01-08 13:36:40,187 - 10 - training_embed.py - training - Epoch: [5][1300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.1442 (351.4098)	
2019-01-08 13:36:40,428 - 10 - training_embed.py - training - Epoch: [5][1400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.9566 (351.4083)	
2019-01-08 13:36:40,666 - 10 - training_embed.py - training - Epoch: [5][1500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.2775 (351.3869)	
2019-01-08 13:36:40,906 - 10 - training_embed.py - training - Epoch: [5][1600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.7464 (351.3529)	
2019-01-08 13:36:41,124 - 10 - training_embed.py - training - Epoch: [5][1700/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 352.4284 (351.3480)	
2019-01-08 13:36:41,370 - 10 - training_embed.py - training - Epoch: [5][1800/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 348.1526 (351.3419)	
2019-01-08 13:36:41,617 - 10 - training_embed.py - training - Epoch: [5][1900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.2690 (351.3337)	
2019-01-08 13:36:41,839 - 10 - training_embed.py - training - Epoch: [5][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 346.2256 (351.3313)	
2019-01-08 13:36:42,071 - 10 - training_embed.py - training - Epoch: [5][2100/2109]	Time 0.007 (0.003)	Data 0.006 (0.001)	Loss 345.9248 (351.3367)	
2019-01-08 13:36:42,088 - 10 - training_embed.py - training - loss: 351.299027
2019-01-08 13:36:42,088 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 13:36:42,517 - 10 - training_embed.py - training - Epoch: [6][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.3952 (351.1844)	
2019-01-08 13:36:42,728 - 10 - training_embed.py - training - Epoch: [6][200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.4178 (351.0172)	
2019-01-08 13:36:42,955 - 10 - training_embed.py - training - Epoch: [6][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.8192 (351.1260)	
2019-01-08 13:36:43,171 - 10 - training_embed.py - training - Epoch: [6][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.5345 (351.2553)	
2019-01-08 13:36:43,396 - 10 - training_embed.py - training - Epoch: [6][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.7399 (351.1392)	
2019-01-08 13:36:43,618 - 10 - training_embed.py - training - Epoch: [6][600/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 349.5182 (351.1234)	
2019-01-08 13:36:43,841 - 10 - training_embed.py - training - Epoch: [6][700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.5923 (351.1007)	
2019-01-08 13:36:44,066 - 10 - training_embed.py - training - Epoch: [6][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.7503 (351.0656)	
2019-01-08 13:36:44,272 - 10 - training_embed.py - training - Epoch: [6][900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.9352 (351.0729)	
2019-01-08 13:36:44,495 - 10 - training_embed.py - training - Epoch: [6][1000/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 350.9766 (351.0396)	
2019-01-08 13:36:44,722 - 10 - training_embed.py - training - Epoch: [6][1100/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 347.4353 (351.0333)	
2019-01-08 13:36:44,954 - 10 - training_embed.py - training - Epoch: [6][1200/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 349.2943 (351.0018)	
2019-01-08 13:36:45,195 - 10 - training_embed.py - training - Epoch: [6][1300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.5267 (351.0095)	
2019-01-08 13:36:45,433 - 10 - training_embed.py - training - Epoch: [6][1400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.1648 (351.0001)	
2019-01-08 13:36:45,654 - 10 - training_embed.py - training - Epoch: [6][1500/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 350.9605 (350.9932)	
2019-01-08 13:36:45,864 - 10 - training_embed.py - training - Epoch: [6][1600/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 355.3716 (350.9933)	
2019-01-08 13:36:46,090 - 10 - training_embed.py - training - Epoch: [6][1700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.0421 (350.9801)	
2019-01-08 13:36:46,307 - 10 - training_embed.py - training - Epoch: [6][1800/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 352.9345 (350.9581)	
2019-01-08 13:36:46,537 - 10 - training_embed.py - training - Epoch: [6][1900/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 351.0383 (350.9333)	
2019-01-08 13:36:46,770 - 10 - training_embed.py - training - Epoch: [6][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.6197 (350.9261)	
2019-01-08 13:36:46,999 - 10 - training_embed.py - training - Epoch: [6][2100/2109]	Time 0.008 (0.003)	Data 0.007 (0.001)	Loss 345.7784 (350.8931)	
2019-01-08 13:36:47,014 - 10 - training_embed.py - training - loss: 350.857313
2019-01-08 13:36:47,014 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 13:36:47,501 - 10 - training_embed.py - training - Epoch: [7][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.3910 (350.4205)	
2019-01-08 13:36:47,720 - 10 - training_embed.py - training - Epoch: [7][200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 345.7508 (350.4259)	
2019-01-08 13:36:47,939 - 10 - training_embed.py - training - Epoch: [7][300/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 353.0779 (350.3852)	
2019-01-08 13:36:48,160 - 10 - training_embed.py - training - Epoch: [7][400/2109]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 351.2019 (350.4483)	
2019-01-08 13:36:48,385 - 10 - training_embed.py - training - Epoch: [7][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.4884 (350.4402)	
2019-01-08 13:36:48,606 - 10 - training_embed.py - training - Epoch: [7][600/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 351.3612 (350.4732)	
2019-01-08 13:36:48,833 - 10 - training_embed.py - training - Epoch: [7][700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.9564 (350.4927)	
2019-01-08 13:36:49,054 - 10 - training_embed.py - training - Epoch: [7][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.8798 (350.4867)	
2019-01-08 13:36:49,261 - 10 - training_embed.py - training - Epoch: [7][900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.2799 (350.4685)	
2019-01-08 13:36:49,476 - 10 - training_embed.py - training - Epoch: [7][1000/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 349.6361 (350.4783)	
2019-01-08 13:36:49,710 - 10 - training_embed.py - training - Epoch: [7][1100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.9678 (350.4703)	
2019-01-08 13:36:49,946 - 10 - training_embed.py - training - Epoch: [7][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.7357 (350.4816)	
2019-01-08 13:36:50,181 - 10 - training_embed.py - training - Epoch: [7][1300/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 347.1228 (350.4666)	
2019-01-08 13:36:50,404 - 10 - training_embed.py - training - Epoch: [7][1400/2109]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 348.5036 (350.4756)	
2019-01-08 13:36:50,624 - 10 - training_embed.py - training - Epoch: [7][1500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.4229 (350.5031)	
2019-01-08 13:36:50,846 - 10 - training_embed.py - training - Epoch: [7][1600/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 353.3518 (350.4913)	
2019-01-08 13:36:51,075 - 10 - training_embed.py - training - Epoch: [7][1700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.1101 (350.4787)	
2019-01-08 13:36:51,304 - 10 - training_embed.py - training - Epoch: [7][1800/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 349.1035 (350.4764)	
2019-01-08 13:36:51,531 - 10 - training_embed.py - training - Epoch: [7][1900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.5543 (350.4851)	
2019-01-08 13:36:51,765 - 10 - training_embed.py - training - Epoch: [7][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.0067 (350.4725)	
2019-01-08 13:36:51,971 - 10 - training_embed.py - training - Epoch: [7][2100/2109]	Time 0.005 (0.003)	Data 0.004 (0.001)	Loss 347.6800 (350.4528)	
2019-01-08 13:36:51,988 - 10 - training_embed.py - training - loss: 350.411437
2019-01-08 13:36:51,988 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 13:36:52,422 - 10 - training_embed.py - training - Epoch: [8][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.1164 (350.1511)	
2019-01-08 13:36:52,653 - 10 - training_embed.py - training - Epoch: [8][200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.3682 (350.3085)	
2019-01-08 13:36:52,873 - 10 - training_embed.py - training - Epoch: [8][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.3731 (350.3207)	
2019-01-08 13:36:53,090 - 10 - training_embed.py - training - Epoch: [8][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.9309 (350.2385)	
2019-01-08 13:36:53,311 - 10 - training_embed.py - training - Epoch: [8][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 346.4697 (350.1648)	
2019-01-08 13:36:53,538 - 10 - training_embed.py - training - Epoch: [8][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.1837 (350.1146)	
2019-01-08 13:36:53,752 - 10 - training_embed.py - training - Epoch: [8][700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.8185 (350.1093)	
2019-01-08 13:36:53,978 - 10 - training_embed.py - training - Epoch: [8][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.2065 (350.1506)	
2019-01-08 13:36:54,207 - 10 - training_embed.py - training - Epoch: [8][900/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 350.2173 (350.1480)	
2019-01-08 13:36:54,434 - 10 - training_embed.py - training - Epoch: [8][1000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 346.5298 (350.1493)	
2019-01-08 13:36:54,671 - 10 - training_embed.py - training - Epoch: [8][1100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.3232 (350.1355)	
2019-01-08 13:36:54,903 - 10 - training_embed.py - training - Epoch: [8][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.0407 (350.0905)	
2019-01-08 13:36:55,128 - 10 - training_embed.py - training - Epoch: [8][1300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.5439 (350.0826)	
2019-01-08 13:36:55,354 - 10 - training_embed.py - training - Epoch: [8][1400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.1914 (350.0542)	
2019-01-08 13:36:55,588 - 10 - training_embed.py - training - Epoch: [8][1500/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 348.5143 (350.0602)	
2019-01-08 13:36:55,811 - 10 - training_embed.py - training - Epoch: [8][1600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.0434 (350.0377)	
2019-01-08 13:36:56,031 - 10 - training_embed.py - training - Epoch: [8][1700/2109]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 349.0062 (350.0222)	
2019-01-08 13:36:56,250 - 10 - training_embed.py - training - Epoch: [8][1800/2109]	Time 0.004 (0.003)	Data 0.001 (0.001)	Loss 346.5550 (350.0287)	
2019-01-08 13:36:56,481 - 10 - training_embed.py - training - Epoch: [8][1900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.0591 (349.9990)	
2019-01-08 13:36:56,710 - 10 - training_embed.py - training - Epoch: [8][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.5930 (350.0059)	
2019-01-08 13:36:56,960 - 10 - training_embed.py - training - Epoch: [8][2100/2109]	Time 0.006 (0.003)	Data 0.005 (0.001)	Loss 350.8380 (349.9995)	
2019-01-08 13:36:56,975 - 10 - training_embed.py - training - loss: 349.964669
2019-01-08 13:36:56,975 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 13:36:57,455 - 10 - training_embed.py - training - Epoch: [9][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 346.2476 (349.8371)	
2019-01-08 13:36:57,697 - 10 - training_embed.py - training - Epoch: [9][200/2109]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 347.0726 (349.6791)	
2019-01-08 13:36:57,914 - 10 - training_embed.py - training - Epoch: [9][300/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 355.8803 (349.6974)	
2019-01-08 13:36:58,145 - 10 - training_embed.py - training - Epoch: [9][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.0444 (349.7339)	
2019-01-08 13:36:58,367 - 10 - training_embed.py - training - Epoch: [9][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.8599 (349.7671)	
2019-01-08 13:36:58,594 - 10 - training_embed.py - training - Epoch: [9][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.9834 (349.7806)	
2019-01-08 13:36:58,808 - 10 - training_embed.py - training - Epoch: [9][700/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 350.0638 (349.7527)	
2019-01-08 13:36:59,036 - 10 - training_embed.py - training - Epoch: [9][800/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 351.9334 (349.7471)	
2019-01-08 13:36:59,256 - 10 - training_embed.py - training - Epoch: [9][900/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 347.9261 (349.7378)	
2019-01-08 13:36:59,482 - 10 - training_embed.py - training - Epoch: [9][1000/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 348.5694 (349.6873)	
2019-01-08 13:36:59,701 - 10 - training_embed.py - training - Epoch: [9][1100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.3657 (349.7027)	
2019-01-08 13:36:59,914 - 10 - training_embed.py - training - Epoch: [9][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.4311 (349.6422)	
2019-01-08 13:37:00,157 - 10 - training_embed.py - training - Epoch: [9][1300/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 346.8384 (349.6235)	
2019-01-08 13:37:00,396 - 10 - training_embed.py - training - Epoch: [9][1400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 353.0935 (349.6188)	
2019-01-08 13:37:00,616 - 10 - training_embed.py - training - Epoch: [9][1500/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 347.8922 (349.6148)	
2019-01-08 13:37:00,823 - 10 - training_embed.py - training - Epoch: [9][1600/2109]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 347.0811 (349.6244)	
2019-01-08 13:37:01,050 - 10 - training_embed.py - training - Epoch: [9][1700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.9720 (349.6154)	
2019-01-08 13:37:01,276 - 10 - training_embed.py - training - Epoch: [9][1800/2109]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 349.4666 (349.5928)	
2019-01-08 13:37:01,493 - 10 - training_embed.py - training - Epoch: [9][1900/2109]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 350.3005 (349.5925)	
2019-01-08 13:37:01,726 - 10 - training_embed.py - training - Epoch: [9][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 351.8416 (349.5720)	
2019-01-08 13:37:01,949 - 10 - training_embed.py - training - Epoch: [9][2100/2109]	Time 0.007 (0.003)	Data 0.005 (0.001)	Loss 353.6494 (349.5540)	
2019-01-08 13:37:01,964 - 10 - training_embed.py - training - loss: 349.515286
2019-01-08 13:37:01,964 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 13:37:02,403 - 10 - training_embed.py - training - Epoch: [10][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.0338 (349.2740)	
2019-01-08 13:37:02,639 - 10 - training_embed.py - training - Epoch: [10][200/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 351.5073 (349.3388)	
2019-01-08 13:37:02,868 - 10 - training_embed.py - training - Epoch: [10][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.3150 (349.4010)	
2019-01-08 13:37:03,103 - 10 - training_embed.py - training - Epoch: [10][400/2109]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 349.1022 (349.4249)	
2019-01-08 13:37:03,309 - 10 - training_embed.py - training - Epoch: [10][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 345.4891 (349.3528)	
2019-01-08 13:37:03,531 - 10 - training_embed.py - training - Epoch: [10][600/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 348.7032 (349.3569)	
2019-01-08 13:37:03,759 - 10 - training_embed.py - training - Epoch: [10][700/2109]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 350.4236 (349.2574)	
2019-01-08 13:37:03,974 - 10 - training_embed.py - training - Epoch: [10][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 346.5530 (349.2437)	
2019-01-08 13:37:04,189 - 10 - training_embed.py - training - Epoch: [10][900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 355.4463 (349.2682)	
2019-01-08 13:37:04,422 - 10 - training_embed.py - training - Epoch: [10][1000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.4817 (349.2305)	
2019-01-08 13:37:04,659 - 10 - training_embed.py - training - Epoch: [10][1100/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 346.0077 (349.1839)	
2019-01-08 13:37:04,878 - 10 - training_embed.py - training - Epoch: [10][1200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 346.5829 (349.1890)	
2019-01-08 13:37:05,113 - 10 - training_embed.py - training - Epoch: [10][1300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 346.2595 (349.1904)	
2019-01-08 13:37:05,332 - 10 - training_embed.py - training - Epoch: [10][1400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.1228 (349.1838)	
2019-01-08 13:37:05,556 - 10 - training_embed.py - training - Epoch: [10][1500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.9449 (349.1481)	
2019-01-08 13:37:05,782 - 10 - training_embed.py - training - Epoch: [10][1600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.8983 (349.1512)	
2019-01-08 13:37:06,020 - 10 - training_embed.py - training - Epoch: [10][1700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 352.4040 (349.1653)	
2019-01-08 13:37:06,254 - 10 - training_embed.py - training - Epoch: [10][1800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 346.2163 (349.1470)	
2019-01-08 13:37:06,506 - 10 - training_embed.py - training - Epoch: [10][1900/2109]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 346.8230 (349.1246)	
2019-01-08 13:37:06,779 - 10 - training_embed.py - training - Epoch: [10][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.7014 (349.1110)	
2019-01-08 13:37:07,026 - 10 - training_embed.py - training - Epoch: [10][2100/2109]	Time 0.006 (0.003)	Data 0.004 (0.001)	Loss 347.6395 (349.1014)	
2019-01-08 13:37:07,043 - 10 - training_embed.py - training - loss: 349.062790
2019-01-08 13:37:07,044 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 13:37:07,511 - 10 - training_embed.py - training - Epoch: [11][100/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.1478 (348.8521)	
2019-01-08 13:37:07,754 - 10 - training_embed.py - training - Epoch: [11][200/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.0532 (348.9574)	
2019-01-08 13:37:07,974 - 10 - training_embed.py - training - Epoch: [11][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 345.5518 (348.9792)	
2019-01-08 13:37:08,240 - 10 - training_embed.py - training - Epoch: [11][400/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 350.2702 (348.9018)	
2019-01-08 13:37:08,458 - 10 - training_embed.py - training - Epoch: [11][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 346.6755 (348.9766)	
2019-01-08 13:37:08,679 - 10 - training_embed.py - training - Epoch: [11][600/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 350.1722 (348.9145)	
2019-01-08 13:37:08,901 - 10 - training_embed.py - training - Epoch: [11][700/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.1786 (348.8982)	
2019-01-08 13:37:09,135 - 10 - training_embed.py - training - Epoch: [11][800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.1270 (348.8403)	
2019-01-08 13:37:09,367 - 10 - training_embed.py - training - Epoch: [11][900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.9659 (348.8413)	
2019-01-08 13:37:09,582 - 10 - training_embed.py - training - Epoch: [11][1000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.2595 (348.7963)	
2019-01-08 13:37:09,809 - 10 - training_embed.py - training - Epoch: [11][1100/2109]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 345.3832 (348.7297)	
2019-01-08 13:37:10,027 - 10 - training_embed.py - training - Epoch: [11][1200/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 351.3101 (348.7138)	
2019-01-08 13:37:10,257 - 10 - training_embed.py - training - Epoch: [11][1300/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 345.4429 (348.6868)	
2019-01-08 13:37:10,487 - 10 - training_embed.py - training - Epoch: [11][1400/2109]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 349.3703 (348.6687)	
2019-01-08 13:37:10,780 - 10 - training_embed.py - training - Epoch: [11][1500/2109]	Time 0.002 (0.003)	Data 0.000 (0.001)	Loss 352.1511 (348.6616)	
2019-01-08 13:37:10,991 - 10 - training_embed.py - training - Epoch: [11][1600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 350.1807 (348.6453)	
2019-01-08 13:37:11,198 - 10 - training_embed.py - training - Epoch: [11][1700/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 350.4790 (348.6451)	
2019-01-08 13:37:11,427 - 10 - training_embed.py - training - Epoch: [11][1800/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.9273 (348.6456)	
2019-01-08 13:37:11,654 - 10 - training_embed.py - training - Epoch: [11][1900/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 347.5045 (348.6640)	
2019-01-08 13:37:11,871 - 10 - training_embed.py - training - Epoch: [11][2000/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 345.4518 (348.6564)	
2019-01-08 13:37:12,084 - 10 - training_embed.py - training - Epoch: [11][2100/2109]	Time 0.007 (0.003)	Data 0.006 (0.001)	Loss 347.2155 (348.6477)	
2019-01-08 13:37:12,098 - 10 - training_embed.py - training - loss: 348.608507
2019-01-08 13:37:12,179 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 13:37:13,188 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1008.217 ms ~ 0.017 min ~ 1.008 sec
2019-01-08 13:37:14,066 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1887.108 ms ~ 0.031 min ~ 1.887 sec
2019-01-08 13:37:14,066 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 13:37:14,067 - 10 - corpus.py - subactivity_sampler - 0 / 173
2019-01-08 13:37:14,067 - 10 - corpus.py - subactivity_sampler - [ 83467.  19967. 123196.  10103. 128747.   7990. 122541.  43722.]
2019-01-08 13:38:38,782 - 10 - corpus.py - subactivity_sampler - 20 / 173
2019-01-08 13:38:38,782 - 10 - corpus.py - subactivity_sampler - [ 83554.  19152. 124392.   9373. 129076.   7488. 123575.  43123.]
2019-01-08 13:39:35,650 - 10 - corpus.py - subactivity_sampler - 40 / 173
2019-01-08 13:39:35,650 - 10 - corpus.py - subactivity_sampler - [ 83730.  18419. 125086.   9094. 129303.   7165. 124338.  42598.]
2019-01-08 13:40:47,506 - 10 - corpus.py - subactivity_sampler - 60 / 173
2019-01-08 13:40:47,506 - 10 - corpus.py - subactivity_sampler - [ 83719.  17596. 126621.   8176. 129599.   6651. 125368.  42003.]
2019-01-08 13:41:59,077 - 10 - corpus.py - subactivity_sampler - 80 / 173
2019-01-08 13:41:59,077 - 10 - corpus.py - subactivity_sampler - [ 83666.  16797. 127698.   7711. 129562.   6094. 128939.  39266.]
2019-01-08 13:43:16,137 - 10 - corpus.py - subactivity_sampler - 100 / 173
2019-01-08 13:43:16,137 - 10 - corpus.py - subactivity_sampler - [ 83541.  16189. 128715.   7529. 129288.   5618. 130627.  38226.]
2019-01-08 13:44:00,680 - 10 - corpus.py - subactivity_sampler - 120 / 173
2019-01-08 13:44:00,681 - 10 - corpus.py - subactivity_sampler - [ 83605.  15544. 129788.   6926. 129154.   5291. 131818.  37607.]
2019-01-08 13:44:53,889 - 10 - corpus.py - subactivity_sampler - 140 / 173
2019-01-08 13:44:53,890 - 10 - corpus.py - subactivity_sampler - [ 83545.  14920. 130421.   6643. 129212.   4903. 133013.  37076.]
2019-01-08 13:45:59,585 - 10 - corpus.py - subactivity_sampler - 160 / 173
2019-01-08 13:45:59,585 - 10 - corpus.py - subactivity_sampler - [ 83262.  14342. 130890.   6497. 130274.   4704. 133701.  36063.]
2019-01-08 13:46:35,722 - 10 - corpus.py - subactivity_sampler - [ 83278.  14070. 131184.   6349. 130397.   4575. 133940.  35940.]
2019-01-08 13:46:35,722 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 561655.962 ms ~ 9.361 min ~ 561.656 sec
2019-01-08 13:46:35,723 - 10 - corpus.py - ordering_sampler - .
2019-01-08 13:46:38,616 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 13:46:38,616 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 13.   8.  30.   2. 103.   7.   4.]
2019-01-08 13:46:38,616 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 13:46:38,634 - 10 - corpus.py - rho_sampling - ['48.1054', '10.4458', '610.9059', '82.8764', '2.4649', '315.0575', '5.2371']
2019-01-08 13:46:38,634 - 10 - pipeline.py - baseline - Iteration 4
2019-01-08 13:46:38,825 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 13:46:38,839 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 13:46:38,839 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 17', '1: 12', '2: 11', '3: 10', '4: 13', '5: 16', '6: 14', '7: 15']
2019-01-08 13:46:38,863 - 10 - accuracy_class.py - mof_val - frames true: 187476	frames overall : 539733
2019-01-08 13:46:38,864 - 10 - corpus.py - accuracy_corpus - Action: friedegg
2019-01-08 13:46:38,864 - 10 - corpus.py - accuracy_corpus - MoF val: 0.34734952281961634
2019-01-08 13:46:38,864 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.34734952281961634
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 17933
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - mof_classes - label 10: 0.000000  0 / 10971
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - mof_classes - label 11: 0.415162  28657 / 69026
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - mof_classes - label 12: 0.046526  1559 / 33508
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - mof_classes - label 13: 0.345480  109967 / 318302
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - mof_classes - label 14: 0.341450  8778 / 25708
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - mof_classes - label 15: 0.455731  14577 / 31986
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 2201
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - mof_classes - label 17: 0.795335  23938 / 30098
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - mof_classes - average class mof: 0.266632
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 17933
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - iou_classes - label 10: 0.000000  0 / 17320
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - iou_classes - label 11: 0.167045  28657 / 171553
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - iou_classes - label 12: 0.033877  1559 / 46019
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - iou_classes - label 13: 0.324643  109967 / 338732
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - iou_classes - label 14: 0.058183  8778 / 150870
2019-01-08 13:46:38,864 - 10 - accuracy_class.py - iou_classes - label 15: 0.273238  14577 / 53349
2019-01-08 13:46:38,865 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 6776
2019-01-08 13:46:38,865 - 10 - accuracy_class.py - iou_classes - label 17: 0.267649  23938 / 89438
2019-01-08 13:46:38,865 - 10 - accuracy_class.py - iou_classes - average IoU: 0.140579
2019-01-08 13:46:38,865 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.124959
2019-01-08 13:46:45,179 - 10 - f1_score.py - f1 - f1 score: 0.259890
2019-01-08 13:46:45,211 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 6577.451 ms ~ 0.110 min ~ 6.577 sec
2019-01-08 13:46:45,212 - 10 - corpus.py - embedding_training - .
2019-01-08 13:46:45,212 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 13:46:45,212 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 13:46:45,212 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 13:46:49,172 - 10 - training_embed.py - training - create model
2019-01-08 13:46:49,172 - 10 - training_embed.py - training - epochs: 12
2019-01-08 13:46:49,172 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 13:46:49,543 - 10 - training_embed.py - training - Epoch: [0][100/2109]	Time 0.002 (0.004)	Data 0.001 (0.003)	Loss 354.9513 (353.2826)	
2019-01-08 13:46:49,759 - 10 - training_embed.py - training - Epoch: [0][200/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 354.9884 (353.1548)	
2019-01-08 13:46:49,975 - 10 - training_embed.py - training - Epoch: [0][300/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 349.2260 (353.1989)	
2019-01-08 13:46:50,204 - 10 - training_embed.py - training - Epoch: [0][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 349.1319 (353.1899)	
2019-01-08 13:46:50,427 - 10 - training_embed.py - training - Epoch: [0][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.3365 (353.1758)	
2019-01-08 13:46:50,645 - 10 - training_embed.py - training - Epoch: [0][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.5837 (353.2216)	
2019-01-08 13:46:50,875 - 10 - training_embed.py - training - Epoch: [0][700/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.1049 (353.1578)	
2019-01-08 13:46:51,084 - 10 - training_embed.py - training - Epoch: [0][800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.9715 (353.1645)	
2019-01-08 13:46:51,319 - 10 - training_embed.py - training - Epoch: [0][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 362.5129 (353.1428)	
2019-01-08 13:46:51,545 - 10 - training_embed.py - training - Epoch: [0][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.5793 (353.1388)	
2019-01-08 13:46:51,767 - 10 - training_embed.py - training - Epoch: [0][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.7004 (353.1380)	
2019-01-08 13:46:51,978 - 10 - training_embed.py - training - Epoch: [0][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.6392 (353.0941)	
2019-01-08 13:46:52,184 - 10 - training_embed.py - training - Epoch: [0][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.7077 (353.0521)	
2019-01-08 13:46:52,404 - 10 - training_embed.py - training - Epoch: [0][1400/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.0254 (353.0525)	
2019-01-08 13:46:52,650 - 10 - training_embed.py - training - Epoch: [0][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.2614 (353.0058)	
2019-01-08 13:46:52,884 - 10 - training_embed.py - training - Epoch: [0][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.0717 (353.0010)	
2019-01-08 13:46:53,097 - 10 - training_embed.py - training - Epoch: [0][1700/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 355.8755 (352.9742)	
2019-01-08 13:46:53,314 - 10 - training_embed.py - training - Epoch: [0][1800/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 353.1656 (352.9714)	
2019-01-08 13:46:53,535 - 10 - training_embed.py - training - Epoch: [0][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.6959 (352.9476)	
2019-01-08 13:46:53,774 - 10 - training_embed.py - training - Epoch: [0][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.2781 (352.9263)	
2019-01-08 13:46:53,992 - 10 - training_embed.py - training - Epoch: [0][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 356.8256 (352.9327)	
2019-01-08 13:46:54,005 - 10 - training_embed.py - training - loss: 352.893435
2019-01-08 13:46:54,006 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 13:46:54,449 - 10 - training_embed.py - training - Epoch: [1][100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 352.2532 (352.6117)	
2019-01-08 13:46:54,671 - 10 - training_embed.py - training - Epoch: [1][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.1135 (352.4628)	
2019-01-08 13:46:54,897 - 10 - training_embed.py - training - Epoch: [1][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.3868 (352.5581)	
2019-01-08 13:46:55,112 - 10 - training_embed.py - training - Epoch: [1][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.2939 (352.4881)	
2019-01-08 13:46:55,344 - 10 - training_embed.py - training - Epoch: [1][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.2376 (352.4820)	
2019-01-08 13:46:55,567 - 10 - training_embed.py - training - Epoch: [1][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.4536 (352.5189)	
2019-01-08 13:46:55,771 - 10 - training_embed.py - training - Epoch: [1][700/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.0749 (352.5082)	
2019-01-08 13:46:55,999 - 10 - training_embed.py - training - Epoch: [1][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.5036 (352.5645)	
2019-01-08 13:46:56,224 - 10 - training_embed.py - training - Epoch: [1][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.7002 (352.5705)	
2019-01-08 13:46:56,455 - 10 - training_embed.py - training - Epoch: [1][1000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 351.0321 (352.5391)	
2019-01-08 13:46:56,694 - 10 - training_embed.py - training - Epoch: [1][1100/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.5141 (352.5453)	
2019-01-08 13:46:56,918 - 10 - training_embed.py - training - Epoch: [1][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.0256 (352.5039)	
2019-01-08 13:46:57,133 - 10 - training_embed.py - training - Epoch: [1][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.5402 (352.4953)	
2019-01-08 13:46:57,367 - 10 - training_embed.py - training - Epoch: [1][1400/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.0194 (352.4613)	
2019-01-08 13:46:57,594 - 10 - training_embed.py - training - Epoch: [1][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.3279 (352.4665)	
2019-01-08 13:46:57,832 - 10 - training_embed.py - training - Epoch: [1][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.0538 (352.4756)	
2019-01-08 13:46:58,058 - 10 - training_embed.py - training - Epoch: [1][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.9739 (352.4519)	
2019-01-08 13:46:58,264 - 10 - training_embed.py - training - Epoch: [1][1800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 349.2880 (352.4375)	
2019-01-08 13:46:58,495 - 10 - training_embed.py - training - Epoch: [1][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.5760 (352.4101)	
2019-01-08 13:46:58,708 - 10 - training_embed.py - training - Epoch: [1][2000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.6815 (352.4022)	
2019-01-08 13:46:58,946 - 10 - training_embed.py - training - Epoch: [1][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 353.8239 (352.3927)	
2019-01-08 13:46:58,961 - 10 - training_embed.py - training - loss: 352.353965
2019-01-08 13:46:58,961 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 13:46:59,393 - 10 - training_embed.py - training - Epoch: [2][100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 351.4085 (351.9674)	
2019-01-08 13:46:59,609 - 10 - training_embed.py - training - Epoch: [2][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.8466 (352.0242)	
2019-01-08 13:46:59,846 - 10 - training_embed.py - training - Epoch: [2][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.1371 (352.0727)	
2019-01-08 13:47:00,076 - 10 - training_embed.py - training - Epoch: [2][400/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.9202 (351.9959)	
2019-01-08 13:47:00,316 - 10 - training_embed.py - training - Epoch: [2][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.0443 (351.8868)	
2019-01-08 13:47:00,524 - 10 - training_embed.py - training - Epoch: [2][600/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.0496 (351.9157)	
2019-01-08 13:47:00,740 - 10 - training_embed.py - training - Epoch: [2][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.6543 (351.9249)	
2019-01-08 13:47:00,978 - 10 - training_embed.py - training - Epoch: [2][800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 355.1385 (351.9501)	
2019-01-08 13:47:01,198 - 10 - training_embed.py - training - Epoch: [2][900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 354.8513 (351.9200)	
2019-01-08 13:47:01,436 - 10 - training_embed.py - training - Epoch: [2][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.1701 (351.8842)	
2019-01-08 13:47:01,643 - 10 - training_embed.py - training - Epoch: [2][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.6567 (351.9031)	
2019-01-08 13:47:01,854 - 10 - training_embed.py - training - Epoch: [2][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.8713 (351.9206)	
2019-01-08 13:47:02,076 - 10 - training_embed.py - training - Epoch: [2][1300/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.0325 (351.9250)	
2019-01-08 13:47:02,293 - 10 - training_embed.py - training - Epoch: [2][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.7209 (351.9289)	
2019-01-08 13:47:02,523 - 10 - training_embed.py - training - Epoch: [2][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.1133 (351.9198)	
2019-01-08 13:47:02,745 - 10 - training_embed.py - training - Epoch: [2][1600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 347.0693 (351.9073)	
2019-01-08 13:47:02,953 - 10 - training_embed.py - training - Epoch: [2][1700/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 349.1648 (351.9153)	
2019-01-08 13:47:03,169 - 10 - training_embed.py - training - Epoch: [2][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.6752 (351.9107)	
2019-01-08 13:47:03,427 - 10 - training_embed.py - training - Epoch: [2][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.6168 (351.9037)	
2019-01-08 13:47:03,650 - 10 - training_embed.py - training - Epoch: [2][2000/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 350.1577 (351.8988)	
2019-01-08 13:47:03,874 - 10 - training_embed.py - training - Epoch: [2][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 349.7323 (351.8549)	
2019-01-08 13:47:03,888 - 10 - training_embed.py - training - loss: 351.814129
2019-01-08 13:47:03,888 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 13:47:04,324 - 10 - training_embed.py - training - Epoch: [3][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.9915 (351.5685)	
2019-01-08 13:47:04,545 - 10 - training_embed.py - training - Epoch: [3][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.9418 (351.4369)	
2019-01-08 13:47:04,765 - 10 - training_embed.py - training - Epoch: [3][300/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 351.0795 (351.4882)	
2019-01-08 13:47:04,994 - 10 - training_embed.py - training - Epoch: [3][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.3316 (351.4739)	
2019-01-08 13:47:05,203 - 10 - training_embed.py - training - Epoch: [3][500/2109]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 350.6273 (351.5066)	
2019-01-08 13:47:05,421 - 10 - training_embed.py - training - Epoch: [3][600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 356.2169 (351.5251)	
2019-01-08 13:47:05,637 - 10 - training_embed.py - training - Epoch: [3][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.7360 (351.4869)	
2019-01-08 13:47:05,872 - 10 - training_embed.py - training - Epoch: [3][800/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.5096 (351.5131)	
2019-01-08 13:47:06,093 - 10 - training_embed.py - training - Epoch: [3][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.6237 (351.4764)	
2019-01-08 13:47:06,319 - 10 - training_embed.py - training - Epoch: [3][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.2982 (351.4836)	
2019-01-08 13:47:06,537 - 10 - training_embed.py - training - Epoch: [3][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.5308 (351.4940)	
2019-01-08 13:47:06,754 - 10 - training_embed.py - training - Epoch: [3][1200/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 352.5587 (351.4730)	
2019-01-08 13:47:06,984 - 10 - training_embed.py - training - Epoch: [3][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.5949 (351.4536)	
2019-01-08 13:47:07,188 - 10 - training_embed.py - training - Epoch: [3][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.1130 (351.4565)	
2019-01-08 13:47:07,425 - 10 - training_embed.py - training - Epoch: [3][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.9106 (351.4209)	
2019-01-08 13:47:07,632 - 10 - training_embed.py - training - Epoch: [3][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.3009 (351.3886)	
2019-01-08 13:47:07,859 - 10 - training_embed.py - training - Epoch: [3][1700/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 351.9330 (351.3719)	
2019-01-08 13:47:08,086 - 10 - training_embed.py - training - Epoch: [3][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.4903 (351.3474)	
2019-01-08 13:47:08,307 - 10 - training_embed.py - training - Epoch: [3][1900/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 349.9353 (351.3282)	
2019-01-08 13:47:08,536 - 10 - training_embed.py - training - Epoch: [3][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.4393 (351.3141)	
2019-01-08 13:47:08,755 - 10 - training_embed.py - training - Epoch: [3][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 349.7560 (351.3056)	
2019-01-08 13:47:08,772 - 10 - training_embed.py - training - loss: 351.270654
2019-01-08 13:47:08,772 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 13:47:09,223 - 10 - training_embed.py - training - Epoch: [4][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.3206 (351.0732)	
2019-01-08 13:47:09,449 - 10 - training_embed.py - training - Epoch: [4][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.3016 (350.8576)	
2019-01-08 13:47:09,675 - 10 - training_embed.py - training - Epoch: [4][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.0196 (350.8004)	
2019-01-08 13:47:09,912 - 10 - training_embed.py - training - Epoch: [4][400/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.6420 (350.8196)	
2019-01-08 13:47:10,123 - 10 - training_embed.py - training - Epoch: [4][500/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 349.2036 (350.8666)	
2019-01-08 13:47:10,355 - 10 - training_embed.py - training - Epoch: [4][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.1980 (350.8790)	
2019-01-08 13:47:10,582 - 10 - training_embed.py - training - Epoch: [4][700/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 349.9871 (350.9068)	
2019-01-08 13:47:10,801 - 10 - training_embed.py - training - Epoch: [4][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.3422 (350.9229)	
2019-01-08 13:47:11,032 - 10 - training_embed.py - training - Epoch: [4][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.8989 (350.8881)	
2019-01-08 13:47:11,241 - 10 - training_embed.py - training - Epoch: [4][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.0413 (350.8948)	
2019-01-08 13:47:11,465 - 10 - training_embed.py - training - Epoch: [4][1100/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.6995 (350.8381)	
2019-01-08 13:47:11,693 - 10 - training_embed.py - training - Epoch: [4][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 351.4167 (350.8523)	
2019-01-08 13:47:11,925 - 10 - training_embed.py - training - Epoch: [4][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.2207 (350.8041)	
2019-01-08 13:47:12,168 - 10 - training_embed.py - training - Epoch: [4][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.5683 (350.7841)	
2019-01-08 13:47:12,405 - 10 - training_embed.py - training - Epoch: [4][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.4318 (350.7851)	
2019-01-08 13:47:12,618 - 10 - training_embed.py - training - Epoch: [4][1600/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 351.6657 (350.7654)	
2019-01-08 13:47:12,833 - 10 - training_embed.py - training - Epoch: [4][1700/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 351.0203 (350.7497)	
2019-01-08 13:47:13,066 - 10 - training_embed.py - training - Epoch: [4][1800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 354.1860 (350.7427)	
2019-01-08 13:47:13,300 - 10 - training_embed.py - training - Epoch: [4][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.8541 (350.7574)	
2019-01-08 13:47:13,524 - 10 - training_embed.py - training - Epoch: [4][2000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 353.7116 (350.7678)	
2019-01-08 13:47:13,732 - 10 - training_embed.py - training - Epoch: [4][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 354.9632 (350.7602)	
2019-01-08 13:47:13,747 - 10 - training_embed.py - training - loss: 350.725255
2019-01-08 13:47:13,748 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 13:47:14,226 - 10 - training_embed.py - training - Epoch: [5][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.4649 (350.4158)	
2019-01-08 13:47:14,438 - 10 - training_embed.py - training - Epoch: [5][200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 355.0989 (350.3758)	
2019-01-08 13:47:14,667 - 10 - training_embed.py - training - Epoch: [5][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.7484 (350.2967)	
2019-01-08 13:47:14,886 - 10 - training_embed.py - training - Epoch: [5][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.1332 (350.2697)	
2019-01-08 13:47:15,100 - 10 - training_embed.py - training - Epoch: [5][500/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 347.9855 (350.2752)	
2019-01-08 13:47:15,326 - 10 - training_embed.py - training - Epoch: [5][600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.9768 (350.2868)	
2019-01-08 13:47:15,556 - 10 - training_embed.py - training - Epoch: [5][700/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 349.4454 (350.2943)	
2019-01-08 13:47:15,798 - 10 - training_embed.py - training - Epoch: [5][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.4529 (350.3149)	
2019-01-08 13:47:16,028 - 10 - training_embed.py - training - Epoch: [5][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.2380 (350.2897)	
2019-01-08 13:47:16,245 - 10 - training_embed.py - training - Epoch: [5][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.3503 (350.2501)	
2019-01-08 13:47:16,470 - 10 - training_embed.py - training - Epoch: [5][1100/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.2501 (350.2385)	
2019-01-08 13:47:16,703 - 10 - training_embed.py - training - Epoch: [5][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.2144 (350.2541)	
2019-01-08 13:47:16,932 - 10 - training_embed.py - training - Epoch: [5][1300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 354.3390 (350.2918)	
2019-01-08 13:47:17,161 - 10 - training_embed.py - training - Epoch: [5][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.0569 (350.2901)	
2019-01-08 13:47:17,387 - 10 - training_embed.py - training - Epoch: [5][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.9207 (350.2814)	
2019-01-08 13:47:17,591 - 10 - training_embed.py - training - Epoch: [5][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.6598 (350.2472)	
2019-01-08 13:47:17,817 - 10 - training_embed.py - training - Epoch: [5][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.5097 (350.2471)	
2019-01-08 13:47:18,036 - 10 - training_embed.py - training - Epoch: [5][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.4094 (350.2321)	
2019-01-08 13:47:18,265 - 10 - training_embed.py - training - Epoch: [5][1900/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 347.9073 (350.2132)	
2019-01-08 13:47:18,478 - 10 - training_embed.py - training - Epoch: [5][2000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 343.6610 (350.2124)	
2019-01-08 13:47:18,695 - 10 - training_embed.py - training - Epoch: [5][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 345.2986 (350.2154)	
2019-01-08 13:47:18,710 - 10 - training_embed.py - training - loss: 350.177027
2019-01-08 13:47:18,710 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 13:47:19,148 - 10 - training_embed.py - training - Epoch: [6][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.8785 (350.1463)	
2019-01-08 13:47:19,389 - 10 - training_embed.py - training - Epoch: [6][200/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 352.8923 (349.7860)	
2019-01-08 13:47:19,612 - 10 - training_embed.py - training - Epoch: [6][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.5239 (349.9197)	
2019-01-08 13:47:19,829 - 10 - training_embed.py - training - Epoch: [6][400/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.8443 (350.0023)	
2019-01-08 13:47:20,042 - 10 - training_embed.py - training - Epoch: [6][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.4305 (349.9223)	
2019-01-08 13:47:20,277 - 10 - training_embed.py - training - Epoch: [6][600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.0262 (349.8929)	
2019-01-08 13:47:20,505 - 10 - training_embed.py - training - Epoch: [6][700/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.8055 (349.8990)	
2019-01-08 13:47:20,752 - 10 - training_embed.py - training - Epoch: [6][800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 353.6879 (349.8631)	
2019-01-08 13:47:20,976 - 10 - training_embed.py - training - Epoch: [6][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.9519 (349.8794)	
2019-01-08 13:47:21,214 - 10 - training_embed.py - training - Epoch: [6][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.3612 (349.8409)	
2019-01-08 13:47:21,460 - 10 - training_embed.py - training - Epoch: [6][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.3699 (349.8429)	
2019-01-08 13:47:21,698 - 10 - training_embed.py - training - Epoch: [6][1200/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 348.3144 (349.8052)	
2019-01-08 13:47:21,913 - 10 - training_embed.py - training - Epoch: [6][1300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 346.0615 (349.7871)	
2019-01-08 13:47:22,140 - 10 - training_embed.py - training - Epoch: [6][1400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 347.8605 (349.7745)	
2019-01-08 13:47:22,359 - 10 - training_embed.py - training - Epoch: [6][1500/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 347.7620 (349.7740)	
2019-01-08 13:47:22,585 - 10 - training_embed.py - training - Epoch: [6][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.8318 (349.7745)	
2019-01-08 13:47:22,808 - 10 - training_embed.py - training - Epoch: [6][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.1913 (349.7604)	
2019-01-08 13:47:23,029 - 10 - training_embed.py - training - Epoch: [6][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.1486 (349.7391)	
2019-01-08 13:47:23,237 - 10 - training_embed.py - training - Epoch: [6][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.2473 (349.7088)	
2019-01-08 13:47:23,474 - 10 - training_embed.py - training - Epoch: [6][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.6298 (349.7021)	
2019-01-08 13:47:23,714 - 10 - training_embed.py - training - Epoch: [6][2100/2109]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 347.4101 (349.6634)	
2019-01-08 13:47:23,729 - 10 - training_embed.py - training - loss: 349.627489
2019-01-08 13:47:23,729 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 13:47:24,188 - 10 - training_embed.py - training - Epoch: [7][100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.4663 (349.0261)	
2019-01-08 13:47:24,411 - 10 - training_embed.py - training - Epoch: [7][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.1167 (349.1052)	
2019-01-08 13:47:24,631 - 10 - training_embed.py - training - Epoch: [7][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.3471 (349.0757)	
2019-01-08 13:47:24,838 - 10 - training_embed.py - training - Epoch: [7][400/2109]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 350.4631 (349.1494)	
2019-01-08 13:47:25,075 - 10 - training_embed.py - training - Epoch: [7][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.5653 (349.1190)	
2019-01-08 13:47:25,301 - 10 - training_embed.py - training - Epoch: [7][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.8015 (349.1502)	
2019-01-08 13:47:25,522 - 10 - training_embed.py - training - Epoch: [7][700/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 353.0399 (349.1944)	
2019-01-08 13:47:25,753 - 10 - training_embed.py - training - Epoch: [7][800/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.9970 (349.1725)	
2019-01-08 13:47:25,969 - 10 - training_embed.py - training - Epoch: [7][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.4510 (349.1515)	
2019-01-08 13:47:26,201 - 10 - training_embed.py - training - Epoch: [7][1000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 346.6427 (349.1699)	
2019-01-08 13:47:26,408 - 10 - training_embed.py - training - Epoch: [7][1100/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 349.2978 (349.1590)	
2019-01-08 13:47:26,654 - 10 - training_embed.py - training - Epoch: [7][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.4814 (349.1541)	
2019-01-08 13:47:26,883 - 10 - training_embed.py - training - Epoch: [7][1300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 346.5397 (349.1364)	
2019-01-08 13:47:27,099 - 10 - training_embed.py - training - Epoch: [7][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.1338 (349.1396)	
2019-01-08 13:47:27,344 - 10 - training_embed.py - training - Epoch: [7][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.3156 (349.1637)	
2019-01-08 13:47:27,578 - 10 - training_embed.py - training - Epoch: [7][1600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 351.2573 (349.1544)	
2019-01-08 13:47:27,819 - 10 - training_embed.py - training - Epoch: [7][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.3494 (349.1321)	
2019-01-08 13:47:28,047 - 10 - training_embed.py - training - Epoch: [7][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.9403 (349.1291)	
2019-01-08 13:47:28,272 - 10 - training_embed.py - training - Epoch: [7][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.3291 (349.1431)	
2019-01-08 13:47:28,504 - 10 - training_embed.py - training - Epoch: [7][2000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 347.7714 (349.1281)	
2019-01-08 13:47:28,740 - 10 - training_embed.py - training - Epoch: [7][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 348.0955 (349.1137)	
2019-01-08 13:47:28,758 - 10 - training_embed.py - training - loss: 349.073000
2019-01-08 13:47:28,758 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 13:47:29,223 - 10 - training_embed.py - training - Epoch: [8][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.9243 (348.7087)	
2019-01-08 13:47:29,441 - 10 - training_embed.py - training - Epoch: [8][200/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 346.4195 (348.8750)	
2019-01-08 13:47:29,671 - 10 - training_embed.py - training - Epoch: [8][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.9269 (348.9425)	
2019-01-08 13:47:29,904 - 10 - training_embed.py - training - Epoch: [8][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.7849 (348.8489)	
2019-01-08 13:47:30,143 - 10 - training_embed.py - training - Epoch: [8][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.7460 (348.7640)	
2019-01-08 13:47:30,358 - 10 - training_embed.py - training - Epoch: [8][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.6335 (348.6939)	
2019-01-08 13:47:30,577 - 10 - training_embed.py - training - Epoch: [8][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.5227 (348.6840)	
2019-01-08 13:47:30,807 - 10 - training_embed.py - training - Epoch: [8][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.9076 (348.7214)	
2019-01-08 13:47:31,044 - 10 - training_embed.py - training - Epoch: [8][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.0651 (348.7194)	
2019-01-08 13:47:31,255 - 10 - training_embed.py - training - Epoch: [8][1000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 342.0486 (348.7332)	
2019-01-08 13:47:31,484 - 10 - training_embed.py - training - Epoch: [8][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.5433 (348.7047)	
2019-01-08 13:47:31,713 - 10 - training_embed.py - training - Epoch: [8][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.9766 (348.6682)	
2019-01-08 13:47:31,940 - 10 - training_embed.py - training - Epoch: [8][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.2794 (348.6643)	
2019-01-08 13:47:32,170 - 10 - training_embed.py - training - Epoch: [8][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.1583 (348.6369)	
2019-01-08 13:47:32,397 - 10 - training_embed.py - training - Epoch: [8][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.2386 (348.6307)	
2019-01-08 13:47:32,623 - 10 - training_embed.py - training - Epoch: [8][1600/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 347.4624 (348.6110)	
2019-01-08 13:47:32,836 - 10 - training_embed.py - training - Epoch: [8][1700/2109]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 349.8104 (348.6031)	
2019-01-08 13:47:33,061 - 10 - training_embed.py - training - Epoch: [8][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.2073 (348.5991)	
2019-01-08 13:47:33,277 - 10 - training_embed.py - training - Epoch: [8][1900/2109]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 349.7610 (348.5688)	
2019-01-08 13:47:33,499 - 10 - training_embed.py - training - Epoch: [8][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.6577 (348.5650)	
2019-01-08 13:47:33,727 - 10 - training_embed.py - training - Epoch: [8][2100/2109]	Time 0.008 (0.002)	Data 0.006 (0.001)	Loss 348.4864 (348.5516)	
2019-01-08 13:47:33,742 - 10 - training_embed.py - training - loss: 348.516007
2019-01-08 13:47:33,742 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 13:47:34,202 - 10 - training_embed.py - training - Epoch: [9][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.6790 (348.2651)	
2019-01-08 13:47:34,423 - 10 - training_embed.py - training - Epoch: [9][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.4430 (348.1868)	
2019-01-08 13:47:34,644 - 10 - training_embed.py - training - Epoch: [9][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.4865 (348.2320)	
2019-01-08 13:47:34,865 - 10 - training_embed.py - training - Epoch: [9][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.6316 (348.2779)	
2019-01-08 13:47:35,090 - 10 - training_embed.py - training - Epoch: [9][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.4337 (348.3326)	
2019-01-08 13:47:35,301 - 10 - training_embed.py - training - Epoch: [9][600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 349.7227 (348.3362)	
2019-01-08 13:47:35,514 - 10 - training_embed.py - training - Epoch: [9][700/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 347.4006 (348.2921)	
2019-01-08 13:47:35,736 - 10 - training_embed.py - training - Epoch: [9][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.9669 (348.2591)	
2019-01-08 13:47:35,943 - 10 - training_embed.py - training - Epoch: [9][900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 346.6740 (348.2374)	
2019-01-08 13:47:36,182 - 10 - training_embed.py - training - Epoch: [9][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.5369 (348.1815)	
2019-01-08 13:47:36,420 - 10 - training_embed.py - training - Epoch: [9][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.6698 (348.1937)	
2019-01-08 13:47:36,644 - 10 - training_embed.py - training - Epoch: [9][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.8571 (348.1412)	
2019-01-08 13:47:36,865 - 10 - training_embed.py - training - Epoch: [9][1300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 345.4833 (348.1192)	
2019-01-08 13:47:37,103 - 10 - training_embed.py - training - Epoch: [9][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.4522 (348.1085)	
2019-01-08 13:47:37,320 - 10 - training_embed.py - training - Epoch: [9][1500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 346.7552 (348.1063)	
2019-01-08 13:47:37,539 - 10 - training_embed.py - training - Epoch: [9][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 343.9755 (348.1124)	
2019-01-08 13:47:37,752 - 10 - training_embed.py - training - Epoch: [9][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.1404 (348.0963)	
2019-01-08 13:47:37,973 - 10 - training_embed.py - training - Epoch: [9][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.8869 (348.0648)	
2019-01-08 13:47:38,198 - 10 - training_embed.py - training - Epoch: [9][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.6694 (348.0561)	
2019-01-08 13:47:38,404 - 10 - training_embed.py - training - Epoch: [9][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.4308 (348.0229)	
2019-01-08 13:47:38,647 - 10 - training_embed.py - training - Epoch: [9][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 351.5393 (347.9947)	
2019-01-08 13:47:38,661 - 10 - training_embed.py - training - loss: 347.954972
2019-01-08 13:47:38,661 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 13:47:39,114 - 10 - training_embed.py - training - Epoch: [10][100/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 348.4084 (347.6598)	
2019-01-08 13:47:39,344 - 10 - training_embed.py - training - Epoch: [10][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.9099 (347.7171)	
2019-01-08 13:47:39,564 - 10 - training_embed.py - training - Epoch: [10][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.0457 (347.7480)	
2019-01-08 13:47:39,808 - 10 - training_embed.py - training - Epoch: [10][400/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 345.5744 (347.7656)	
2019-01-08 13:47:40,022 - 10 - training_embed.py - training - Epoch: [10][500/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 343.2404 (347.7266)	
2019-01-08 13:47:40,227 - 10 - training_embed.py - training - Epoch: [10][600/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 346.9695 (347.7054)	
2019-01-08 13:47:40,443 - 10 - training_embed.py - training - Epoch: [10][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.7568 (347.5987)	
2019-01-08 13:47:40,669 - 10 - training_embed.py - training - Epoch: [10][800/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 346.7786 (347.6054)	
2019-01-08 13:47:40,890 - 10 - training_embed.py - training - Epoch: [10][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.5978 (347.6185)	
2019-01-08 13:47:41,117 - 10 - training_embed.py - training - Epoch: [10][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.5091 (347.5700)	
2019-01-08 13:47:41,338 - 10 - training_embed.py - training - Epoch: [10][1100/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 343.4894 (347.5061)	
2019-01-08 13:47:41,556 - 10 - training_embed.py - training - Epoch: [10][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.0327 (347.5235)	
2019-01-08 13:47:41,784 - 10 - training_embed.py - training - Epoch: [10][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.9246 (347.5383)	
2019-01-08 13:47:41,997 - 10 - training_embed.py - training - Epoch: [10][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.0642 (347.5378)	
2019-01-08 13:47:42,231 - 10 - training_embed.py - training - Epoch: [10][1500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.1646 (347.4834)	
2019-01-08 13:47:42,454 - 10 - training_embed.py - training - Epoch: [10][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.3021 (347.4699)	
2019-01-08 13:47:42,675 - 10 - training_embed.py - training - Epoch: [10][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.9735 (347.4781)	
2019-01-08 13:47:42,888 - 10 - training_embed.py - training - Epoch: [10][1800/2109]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 345.5078 (347.4704)	
2019-01-08 13:47:43,116 - 10 - training_embed.py - training - Epoch: [10][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.7152 (347.4539)	
2019-01-08 13:47:43,347 - 10 - training_embed.py - training - Epoch: [10][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.4286 (347.4379)	
2019-01-08 13:47:43,584 - 10 - training_embed.py - training - Epoch: [10][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 346.3585 (347.4287)	
2019-01-08 13:47:43,598 - 10 - training_embed.py - training - loss: 347.389332
2019-01-08 13:47:43,598 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 13:47:44,031 - 10 - training_embed.py - training - Epoch: [11][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.1953 (347.0856)	
2019-01-08 13:47:44,285 - 10 - training_embed.py - training - Epoch: [11][200/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 346.9427 (347.3063)	
2019-01-08 13:47:44,509 - 10 - training_embed.py - training - Epoch: [11][300/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 343.4350 (347.3635)	
2019-01-08 13:47:44,731 - 10 - training_embed.py - training - Epoch: [11][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.0317 (347.2755)	
2019-01-08 13:47:44,941 - 10 - training_embed.py - training - Epoch: [11][500/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 344.3187 (347.3096)	
2019-01-08 13:47:45,149 - 10 - training_embed.py - training - Epoch: [11][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.7662 (347.2319)	
2019-01-08 13:47:45,390 - 10 - training_embed.py - training - Epoch: [11][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.7254 (347.2043)	
2019-01-08 13:47:45,598 - 10 - training_embed.py - training - Epoch: [11][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.2894 (347.1051)	
2019-01-08 13:47:45,828 - 10 - training_embed.py - training - Epoch: [11][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.8470 (347.0850)	
2019-01-08 13:47:46,031 - 10 - training_embed.py - training - Epoch: [11][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.5323 (347.0442)	
2019-01-08 13:47:46,242 - 10 - training_embed.py - training - Epoch: [11][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.8856 (346.9946)	
2019-01-08 13:47:46,467 - 10 - training_embed.py - training - Epoch: [11][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.4190 (346.9605)	
2019-01-08 13:47:46,691 - 10 - training_embed.py - training - Epoch: [11][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 343.8196 (346.9314)	
2019-01-08 13:47:46,901 - 10 - training_embed.py - training - Epoch: [11][1400/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 345.4317 (346.9209)	
2019-01-08 13:47:47,135 - 10 - training_embed.py - training - Epoch: [11][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.4941 (346.9099)	
2019-01-08 13:47:47,340 - 10 - training_embed.py - training - Epoch: [11][1600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 346.8134 (346.8816)	
2019-01-08 13:47:47,549 - 10 - training_embed.py - training - Epoch: [11][1700/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 347.4094 (346.8769)	
2019-01-08 13:47:47,775 - 10 - training_embed.py - training - Epoch: [11][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.4187 (346.8582)	
2019-01-08 13:47:47,993 - 10 - training_embed.py - training - Epoch: [11][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.9948 (346.8705)	
2019-01-08 13:47:48,244 - 10 - training_embed.py - training - Epoch: [11][2000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 343.5399 (346.8648)	
2019-01-08 13:47:48,487 - 10 - training_embed.py - training - Epoch: [11][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 344.8069 (346.8603)	
2019-01-08 13:47:48,501 - 10 - training_embed.py - training - loss: 346.820392
2019-01-08 13:47:48,575 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 13:47:49,534 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 958.318 ms ~ 0.016 min ~ 0.958 sec
2019-01-08 13:47:50,357 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1781.829 ms ~ 0.030 min ~ 1.782 sec
2019-01-08 13:47:50,357 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 13:47:50,358 - 10 - corpus.py - subactivity_sampler - 0 / 173
2019-01-08 13:47:50,358 - 10 - corpus.py - subactivity_sampler - [ 83278.  14070. 131184.   6349. 130397.   4575. 133940.  35940.]
2019-01-08 13:49:15,537 - 10 - corpus.py - subactivity_sampler - 20 / 173
2019-01-08 13:49:15,537 - 10 - corpus.py - subactivity_sampler - [ 83279.  13832. 131447.   6165. 130659.   4480. 134929.  34942.]
2019-01-08 13:50:11,255 - 10 - corpus.py - subactivity_sampler - 40 / 173
2019-01-08 13:50:11,255 - 10 - corpus.py - subactivity_sampler - [ 83306.  13452. 131831.   6019. 130909.   4323. 135491.  34402.]
2019-01-08 13:51:23,308 - 10 - corpus.py - subactivity_sampler - 60 / 173
2019-01-08 13:51:23,308 - 10 - corpus.py - subactivity_sampler - [ 83120.  13208. 132254.   5848. 131566.   4134. 136254.  33349.]
2019-01-08 13:52:35,626 - 10 - corpus.py - subactivity_sampler - 80 / 173
2019-01-08 13:52:35,627 - 10 - corpus.py - subactivity_sampler - [ 83143.  12693. 133154.   5670. 131556.   3996. 136870.  32651.]
2019-01-08 13:53:52,815 - 10 - corpus.py - subactivity_sampler - 100 / 173
2019-01-08 13:53:52,816 - 10 - corpus.py - subactivity_sampler - [ 83098.  12483. 133397.   5603. 131247.   3875. 137925.  32105.]
2019-01-08 13:54:37,373 - 10 - corpus.py - subactivity_sampler - 120 / 173
2019-01-08 13:54:37,374 - 10 - corpus.py - subactivity_sampler - [ 83067.  12245. 133865.   5467. 130947.   3782. 138632.  31728.]
2019-01-08 13:55:28,727 - 10 - corpus.py - subactivity_sampler - 140 / 173
2019-01-08 13:55:28,728 - 10 - corpus.py - subactivity_sampler - [ 83131.  12048. 133941.   5421. 131098.   3662. 139136.  31296.]
2019-01-08 13:56:34,373 - 10 - corpus.py - subactivity_sampler - 160 / 173
2019-01-08 13:56:34,373 - 10 - corpus.py - subactivity_sampler - [ 82873.  11932. 134289.   5312. 131012.   3606. 139682.  31027.]
2019-01-08 13:57:10,500 - 10 - corpus.py - subactivity_sampler - [ 82865.  11812. 134500.   5220. 130841.   3571. 140185.  30739.]
2019-01-08 13:57:10,501 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 560143.202 ms ~ 9.336 min ~ 560.143 sec
2019-01-08 13:57:10,501 - 10 - corpus.py - ordering_sampler - .
2019-01-08 13:57:13,379 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 13:57:13,379 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 9. 10.  2.  1. 86.  2.  7.]
2019-01-08 13:57:13,379 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 13:57:13,392 - 10 - corpus.py - rho_sampling - ['54.0481', '0.7993', '7.5038', '13.6190', '-1.0867', '17.3222', '1.6897']
2019-01-08 13:57:13,392 - 10 - pipeline.py - baseline - Iteration 5
2019-01-08 13:57:13,586 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 13:57:13,600 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 13:57:13,600 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 17', '1: 12', '2: 11', '3: 10', '4: 13', '5: 16', '6: 14', '7: 15']
2019-01-08 13:57:13,622 - 10 - accuracy_class.py - mof_val - frames true: 187893	frames overall : 539733
2019-01-08 13:57:13,622 - 10 - corpus.py - accuracy_corpus - Action: friedegg
2019-01-08 13:57:13,622 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3481221270517089
2019-01-08 13:57:13,622 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3481221270517089
2019-01-08 13:57:13,622 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 17933
2019-01-08 13:57:13,622 - 10 - accuracy_class.py - mof_classes - label 10: 0.000000  0 / 10971
2019-01-08 13:57:13,622 - 10 - accuracy_class.py - mof_classes - label 11: 0.440472  30404 / 69026
2019-01-08 13:57:13,622 - 10 - accuracy_class.py - mof_classes - label 12: 0.040438  1355 / 33508
2019-01-08 13:57:13,622 - 10 - accuracy_class.py - mof_classes - label 13: 0.346526  110300 / 318302
2019-01-08 13:57:13,622 - 10 - accuracy_class.py - mof_classes - label 14: 0.386494  9936 / 25708
2019-01-08 13:57:13,622 - 10 - accuracy_class.py - mof_classes - label 15: 0.368224  11778 / 31986
2019-01-08 13:57:13,622 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 2201
2019-01-08 13:57:13,622 - 10 - accuracy_class.py - mof_classes - label 17: 0.801382  24120 / 30098
2019-01-08 13:57:13,622 - 10 - accuracy_class.py - mof_classes - average class mof: 0.264837
2019-01-08 13:57:13,623 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 17933
2019-01-08 13:57:13,623 - 10 - accuracy_class.py - iou_classes - label 10: 0.000000  0 / 16191
2019-01-08 13:57:13,623 - 10 - accuracy_class.py - iou_classes - label 11: 0.175622  30404 / 173122
2019-01-08 13:57:13,623 - 10 - accuracy_class.py - iou_classes - label 12: 0.030820  1355 / 43965
2019-01-08 13:57:13,623 - 10 - accuracy_class.py - iou_classes - label 13: 0.325519  110300 / 338843
2019-01-08 13:57:13,623 - 10 - accuracy_class.py - iou_classes - label 14: 0.063710  9936 / 155957
2019-01-08 13:57:13,623 - 10 - accuracy_class.py - iou_classes - label 15: 0.231181  11778 / 50947
2019-01-08 13:57:13,623 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 5772
2019-01-08 13:57:13,623 - 10 - accuracy_class.py - iou_classes - label 17: 0.271490  24120 / 88843
2019-01-08 13:57:13,623 - 10 - accuracy_class.py - iou_classes - average IoU: 0.137293
2019-01-08 13:57:13,623 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.122038
2019-01-08 13:57:19,975 - 10 - f1_score.py - f1 - f1 score: 0.254024
2019-01-08 13:57:20,009 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 6617.037 ms ~ 0.110 min ~ 6.617 sec
2019-01-08 13:57:20,010 - 10 - corpus.py - embedding_training - .
2019-01-08 13:57:20,010 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 13:57:20,010 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 13:57:20,010 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 13:57:23,938 - 10 - training_embed.py - training - create model
2019-01-08 13:57:23,939 - 10 - training_embed.py - training - epochs: 12
2019-01-08 13:57:23,940 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 13:57:24,310 - 10 - training_embed.py - training - Epoch: [0][100/2109]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 355.3195 (353.2040)	
2019-01-08 13:57:24,522 - 10 - training_embed.py - training - Epoch: [0][200/2109]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 354.1016 (353.0455)	
2019-01-08 13:57:24,739 - 10 - training_embed.py - training - Epoch: [0][300/2109]	Time 0.002 (0.003)	Data 0.000 (0.002)	Loss 351.0207 (353.1374)	
2019-01-08 13:57:24,981 - 10 - training_embed.py - training - Epoch: [0][400/2109]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 348.5830 (353.1091)	
2019-01-08 13:57:25,208 - 10 - training_embed.py - training - Epoch: [0][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.3778 (353.1028)	
2019-01-08 13:57:25,445 - 10 - training_embed.py - training - Epoch: [0][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 346.3808 (353.1220)	
2019-01-08 13:57:25,665 - 10 - training_embed.py - training - Epoch: [0][700/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.0614 (353.0472)	
2019-01-08 13:57:25,888 - 10 - training_embed.py - training - Epoch: [0][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.4922 (353.0565)	
2019-01-08 13:57:26,125 - 10 - training_embed.py - training - Epoch: [0][900/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 360.7813 (353.0323)	
2019-01-08 13:57:26,354 - 10 - training_embed.py - training - Epoch: [0][1000/2109]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 351.0653 (353.0247)	
2019-01-08 13:57:26,584 - 10 - training_embed.py - training - Epoch: [0][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.2310 (353.0106)	
2019-01-08 13:57:26,791 - 10 - training_embed.py - training - Epoch: [0][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.2327 (352.9723)	
2019-01-08 13:57:27,002 - 10 - training_embed.py - training - Epoch: [0][1300/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 352.0891 (352.9209)	
2019-01-08 13:57:27,222 - 10 - training_embed.py - training - Epoch: [0][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.5133 (352.9090)	
2019-01-08 13:57:27,432 - 10 - training_embed.py - training - Epoch: [0][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.4701 (352.8642)	
2019-01-08 13:57:27,658 - 10 - training_embed.py - training - Epoch: [0][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.5318 (352.8468)	
2019-01-08 13:57:27,886 - 10 - training_embed.py - training - Epoch: [0][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.6501 (352.8108)	
2019-01-08 13:57:28,120 - 10 - training_embed.py - training - Epoch: [0][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.7750 (352.7973)	
2019-01-08 13:57:28,367 - 10 - training_embed.py - training - Epoch: [0][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.1397 (352.7669)	
2019-01-08 13:57:28,614 - 10 - training_embed.py - training - Epoch: [0][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.0651 (352.7451)	
2019-01-08 13:57:28,850 - 10 - training_embed.py - training - Epoch: [0][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 357.1042 (352.7516)	
2019-01-08 13:57:28,865 - 10 - training_embed.py - training - loss: 352.713678
2019-01-08 13:57:28,865 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 13:57:29,281 - 10 - training_embed.py - training - Epoch: [1][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.1875 (352.3146)	
2019-01-08 13:57:29,503 - 10 - training_embed.py - training - Epoch: [1][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0510 (352.2092)	
2019-01-08 13:57:29,726 - 10 - training_embed.py - training - Epoch: [1][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.1569 (352.3388)	
2019-01-08 13:57:29,967 - 10 - training_embed.py - training - Epoch: [1][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.1029 (352.2817)	
2019-01-08 13:57:30,201 - 10 - training_embed.py - training - Epoch: [1][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.6281 (352.2899)	
2019-01-08 13:57:30,410 - 10 - training_embed.py - training - Epoch: [1][600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.1404 (352.3160)	
2019-01-08 13:57:30,642 - 10 - training_embed.py - training - Epoch: [1][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.5579 (352.3048)	
2019-01-08 13:57:30,884 - 10 - training_embed.py - training - Epoch: [1][800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 352.1648 (352.3676)	
2019-01-08 13:57:31,099 - 10 - training_embed.py - training - Epoch: [1][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.7457 (352.3744)	
2019-01-08 13:57:31,340 - 10 - training_embed.py - training - Epoch: [1][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.3299 (352.3506)	
2019-01-08 13:57:31,580 - 10 - training_embed.py - training - Epoch: [1][1100/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.2824 (352.3314)	
2019-01-08 13:57:31,809 - 10 - training_embed.py - training - Epoch: [1][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.9303 (352.2991)	
2019-01-08 13:57:32,053 - 10 - training_embed.py - training - Epoch: [1][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.4936 (352.2913)	
2019-01-08 13:57:32,274 - 10 - training_embed.py - training - Epoch: [1][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.7576 (352.2650)	
2019-01-08 13:57:32,515 - 10 - training_embed.py - training - Epoch: [1][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.9148 (352.2660)	
2019-01-08 13:57:32,722 - 10 - training_embed.py - training - Epoch: [1][1600/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 350.3418 (352.2696)	
2019-01-08 13:57:32,939 - 10 - training_embed.py - training - Epoch: [1][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.8523 (352.2450)	
2019-01-08 13:57:33,155 - 10 - training_embed.py - training - Epoch: [1][1800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 351.6291 (352.2305)	
2019-01-08 13:57:33,405 - 10 - training_embed.py - training - Epoch: [1][1900/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.8748 (352.1940)	
2019-01-08 13:57:33,635 - 10 - training_embed.py - training - Epoch: [1][2000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 351.1670 (352.1776)	
2019-01-08 13:57:33,857 - 10 - training_embed.py - training - Epoch: [1][2100/2109]	Time 0.007 (0.002)	Data 0.006 (0.001)	Loss 353.1082 (352.1654)	
2019-01-08 13:57:33,871 - 10 - training_embed.py - training - loss: 352.126291
2019-01-08 13:57:33,871 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 13:57:34,290 - 10 - training_embed.py - training - Epoch: [2][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.8909 (351.6246)	
2019-01-08 13:57:34,522 - 10 - training_embed.py - training - Epoch: [2][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.8279 (351.7146)	
2019-01-08 13:57:34,737 - 10 - training_embed.py - training - Epoch: [2][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.2210 (351.7425)	
2019-01-08 13:57:34,970 - 10 - training_embed.py - training - Epoch: [2][400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 349.9749 (351.7040)	
2019-01-08 13:57:35,191 - 10 - training_embed.py - training - Epoch: [2][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.7648 (351.6088)	
2019-01-08 13:57:35,415 - 10 - training_embed.py - training - Epoch: [2][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.8936 (351.6270)	
2019-01-08 13:57:35,638 - 10 - training_embed.py - training - Epoch: [2][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.9290 (351.6500)	
2019-01-08 13:57:35,852 - 10 - training_embed.py - training - Epoch: [2][800/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 354.4956 (351.6725)	
2019-01-08 13:57:36,090 - 10 - training_embed.py - training - Epoch: [2][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.9966 (351.6406)	
2019-01-08 13:57:36,297 - 10 - training_embed.py - training - Epoch: [2][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.7272 (351.6032)	
2019-01-08 13:57:36,521 - 10 - training_embed.py - training - Epoch: [2][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.2891 (351.6329)	
2019-01-08 13:57:36,735 - 10 - training_embed.py - training - Epoch: [2][1200/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 353.1034 (351.6537)	
2019-01-08 13:57:36,959 - 10 - training_embed.py - training - Epoch: [2][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.0411 (351.6614)	
2019-01-08 13:57:37,183 - 10 - training_embed.py - training - Epoch: [2][1400/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.9887 (351.6661)	
2019-01-08 13:57:37,405 - 10 - training_embed.py - training - Epoch: [2][1500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 355.1386 (351.6524)	
2019-01-08 13:57:37,627 - 10 - training_embed.py - training - Epoch: [2][1600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 347.3476 (351.6496)	
2019-01-08 13:57:37,844 - 10 - training_embed.py - training - Epoch: [2][1700/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 349.2299 (351.6495)	
2019-01-08 13:57:38,087 - 10 - training_embed.py - training - Epoch: [2][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.1509 (351.6415)	
2019-01-08 13:57:38,296 - 10 - training_embed.py - training - Epoch: [2][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 351.1630 (351.6337)	
2019-01-08 13:57:38,540 - 10 - training_embed.py - training - Epoch: [2][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.1651 (351.6283)	
2019-01-08 13:57:38,757 - 10 - training_embed.py - training - Epoch: [2][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 349.5910 (351.5789)	
2019-01-08 13:57:38,771 - 10 - training_embed.py - training - loss: 351.537827
2019-01-08 13:57:38,772 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 13:57:39,219 - 10 - training_embed.py - training - Epoch: [3][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.3167 (351.1884)	
2019-01-08 13:57:39,435 - 10 - training_embed.py - training - Epoch: [3][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.0423 (351.1306)	
2019-01-08 13:57:39,667 - 10 - training_embed.py - training - Epoch: [3][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.5664 (351.1285)	
2019-01-08 13:57:39,883 - 10 - training_embed.py - training - Epoch: [3][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.9792 (351.1023)	
2019-01-08 13:57:40,099 - 10 - training_embed.py - training - Epoch: [3][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.2093 (351.1378)	
2019-01-08 13:57:40,322 - 10 - training_embed.py - training - Epoch: [3][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.9312 (351.1695)	
2019-01-08 13:57:40,568 - 10 - training_embed.py - training - Epoch: [3][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.4635 (351.1357)	
2019-01-08 13:57:40,793 - 10 - training_embed.py - training - Epoch: [3][800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 353.1130 (351.1509)	
2019-01-08 13:57:41,010 - 10 - training_embed.py - training - Epoch: [3][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.9838 (351.1132)	
2019-01-08 13:57:41,223 - 10 - training_embed.py - training - Epoch: [3][1000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 347.7083 (351.1344)	
2019-01-08 13:57:41,447 - 10 - training_embed.py - training - Epoch: [3][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.0477 (351.1382)	
2019-01-08 13:57:41,671 - 10 - training_embed.py - training - Epoch: [3][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.4604 (351.1114)	
2019-01-08 13:57:41,898 - 10 - training_embed.py - training - Epoch: [3][1300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.9393 (351.0977)	
2019-01-08 13:57:42,123 - 10 - training_embed.py - training - Epoch: [3][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.3506 (351.0971)	
2019-01-08 13:57:42,339 - 10 - training_embed.py - training - Epoch: [3][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.7788 (351.0473)	
2019-01-08 13:57:42,565 - 10 - training_embed.py - training - Epoch: [3][1600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 347.2346 (351.0263)	
2019-01-08 13:57:42,790 - 10 - training_embed.py - training - Epoch: [3][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.5852 (351.0180)	
2019-01-08 13:57:43,009 - 10 - training_embed.py - training - Epoch: [3][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.5309 (351.0030)	
2019-01-08 13:57:43,227 - 10 - training_embed.py - training - Epoch: [3][1900/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 350.3393 (350.9943)	
2019-01-08 13:57:43,446 - 10 - training_embed.py - training - Epoch: [3][2000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.9655 (350.9827)	
2019-01-08 13:57:43,665 - 10 - training_embed.py - training - Epoch: [3][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 349.0919 (350.9789)	
2019-01-08 13:57:43,680 - 10 - training_embed.py - training - loss: 350.945170
2019-01-08 13:57:43,680 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 13:57:44,066 - 10 - training_embed.py - training - Epoch: [4][100/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 351.3524 (350.8475)	
2019-01-08 13:57:44,280 - 10 - training_embed.py - training - Epoch: [4][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.2372 (350.5752)	
2019-01-08 13:57:44,522 - 10 - training_embed.py - training - Epoch: [4][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.8557 (350.4973)	
2019-01-08 13:57:44,744 - 10 - training_embed.py - training - Epoch: [4][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.0547 (350.5103)	
2019-01-08 13:57:44,961 - 10 - training_embed.py - training - Epoch: [4][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.8310 (350.4952)	
2019-01-08 13:57:45,183 - 10 - training_embed.py - training - Epoch: [4][600/2109]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 350.7037 (350.5312)	
2019-01-08 13:57:45,412 - 10 - training_embed.py - training - Epoch: [4][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.5513 (350.5675)	
2019-01-08 13:57:45,633 - 10 - training_embed.py - training - Epoch: [4][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.8401 (350.5973)	
2019-01-08 13:57:45,850 - 10 - training_embed.py - training - Epoch: [4][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.1294 (350.5586)	
2019-01-08 13:57:46,070 - 10 - training_embed.py - training - Epoch: [4][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.5379 (350.5688)	
2019-01-08 13:57:46,288 - 10 - training_embed.py - training - Epoch: [4][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.3187 (350.5102)	
2019-01-08 13:57:46,519 - 10 - training_embed.py - training - Epoch: [4][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.1367 (350.5100)	
2019-01-08 13:57:46,745 - 10 - training_embed.py - training - Epoch: [4][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.1114 (350.4675)	
2019-01-08 13:57:46,969 - 10 - training_embed.py - training - Epoch: [4][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.9304 (350.4476)	
2019-01-08 13:57:47,182 - 10 - training_embed.py - training - Epoch: [4][1500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.1319 (350.4480)	
2019-01-08 13:57:47,404 - 10 - training_embed.py - training - Epoch: [4][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.9008 (350.4248)	
2019-01-08 13:57:47,623 - 10 - training_embed.py - training - Epoch: [4][1700/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 350.8317 (350.4030)	
2019-01-08 13:57:47,849 - 10 - training_embed.py - training - Epoch: [4][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.9829 (350.3848)	
2019-01-08 13:57:48,087 - 10 - training_embed.py - training - Epoch: [4][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.4798 (350.3897)	
2019-01-08 13:57:48,309 - 10 - training_embed.py - training - Epoch: [4][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.1853 (350.3980)	
2019-01-08 13:57:48,520 - 10 - training_embed.py - training - Epoch: [4][2100/2109]	Time 0.007 (0.002)	Data 0.006 (0.001)	Loss 355.5252 (350.3851)	
2019-01-08 13:57:48,535 - 10 - training_embed.py - training - loss: 350.350213
2019-01-08 13:57:48,535 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 13:57:48,963 - 10 - training_embed.py - training - Epoch: [5][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.4351 (350.1235)	
2019-01-08 13:57:49,173 - 10 - training_embed.py - training - Epoch: [5][200/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 355.1384 (350.0425)	
2019-01-08 13:57:49,407 - 10 - training_embed.py - training - Epoch: [5][300/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 349.2724 (349.9664)	
2019-01-08 13:57:49,620 - 10 - training_embed.py - training - Epoch: [5][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.0633 (349.9355)	
2019-01-08 13:57:49,839 - 10 - training_embed.py - training - Epoch: [5][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.2187 (349.8812)	
2019-01-08 13:57:50,080 - 10 - training_embed.py - training - Epoch: [5][600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 351.0011 (349.8767)	
2019-01-08 13:57:50,299 - 10 - training_embed.py - training - Epoch: [5][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.8855 (349.8941)	
2019-01-08 13:57:50,529 - 10 - training_embed.py - training - Epoch: [5][800/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.2574 (349.8957)	
2019-01-08 13:57:50,747 - 10 - training_embed.py - training - Epoch: [5][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.8234 (349.8845)	
2019-01-08 13:57:50,955 - 10 - training_embed.py - training - Epoch: [5][1000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.5593 (349.8575)	
2019-01-08 13:57:51,175 - 10 - training_embed.py - training - Epoch: [5][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.5362 (349.8247)	
2019-01-08 13:57:51,406 - 10 - training_embed.py - training - Epoch: [5][1200/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 350.1917 (349.8353)	
2019-01-08 13:57:51,635 - 10 - training_embed.py - training - Epoch: [5][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.5079 (349.8659)	
2019-01-08 13:57:51,858 - 10 - training_embed.py - training - Epoch: [5][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.7049 (349.8712)	
2019-01-08 13:57:52,067 - 10 - training_embed.py - training - Epoch: [5][1500/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.5203 (349.8593)	
2019-01-08 13:57:52,277 - 10 - training_embed.py - training - Epoch: [5][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.1788 (349.8250)	
2019-01-08 13:57:52,516 - 10 - training_embed.py - training - Epoch: [5][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.8921 (349.8172)	
2019-01-08 13:57:52,743 - 10 - training_embed.py - training - Epoch: [5][1800/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 344.4950 (349.8041)	
2019-01-08 13:57:52,985 - 10 - training_embed.py - training - Epoch: [5][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.8572 (349.7901)	
2019-01-08 13:57:53,209 - 10 - training_embed.py - training - Epoch: [5][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 343.2763 (349.7867)	
2019-01-08 13:57:53,422 - 10 - training_embed.py - training - Epoch: [5][2100/2109]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 344.8135 (349.7896)	
2019-01-08 13:57:53,437 - 10 - training_embed.py - training - loss: 349.751286
2019-01-08 13:57:53,437 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 13:57:53,875 - 10 - training_embed.py - training - Epoch: [6][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.3415 (349.6521)	
2019-01-08 13:57:54,108 - 10 - training_embed.py - training - Epoch: [6][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.7071 (349.4353)	
2019-01-08 13:57:54,317 - 10 - training_embed.py - training - Epoch: [6][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.6013 (349.4899)	
2019-01-08 13:57:54,550 - 10 - training_embed.py - training - Epoch: [6][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.2232 (349.5827)	
2019-01-08 13:57:54,769 - 10 - training_embed.py - training - Epoch: [6][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.0268 (349.4986)	
2019-01-08 13:57:55,020 - 10 - training_embed.py - training - Epoch: [6][600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.4907 (349.4608)	
2019-01-08 13:57:55,254 - 10 - training_embed.py - training - Epoch: [6][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.7794 (349.4423)	
2019-01-08 13:57:55,464 - 10 - training_embed.py - training - Epoch: [6][800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 351.2171 (349.4237)	
2019-01-08 13:57:55,683 - 10 - training_embed.py - training - Epoch: [6][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.7827 (349.4419)	
2019-01-08 13:57:55,896 - 10 - training_embed.py - training - Epoch: [6][1000/2109]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 349.5247 (349.3898)	
2019-01-08 13:57:56,124 - 10 - training_embed.py - training - Epoch: [6][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.8506 (349.3821)	
2019-01-08 13:57:56,326 - 10 - training_embed.py - training - Epoch: [6][1200/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 346.7318 (349.3531)	
2019-01-08 13:57:56,556 - 10 - training_embed.py - training - Epoch: [6][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.4630 (349.3336)	
2019-01-08 13:57:56,775 - 10 - training_embed.py - training - Epoch: [6][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.0357 (349.3202)	
2019-01-08 13:57:56,985 - 10 - training_embed.py - training - Epoch: [6][1500/2109]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 347.1706 (349.3197)	
2019-01-08 13:57:57,201 - 10 - training_embed.py - training - Epoch: [6][1600/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 353.6163 (349.3101)	
2019-01-08 13:57:57,419 - 10 - training_embed.py - training - Epoch: [6][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.3809 (349.2870)	
2019-01-08 13:57:57,639 - 10 - training_embed.py - training - Epoch: [6][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.7957 (349.2658)	
2019-01-08 13:57:57,844 - 10 - training_embed.py - training - Epoch: [6][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.9182 (349.2395)	
2019-01-08 13:57:58,064 - 10 - training_embed.py - training - Epoch: [6][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.3950 (349.2315)	
2019-01-08 13:57:58,274 - 10 - training_embed.py - training - Epoch: [6][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 347.1720 (349.1859)	
2019-01-08 13:57:58,293 - 10 - training_embed.py - training - loss: 349.150751
2019-01-08 13:57:58,293 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 13:57:58,748 - 10 - training_embed.py - training - Epoch: [7][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.7952 (348.5582)	
2019-01-08 13:57:58,985 - 10 - training_embed.py - training - Epoch: [7][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.5519 (348.5533)	
2019-01-08 13:57:59,205 - 10 - training_embed.py - training - Epoch: [7][300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.7238 (348.5755)	
2019-01-08 13:57:59,411 - 10 - training_embed.py - training - Epoch: [7][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.8808 (348.6256)	
2019-01-08 13:57:59,643 - 10 - training_embed.py - training - Epoch: [7][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.5442 (348.6396)	
2019-01-08 13:57:59,864 - 10 - training_embed.py - training - Epoch: [7][600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 348.7972 (348.6620)	
2019-01-08 13:58:00,098 - 10 - training_embed.py - training - Epoch: [7][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.1004 (348.6893)	
2019-01-08 13:58:00,330 - 10 - training_embed.py - training - Epoch: [7][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.3434 (348.6708)	
2019-01-08 13:58:00,538 - 10 - training_embed.py - training - Epoch: [7][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.0774 (348.6600)	
2019-01-08 13:58:00,759 - 10 - training_embed.py - training - Epoch: [7][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.1024 (348.6905)	
2019-01-08 13:58:00,984 - 10 - training_embed.py - training - Epoch: [7][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.3065 (348.6826)	
2019-01-08 13:58:01,219 - 10 - training_embed.py - training - Epoch: [7][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.5079 (348.6739)	
2019-01-08 13:58:01,441 - 10 - training_embed.py - training - Epoch: [7][1300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 344.6200 (348.6425)	
2019-01-08 13:58:01,666 - 10 - training_embed.py - training - Epoch: [7][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.5169 (348.6342)	
2019-01-08 13:58:01,887 - 10 - training_embed.py - training - Epoch: [7][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.8895 (348.6524)	
2019-01-08 13:58:02,121 - 10 - training_embed.py - training - Epoch: [7][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.5692 (348.6367)	
2019-01-08 13:58:02,328 - 10 - training_embed.py - training - Epoch: [7][1700/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 348.5609 (348.6140)	
2019-01-08 13:58:02,569 - 10 - training_embed.py - training - Epoch: [7][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.9673 (348.6097)	
2019-01-08 13:58:02,795 - 10 - training_embed.py - training - Epoch: [7][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.2665 (348.6205)	
2019-01-08 13:58:03,033 - 10 - training_embed.py - training - Epoch: [7][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.7643 (348.6035)	
2019-01-08 13:58:03,278 - 10 - training_embed.py - training - Epoch: [7][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 346.7002 (348.5848)	
2019-01-08 13:58:03,293 - 10 - training_embed.py - training - loss: 348.544314
2019-01-08 13:58:03,293 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 13:58:03,723 - 10 - training_embed.py - training - Epoch: [8][100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.7832 (348.0159)	
2019-01-08 13:58:03,929 - 10 - training_embed.py - training - Epoch: [8][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.4261 (348.1809)	
2019-01-08 13:58:04,145 - 10 - training_embed.py - training - Epoch: [8][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.7637 (348.2610)	
2019-01-08 13:58:04,366 - 10 - training_embed.py - training - Epoch: [8][400/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 346.5901 (348.2092)	
2019-01-08 13:58:04,593 - 10 - training_embed.py - training - Epoch: [8][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.4456 (348.1911)	
2019-01-08 13:58:04,818 - 10 - training_embed.py - training - Epoch: [8][600/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 344.9101 (348.1363)	
2019-01-08 13:58:05,042 - 10 - training_embed.py - training - Epoch: [8][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.3799 (348.1378)	
2019-01-08 13:58:05,258 - 10 - training_embed.py - training - Epoch: [8][800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.6930 (348.1623)	
2019-01-08 13:58:05,467 - 10 - training_embed.py - training - Epoch: [8][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.5692 (348.1457)	
2019-01-08 13:58:05,707 - 10 - training_embed.py - training - Epoch: [8][1000/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 342.4741 (348.1592)	
2019-01-08 13:58:05,926 - 10 - training_embed.py - training - Epoch: [8][1100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 349.7629 (348.1475)	
2019-01-08 13:58:06,147 - 10 - training_embed.py - training - Epoch: [8][1200/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 350.2166 (348.1128)	
2019-01-08 13:58:06,371 - 10 - training_embed.py - training - Epoch: [8][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.5873 (348.1095)	
2019-01-08 13:58:06,588 - 10 - training_embed.py - training - Epoch: [8][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.7469 (348.0779)	
2019-01-08 13:58:06,825 - 10 - training_embed.py - training - Epoch: [8][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.4591 (348.0707)	
2019-01-08 13:58:07,049 - 10 - training_embed.py - training - Epoch: [8][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.0410 (348.0498)	
2019-01-08 13:58:07,273 - 10 - training_embed.py - training - Epoch: [8][1700/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.5329 (348.0318)	
2019-01-08 13:58:07,483 - 10 - training_embed.py - training - Epoch: [8][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.9535 (348.0248)	
2019-01-08 13:58:07,691 - 10 - training_embed.py - training - Epoch: [8][1900/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 349.4326 (347.9967)	
2019-01-08 13:58:07,919 - 10 - training_embed.py - training - Epoch: [8][2000/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 349.1808 (347.9896)	
2019-01-08 13:58:08,159 - 10 - training_embed.py - training - Epoch: [8][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 348.4129 (347.9718)	
2019-01-08 13:58:08,173 - 10 - training_embed.py - training - loss: 347.934633
2019-01-08 13:58:08,173 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 13:58:08,607 - 10 - training_embed.py - training - Epoch: [9][100/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 345.9647 (347.6468)	
2019-01-08 13:58:08,819 - 10 - training_embed.py - training - Epoch: [9][200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 347.0682 (347.5077)	
2019-01-08 13:58:09,043 - 10 - training_embed.py - training - Epoch: [9][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.4593 (347.5835)	
2019-01-08 13:58:09,273 - 10 - training_embed.py - training - Epoch: [9][400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 347.4517 (347.6027)	
2019-01-08 13:58:09,497 - 10 - training_embed.py - training - Epoch: [9][500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.6071 (347.6745)	
2019-01-08 13:58:09,734 - 10 - training_embed.py - training - Epoch: [9][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.4369 (347.7037)	
2019-01-08 13:58:09,946 - 10 - training_embed.py - training - Epoch: [9][700/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 346.9055 (347.6664)	
2019-01-08 13:58:10,162 - 10 - training_embed.py - training - Epoch: [9][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.2847 (347.6397)	
2019-01-08 13:58:10,374 - 10 - training_embed.py - training - Epoch: [9][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.2697 (347.6219)	
2019-01-08 13:58:10,618 - 10 - training_embed.py - training - Epoch: [9][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.6977 (347.5729)	
2019-01-08 13:58:10,851 - 10 - training_embed.py - training - Epoch: [9][1100/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.0474 (347.5742)	
2019-01-08 13:58:11,071 - 10 - training_embed.py - training - Epoch: [9][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.1638 (347.5156)	
2019-01-08 13:58:11,276 - 10 - training_embed.py - training - Epoch: [9][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 343.8433 (347.4935)	
2019-01-08 13:58:11,481 - 10 - training_embed.py - training - Epoch: [9][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.8702 (347.4792)	
2019-01-08 13:58:11,723 - 10 - training_embed.py - training - Epoch: [9][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.6912 (347.4673)	
2019-01-08 13:58:11,941 - 10 - training_embed.py - training - Epoch: [9][1600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 342.5230 (347.4738)	
2019-01-08 13:58:12,193 - 10 - training_embed.py - training - Epoch: [9][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.1157 (347.4659)	
2019-01-08 13:58:12,419 - 10 - training_embed.py - training - Epoch: [9][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.9887 (347.4435)	
2019-01-08 13:58:12,658 - 10 - training_embed.py - training - Epoch: [9][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 347.0907 (347.4347)	
2019-01-08 13:58:12,889 - 10 - training_embed.py - training - Epoch: [9][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.5962 (347.3952)	
2019-01-08 13:58:13,113 - 10 - training_embed.py - training - Epoch: [9][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 352.1291 (347.3576)	
2019-01-08 13:58:13,127 - 10 - training_embed.py - training - loss: 347.320227
2019-01-08 13:58:13,127 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 13:58:13,551 - 10 - training_embed.py - training - Epoch: [10][100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 347.7238 (346.9986)	
2019-01-08 13:58:13,761 - 10 - training_embed.py - training - Epoch: [10][200/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 348.6137 (347.0508)	
2019-01-08 13:58:13,974 - 10 - training_embed.py - training - Epoch: [10][300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 346.5580 (347.0802)	
2019-01-08 13:58:14,213 - 10 - training_embed.py - training - Epoch: [10][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 343.9861 (347.1212)	
2019-01-08 13:58:14,419 - 10 - training_embed.py - training - Epoch: [10][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 342.8680 (347.0700)	
2019-01-08 13:58:14,647 - 10 - training_embed.py - training - Epoch: [10][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.7769 (347.0429)	
2019-01-08 13:58:14,883 - 10 - training_embed.py - training - Epoch: [10][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.3802 (346.9375)	
2019-01-08 13:58:15,111 - 10 - training_embed.py - training - Epoch: [10][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.6891 (346.9242)	
2019-01-08 13:58:15,342 - 10 - training_embed.py - training - Epoch: [10][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.9048 (346.9419)	
2019-01-08 13:58:15,575 - 10 - training_embed.py - training - Epoch: [10][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.4578 (346.9024)	
2019-01-08 13:58:15,806 - 10 - training_embed.py - training - Epoch: [10][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.9268 (346.8432)	
2019-01-08 13:58:16,028 - 10 - training_embed.py - training - Epoch: [10][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.1009 (346.8543)	
2019-01-08 13:58:16,257 - 10 - training_embed.py - training - Epoch: [10][1300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 345.1014 (346.8764)	
2019-01-08 13:58:16,493 - 10 - training_embed.py - training - Epoch: [10][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.5702 (346.8719)	
2019-01-08 13:58:16,710 - 10 - training_embed.py - training - Epoch: [10][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.2706 (346.8288)	
2019-01-08 13:58:16,943 - 10 - training_embed.py - training - Epoch: [10][1600/2109]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 347.2070 (346.8075)	
2019-01-08 13:58:17,160 - 10 - training_embed.py - training - Epoch: [10][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.6288 (346.8126)	
2019-01-08 13:58:17,389 - 10 - training_embed.py - training - Epoch: [10][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.5367 (346.7925)	
2019-01-08 13:58:17,615 - 10 - training_embed.py - training - Epoch: [10][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.5042 (346.7676)	
2019-01-08 13:58:17,847 - 10 - training_embed.py - training - Epoch: [10][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.2979 (346.7501)	
2019-01-08 13:58:18,090 - 10 - training_embed.py - training - Epoch: [10][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 346.1685 (346.7407)	
2019-01-08 13:58:18,105 - 10 - training_embed.py - training - loss: 346.700308
2019-01-08 13:58:18,105 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 13:58:18,531 - 10 - training_embed.py - training - Epoch: [11][100/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 347.9182 (346.3357)	
2019-01-08 13:58:18,746 - 10 - training_embed.py - training - Epoch: [11][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.5140 (346.5765)	
2019-01-08 13:58:18,977 - 10 - training_embed.py - training - Epoch: [11][300/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 342.3558 (346.5748)	
2019-01-08 13:58:19,198 - 10 - training_embed.py - training - Epoch: [11][400/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 347.7537 (346.4925)	
2019-01-08 13:58:19,431 - 10 - training_embed.py - training - Epoch: [11][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.9716 (346.5379)	
2019-01-08 13:58:19,644 - 10 - training_embed.py - training - Epoch: [11][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.4608 (346.4476)	
2019-01-08 13:58:19,863 - 10 - training_embed.py - training - Epoch: [11][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.5796 (346.4343)	
2019-01-08 13:58:20,090 - 10 - training_embed.py - training - Epoch: [11][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.8068 (346.3324)	
2019-01-08 13:58:20,304 - 10 - training_embed.py - training - Epoch: [11][900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 345.4143 (346.3305)	
2019-01-08 13:58:20,534 - 10 - training_embed.py - training - Epoch: [11][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.0070 (346.3002)	
2019-01-08 13:58:20,754 - 10 - training_embed.py - training - Epoch: [11][1100/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 343.5993 (346.2577)	
2019-01-08 13:58:20,970 - 10 - training_embed.py - training - Epoch: [11][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.5528 (346.2225)	
2019-01-08 13:58:21,196 - 10 - training_embed.py - training - Epoch: [11][1300/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 343.9442 (346.2024)	
2019-01-08 13:58:21,422 - 10 - training_embed.py - training - Epoch: [11][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.0507 (346.1909)	
2019-01-08 13:58:21,634 - 10 - training_embed.py - training - Epoch: [11][1500/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 349.3241 (346.1721)	
2019-01-08 13:58:21,861 - 10 - training_embed.py - training - Epoch: [11][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.0284 (346.1384)	
2019-01-08 13:58:22,080 - 10 - training_embed.py - training - Epoch: [11][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.4956 (346.1382)	
2019-01-08 13:58:22,298 - 10 - training_embed.py - training - Epoch: [11][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.9054 (346.1306)	
2019-01-08 13:58:22,544 - 10 - training_embed.py - training - Epoch: [11][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 347.1181 (346.1434)	
2019-01-08 13:58:22,774 - 10 - training_embed.py - training - Epoch: [11][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 342.9139 (346.1334)	
2019-01-08 13:58:23,009 - 10 - training_embed.py - training - Epoch: [11][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 343.8441 (346.1149)	
2019-01-08 13:58:23,024 - 10 - training_embed.py - training - loss: 346.075551
2019-01-08 13:58:23,101 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 13:58:24,074 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 973.049 ms ~ 0.016 min ~ 0.973 sec
2019-01-08 13:58:24,923 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1822.332 ms ~ 0.030 min ~ 1.822 sec
2019-01-08 13:58:24,923 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 13:58:24,924 - 10 - corpus.py - subactivity_sampler - 0 / 173
2019-01-08 13:58:24,924 - 10 - corpus.py - subactivity_sampler - [ 82865.  11812. 134500.   5220. 130841.   3571. 140185.  30739.]
2019-01-08 13:59:49,941 - 10 - corpus.py - subactivity_sampler - 20 / 173
2019-01-08 13:59:49,942 - 10 - corpus.py - subactivity_sampler - [ 82853.  11766. 134582.   5172. 130883.   3521. 140827.  30129.]
2019-01-08 14:00:45,302 - 10 - corpus.py - subactivity_sampler - 40 / 173
2019-01-08 14:00:45,302 - 10 - corpus.py - subactivity_sampler - [ 82810.  11445. 134920.   5123. 131222.   3497. 141287.  29429.]
2019-01-08 14:01:56,881 - 10 - corpus.py - subactivity_sampler - 60 / 173
2019-01-08 14:01:56,881 - 10 - corpus.py - subactivity_sampler - [ 82725.  11350. 135250.   5541. 131017.   3168. 141498.  29184.]
2019-01-08 14:03:08,262 - 10 - corpus.py - subactivity_sampler - 80 / 173
2019-01-08 14:03:08,263 - 10 - corpus.py - subactivity_sampler - [ 82706.  11169. 135461.   5491. 130777.   3120. 141878.  29131.]
2019-01-08 14:04:25,058 - 10 - corpus.py - subactivity_sampler - 100 / 173
2019-01-08 14:04:25,058 - 10 - corpus.py - subactivity_sampler - [ 82644.  11042. 135638.   5475. 130808.   3059. 142233.  28834.]
2019-01-08 14:05:09,472 - 10 - corpus.py - subactivity_sampler - 120 / 173
2019-01-08 14:05:09,473 - 10 - corpus.py - subactivity_sampler - [ 82518.  10875. 135740.   5467. 130817.   3025. 142673.  28618.]
2019-01-08 14:06:00,718 - 10 - corpus.py - subactivity_sampler - 140 / 173
2019-01-08 14:06:00,718 - 10 - corpus.py - subactivity_sampler - [ 82492.  10776. 135734.   5492. 131097.   2979. 143075.  28088.]
2019-01-08 14:07:05,914 - 10 - corpus.py - subactivity_sampler - 160 / 173
2019-01-08 14:07:05,914 - 10 - corpus.py - subactivity_sampler - [ 82405.  10721. 135757.   5466. 131081.   2919. 143557.  27827.]
2019-01-08 14:07:41,605 - 10 - corpus.py - subactivity_sampler - [ 82322.  10662. 135854.   5513. 131238.   2887. 143523.  27734.]
2019-01-08 14:07:41,605 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 556682.095 ms ~ 9.278 min ~ 556.682 sec
2019-01-08 14:07:41,605 - 10 - corpus.py - ordering_sampler - .
2019-01-08 14:07:45,665 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 14:07:45,665 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 15.  44. 101.   5. 194.   4.   8.]
2019-01-08 14:07:45,665 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 14:07:45,682 - 10 - corpus.py - rho_sampling - ['48.3744', '13.4320', '5.8275', '5.6863', '65.1722', '5.4997', '15.6301']
2019-01-08 14:07:45,682 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 14:07:45,847 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 14:07:45,857 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 14:07:45,857 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 17', '1: 12', '2: 11', '3: 16', '4: 13', '5: 10', '6: 15', '7: 14']
2019-01-08 14:07:45,880 - 10 - accuracy_class.py - mof_val - frames true: 185584	frames overall : 539733
2019-01-08 14:07:45,880 - 10 - corpus.py - accuracy_corpus - Action: friedegg
2019-01-08 14:07:45,880 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3438440858720886
2019-01-08 14:07:45,880 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3404850175920316
2019-01-08 14:07:45,880 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 17933
2019-01-08 14:07:45,880 - 10 - accuracy_class.py - mof_classes - label 10: 0.000000  0 / 10971
2019-01-08 14:07:45,880 - 10 - accuracy_class.py - mof_classes - label 11: 0.421030  29062 / 69026
2019-01-08 14:07:45,880 - 10 - accuracy_class.py - mof_classes - label 12: 0.036887  1236 / 33508
2019-01-08 14:07:45,880 - 10 - accuracy_class.py - mof_classes - label 13: 0.345769  110059 / 318302
2019-01-08 14:07:45,880 - 10 - accuracy_class.py - mof_classes - label 14: 0.140812  3620 / 25708
2019-01-08 14:07:45,880 - 10 - accuracy_class.py - mof_classes - label 15: 0.546333  17475 / 31986
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 2201
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - mof_classes - label 17: 0.801781  24132 / 30098
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - mof_classes - average class mof: 0.254735
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 17933
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - iou_classes - label 10: 0.000000  0 / 13858
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - iou_classes - label 11: 0.165296  29062 / 175818
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - iou_classes - label 12: 0.028788  1236 / 42934
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - iou_classes - label 13: 0.324198  110059 / 339481
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - iou_classes - label 14: 0.072659  3620 / 49822
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - iou_classes - label 15: 0.110577  17475 / 158034
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 7714
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - iou_classes - label 17: 0.273333  24132 / 88288
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - iou_classes - average IoU: 0.121856
2019-01-08 14:07:45,881 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.108317
2019-01-08 14:07:52,137 - 10 - f1_score.py - f1 - f1 score: 0.239934
2019-01-08 14:07:52,166 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 6484.164 ms ~ 0.108 min ~ 6.484 sec
2019-01-08 14:07:52,166 - 10 - corpus.py - embedding_training - .
2019-01-08 14:07:52,166 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 14:07:52,166 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 14:07:52,166 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 14:07:56,112 - 10 - training_embed.py - training - create model
2019-01-08 14:07:56,113 - 10 - training_embed.py - training - epochs: 12
2019-01-08 14:07:56,113 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 14:07:56,486 - 10 - training_embed.py - training - Epoch: [0][100/2109]	Time 0.002 (0.004)	Data 0.001 (0.003)	Loss 356.4957 (352.8360)	
2019-01-08 14:07:56,709 - 10 - training_embed.py - training - Epoch: [0][200/2109]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 353.6680 (352.8529)	
2019-01-08 14:07:56,933 - 10 - training_embed.py - training - Epoch: [0][300/2109]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 349.4550 (352.9687)	
2019-01-08 14:07:57,162 - 10 - training_embed.py - training - Epoch: [0][400/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 348.3776 (352.9530)	
2019-01-08 14:07:57,390 - 10 - training_embed.py - training - Epoch: [0][500/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 354.2528 (352.8995)	
2019-01-08 14:07:57,617 - 10 - training_embed.py - training - Epoch: [0][600/2109]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 345.4806 (352.8809)	
2019-01-08 14:07:57,829 - 10 - training_embed.py - training - Epoch: [0][700/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 353.6791 (352.8009)	
2019-01-08 14:07:58,046 - 10 - training_embed.py - training - Epoch: [0][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.1994 (352.8274)	
2019-01-08 14:07:58,267 - 10 - training_embed.py - training - Epoch: [0][900/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 359.4948 (352.8134)	
2019-01-08 14:07:58,490 - 10 - training_embed.py - training - Epoch: [0][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.2930 (352.8007)	
2019-01-08 14:07:58,717 - 10 - training_embed.py - training - Epoch: [0][1100/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.6703 (352.7894)	
2019-01-08 14:07:58,931 - 10 - training_embed.py - training - Epoch: [0][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.3120 (352.7374)	
2019-01-08 14:07:59,160 - 10 - training_embed.py - training - Epoch: [0][1300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.6580 (352.7058)	
2019-01-08 14:07:59,381 - 10 - training_embed.py - training - Epoch: [0][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.8027 (352.6989)	
2019-01-08 14:07:59,628 - 10 - training_embed.py - training - Epoch: [0][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.4313 (352.6535)	
2019-01-08 14:07:59,865 - 10 - training_embed.py - training - Epoch: [0][1600/2109]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 350.3167 (352.6410)	
2019-01-08 14:08:00,118 - 10 - training_embed.py - training - Epoch: [0][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.8014 (352.6037)	
2019-01-08 14:08:00,337 - 10 - training_embed.py - training - Epoch: [0][1800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 351.7335 (352.5908)	
2019-01-08 14:08:00,547 - 10 - training_embed.py - training - Epoch: [0][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.8052 (352.5594)	
2019-01-08 14:08:00,779 - 10 - training_embed.py - training - Epoch: [0][2000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.7080 (352.5359)	
2019-01-08 14:08:00,997 - 10 - training_embed.py - training - Epoch: [0][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 355.7003 (352.5473)	
2019-01-08 14:08:01,013 - 10 - training_embed.py - training - loss: 352.510540
2019-01-08 14:08:01,014 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 14:08:01,463 - 10 - training_embed.py - training - Epoch: [1][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.4301 (352.1202)	
2019-01-08 14:08:01,684 - 10 - training_embed.py - training - Epoch: [1][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 357.1133 (352.0868)	
2019-01-08 14:08:01,906 - 10 - training_embed.py - training - Epoch: [1][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0585 (352.1233)	
2019-01-08 14:08:02,125 - 10 - training_embed.py - training - Epoch: [1][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.9768 (352.0413)	
2019-01-08 14:08:02,347 - 10 - training_embed.py - training - Epoch: [1][500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.5401 (352.0863)	
2019-01-08 14:08:02,562 - 10 - training_embed.py - training - Epoch: [1][600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 351.6090 (352.1094)	
2019-01-08 14:08:02,769 - 10 - training_embed.py - training - Epoch: [1][700/2109]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 351.3556 (352.0990)	
2019-01-08 14:08:02,992 - 10 - training_embed.py - training - Epoch: [1][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.9293 (352.1446)	
2019-01-08 14:08:03,219 - 10 - training_embed.py - training - Epoch: [1][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.2562 (352.1395)	
2019-01-08 14:08:03,425 - 10 - training_embed.py - training - Epoch: [1][1000/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 351.0788 (352.1233)	
2019-01-08 14:08:03,658 - 10 - training_embed.py - training - Epoch: [1][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.9639 (352.0973)	
2019-01-08 14:08:03,873 - 10 - training_embed.py - training - Epoch: [1][1200/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 351.5595 (352.0623)	
2019-01-08 14:08:04,110 - 10 - training_embed.py - training - Epoch: [1][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.8678 (352.0599)	
2019-01-08 14:08:04,337 - 10 - training_embed.py - training - Epoch: [1][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.0645 (352.0323)	
2019-01-08 14:08:04,561 - 10 - training_embed.py - training - Epoch: [1][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.1700 (352.0394)	
2019-01-08 14:08:04,803 - 10 - training_embed.py - training - Epoch: [1][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.4899 (352.0435)	
2019-01-08 14:08:05,026 - 10 - training_embed.py - training - Epoch: [1][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.7340 (352.0189)	
2019-01-08 14:08:05,256 - 10 - training_embed.py - training - Epoch: [1][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.3795 (351.9987)	
2019-01-08 14:08:05,466 - 10 - training_embed.py - training - Epoch: [1][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.7773 (351.9543)	
2019-01-08 14:08:05,689 - 10 - training_embed.py - training - Epoch: [1][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.7794 (351.9371)	
2019-01-08 14:08:05,917 - 10 - training_embed.py - training - Epoch: [1][2100/2109]	Time 0.008 (0.002)	Data 0.007 (0.001)	Loss 352.9468 (351.9303)	
2019-01-08 14:08:05,934 - 10 - training_embed.py - training - loss: 351.892073
2019-01-08 14:08:05,935 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 14:08:06,364 - 10 - training_embed.py - training - Epoch: [2][100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 350.4181 (351.2945)	
2019-01-08 14:08:06,586 - 10 - training_embed.py - training - Epoch: [2][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.6349 (351.4603)	
2019-01-08 14:08:06,807 - 10 - training_embed.py - training - Epoch: [2][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.2149 (351.4865)	
2019-01-08 14:08:07,012 - 10 - training_embed.py - training - Epoch: [2][400/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.1864 (351.4680)	
2019-01-08 14:08:07,242 - 10 - training_embed.py - training - Epoch: [2][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.2004 (351.3945)	
2019-01-08 14:08:07,454 - 10 - training_embed.py - training - Epoch: [2][600/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 353.8983 (351.4084)	
2019-01-08 14:08:07,681 - 10 - training_embed.py - training - Epoch: [2][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.8438 (351.4408)	
2019-01-08 14:08:07,908 - 10 - training_embed.py - training - Epoch: [2][800/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 354.5515 (351.4376)	
2019-01-08 14:08:08,141 - 10 - training_embed.py - training - Epoch: [2][900/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.4623 (351.3925)	
2019-01-08 14:08:08,385 - 10 - training_embed.py - training - Epoch: [2][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.4261 (351.3616)	
2019-01-08 14:08:08,606 - 10 - training_embed.py - training - Epoch: [2][1100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.8043 (351.3765)	
2019-01-08 14:08:08,815 - 10 - training_embed.py - training - Epoch: [2][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.3813 (351.3936)	
2019-01-08 14:08:09,034 - 10 - training_embed.py - training - Epoch: [2][1300/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 353.6820 (351.3946)	
2019-01-08 14:08:09,283 - 10 - training_embed.py - training - Epoch: [2][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.0247 (351.3943)	
2019-01-08 14:08:09,517 - 10 - training_embed.py - training - Epoch: [2][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.3201 (351.3734)	
2019-01-08 14:08:09,740 - 10 - training_embed.py - training - Epoch: [2][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.0193 (351.3651)	
2019-01-08 14:08:09,958 - 10 - training_embed.py - training - Epoch: [2][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.1375 (351.3753)	
2019-01-08 14:08:10,172 - 10 - training_embed.py - training - Epoch: [2][1800/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 352.5182 (351.3674)	
2019-01-08 14:08:10,402 - 10 - training_embed.py - training - Epoch: [2][1900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 352.4434 (351.3564)	
2019-01-08 14:08:10,618 - 10 - training_embed.py - training - Epoch: [2][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.0127 (351.3582)	
2019-01-08 14:08:10,849 - 10 - training_embed.py - training - Epoch: [2][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 348.6566 (351.3125)	
2019-01-08 14:08:10,864 - 10 - training_embed.py - training - loss: 351.271756
2019-01-08 14:08:10,864 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 14:08:11,299 - 10 - training_embed.py - training - Epoch: [3][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.2563 (350.9532)	
2019-01-08 14:08:11,523 - 10 - training_embed.py - training - Epoch: [3][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.5169 (350.9208)	
2019-01-08 14:08:11,747 - 10 - training_embed.py - training - Epoch: [3][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.2940 (350.9185)	
2019-01-08 14:08:11,975 - 10 - training_embed.py - training - Epoch: [3][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.2617 (350.8789)	
2019-01-08 14:08:12,198 - 10 - training_embed.py - training - Epoch: [3][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.7889 (350.8754)	
2019-01-08 14:08:12,401 - 10 - training_embed.py - training - Epoch: [3][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.0186 (350.9179)	
2019-01-08 14:08:12,615 - 10 - training_embed.py - training - Epoch: [3][700/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 347.5509 (350.8691)	
2019-01-08 14:08:12,852 - 10 - training_embed.py - training - Epoch: [3][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.7329 (350.8693)	
2019-01-08 14:08:13,067 - 10 - training_embed.py - training - Epoch: [3][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.3108 (350.8341)	
2019-01-08 14:08:13,297 - 10 - training_embed.py - training - Epoch: [3][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.8468 (350.8501)	
2019-01-08 14:08:13,511 - 10 - training_embed.py - training - Epoch: [3][1100/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.5784 (350.8409)	
2019-01-08 14:08:13,729 - 10 - training_embed.py - training - Epoch: [3][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.8157 (350.8159)	
2019-01-08 14:08:13,961 - 10 - training_embed.py - training - Epoch: [3][1300/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.6362 (350.8071)	
2019-01-08 14:08:14,178 - 10 - training_embed.py - training - Epoch: [3][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.5200 (350.8056)	
2019-01-08 14:08:14,412 - 10 - training_embed.py - training - Epoch: [3][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 356.4738 (350.7648)	
2019-01-08 14:08:14,636 - 10 - training_embed.py - training - Epoch: [3][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.3549 (350.7432)	
2019-01-08 14:08:14,850 - 10 - training_embed.py - training - Epoch: [3][1700/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 352.5747 (350.7279)	
2019-01-08 14:08:15,071 - 10 - training_embed.py - training - Epoch: [3][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.9611 (350.7085)	
2019-01-08 14:08:15,311 - 10 - training_embed.py - training - Epoch: [3][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.4029 (350.6920)	
2019-01-08 14:08:15,553 - 10 - training_embed.py - training - Epoch: [3][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.1637 (350.6863)	
2019-01-08 14:08:15,785 - 10 - training_embed.py - training - Epoch: [3][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 349.0996 (350.6841)	
2019-01-08 14:08:15,799 - 10 - training_embed.py - training - loss: 350.647210
2019-01-08 14:08:15,800 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 14:08:16,232 - 10 - training_embed.py - training - Epoch: [4][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.2405 (350.4535)	
2019-01-08 14:08:16,455 - 10 - training_embed.py - training - Epoch: [4][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.5748 (350.2585)	
2019-01-08 14:08:16,695 - 10 - training_embed.py - training - Epoch: [4][300/2109]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 347.0039 (350.2370)	
2019-01-08 14:08:16,934 - 10 - training_embed.py - training - Epoch: [4][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.3072 (350.1830)	
2019-01-08 14:08:17,160 - 10 - training_embed.py - training - Epoch: [4][500/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.8388 (350.1910)	
2019-01-08 14:08:17,382 - 10 - training_embed.py - training - Epoch: [4][600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.8799 (350.2223)	
2019-01-08 14:08:17,604 - 10 - training_embed.py - training - Epoch: [4][700/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.6559 (350.2500)	
2019-01-08 14:08:17,822 - 10 - training_embed.py - training - Epoch: [4][800/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.3919 (350.2856)	
2019-01-08 14:08:18,042 - 10 - training_embed.py - training - Epoch: [4][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.4351 (350.2501)	
2019-01-08 14:08:18,259 - 10 - training_embed.py - training - Epoch: [4][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.7398 (350.2603)	
2019-01-08 14:08:18,477 - 10 - training_embed.py - training - Epoch: [4][1100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 354.3234 (350.2190)	
2019-01-08 14:08:18,699 - 10 - training_embed.py - training - Epoch: [4][1200/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 350.2711 (350.2104)	
2019-01-08 14:08:18,912 - 10 - training_embed.py - training - Epoch: [4][1300/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 347.2006 (350.1606)	
2019-01-08 14:08:19,144 - 10 - training_embed.py - training - Epoch: [4][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.0936 (350.1338)	
2019-01-08 14:08:19,374 - 10 - training_embed.py - training - Epoch: [4][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.3271 (350.1314)	
2019-01-08 14:08:19,586 - 10 - training_embed.py - training - Epoch: [4][1600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 348.6442 (350.1084)	
2019-01-08 14:08:19,822 - 10 - training_embed.py - training - Epoch: [4][1700/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.4617 (350.0781)	
2019-01-08 14:08:20,053 - 10 - training_embed.py - training - Epoch: [4][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.6556 (350.0613)	
2019-01-08 14:08:20,268 - 10 - training_embed.py - training - Epoch: [4][1900/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 351.7053 (350.0642)	
2019-01-08 14:08:20,493 - 10 - training_embed.py - training - Epoch: [4][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.8247 (350.0700)	
2019-01-08 14:08:20,723 - 10 - training_embed.py - training - Epoch: [4][2100/2109]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 353.6880 (350.0547)	
2019-01-08 14:08:20,736 - 10 - training_embed.py - training - loss: 350.019791
2019-01-08 14:08:20,737 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 14:08:21,194 - 10 - training_embed.py - training - Epoch: [5][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.8130 (349.6883)	
2019-01-08 14:08:21,413 - 10 - training_embed.py - training - Epoch: [5][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 352.4267 (349.6134)	
2019-01-08 14:08:21,648 - 10 - training_embed.py - training - Epoch: [5][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.4756 (349.5368)	
2019-01-08 14:08:21,854 - 10 - training_embed.py - training - Epoch: [5][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.7069 (349.4616)	
2019-01-08 14:08:22,065 - 10 - training_embed.py - training - Epoch: [5][500/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 349.5464 (349.4488)	
2019-01-08 14:08:22,282 - 10 - training_embed.py - training - Epoch: [5][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.6142 (349.4688)	
2019-01-08 14:08:22,505 - 10 - training_embed.py - training - Epoch: [5][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.5182 (349.4623)	
2019-01-08 14:08:22,743 - 10 - training_embed.py - training - Epoch: [5][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.1729 (349.4990)	
2019-01-08 14:08:22,972 - 10 - training_embed.py - training - Epoch: [5][900/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 350.9522 (349.4934)	
2019-01-08 14:08:23,184 - 10 - training_embed.py - training - Epoch: [5][1000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 353.6072 (349.4486)	
2019-01-08 14:08:23,414 - 10 - training_embed.py - training - Epoch: [5][1100/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.1964 (349.4369)	
2019-01-08 14:08:23,640 - 10 - training_embed.py - training - Epoch: [5][1200/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 349.6497 (349.4477)	
2019-01-08 14:08:23,860 - 10 - training_embed.py - training - Epoch: [5][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 355.7376 (349.4778)	
2019-01-08 14:08:24,078 - 10 - training_embed.py - training - Epoch: [5][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.9054 (349.4860)	
2019-01-08 14:08:24,294 - 10 - training_embed.py - training - Epoch: [5][1500/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.9280 (349.4889)	
2019-01-08 14:08:24,500 - 10 - training_embed.py - training - Epoch: [5][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.1787 (349.4588)	
2019-01-08 14:08:24,738 - 10 - training_embed.py - training - Epoch: [5][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.7318 (349.4552)	
2019-01-08 14:08:24,964 - 10 - training_embed.py - training - Epoch: [5][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.3283 (349.4374)	
2019-01-08 14:08:25,193 - 10 - training_embed.py - training - Epoch: [5][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.5503 (349.4234)	
2019-01-08 14:08:25,409 - 10 - training_embed.py - training - Epoch: [5][2000/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 343.1898 (349.4223)	
2019-01-08 14:08:25,643 - 10 - training_embed.py - training - Epoch: [5][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 345.6584 (349.4250)	
2019-01-08 14:08:25,657 - 10 - training_embed.py - training - loss: 349.388168
2019-01-08 14:08:25,657 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 14:08:26,101 - 10 - training_embed.py - training - Epoch: [6][100/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 348.8225 (349.3193)	
2019-01-08 14:08:26,319 - 10 - training_embed.py - training - Epoch: [6][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.8649 (349.0007)	
2019-01-08 14:08:26,545 - 10 - training_embed.py - training - Epoch: [6][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 354.6660 (349.0529)	
2019-01-08 14:08:26,775 - 10 - training_embed.py - training - Epoch: [6][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.1758 (349.1866)	
2019-01-08 14:08:26,994 - 10 - training_embed.py - training - Epoch: [6][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.6506 (349.0973)	
2019-01-08 14:08:27,232 - 10 - training_embed.py - training - Epoch: [6][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.6603 (349.0471)	
2019-01-08 14:08:27,452 - 10 - training_embed.py - training - Epoch: [6][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.8565 (349.0397)	
2019-01-08 14:08:27,682 - 10 - training_embed.py - training - Epoch: [6][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.1268 (349.0403)	
2019-01-08 14:08:27,899 - 10 - training_embed.py - training - Epoch: [6][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.1851 (349.0600)	
2019-01-08 14:08:28,122 - 10 - training_embed.py - training - Epoch: [6][1000/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.6677 (348.9989)	
2019-01-08 14:08:28,342 - 10 - training_embed.py - training - Epoch: [6][1100/2109]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 344.0590 (348.9941)	
2019-01-08 14:08:28,558 - 10 - training_embed.py - training - Epoch: [6][1200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 346.3070 (348.9458)	
2019-01-08 14:08:28,786 - 10 - training_embed.py - training - Epoch: [6][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.8038 (348.9271)	
2019-01-08 14:08:29,002 - 10 - training_embed.py - training - Epoch: [6][1400/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 346.5200 (348.9041)	
2019-01-08 14:08:29,215 - 10 - training_embed.py - training - Epoch: [6][1500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 347.1612 (348.9019)	
2019-01-08 14:08:29,428 - 10 - training_embed.py - training - Epoch: [6][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.6606 (348.8958)	
2019-01-08 14:08:29,659 - 10 - training_embed.py - training - Epoch: [6][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.9600 (348.8724)	
2019-01-08 14:08:29,860 - 10 - training_embed.py - training - Epoch: [6][1800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 350.8245 (348.8541)	
2019-01-08 14:08:30,097 - 10 - training_embed.py - training - Epoch: [6][1900/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 350.5946 (348.8272)	
2019-01-08 14:08:30,310 - 10 - training_embed.py - training - Epoch: [6][2000/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 348.6515 (348.8256)	
2019-01-08 14:08:30,541 - 10 - training_embed.py - training - Epoch: [6][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 348.5457 (348.7932)	
2019-01-08 14:08:30,557 - 10 - training_embed.py - training - loss: 348.754819
2019-01-08 14:08:30,557 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 14:08:30,996 - 10 - training_embed.py - training - Epoch: [7][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.2794 (348.0613)	
2019-01-08 14:08:31,223 - 10 - training_embed.py - training - Epoch: [7][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.8529 (348.1422)	
2019-01-08 14:08:31,446 - 10 - training_embed.py - training - Epoch: [7][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.1140 (348.1651)	
2019-01-08 14:08:31,666 - 10 - training_embed.py - training - Epoch: [7][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.8354 (348.2359)	
2019-01-08 14:08:31,885 - 10 - training_embed.py - training - Epoch: [7][500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 351.1223 (348.2291)	
2019-01-08 14:08:32,107 - 10 - training_embed.py - training - Epoch: [7][600/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 347.6592 (348.2585)	
2019-01-08 14:08:32,328 - 10 - training_embed.py - training - Epoch: [7][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 353.8080 (348.3010)	
2019-01-08 14:08:32,558 - 10 - training_embed.py - training - Epoch: [7][800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.9093 (348.2657)	
2019-01-08 14:08:32,787 - 10 - training_embed.py - training - Epoch: [7][900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 346.8548 (348.2470)	
2019-01-08 14:08:33,022 - 10 - training_embed.py - training - Epoch: [7][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.3911 (348.2753)	
2019-01-08 14:08:33,251 - 10 - training_embed.py - training - Epoch: [7][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.1722 (348.2682)	
2019-01-08 14:08:33,466 - 10 - training_embed.py - training - Epoch: [7][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.3430 (348.2503)	
2019-01-08 14:08:33,699 - 10 - training_embed.py - training - Epoch: [7][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.3166 (348.2200)	
2019-01-08 14:08:33,931 - 10 - training_embed.py - training - Epoch: [7][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.5113 (348.2202)	
2019-01-08 14:08:34,149 - 10 - training_embed.py - training - Epoch: [7][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.8627 (348.2376)	
2019-01-08 14:08:34,377 - 10 - training_embed.py - training - Epoch: [7][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.3564 (348.2144)	
2019-01-08 14:08:34,600 - 10 - training_embed.py - training - Epoch: [7][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.9524 (348.1960)	
2019-01-08 14:08:34,828 - 10 - training_embed.py - training - Epoch: [7][1800/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 346.9938 (348.1862)	
2019-01-08 14:08:35,045 - 10 - training_embed.py - training - Epoch: [7][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 350.3205 (348.1864)	
2019-01-08 14:08:35,267 - 10 - training_embed.py - training - Epoch: [7][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.3221 (348.1730)	
2019-01-08 14:08:35,481 - 10 - training_embed.py - training - Epoch: [7][2100/2109]	Time 0.007 (0.002)	Data 0.006 (0.001)	Loss 347.2103 (348.1545)	
2019-01-08 14:08:35,495 - 10 - training_embed.py - training - loss: 348.114987
2019-01-08 14:08:35,495 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 14:08:35,945 - 10 - training_embed.py - training - Epoch: [8][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.7435 (347.6076)	
2019-01-08 14:08:36,174 - 10 - training_embed.py - training - Epoch: [8][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.7634 (347.8234)	
2019-01-08 14:08:36,389 - 10 - training_embed.py - training - Epoch: [8][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.6806 (347.8556)	
2019-01-08 14:08:36,609 - 10 - training_embed.py - training - Epoch: [8][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.8780 (347.8046)	
2019-01-08 14:08:36,839 - 10 - training_embed.py - training - Epoch: [8][500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 345.3230 (347.7675)	
2019-01-08 14:08:37,070 - 10 - training_embed.py - training - Epoch: [8][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.8642 (347.7287)	
2019-01-08 14:08:37,290 - 10 - training_embed.py - training - Epoch: [8][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.7704 (347.7077)	
2019-01-08 14:08:37,519 - 10 - training_embed.py - training - Epoch: [8][800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.5906 (347.7366)	
2019-01-08 14:08:37,730 - 10 - training_embed.py - training - Epoch: [8][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.9937 (347.7329)	
2019-01-08 14:08:37,959 - 10 - training_embed.py - training - Epoch: [8][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 342.3523 (347.7487)	
2019-01-08 14:08:38,167 - 10 - training_embed.py - training - Epoch: [8][1100/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 349.8261 (347.7380)	
2019-01-08 14:08:38,392 - 10 - training_embed.py - training - Epoch: [8][1200/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 349.8122 (347.7039)	
2019-01-08 14:08:38,607 - 10 - training_embed.py - training - Epoch: [8][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 343.5794 (347.6947)	
2019-01-08 14:08:38,849 - 10 - training_embed.py - training - Epoch: [8][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.3008 (347.6681)	
2019-01-08 14:08:39,076 - 10 - training_embed.py - training - Epoch: [8][1500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 346.3481 (347.6333)	
2019-01-08 14:08:39,300 - 10 - training_embed.py - training - Epoch: [8][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.8328 (347.6064)	
2019-01-08 14:08:39,518 - 10 - training_embed.py - training - Epoch: [8][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.8688 (347.5864)	
2019-01-08 14:08:39,757 - 10 - training_embed.py - training - Epoch: [8][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.2364 (347.5782)	
2019-01-08 14:08:39,976 - 10 - training_embed.py - training - Epoch: [8][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.4945 (347.5469)	
2019-01-08 14:08:40,201 - 10 - training_embed.py - training - Epoch: [8][2000/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 350.6026 (347.5321)	
2019-01-08 14:08:40,433 - 10 - training_embed.py - training - Epoch: [8][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 347.6586 (347.5051)	
2019-01-08 14:08:40,447 - 10 - training_embed.py - training - loss: 347.471138
2019-01-08 14:08:40,447 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 14:08:40,903 - 10 - training_embed.py - training - Epoch: [9][100/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 346.3767 (347.2584)	
2019-01-08 14:08:41,125 - 10 - training_embed.py - training - Epoch: [9][200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.7930 (347.0989)	
2019-01-08 14:08:41,354 - 10 - training_embed.py - training - Epoch: [9][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.4268 (347.1644)	
2019-01-08 14:08:41,581 - 10 - training_embed.py - training - Epoch: [9][400/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 346.8895 (347.2036)	
2019-01-08 14:08:41,802 - 10 - training_embed.py - training - Epoch: [9][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.7728 (347.2547)	
2019-01-08 14:08:42,033 - 10 - training_embed.py - training - Epoch: [9][600/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 349.2171 (347.2779)	
2019-01-08 14:08:42,237 - 10 - training_embed.py - training - Epoch: [9][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.8487 (347.2219)	
2019-01-08 14:08:42,452 - 10 - training_embed.py - training - Epoch: [9][800/2109]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 351.2480 (347.1909)	
2019-01-08 14:08:42,673 - 10 - training_embed.py - training - Epoch: [9][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.8830 (347.1548)	
2019-01-08 14:08:42,904 - 10 - training_embed.py - training - Epoch: [9][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.6545 (347.0986)	
2019-01-08 14:08:43,116 - 10 - training_embed.py - training - Epoch: [9][1100/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 350.0528 (347.1123)	
2019-01-08 14:08:43,342 - 10 - training_embed.py - training - Epoch: [9][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.8301 (347.0546)	
2019-01-08 14:08:43,555 - 10 - training_embed.py - training - Epoch: [9][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.6650 (347.0327)	
2019-01-08 14:08:43,783 - 10 - training_embed.py - training - Epoch: [9][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.4813 (347.0137)	
2019-01-08 14:08:44,024 - 10 - training_embed.py - training - Epoch: [9][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.3041 (346.9993)	
2019-01-08 14:08:44,240 - 10 - training_embed.py - training - Epoch: [9][1600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 342.2887 (346.9928)	
2019-01-08 14:08:44,472 - 10 - training_embed.py - training - Epoch: [9][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.5985 (346.9792)	
2019-01-08 14:08:44,690 - 10 - training_embed.py - training - Epoch: [9][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.3980 (346.9462)	
2019-01-08 14:08:44,914 - 10 - training_embed.py - training - Epoch: [9][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.4665 (346.9317)	
2019-01-08 14:08:45,130 - 10 - training_embed.py - training - Epoch: [9][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.7828 (346.8940)	
2019-01-08 14:08:45,359 - 10 - training_embed.py - training - Epoch: [9][2100/2109]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 350.8353 (346.8615)	
2019-01-08 14:08:45,374 - 10 - training_embed.py - training - loss: 346.822375
2019-01-08 14:08:45,374 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 14:08:45,811 - 10 - training_embed.py - training - Epoch: [10][100/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 346.8274 (346.5520)	
2019-01-08 14:08:46,020 - 10 - training_embed.py - training - Epoch: [10][200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 347.9656 (346.4301)	
2019-01-08 14:08:46,223 - 10 - training_embed.py - training - Epoch: [10][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.9128 (346.4960)	
2019-01-08 14:08:46,460 - 10 - training_embed.py - training - Epoch: [10][400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 343.4751 (346.5489)	
2019-01-08 14:08:46,671 - 10 - training_embed.py - training - Epoch: [10][500/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 342.3957 (346.4995)	
2019-01-08 14:08:46,916 - 10 - training_embed.py - training - Epoch: [10][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.4021 (346.4884)	
2019-01-08 14:08:47,123 - 10 - training_embed.py - training - Epoch: [10][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.9968 (346.3829)	
2019-01-08 14:08:47,349 - 10 - training_embed.py - training - Epoch: [10][800/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 345.2136 (346.3672)	
2019-01-08 14:08:47,587 - 10 - training_embed.py - training - Epoch: [10][900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 351.5940 (346.3883)	
2019-01-08 14:08:47,818 - 10 - training_embed.py - training - Epoch: [10][1000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.9603 (346.3466)	
2019-01-08 14:08:48,064 - 10 - training_embed.py - training - Epoch: [10][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.0256 (346.3001)	
2019-01-08 14:08:48,283 - 10 - training_embed.py - training - Epoch: [10][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.9896 (346.3223)	
2019-01-08 14:08:48,501 - 10 - training_embed.py - training - Epoch: [10][1300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.4119 (346.3341)	
2019-01-08 14:08:48,715 - 10 - training_embed.py - training - Epoch: [10][1400/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 347.1902 (346.3347)	
2019-01-08 14:08:48,953 - 10 - training_embed.py - training - Epoch: [10][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.4467 (346.2734)	
2019-01-08 14:08:49,174 - 10 - training_embed.py - training - Epoch: [10][1600/2109]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 346.3578 (346.2576)	
2019-01-08 14:08:49,395 - 10 - training_embed.py - training - Epoch: [10][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.7020 (346.2595)	
2019-01-08 14:08:49,614 - 10 - training_embed.py - training - Epoch: [10][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 343.8750 (346.2425)	
2019-01-08 14:08:49,851 - 10 - training_embed.py - training - Epoch: [10][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.5036 (346.2199)	
2019-01-08 14:08:50,087 - 10 - training_embed.py - training - Epoch: [10][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.8131 (346.2091)	
2019-01-08 14:08:50,318 - 10 - training_embed.py - training - Epoch: [10][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 346.4978 (346.2085)	
2019-01-08 14:08:50,332 - 10 - training_embed.py - training - loss: 346.167292
2019-01-08 14:08:50,332 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 14:08:50,771 - 10 - training_embed.py - training - Epoch: [11][100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.3215 (346.0482)	
2019-01-08 14:08:50,987 - 10 - training_embed.py - training - Epoch: [11][200/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 347.8466 (346.2052)	
2019-01-08 14:08:51,209 - 10 - training_embed.py - training - Epoch: [11][300/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 343.3477 (346.1217)	
2019-01-08 14:08:51,444 - 10 - training_embed.py - training - Epoch: [11][400/2109]	Time 0.008 (0.002)	Data 0.007 (0.001)	Loss 348.8609 (346.0415)	
2019-01-08 14:08:51,684 - 10 - training_embed.py - training - Epoch: [11][500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 344.6097 (346.0738)	
2019-01-08 14:08:51,903 - 10 - training_embed.py - training - Epoch: [11][600/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 347.2397 (345.9730)	
2019-01-08 14:08:52,121 - 10 - training_embed.py - training - Epoch: [11][700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 346.0209 (345.9710)	
2019-01-08 14:08:52,328 - 10 - training_embed.py - training - Epoch: [11][800/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 344.8023 (345.8578)	
2019-01-08 14:08:52,579 - 10 - training_embed.py - training - Epoch: [11][900/2109]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 343.0077 (345.8325)	
2019-01-08 14:08:52,805 - 10 - training_embed.py - training - Epoch: [11][1000/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 348.4666 (345.7841)	
2019-01-08 14:08:53,030 - 10 - training_embed.py - training - Epoch: [11][1100/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 342.7511 (345.7373)	
2019-01-08 14:08:53,243 - 10 - training_embed.py - training - Epoch: [11][1200/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 349.1634 (345.7107)	
2019-01-08 14:08:53,446 - 10 - training_embed.py - training - Epoch: [11][1300/2109]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 345.6768 (345.6779)	
2019-01-08 14:08:53,673 - 10 - training_embed.py - training - Epoch: [11][1400/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.7061 (345.6695)	
2019-01-08 14:08:53,899 - 10 - training_embed.py - training - Epoch: [11][1500/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 348.5535 (345.6493)	
2019-01-08 14:08:54,135 - 10 - training_embed.py - training - Epoch: [11][1600/2109]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 345.3989 (345.6155)	
2019-01-08 14:08:54,376 - 10 - training_embed.py - training - Epoch: [11][1700/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.0216 (345.6084)	
2019-01-08 14:08:54,595 - 10 - training_embed.py - training - Epoch: [11][1800/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 343.7981 (345.5892)	
2019-01-08 14:08:54,836 - 10 - training_embed.py - training - Epoch: [11][1900/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 345.2520 (345.5868)	
2019-01-08 14:08:55,072 - 10 - training_embed.py - training - Epoch: [11][2000/2109]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 341.9630 (345.5674)	
2019-01-08 14:08:55,320 - 10 - training_embed.py - training - Epoch: [11][2100/2109]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 343.9340 (345.5479)	
2019-01-08 14:08:55,334 - 10 - training_embed.py - training - loss: 345.507019
2019-01-08 14:08:55,410 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 14:08:56,378 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 967.761 ms ~ 0.016 min ~ 0.968 sec
2019-01-08 14:08:57,204 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1793.812 ms ~ 0.030 min ~ 1.794 sec
2019-01-08 14:08:57,204 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 14:08:57,204 - 10 - corpus.py - subactivity_sampler - 0 / 173
2019-01-08 14:08:57,205 - 10 - corpus.py - subactivity_sampler - [ 82322.  10662. 135854.   5513. 131238.   2887. 143523.  27734.]
2019-01-08 14:10:22,761 - 10 - corpus.py - subactivity_sampler - 20 / 173
2019-01-08 14:10:22,762 - 10 - corpus.py - subactivity_sampler - [ 82212.  10540. 135209.   5498. 131836.   2872. 144153.  27413.]
2019-01-08 14:11:18,165 - 10 - corpus.py - subactivity_sampler - 40 / 173
2019-01-08 14:11:18,165 - 10 - corpus.py - subactivity_sampler - [ 82198.  10433. 135307.   5479. 131827.   2877. 144213.  27399.]
2019-01-08 14:12:30,003 - 10 - corpus.py - subactivity_sampler - 60 / 173
2019-01-08 14:12:30,003 - 10 - corpus.py - subactivity_sampler - [ 82218.  10282. 135467.   5473. 131848.   2865. 144234.  27346.]
2019-01-08 14:13:41,693 - 10 - corpus.py - subactivity_sampler - 80 / 173
2019-01-08 14:13:41,693 - 10 - corpus.py - subactivity_sampler - [ 82188.  10096. 135658.   5450. 131988.   2843. 144175.  27335.]
2019-01-08 14:14:58,897 - 10 - corpus.py - subactivity_sampler - 100 / 173
2019-01-08 14:14:58,897 - 10 - corpus.py - subactivity_sampler - [ 81937.   9999. 135893.   5447. 131998.   2825. 144495.  27139.]
2019-01-08 14:15:43,442 - 10 - corpus.py - subactivity_sampler - 120 / 173
2019-01-08 14:15:43,442 - 10 - corpus.py - subactivity_sampler - [ 81815.   9910. 135831.   5445. 132042.   2816. 144780.  27094.]
2019-01-08 14:16:34,700 - 10 - corpus.py - subactivity_sampler - 140 / 173
2019-01-08 14:16:34,700 - 10 - corpus.py - subactivity_sampler - [ 81750.   9864. 135911.   5758. 132114.   2805. 144904.  26627.]
2019-01-08 14:17:40,007 - 10 - corpus.py - subactivity_sampler - 160 / 173
2019-01-08 14:17:40,008 - 10 - corpus.py - subactivity_sampler - [ 81706.   9693. 135055.   5750. 132469.   2790. 145996.  26274.]
2019-01-08 14:18:15,727 - 10 - corpus.py - subactivity_sampler - [ 81701.   9668. 135277.   5748. 132548.   2780. 146195.  25816.]
2019-01-08 14:18:15,727 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 558523.136 ms ~ 9.309 min ~ 558.523 sec
2019-01-08 14:18:15,727 - 10 - corpus.py - ordering_sampler - .
2019-01-08 14:18:19,772 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 14:18:19,772 - 10 - corpus.py - ordering_sampler - inv_count_vec: [13.  9. 80.  6. 75.  7.  7.]
2019-01-08 14:18:19,772 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 14:18:19,793 - 10 - corpus.py - rho_sampling - ['52.1166', '32.4659', '49.3809', '120.7610', '4.0691', '23.5048', '8.7595']
2019-01-08 14:18:19,793 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 14:18:19,956 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 14:18:19,966 - 10 - accuracy_class.py - mof - # gt_labels: 9   # pr_labels: 8
2019-01-08 14:18:19,966 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 17', '1: 12', '2: 11', '3: 16', '4: 13', '5: 10', '6: 15', '7: 14']
2019-01-08 14:18:19,986 - 10 - accuracy_class.py - mof_val - frames true: 187665	frames overall : 539733
2019-01-08 14:18:19,987 - 10 - corpus.py - accuracy_corpus - Action: friedegg
2019-01-08 14:18:19,987 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3476996959607806
2019-01-08 14:18:19,987 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3476996959607806
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 17933
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - mof_classes - label 10: 0.000000  0 / 10971
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - mof_classes - label 11: 0.419248  28939 / 69026
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - mof_classes - label 12: 0.035633  1194 / 33508
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - mof_classes - label 13: 0.351606  111917 / 318302
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - mof_classes - label 14: 0.129104  3319 / 25708
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - mof_classes - label 15: 0.573032  18329 / 31986
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 2201
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - mof_classes - label 17: 0.796299  23967 / 30098
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - mof_classes - average class mof: 0.256102
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 17933
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - iou_classes - label 10: 0.000000  0 / 13751
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - iou_classes - label 11: 0.165022  28939 / 175364
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - iou_classes - label 12: 0.028441  1194 / 41982
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - iou_classes - label 13: 0.330204  111917 / 338933
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - iou_classes - label 14: 0.068852  3319 / 48205
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - iou_classes - label 15: 0.114662  18329 / 159852
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 7949
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - iou_classes - label 17: 0.272873  23967 / 87832
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - iou_classes - average IoU: 0.122507
2019-01-08 14:18:19,987 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.108895
2019-01-08 14:18:26,249 - 10 - f1_score.py - f1 - f1 score: 0.242870
2019-01-08 14:18:26,277 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 6484.703 ms ~ 0.108 min ~ 6.485 sec
2019-01-08 14:18:26,303 - 10 - utils.py - wrap - <function baseline at 0x7f3991d24e18> took 4483698.367 ms ~ 74.728 min ~ 4483.698 sec
2019-01-08 14:18:26,303 - 10 - utils.py - update_opt_str - batch_size: 256
2019-01-08 14:18:26,303 - 10 - utils.py - update_opt_str - bg: False
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - bg_trh: 85
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - data: /media/data/kukleva/lab/Breakfast/feat/s1
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - data_type: 2
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - dataset: bf
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - dataset_root: /media/data/kukleva/lab/Breakfast
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - embed_dim: 30
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - epochs: 12
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - ext: txt
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - feature_dim: 64
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - full: True
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - gmm: 1
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - gmms: one
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - gt: /media/data/kukleva/lab/Breakfast/feat/s1/groundTruth
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - gt_training: False
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - log: DEBUG
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - log_str: slim.mallow._salat_!bg_data2_bf_embed30_epochs12_full_gmm1_one_!gt_lr1e-10_!lr_ordering_reg0.1_!viterbi_!zeros_
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - lr: 1e-10
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - lr_adj: False
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - momentum: 0.9
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - num_workers: 4
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - ordering: True
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - prefix: slim.mallow.
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - reg_cov: 0.1
2019-01-08 14:18:26,304 - 10 - utils.py - update_opt_str - resume: False
2019-01-08 14:18:26,305 - 10 - utils.py - update_opt_str - resume_str: 
2019-01-08 14:18:26,305 - 10 - utils.py - update_opt_str - save_embed_feat: False
2019-01-08 14:18:26,305 - 10 - utils.py - update_opt_str - save_likelihood: False
2019-01-08 14:18:26,305 - 10 - utils.py - update_opt_str - save_model: False
2019-01-08 14:18:26,305 - 10 - utils.py - update_opt_str - seed: 0
2019-01-08 14:18:26,305 - 10 - utils.py - update_opt_str - subaction: salat
2019-01-08 14:18:26,305 - 10 - utils.py - update_opt_str - vis: False
2019-01-08 14:18:26,305 - 10 - utils.py - update_opt_str - viterbi: False
2019-01-08 14:18:26,305 - 10 - utils.py - update_opt_str - weight_decay: 0.0001
2019-01-08 14:18:26,305 - 10 - utils.py - update_opt_str - zeros: False
2019-01-08 14:18:26,325 - 10 - utils.py - wrap - <function GroundTruth.load_gt at 0x7f39322b36a8> took 20.126 ms ~ 0.000 min ~ 0.020 sec
2019-01-08 14:18:26,447 - 10 - corpus.py - __init__ - salat  subactions: 7
2019-01-08 14:18:26,448 - 10 - corpus.py - _init_videos - .
2019-01-08 14:18:46,890 - 10 - corpus.py - _init_videos - gt statistic: Counter({32: 313690, 34: 165624, 33: 50285, 14: 8742, 4: 8463, 35: 7789, 18: 4335})
2019-01-08 14:18:46,890 - 10 - corpus.py - _update_fg_mask - .
2019-01-08 14:18:46,934 - 10 - corpus.py - __init__ - min: -48.370804  max: 38.258369  avg: 0.020495
2019-01-08 14:18:46,934 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 14:18:46,973 - 10 - corpus.py - rho_sampling - ['64.0767', '103.1747', '929.0535', '90.9104', '17.1809', '164.1557']
2019-01-08 14:18:46,973 - 10 - pipeline.py - baseline - Iteration 0
2019-01-08 14:18:47,164 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 14:18:47,175 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 14:18:47,175 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 14', '1: 18', '2: 32', '3: 35', '4: 34', '5: 4', '6: 33']
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - mof_val - frames true: 117810	frames overall : 558928
2019-01-08 14:18:47,193 - 10 - corpus.py - accuracy_corpus - Action: salat
2019-01-08 14:18:47,193 - 10 - corpus.py - accuracy_corpus - MoF val: 0.2107784902527696
2019-01-08 14:18:47,193 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.0
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 23956
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - mof_classes - label 4: 0.148167  1160 / 7829
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - mof_classes - label 14: 0.976763  5885 / 6025
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - mof_classes - label 18: 0.313936  820 / 2612
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - mof_classes - label 32: 0.177689  54463 / 306508
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - mof_classes - label 33: 0.438126  18637 / 42538
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - mof_classes - label 34: 0.225434  36845 / 163440
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - mof_classes - label 35: 0.000000  0 / 6020
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - mof_classes - average class mof: 0.285014
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 23956
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - iou_classes - label 4: 0.013415  1160 / 86469
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - iou_classes - label 14: 0.073510  5885 / 80057
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - iou_classes - label 18: 0.010039  820 / 81683
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - iou_classes - label 32: 0.164089  54463 / 331912
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - iou_classes - label 33: 0.179772  18637 / 103670
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - iou_classes - label 34: 0.178494  36845 / 206422
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - iou_classes - label 35: 0.000000  0 / 85877
2019-01-08 14:18:47,193 - 10 - accuracy_class.py - iou_classes - average IoU: 0.088474
2019-01-08 14:18:47,194 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.077415
2019-01-08 14:18:56,266 - 10 - f1_score.py - f1 - f1 score: 0.255864
2019-01-08 14:18:56,290 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 9317.026 ms ~ 0.155 min ~ 9.317 sec
2019-01-08 14:18:56,290 - 10 - corpus.py - embedding_training - .
2019-01-08 14:18:56,290 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 14:18:56,290 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 14:18:56,290 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 14:19:00,174 - 10 - training_embed.py - training - create model
2019-01-08 14:19:00,175 - 10 - training_embed.py - training - epochs: 12
2019-01-08 14:19:00,175 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 14:19:00,533 - 10 - training_embed.py - training - Epoch: [0][100/2184]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 305.5966 (307.4465)	
2019-01-08 14:19:00,738 - 10 - training_embed.py - training - Epoch: [0][200/2184]	Time 0.003 (0.003)	Data 0.002 (0.002)	Loss 307.2037 (307.2567)	
2019-01-08 14:19:00,960 - 10 - training_embed.py - training - Epoch: [0][300/2184]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 308.6679 (307.1164)	
2019-01-08 14:19:01,198 - 10 - training_embed.py - training - Epoch: [0][400/2184]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 302.9954 (307.1623)	
2019-01-08 14:19:01,409 - 10 - training_embed.py - training - Epoch: [0][500/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 304.3896 (307.0708)	
2019-01-08 14:19:01,620 - 10 - training_embed.py - training - Epoch: [0][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6415 (307.0507)	
2019-01-08 14:19:01,838 - 10 - training_embed.py - training - Epoch: [0][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7522 (307.1143)	
2019-01-08 14:19:02,043 - 10 - training_embed.py - training - Epoch: [0][800/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.7514 (307.0951)	
2019-01-08 14:19:02,268 - 10 - training_embed.py - training - Epoch: [0][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6956 (307.1287)	
2019-01-08 14:19:02,487 - 10 - training_embed.py - training - Epoch: [0][1000/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 309.7223 (307.1508)	
2019-01-08 14:19:02,697 - 10 - training_embed.py - training - Epoch: [0][1100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.3467 (307.1434)	
2019-01-08 14:19:02,908 - 10 - training_embed.py - training - Epoch: [0][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.1404 (307.1338)	
2019-01-08 14:19:03,131 - 10 - training_embed.py - training - Epoch: [0][1300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.5289 (307.1407)	
2019-01-08 14:19:03,347 - 10 - training_embed.py - training - Epoch: [0][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3803 (307.1500)	
2019-01-08 14:19:03,603 - 10 - training_embed.py - training - Epoch: [0][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9976 (307.1544)	
2019-01-08 14:19:03,815 - 10 - training_embed.py - training - Epoch: [0][1600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.5220 (307.1402)	
2019-01-08 14:19:04,028 - 10 - training_embed.py - training - Epoch: [0][1700/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 309.3360 (307.1418)	
2019-01-08 14:19:04,254 - 10 - training_embed.py - training - Epoch: [0][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8747 (307.1403)	
2019-01-08 14:19:04,466 - 10 - training_embed.py - training - Epoch: [0][1900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.2246 (307.1484)	
2019-01-08 14:19:04,706 - 10 - training_embed.py - training - Epoch: [0][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6370 (307.1485)	
2019-01-08 14:19:04,923 - 10 - training_embed.py - training - Epoch: [0][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7636 (307.1389)	
2019-01-08 14:19:05,098 - 10 - training_embed.py - training - loss: 307.106299
2019-01-08 14:19:05,099 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 14:19:05,506 - 10 - training_embed.py - training - Epoch: [1][100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.0732 (306.8901)	
2019-01-08 14:19:05,731 - 10 - training_embed.py - training - Epoch: [1][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1929 (307.0525)	
2019-01-08 14:19:05,961 - 10 - training_embed.py - training - Epoch: [1][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9960 (306.9881)	
2019-01-08 14:19:06,175 - 10 - training_embed.py - training - Epoch: [1][400/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 308.6633 (306.9693)	
2019-01-08 14:19:06,394 - 10 - training_embed.py - training - Epoch: [1][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1049 (306.9744)	
2019-01-08 14:19:06,621 - 10 - training_embed.py - training - Epoch: [1][600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.7712 (306.9977)	
2019-01-08 14:19:06,842 - 10 - training_embed.py - training - Epoch: [1][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0090 (306.9913)	
2019-01-08 14:19:07,068 - 10 - training_embed.py - training - Epoch: [1][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2052 (307.0046)	
2019-01-08 14:19:07,280 - 10 - training_embed.py - training - Epoch: [1][900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.8677 (307.0082)	
2019-01-08 14:19:07,492 - 10 - training_embed.py - training - Epoch: [1][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6727 (307.0433)	
2019-01-08 14:19:07,702 - 10 - training_embed.py - training - Epoch: [1][1100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.7706 (307.0552)	
2019-01-08 14:19:07,920 - 10 - training_embed.py - training - Epoch: [1][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.4876 (307.1034)	
2019-01-08 14:19:08,122 - 10 - training_embed.py - training - Epoch: [1][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3353 (307.1114)	
2019-01-08 14:19:08,351 - 10 - training_embed.py - training - Epoch: [1][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8364 (307.1156)	
2019-01-08 14:19:08,560 - 10 - training_embed.py - training - Epoch: [1][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4449 (307.1210)	
2019-01-08 14:19:08,769 - 10 - training_embed.py - training - Epoch: [1][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8407 (307.1306)	
2019-01-08 14:19:08,983 - 10 - training_embed.py - training - Epoch: [1][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0517 (307.1117)	
2019-01-08 14:19:09,208 - 10 - training_embed.py - training - Epoch: [1][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4377 (307.1029)	
2019-01-08 14:19:09,432 - 10 - training_embed.py - training - Epoch: [1][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6998 (307.1156)	
2019-01-08 14:19:09,650 - 10 - training_embed.py - training - Epoch: [1][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.9947 (307.1112)	
2019-01-08 14:19:09,874 - 10 - training_embed.py - training - Epoch: [1][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6456 (307.1087)	
2019-01-08 14:19:10,049 - 10 - training_embed.py - training - loss: 307.073981
2019-01-08 14:19:10,049 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 14:19:10,471 - 10 - training_embed.py - training - Epoch: [2][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0419 (307.0155)	
2019-01-08 14:19:10,702 - 10 - training_embed.py - training - Epoch: [2][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1862 (306.9841)	
2019-01-08 14:19:10,928 - 10 - training_embed.py - training - Epoch: [2][300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.9983 (306.9154)	
2019-01-08 14:19:11,142 - 10 - training_embed.py - training - Epoch: [2][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5491 (306.9664)	
2019-01-08 14:19:11,358 - 10 - training_embed.py - training - Epoch: [2][500/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.6227 (307.0141)	
2019-01-08 14:19:11,583 - 10 - training_embed.py - training - Epoch: [2][600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.9215 (307.0280)	
2019-01-08 14:19:11,785 - 10 - training_embed.py - training - Epoch: [2][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8915 (307.0151)	
2019-01-08 14:19:12,002 - 10 - training_embed.py - training - Epoch: [2][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.0788 (306.9951)	
2019-01-08 14:19:12,216 - 10 - training_embed.py - training - Epoch: [2][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.9736 (307.0253)	
2019-01-08 14:19:12,422 - 10 - training_embed.py - training - Epoch: [2][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1453 (307.0097)	
2019-01-08 14:19:12,656 - 10 - training_embed.py - training - Epoch: [2][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2709 (307.0003)	
2019-01-08 14:19:12,855 - 10 - training_embed.py - training - Epoch: [2][1200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 308.0925 (306.9973)	
2019-01-08 14:19:13,085 - 10 - training_embed.py - training - Epoch: [2][1300/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 302.1328 (307.0204)	
2019-01-08 14:19:13,302 - 10 - training_embed.py - training - Epoch: [2][1400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 308.1397 (307.0397)	
2019-01-08 14:19:13,517 - 10 - training_embed.py - training - Epoch: [2][1500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.9154 (307.0646)	
2019-01-08 14:19:13,738 - 10 - training_embed.py - training - Epoch: [2][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8485 (307.0472)	
2019-01-08 14:19:13,968 - 10 - training_embed.py - training - Epoch: [2][1700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.6434 (307.0575)	
2019-01-08 14:19:14,187 - 10 - training_embed.py - training - Epoch: [2][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0146 (307.0651)	
2019-01-08 14:19:14,419 - 10 - training_embed.py - training - Epoch: [2][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9431 (307.0645)	
2019-01-08 14:19:14,618 - 10 - training_embed.py - training - Epoch: [2][2000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 312.7621 (307.0697)	
2019-01-08 14:19:14,823 - 10 - training_embed.py - training - Epoch: [2][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9458 (307.0658)	
2019-01-08 14:19:15,003 - 10 - training_embed.py - training - loss: 307.041957
2019-01-08 14:19:15,003 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 14:19:15,425 - 10 - training_embed.py - training - Epoch: [3][100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.6162 (307.1803)	
2019-01-08 14:19:15,648 - 10 - training_embed.py - training - Epoch: [3][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.8472 (307.0277)	
2019-01-08 14:19:15,868 - 10 - training_embed.py - training - Epoch: [3][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1279 (307.1007)	
2019-01-08 14:19:16,078 - 10 - training_embed.py - training - Epoch: [3][400/2184]	Time 0.005 (0.002)	Data 0.002 (0.001)	Loss 305.1848 (307.0563)	
2019-01-08 14:19:16,290 - 10 - training_embed.py - training - Epoch: [3][500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.3020 (307.0486)	
2019-01-08 14:19:16,506 - 10 - training_embed.py - training - Epoch: [3][600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.5576 (307.0527)	
2019-01-08 14:19:16,735 - 10 - training_embed.py - training - Epoch: [3][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4606 (306.9895)	
2019-01-08 14:19:16,951 - 10 - training_embed.py - training - Epoch: [3][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.7937 (307.0202)	
2019-01-08 14:19:17,163 - 10 - training_embed.py - training - Epoch: [3][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.0262 (307.0545)	
2019-01-08 14:19:17,367 - 10 - training_embed.py - training - Epoch: [3][1000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.1962 (307.0118)	
2019-01-08 14:19:17,604 - 10 - training_embed.py - training - Epoch: [3][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5418 (307.0221)	
2019-01-08 14:19:17,826 - 10 - training_embed.py - training - Epoch: [3][1200/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 310.5144 (307.0016)	
2019-01-08 14:19:18,056 - 10 - training_embed.py - training - Epoch: [3][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2318 (307.0324)	
2019-01-08 14:19:18,284 - 10 - training_embed.py - training - Epoch: [3][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.3923 (307.0122)	
2019-01-08 14:19:18,501 - 10 - training_embed.py - training - Epoch: [3][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7306 (306.9967)	
2019-01-08 14:19:18,723 - 10 - training_embed.py - training - Epoch: [3][1600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.0530 (306.9901)	
2019-01-08 14:19:18,945 - 10 - training_embed.py - training - Epoch: [3][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5904 (307.0070)	
2019-01-08 14:19:19,192 - 10 - training_embed.py - training - Epoch: [3][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.9972 (307.0078)	
2019-01-08 14:19:19,430 - 10 - training_embed.py - training - Epoch: [3][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4552 (307.0088)	
2019-01-08 14:19:19,653 - 10 - training_embed.py - training - Epoch: [3][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4724 (307.0299)	
2019-01-08 14:19:19,886 - 10 - training_embed.py - training - Epoch: [3][2100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.5139 (307.0353)	
2019-01-08 14:19:20,070 - 10 - training_embed.py - training - loss: 307.010144
2019-01-08 14:19:20,071 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 14:19:20,475 - 10 - training_embed.py - training - Epoch: [4][100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 313.9399 (306.8347)	
2019-01-08 14:19:20,686 - 10 - training_embed.py - training - Epoch: [4][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.8690 (306.9283)	
2019-01-08 14:19:20,895 - 10 - training_embed.py - training - Epoch: [4][300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.1907 (306.9956)	
2019-01-08 14:19:21,112 - 10 - training_embed.py - training - Epoch: [4][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.5486 (306.9164)	
2019-01-08 14:19:21,328 - 10 - training_embed.py - training - Epoch: [4][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1866 (306.9804)	
2019-01-08 14:19:21,551 - 10 - training_embed.py - training - Epoch: [4][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0118 (306.9622)	
2019-01-08 14:19:21,756 - 10 - training_embed.py - training - Epoch: [4][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6012 (306.9648)	
2019-01-08 14:19:21,964 - 10 - training_embed.py - training - Epoch: [4][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4135 (306.9942)	
2019-01-08 14:19:22,173 - 10 - training_embed.py - training - Epoch: [4][900/2184]	Time 0.006 (0.002)	Data 0.001 (0.001)	Loss 304.0959 (307.0237)	
2019-01-08 14:19:22,402 - 10 - training_embed.py - training - Epoch: [4][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5317 (306.9938)	
2019-01-08 14:19:22,619 - 10 - training_embed.py - training - Epoch: [4][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.9568 (306.9646)	
2019-01-08 14:19:22,837 - 10 - training_embed.py - training - Epoch: [4][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7685 (306.9915)	
2019-01-08 14:19:23,052 - 10 - training_embed.py - training - Epoch: [4][1300/2184]	Time 0.006 (0.002)	Data 0.001 (0.001)	Loss 306.0103 (307.0075)	
2019-01-08 14:19:23,259 - 10 - training_embed.py - training - Epoch: [4][1400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.9045 (307.0002)	
2019-01-08 14:19:23,480 - 10 - training_embed.py - training - Epoch: [4][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6072 (307.0216)	
2019-01-08 14:19:23,696 - 10 - training_embed.py - training - Epoch: [4][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6366 (307.0110)	
2019-01-08 14:19:23,918 - 10 - training_embed.py - training - Epoch: [4][1700/2184]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 303.2197 (307.0052)	
2019-01-08 14:19:24,129 - 10 - training_embed.py - training - Epoch: [4][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 313.1243 (306.9966)	
2019-01-08 14:19:24,342 - 10 - training_embed.py - training - Epoch: [4][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.3714 (307.0090)	
2019-01-08 14:19:24,550 - 10 - training_embed.py - training - Epoch: [4][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1369 (307.0151)	
2019-01-08 14:19:24,783 - 10 - training_embed.py - training - Epoch: [4][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3131 (307.0034)	
2019-01-08 14:19:24,961 - 10 - training_embed.py - training - loss: 306.978084
2019-01-08 14:19:24,961 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 14:19:25,372 - 10 - training_embed.py - training - Epoch: [5][100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 303.6700 (307.3208)	
2019-01-08 14:19:25,585 - 10 - training_embed.py - training - Epoch: [5][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4909 (307.2868)	
2019-01-08 14:19:25,801 - 10 - training_embed.py - training - Epoch: [5][300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 303.0926 (307.1410)	
2019-01-08 14:19:26,036 - 10 - training_embed.py - training - Epoch: [5][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6377 (307.1214)	
2019-01-08 14:19:26,246 - 10 - training_embed.py - training - Epoch: [5][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.0565 (307.1044)	
2019-01-08 14:19:26,472 - 10 - training_embed.py - training - Epoch: [5][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.1960 (307.1129)	
2019-01-08 14:19:26,695 - 10 - training_embed.py - training - Epoch: [5][700/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.6034 (307.0783)	
2019-01-08 14:19:26,905 - 10 - training_embed.py - training - Epoch: [5][800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.9651 (307.0478)	
2019-01-08 14:19:27,128 - 10 - training_embed.py - training - Epoch: [5][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7025 (306.9997)	
2019-01-08 14:19:27,345 - 10 - training_embed.py - training - Epoch: [5][1000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 309.3994 (306.9795)	
2019-01-08 14:19:27,568 - 10 - training_embed.py - training - Epoch: [5][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8199 (306.9918)	
2019-01-08 14:19:27,784 - 10 - training_embed.py - training - Epoch: [5][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6236 (307.0125)	
2019-01-08 14:19:27,997 - 10 - training_embed.py - training - Epoch: [5][1300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.6957 (306.9861)	
2019-01-08 14:19:28,209 - 10 - training_embed.py - training - Epoch: [5][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1615 (306.9916)	
2019-01-08 14:19:28,448 - 10 - training_embed.py - training - Epoch: [5][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4189 (306.9976)	
2019-01-08 14:19:28,664 - 10 - training_embed.py - training - Epoch: [5][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5101 (306.9848)	
2019-01-08 14:19:28,892 - 10 - training_embed.py - training - Epoch: [5][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1048 (306.9719)	
2019-01-08 14:19:29,106 - 10 - training_embed.py - training - Epoch: [5][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.3157 (306.9886)	
2019-01-08 14:19:29,318 - 10 - training_embed.py - training - Epoch: [5][1900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.7278 (306.9763)	
2019-01-08 14:19:29,544 - 10 - training_embed.py - training - Epoch: [5][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0434 (306.9878)	
2019-01-08 14:19:29,768 - 10 - training_embed.py - training - Epoch: [5][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.8297 (306.9845)	
2019-01-08 14:19:29,969 - 10 - training_embed.py - training - loss: 306.946179
2019-01-08 14:19:29,970 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 14:19:30,379 - 10 - training_embed.py - training - Epoch: [6][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4926 (306.8598)	
2019-01-08 14:19:30,589 - 10 - training_embed.py - training - Epoch: [6][200/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 304.6205 (306.9576)	
2019-01-08 14:19:30,810 - 10 - training_embed.py - training - Epoch: [6][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.1882 (306.8581)	
2019-01-08 14:19:31,015 - 10 - training_embed.py - training - Epoch: [6][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4397 (306.8899)	
2019-01-08 14:19:31,240 - 10 - training_embed.py - training - Epoch: [6][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.2529 (306.8766)	
2019-01-08 14:19:31,449 - 10 - training_embed.py - training - Epoch: [6][600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.7700 (306.9077)	
2019-01-08 14:19:31,665 - 10 - training_embed.py - training - Epoch: [6][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2070 (306.8656)	
2019-01-08 14:19:31,907 - 10 - training_embed.py - training - Epoch: [6][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.7668 (306.8670)	
2019-01-08 14:19:32,121 - 10 - training_embed.py - training - Epoch: [6][900/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 305.0135 (306.8615)	
2019-01-08 14:19:32,368 - 10 - training_embed.py - training - Epoch: [6][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7363 (306.8790)	
2019-01-08 14:19:32,577 - 10 - training_embed.py - training - Epoch: [6][1100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.6139 (306.8772)	
2019-01-08 14:19:32,792 - 10 - training_embed.py - training - Epoch: [6][1200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.7033 (306.8889)	
2019-01-08 14:19:33,009 - 10 - training_embed.py - training - Epoch: [6][1300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.9854 (306.9205)	
2019-01-08 14:19:33,235 - 10 - training_embed.py - training - Epoch: [6][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4586 (306.9229)	
2019-01-08 14:19:33,470 - 10 - training_embed.py - training - Epoch: [6][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.1689 (306.9317)	
2019-01-08 14:19:33,687 - 10 - training_embed.py - training - Epoch: [6][1600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.8314 (306.9139)	
2019-01-08 14:19:33,890 - 10 - training_embed.py - training - Epoch: [6][1700/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.1313 (306.9311)	
2019-01-08 14:19:34,104 - 10 - training_embed.py - training - Epoch: [6][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0233 (306.9320)	
2019-01-08 14:19:34,337 - 10 - training_embed.py - training - Epoch: [6][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4876 (306.9321)	
2019-01-08 14:19:34,554 - 10 - training_embed.py - training - Epoch: [6][2000/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.2694 (306.9561)	
2019-01-08 14:19:34,786 - 10 - training_embed.py - training - Epoch: [6][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.2505 (306.9518)	
2019-01-08 14:19:34,961 - 10 - training_embed.py - training - loss: 306.913375
2019-01-08 14:19:34,961 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 14:19:35,374 - 10 - training_embed.py - training - Epoch: [7][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0479 (306.7857)	
2019-01-08 14:19:35,608 - 10 - training_embed.py - training - Epoch: [7][200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.3240 (306.8618)	
2019-01-08 14:19:35,820 - 10 - training_embed.py - training - Epoch: [7][300/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 307.9428 (306.8963)	
2019-01-08 14:19:36,067 - 10 - training_embed.py - training - Epoch: [7][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0899 (306.9506)	
2019-01-08 14:19:36,272 - 10 - training_embed.py - training - Epoch: [7][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.5146 (306.9683)	
2019-01-08 14:19:36,490 - 10 - training_embed.py - training - Epoch: [7][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2664 (306.9125)	
2019-01-08 14:19:36,713 - 10 - training_embed.py - training - Epoch: [7][700/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.3784 (306.9632)	
2019-01-08 14:19:36,943 - 10 - training_embed.py - training - Epoch: [7][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2966 (307.0313)	
2019-01-08 14:19:37,177 - 10 - training_embed.py - training - Epoch: [7][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5303 (307.0359)	
2019-01-08 14:19:37,406 - 10 - training_embed.py - training - Epoch: [7][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6826 (307.0211)	
2019-01-08 14:19:37,617 - 10 - training_embed.py - training - Epoch: [7][1100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.3444 (306.9850)	
2019-01-08 14:19:37,827 - 10 - training_embed.py - training - Epoch: [7][1200/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 304.4703 (306.9708)	
2019-01-08 14:19:38,043 - 10 - training_embed.py - training - Epoch: [7][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 312.9794 (306.9856)	
2019-01-08 14:19:38,267 - 10 - training_embed.py - training - Epoch: [7][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3441 (306.9559)	
2019-01-08 14:19:38,479 - 10 - training_embed.py - training - Epoch: [7][1500/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.0856 (306.9610)	
2019-01-08 14:19:38,689 - 10 - training_embed.py - training - Epoch: [7][1600/2184]	Time 0.007 (0.002)	Data 0.001 (0.001)	Loss 307.1431 (306.9603)	
2019-01-08 14:19:38,908 - 10 - training_embed.py - training - Epoch: [7][1700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.4674 (306.9564)	
2019-01-08 14:19:39,128 - 10 - training_embed.py - training - Epoch: [7][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4261 (306.9374)	
2019-01-08 14:19:39,338 - 10 - training_embed.py - training - Epoch: [7][1900/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 301.2689 (306.9295)	
2019-01-08 14:19:39,571 - 10 - training_embed.py - training - Epoch: [7][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0197 (306.9213)	
2019-01-08 14:19:39,804 - 10 - training_embed.py - training - Epoch: [7][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0617 (306.9131)	
2019-01-08 14:19:39,981 - 10 - training_embed.py - training - loss: 306.881826
2019-01-08 14:19:39,982 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 14:19:40,404 - 10 - training_embed.py - training - Epoch: [8][100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.5500 (306.8445)	
2019-01-08 14:19:40,620 - 10 - training_embed.py - training - Epoch: [8][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3949 (306.9043)	
2019-01-08 14:19:40,839 - 10 - training_embed.py - training - Epoch: [8][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0080 (306.8620)	
2019-01-08 14:19:41,046 - 10 - training_embed.py - training - Epoch: [8][400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 309.1869 (306.8268)	
2019-01-08 14:19:41,283 - 10 - training_embed.py - training - Epoch: [8][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2142 (306.7988)	
2019-01-08 14:19:41,515 - 10 - training_embed.py - training - Epoch: [8][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5570 (306.8008)	
2019-01-08 14:19:41,744 - 10 - training_embed.py - training - Epoch: [8][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.4108 (306.8171)	
2019-01-08 14:19:41,975 - 10 - training_embed.py - training - Epoch: [8][800/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 309.1433 (306.8234)	
2019-01-08 14:19:42,194 - 10 - training_embed.py - training - Epoch: [8][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4283 (306.8236)	
2019-01-08 14:19:42,401 - 10 - training_embed.py - training - Epoch: [8][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.5311 (306.8001)	
2019-01-08 14:19:42,625 - 10 - training_embed.py - training - Epoch: [8][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2418 (306.7820)	
2019-01-08 14:19:42,849 - 10 - training_embed.py - training - Epoch: [8][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1942 (306.7965)	
2019-01-08 14:19:43,072 - 10 - training_embed.py - training - Epoch: [8][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.2629 (306.8146)	
2019-01-08 14:19:43,281 - 10 - training_embed.py - training - Epoch: [8][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.0697 (306.8241)	
2019-01-08 14:19:43,499 - 10 - training_embed.py - training - Epoch: [8][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3120 (306.8031)	
2019-01-08 14:19:43,709 - 10 - training_embed.py - training - Epoch: [8][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.9669 (306.8106)	
2019-01-08 14:19:43,931 - 10 - training_embed.py - training - Epoch: [8][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1288 (306.8222)	
2019-01-08 14:19:44,143 - 10 - training_embed.py - training - Epoch: [8][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.2262 (306.8187)	
2019-01-08 14:19:44,366 - 10 - training_embed.py - training - Epoch: [8][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0495 (306.8534)	
2019-01-08 14:19:44,591 - 10 - training_embed.py - training - Epoch: [8][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6503 (306.8573)	
2019-01-08 14:19:44,794 - 10 - training_embed.py - training - Epoch: [8][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.0631 (306.8645)	
2019-01-08 14:19:44,969 - 10 - training_embed.py - training - loss: 306.849747
2019-01-08 14:19:44,969 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 14:19:45,388 - 10 - training_embed.py - training - Epoch: [9][100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.7291 (306.7015)	
2019-01-08 14:19:45,611 - 10 - training_embed.py - training - Epoch: [9][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 312.1978 (306.7153)	
2019-01-08 14:19:45,831 - 10 - training_embed.py - training - Epoch: [9][300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.7274 (306.6855)	
2019-01-08 14:19:46,059 - 10 - training_embed.py - training - Epoch: [9][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0530 (306.7768)	
2019-01-08 14:19:46,283 - 10 - training_embed.py - training - Epoch: [9][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.3165 (306.8515)	
2019-01-08 14:19:46,495 - 10 - training_embed.py - training - Epoch: [9][600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.8419 (306.8290)	
2019-01-08 14:19:46,738 - 10 - training_embed.py - training - Epoch: [9][700/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.7214 (306.8274)	
2019-01-08 14:19:46,959 - 10 - training_embed.py - training - Epoch: [9][800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.8334 (306.8813)	
2019-01-08 14:19:47,178 - 10 - training_embed.py - training - Epoch: [9][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4840 (306.8787)	
2019-01-08 14:19:47,405 - 10 - training_embed.py - training - Epoch: [9][1000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 311.2811 (306.9033)	
2019-01-08 14:19:47,641 - 10 - training_embed.py - training - Epoch: [9][1100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.2805 (306.8669)	
2019-01-08 14:19:47,856 - 10 - training_embed.py - training - Epoch: [9][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6616 (306.8517)	
2019-01-08 14:19:48,079 - 10 - training_embed.py - training - Epoch: [9][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5281 (306.8353)	
2019-01-08 14:19:48,289 - 10 - training_embed.py - training - Epoch: [9][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.3505 (306.8236)	
2019-01-08 14:19:48,494 - 10 - training_embed.py - training - Epoch: [9][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 312.2934 (306.8320)	
2019-01-08 14:19:48,735 - 10 - training_embed.py - training - Epoch: [9][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7005 (306.8303)	
2019-01-08 14:19:48,947 - 10 - training_embed.py - training - Epoch: [9][1700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.8794 (306.8300)	
2019-01-08 14:19:49,183 - 10 - training_embed.py - training - Epoch: [9][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.8726 (306.8225)	
2019-01-08 14:19:49,394 - 10 - training_embed.py - training - Epoch: [9][1900/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.9913 (306.8184)	
2019-01-08 14:19:49,607 - 10 - training_embed.py - training - Epoch: [9][2000/2184]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 308.5426 (306.8283)	
2019-01-08 14:19:49,814 - 10 - training_embed.py - training - Epoch: [9][2100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.5609 (306.8280)	
2019-01-08 14:19:50,001 - 10 - training_embed.py - training - loss: 306.818023
2019-01-08 14:19:50,001 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 14:19:50,419 - 10 - training_embed.py - training - Epoch: [10][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3234 (306.2969)	
2019-01-08 14:19:50,622 - 10 - training_embed.py - training - Epoch: [10][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0072 (306.4059)	
2019-01-08 14:19:50,836 - 10 - training_embed.py - training - Epoch: [10][300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.4266 (306.5368)	
2019-01-08 14:19:51,049 - 10 - training_embed.py - training - Epoch: [10][400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.6934 (306.6448)	
2019-01-08 14:19:51,277 - 10 - training_embed.py - training - Epoch: [10][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.5287 (306.6633)	
2019-01-08 14:19:51,507 - 10 - training_embed.py - training - Epoch: [10][600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 302.8808 (306.6885)	
2019-01-08 14:19:51,729 - 10 - training_embed.py - training - Epoch: [10][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3096 (306.7640)	
2019-01-08 14:19:51,943 - 10 - training_embed.py - training - Epoch: [10][800/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.2249 (306.7791)	
2019-01-08 14:19:52,167 - 10 - training_embed.py - training - Epoch: [10][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9487 (306.7906)	
2019-01-08 14:19:52,399 - 10 - training_embed.py - training - Epoch: [10][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1147 (306.7661)	
2019-01-08 14:19:52,614 - 10 - training_embed.py - training - Epoch: [10][1100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.0346 (306.7517)	
2019-01-08 14:19:52,844 - 10 - training_embed.py - training - Epoch: [10][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4560 (306.7892)	
2019-01-08 14:19:53,058 - 10 - training_embed.py - training - Epoch: [10][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7040 (306.8205)	
2019-01-08 14:19:53,266 - 10 - training_embed.py - training - Epoch: [10][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.9231 (306.8242)	
2019-01-08 14:19:53,475 - 10 - training_embed.py - training - Epoch: [10][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.6462 (306.8149)	
2019-01-08 14:19:53,710 - 10 - training_embed.py - training - Epoch: [10][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.7425 (306.7971)	
2019-01-08 14:19:53,934 - 10 - training_embed.py - training - Epoch: [10][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6517 (306.8059)	
2019-01-08 14:19:54,156 - 10 - training_embed.py - training - Epoch: [10][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0718 (306.8078)	
2019-01-08 14:19:54,368 - 10 - training_embed.py - training - Epoch: [10][1900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 302.0010 (306.8172)	
2019-01-08 14:19:54,587 - 10 - training_embed.py - training - Epoch: [10][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4952 (306.8237)	
2019-01-08 14:19:54,811 - 10 - training_embed.py - training - Epoch: [10][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1398 (306.8201)	
2019-01-08 14:19:54,991 - 10 - training_embed.py - training - loss: 306.785634
2019-01-08 14:19:54,991 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 14:19:55,409 - 10 - training_embed.py - training - Epoch: [11][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.9537 (306.8872)	
2019-01-08 14:19:55,632 - 10 - training_embed.py - training - Epoch: [11][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8419 (306.8453)	
2019-01-08 14:19:55,839 - 10 - training_embed.py - training - Epoch: [11][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9593 (306.8339)	
2019-01-08 14:19:56,063 - 10 - training_embed.py - training - Epoch: [11][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5506 (306.8300)	
2019-01-08 14:19:56,266 - 10 - training_embed.py - training - Epoch: [11][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0138 (306.8387)	
2019-01-08 14:19:56,500 - 10 - training_embed.py - training - Epoch: [11][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5040 (306.8494)	
2019-01-08 14:19:56,725 - 10 - training_embed.py - training - Epoch: [11][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1518 (306.8459)	
2019-01-08 14:19:56,934 - 10 - training_embed.py - training - Epoch: [11][800/2184]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 307.3892 (306.8781)	
2019-01-08 14:19:57,155 - 10 - training_embed.py - training - Epoch: [11][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5715 (306.8701)	
2019-01-08 14:19:57,371 - 10 - training_embed.py - training - Epoch: [11][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.9340 (306.8450)	
2019-01-08 14:19:57,597 - 10 - training_embed.py - training - Epoch: [11][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3241 (306.8528)	
2019-01-08 14:19:57,821 - 10 - training_embed.py - training - Epoch: [11][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1864 (306.8489)	
2019-01-08 14:19:58,041 - 10 - training_embed.py - training - Epoch: [11][1300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.5782 (306.8386)	
2019-01-08 14:19:58,251 - 10 - training_embed.py - training - Epoch: [11][1400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.0610 (306.8177)	
2019-01-08 14:19:58,472 - 10 - training_embed.py - training - Epoch: [11][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.0465 (306.7968)	
2019-01-08 14:19:58,703 - 10 - training_embed.py - training - Epoch: [11][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.4150 (306.8072)	
2019-01-08 14:19:58,938 - 10 - training_embed.py - training - Epoch: [11][1700/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 306.8282 (306.8029)	
2019-01-08 14:19:59,142 - 10 - training_embed.py - training - Epoch: [11][1800/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.2032 (306.8333)	
2019-01-08 14:19:59,361 - 10 - training_embed.py - training - Epoch: [11][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6077 (306.8178)	
2019-01-08 14:19:59,593 - 10 - training_embed.py - training - Epoch: [11][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6215 (306.7932)	
2019-01-08 14:19:59,797 - 10 - training_embed.py - training - Epoch: [11][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8997 (306.7968)	
2019-01-08 14:19:59,986 - 10 - training_embed.py - training - loss: 306.754388
2019-01-08 14:20:00,079 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 14:20:00,945 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 865.864 ms ~ 0.014 min ~ 0.866 sec
2019-01-08 14:20:01,699 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1620.373 ms ~ 0.027 min ~ 1.620 sec
2019-01-08 14:20:01,700 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 14:20:01,700 - 10 - corpus.py - subactivity_sampler - 0 / 163
2019-01-08 14:20:01,700 - 10 - corpus.py - subactivity_sampler - [79917. 79891. 79867. 79857. 79827. 79800. 79769.]
2019-01-08 14:21:18,226 - 10 - corpus.py - subactivity_sampler - 20 / 163
2019-01-08 14:21:18,226 - 10 - corpus.py - subactivity_sampler - [79418. 79923. 80093. 80386. 79690. 80167. 79251.]
2019-01-08 14:22:40,563 - 10 - corpus.py - subactivity_sampler - 40 / 163
2019-01-08 14:22:40,563 - 10 - corpus.py - subactivity_sampler - [79132. 79800. 79810. 80361. 80328. 80762. 78735.]
2019-01-08 14:23:55,804 - 10 - corpus.py - subactivity_sampler - 60 / 163
2019-01-08 14:23:55,804 - 10 - corpus.py - subactivity_sampler - [78453. 80631. 78668. 81424. 79770. 82526. 77456.]
2019-01-08 14:25:04,344 - 10 - corpus.py - subactivity_sampler - 80 / 163
2019-01-08 14:25:04,344 - 10 - corpus.py - subactivity_sampler - [78005. 80646. 78351. 82238. 80086. 82327. 77275.]
2019-01-08 14:26:16,435 - 10 - corpus.py - subactivity_sampler - 100 / 163
2019-01-08 14:26:16,435 - 10 - corpus.py - subactivity_sampler - [76962. 81691. 77788. 82626. 79780. 83297. 76784.]
2019-01-08 14:27:07,756 - 10 - corpus.py - subactivity_sampler - 120 / 163
2019-01-08 14:27:07,756 - 10 - corpus.py - subactivity_sampler - [75778. 82725. 76909. 83516. 79759. 83881. 76360.]
2019-01-08 14:28:08,267 - 10 - corpus.py - subactivity_sampler - 140 / 163
2019-01-08 14:28:08,267 - 10 - corpus.py - subactivity_sampler - [75464. 83059. 75579. 84395. 80814. 84501. 75116.]
2019-01-08 14:29:14,467 - 10 - corpus.py - subactivity_sampler - 160 / 163
2019-01-08 14:29:14,467 - 10 - corpus.py - subactivity_sampler - [74559. 84067. 74556. 85073. 80060. 85591. 75022.]
2019-01-08 14:29:28,351 - 10 - corpus.py - subactivity_sampler - [74275. 84288. 74558. 85195. 79998. 86042. 74572.]
2019-01-08 14:29:28,351 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 566651.845 ms ~ 9.444 min ~ 566.652 sec
2019-01-08 14:29:28,351 - 10 - corpus.py - ordering_sampler - .
2019-01-08 14:29:30,883 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 14:29:30,883 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 6.  1.  0.  0. 51.  2.]
2019-01-08 14:29:30,884 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 14:29:30,901 - 10 - corpus.py - rho_sampling - ['52.3964', '39.9809', '928.1069', '7.5322', '3.6234', '34.2236']
2019-01-08 14:29:30,901 - 10 - pipeline.py - baseline - Iteration 1
2019-01-08 14:29:31,071 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 14:29:31,081 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 14:29:31,081 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 14', '1: 18', '2: 35', '3: 4', '4: 34', '5: 32', '6: 33']
2019-01-08 14:29:31,103 - 10 - accuracy_class.py - mof_val - frames true: 119717	frames overall : 558928
2019-01-08 14:29:31,103 - 10 - corpus.py - accuracy_corpus - Action: salat
2019-01-08 14:29:31,104 - 10 - corpus.py - accuracy_corpus - MoF val: 0.21419037872498783
2019-01-08 14:29:31,104 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.21143152606418
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 23956
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - mof_classes - label 4: 0.150466  1178 / 7829
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - mof_classes - label 14: 0.995353  5997 / 6025
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - mof_classes - label 18: 0.130934  342 / 2612
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - mof_classes - label 32: 0.181362  55589 / 306508
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - mof_classes - label 33: 0.423245  18004 / 42538
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - mof_classes - label 34: 0.236215  38607 / 163440
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - mof_classes - label 35: 0.000000  0 / 6020
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - mof_classes - average class mof: 0.264697
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 23956
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - iou_classes - label 4: 0.012826  1178 / 91846
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - iou_classes - label 14: 0.080710  5997 / 74303
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - iou_classes - label 18: 0.003951  342 / 86558
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - iou_classes - label 32: 0.164972  55589 / 336961
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - iou_classes - label 33: 0.181664  18004 / 99106
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - iou_classes - label 34: 0.188482  38607 / 204831
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - iou_classes - label 35: 0.000000  0 / 80578
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - iou_classes - average IoU: 0.090372
2019-01-08 14:29:31,104 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.079076
2019-01-08 14:29:40,079 - 10 - f1_score.py - f1 - f1 score: 0.287300
2019-01-08 14:29:40,099 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 9198.062 ms ~ 0.153 min ~ 9.198 sec
2019-01-08 14:29:40,100 - 10 - corpus.py - embedding_training - .
2019-01-08 14:29:40,100 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 14:29:40,100 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 14:29:40,100 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 14:29:43,899 - 10 - training_embed.py - training - create model
2019-01-08 14:29:43,899 - 10 - training_embed.py - training - epochs: 12
2019-01-08 14:29:43,899 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 14:29:44,269 - 10 - training_embed.py - training - Epoch: [0][100/2184]	Time 0.003 (0.004)	Data 0.002 (0.002)	Loss 307.1981 (307.2422)	
2019-01-08 14:29:44,490 - 10 - training_embed.py - training - Epoch: [0][200/2184]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 305.1355 (307.0778)	
2019-01-08 14:29:44,712 - 10 - training_embed.py - training - Epoch: [0][300/2184]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 307.2658 (306.9503)	
2019-01-08 14:29:44,930 - 10 - training_embed.py - training - Epoch: [0][400/2184]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 307.0422 (306.9645)	
2019-01-08 14:29:45,151 - 10 - training_embed.py - training - Epoch: [0][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.7148 (306.8304)	
2019-01-08 14:29:45,387 - 10 - training_embed.py - training - Epoch: [0][600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.3904 (306.8512)	
2019-01-08 14:29:45,612 - 10 - training_embed.py - training - Epoch: [0][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.8808 (306.9349)	
2019-01-08 14:29:45,838 - 10 - training_embed.py - training - Epoch: [0][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0804 (306.9154)	
2019-01-08 14:29:46,050 - 10 - training_embed.py - training - Epoch: [0][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.1879 (306.9591)	
2019-01-08 14:29:46,274 - 10 - training_embed.py - training - Epoch: [0][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.1364 (306.9720)	
2019-01-08 14:29:46,507 - 10 - training_embed.py - training - Epoch: [0][1100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.0655 (306.9514)	
2019-01-08 14:29:46,757 - 10 - training_embed.py - training - Epoch: [0][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9259 (306.9508)	
2019-01-08 14:29:46,978 - 10 - training_embed.py - training - Epoch: [0][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4742 (306.9601)	
2019-01-08 14:29:47,210 - 10 - training_embed.py - training - Epoch: [0][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7726 (306.9637)	
2019-01-08 14:29:47,433 - 10 - training_embed.py - training - Epoch: [0][1500/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 306.8785 (306.9644)	
2019-01-08 14:29:47,654 - 10 - training_embed.py - training - Epoch: [0][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3368 (306.9661)	
2019-01-08 14:29:47,870 - 10 - training_embed.py - training - Epoch: [0][1700/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.5169 (306.9651)	
2019-01-08 14:29:48,076 - 10 - training_embed.py - training - Epoch: [0][1800/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.7707 (306.9627)	
2019-01-08 14:29:48,297 - 10 - training_embed.py - training - Epoch: [0][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8309 (306.9644)	
2019-01-08 14:29:48,510 - 10 - training_embed.py - training - Epoch: [0][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4214 (306.9600)	
2019-01-08 14:29:48,738 - 10 - training_embed.py - training - Epoch: [0][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7540 (306.9489)	
2019-01-08 14:29:48,916 - 10 - training_embed.py - training - loss: 306.924917
2019-01-08 14:29:48,916 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 14:29:49,345 - 10 - training_embed.py - training - Epoch: [1][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.1753 (306.7776)	
2019-01-08 14:29:49,556 - 10 - training_embed.py - training - Epoch: [1][200/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 310.8013 (306.7385)	
2019-01-08 14:29:49,782 - 10 - training_embed.py - training - Epoch: [1][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7003 (306.6975)	
2019-01-08 14:29:50,001 - 10 - training_embed.py - training - Epoch: [1][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6307 (306.7023)	
2019-01-08 14:29:50,226 - 10 - training_embed.py - training - Epoch: [1][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8857 (306.7335)	
2019-01-08 14:29:50,451 - 10 - training_embed.py - training - Epoch: [1][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0940 (306.7513)	
2019-01-08 14:29:50,665 - 10 - training_embed.py - training - Epoch: [1][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6545 (306.7336)	
2019-01-08 14:29:50,875 - 10 - training_embed.py - training - Epoch: [1][800/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.6013 (306.7624)	
2019-01-08 14:29:51,100 - 10 - training_embed.py - training - Epoch: [1][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2698 (306.7692)	
2019-01-08 14:29:51,318 - 10 - training_embed.py - training - Epoch: [1][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.3753 (306.7992)	
2019-01-08 14:29:51,565 - 10 - training_embed.py - training - Epoch: [1][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.5965 (306.8143)	
2019-01-08 14:29:51,791 - 10 - training_embed.py - training - Epoch: [1][1200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 310.5607 (306.8455)	
2019-01-08 14:29:52,006 - 10 - training_embed.py - training - Epoch: [1][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6773 (306.8550)	
2019-01-08 14:29:52,238 - 10 - training_embed.py - training - Epoch: [1][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1031 (306.8658)	
2019-01-08 14:29:52,461 - 10 - training_embed.py - training - Epoch: [1][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5651 (306.8582)	
2019-01-08 14:29:52,695 - 10 - training_embed.py - training - Epoch: [1][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4434 (306.8786)	
2019-01-08 14:29:52,917 - 10 - training_embed.py - training - Epoch: [1][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9403 (306.8588)	
2019-01-08 14:29:53,149 - 10 - training_embed.py - training - Epoch: [1][1800/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 304.2517 (306.8594)	
2019-01-08 14:29:53,379 - 10 - training_embed.py - training - Epoch: [1][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.9870 (306.8604)	
2019-01-08 14:29:53,597 - 10 - training_embed.py - training - Epoch: [1][2000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 308.4044 (306.8606)	
2019-01-08 14:29:53,815 - 10 - training_embed.py - training - Epoch: [1][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7406 (306.8683)	
2019-01-08 14:29:54,002 - 10 - training_embed.py - training - loss: 306.839019
2019-01-08 14:29:54,002 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 14:29:54,431 - 10 - training_embed.py - training - Epoch: [2][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6635 (306.6378)	
2019-01-08 14:29:54,682 - 10 - training_embed.py - training - Epoch: [2][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.5232 (306.6274)	
2019-01-08 14:29:54,900 - 10 - training_embed.py - training - Epoch: [2][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8433 (306.5938)	
2019-01-08 14:29:55,133 - 10 - training_embed.py - training - Epoch: [2][400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.0912 (306.6584)	
2019-01-08 14:29:55,342 - 10 - training_embed.py - training - Epoch: [2][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9628 (306.7299)	
2019-01-08 14:29:55,562 - 10 - training_embed.py - training - Epoch: [2][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7701 (306.7315)	
2019-01-08 14:29:55,782 - 10 - training_embed.py - training - Epoch: [2][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6378 (306.7354)	
2019-01-08 14:29:56,011 - 10 - training_embed.py - training - Epoch: [2][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.0615 (306.7421)	
2019-01-08 14:29:56,253 - 10 - training_embed.py - training - Epoch: [2][900/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 311.3604 (306.7769)	
2019-01-08 14:29:56,469 - 10 - training_embed.py - training - Epoch: [2][1000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.2326 (306.7843)	
2019-01-08 14:29:56,693 - 10 - training_embed.py - training - Epoch: [2][1100/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 306.5026 (306.7845)	
2019-01-08 14:29:56,899 - 10 - training_embed.py - training - Epoch: [2][1200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.3488 (306.7767)	
2019-01-08 14:29:57,155 - 10 - training_embed.py - training - Epoch: [2][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.1006 (306.7979)	
2019-01-08 14:29:57,380 - 10 - training_embed.py - training - Epoch: [2][1400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.6450 (306.8046)	
2019-01-08 14:29:57,593 - 10 - training_embed.py - training - Epoch: [2][1500/2184]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 307.6298 (306.8092)	
2019-01-08 14:29:57,805 - 10 - training_embed.py - training - Epoch: [2][1600/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 308.0180 (306.7861)	
2019-01-08 14:29:58,030 - 10 - training_embed.py - training - Epoch: [2][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6027 (306.7743)	
2019-01-08 14:29:58,276 - 10 - training_embed.py - training - Epoch: [2][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7302 (306.7754)	
2019-01-08 14:29:58,500 - 10 - training_embed.py - training - Epoch: [2][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.2902 (306.7767)	
2019-01-08 14:29:58,745 - 10 - training_embed.py - training - Epoch: [2][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 312.0095 (306.7836)	
2019-01-08 14:29:58,973 - 10 - training_embed.py - training - Epoch: [2][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8396 (306.7768)	
2019-01-08 14:29:59,162 - 10 - training_embed.py - training - loss: 306.754038
2019-01-08 14:29:59,162 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 14:29:59,590 - 10 - training_embed.py - training - Epoch: [3][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0286 (306.7659)	
2019-01-08 14:29:59,828 - 10 - training_embed.py - training - Epoch: [3][200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 302.5342 (306.5937)	
2019-01-08 14:30:00,058 - 10 - training_embed.py - training - Epoch: [3][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5040 (306.6828)	
2019-01-08 14:30:00,295 - 10 - training_embed.py - training - Epoch: [3][400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.0585 (306.6562)	
2019-01-08 14:30:00,500 - 10 - training_embed.py - training - Epoch: [3][500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.3043 (306.6533)	
2019-01-08 14:30:00,743 - 10 - training_embed.py - training - Epoch: [3][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3542 (306.6983)	
2019-01-08 14:30:00,981 - 10 - training_embed.py - training - Epoch: [3][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.6707 (306.6358)	
2019-01-08 14:30:01,194 - 10 - training_embed.py - training - Epoch: [3][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0171 (306.6935)	
2019-01-08 14:30:01,406 - 10 - training_embed.py - training - Epoch: [3][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0791 (306.7408)	
2019-01-08 14:30:01,628 - 10 - training_embed.py - training - Epoch: [3][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.8318 (306.7243)	
2019-01-08 14:30:01,863 - 10 - training_embed.py - training - Epoch: [3][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9238 (306.7083)	
2019-01-08 14:30:02,079 - 10 - training_embed.py - training - Epoch: [3][1200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 312.9616 (306.7005)	
2019-01-08 14:30:02,304 - 10 - training_embed.py - training - Epoch: [3][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8457 (306.7061)	
2019-01-08 14:30:02,525 - 10 - training_embed.py - training - Epoch: [3][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9250 (306.7016)	
2019-01-08 14:30:02,760 - 10 - training_embed.py - training - Epoch: [3][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4711 (306.6831)	
2019-01-08 14:30:02,989 - 10 - training_embed.py - training - Epoch: [3][1600/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 306.5452 (306.7056)	
2019-01-08 14:30:03,225 - 10 - training_embed.py - training - Epoch: [3][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.9382 (306.7081)	
2019-01-08 14:30:03,464 - 10 - training_embed.py - training - Epoch: [3][1800/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 303.3013 (306.7021)	
2019-01-08 14:30:03,677 - 10 - training_embed.py - training - Epoch: [3][1900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 302.2107 (306.7113)	
2019-01-08 14:30:03,891 - 10 - training_embed.py - training - Epoch: [3][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3708 (306.7122)	
2019-01-08 14:30:04,102 - 10 - training_embed.py - training - Epoch: [3][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5947 (306.7094)	
2019-01-08 14:30:04,290 - 10 - training_embed.py - training - loss: 306.669426
2019-01-08 14:30:04,290 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 14:30:04,723 - 10 - training_embed.py - training - Epoch: [4][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 313.3924 (306.3229)	
2019-01-08 14:30:04,951 - 10 - training_embed.py - training - Epoch: [4][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.1116 (306.4499)	
2019-01-08 14:30:05,166 - 10 - training_embed.py - training - Epoch: [4][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.0592 (306.5659)	
2019-01-08 14:30:05,375 - 10 - training_embed.py - training - Epoch: [4][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6074 (306.5657)	
2019-01-08 14:30:05,601 - 10 - training_embed.py - training - Epoch: [4][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7279 (306.6392)	
2019-01-08 14:30:05,835 - 10 - training_embed.py - training - Epoch: [4][600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.3727 (306.6468)	
2019-01-08 14:30:06,052 - 10 - training_embed.py - training - Epoch: [4][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7370 (306.6224)	
2019-01-08 14:30:06,267 - 10 - training_embed.py - training - Epoch: [4][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0759 (306.6314)	
2019-01-08 14:30:06,477 - 10 - training_embed.py - training - Epoch: [4][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.5947 (306.6486)	
2019-01-08 14:30:06,715 - 10 - training_embed.py - training - Epoch: [4][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.3231 (306.6246)	
2019-01-08 14:30:06,934 - 10 - training_embed.py - training - Epoch: [4][1100/2184]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 306.8959 (306.5997)	
2019-01-08 14:30:07,173 - 10 - training_embed.py - training - Epoch: [4][1200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.2672 (306.6242)	
2019-01-08 14:30:07,387 - 10 - training_embed.py - training - Epoch: [4][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.2285 (306.6374)	
2019-01-08 14:30:07,614 - 10 - training_embed.py - training - Epoch: [4][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8106 (306.6191)	
2019-01-08 14:30:07,852 - 10 - training_embed.py - training - Epoch: [4][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7362 (306.6256)	
2019-01-08 14:30:08,064 - 10 - training_embed.py - training - Epoch: [4][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7827 (306.6255)	
2019-01-08 14:30:08,297 - 10 - training_embed.py - training - Epoch: [4][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9487 (306.6254)	
2019-01-08 14:30:08,501 - 10 - training_embed.py - training - Epoch: [4][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.9532 (306.6187)	
2019-01-08 14:30:08,735 - 10 - training_embed.py - training - Epoch: [4][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9980 (306.6163)	
2019-01-08 14:30:08,967 - 10 - training_embed.py - training - Epoch: [4][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.6894 (306.6206)	
2019-01-08 14:30:09,199 - 10 - training_embed.py - training - Epoch: [4][2100/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 306.2959 (306.6135)	
2019-01-08 14:30:09,381 - 10 - training_embed.py - training - loss: 306.584572
2019-01-08 14:30:09,381 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 14:30:09,797 - 10 - training_embed.py - training - Epoch: [5][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5651 (306.9336)	
2019-01-08 14:30:10,026 - 10 - training_embed.py - training - Epoch: [5][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.9650 (306.8557)	
2019-01-08 14:30:10,242 - 10 - training_embed.py - training - Epoch: [5][300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 303.0019 (306.7048)	
2019-01-08 14:30:10,450 - 10 - training_embed.py - training - Epoch: [5][400/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 306.3609 (306.6899)	
2019-01-08 14:30:10,682 - 10 - training_embed.py - training - Epoch: [5][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3531 (306.6783)	
2019-01-08 14:30:10,922 - 10 - training_embed.py - training - Epoch: [5][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.7231 (306.6506)	
2019-01-08 14:30:11,142 - 10 - training_embed.py - training - Epoch: [5][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0568 (306.6508)	
2019-01-08 14:30:11,365 - 10 - training_embed.py - training - Epoch: [5][800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.0976 (306.6031)	
2019-01-08 14:30:11,594 - 10 - training_embed.py - training - Epoch: [5][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.9859 (306.5537)	
2019-01-08 14:30:11,831 - 10 - training_embed.py - training - Epoch: [5][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.7232 (306.5253)	
2019-01-08 14:30:12,056 - 10 - training_embed.py - training - Epoch: [5][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.8705 (306.5519)	
2019-01-08 14:30:12,266 - 10 - training_embed.py - training - Epoch: [5][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.1022 (306.5935)	
2019-01-08 14:30:12,472 - 10 - training_embed.py - training - Epoch: [5][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2895 (306.5550)	
2019-01-08 14:30:12,697 - 10 - training_embed.py - training - Epoch: [5][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1630 (306.5675)	
2019-01-08 14:30:12,908 - 10 - training_embed.py - training - Epoch: [5][1500/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 308.6132 (306.5691)	
2019-01-08 14:30:13,136 - 10 - training_embed.py - training - Epoch: [5][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6683 (306.5630)	
2019-01-08 14:30:13,362 - 10 - training_embed.py - training - Epoch: [5][1700/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.2827 (306.5583)	
2019-01-08 14:30:13,568 - 10 - training_embed.py - training - Epoch: [5][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4483 (306.5651)	
2019-01-08 14:30:13,792 - 10 - training_embed.py - training - Epoch: [5][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6534 (306.5562)	
2019-01-08 14:30:14,009 - 10 - training_embed.py - training - Epoch: [5][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6059 (306.5473)	
2019-01-08 14:30:14,238 - 10 - training_embed.py - training - Epoch: [5][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3130 (306.5430)	
2019-01-08 14:30:14,415 - 10 - training_embed.py - training - loss: 306.499292
2019-01-08 14:30:14,415 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 14:30:14,856 - 10 - training_embed.py - training - Epoch: [6][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.0756 (306.1066)	
2019-01-08 14:30:15,076 - 10 - training_embed.py - training - Epoch: [6][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7055 (306.3629)	
2019-01-08 14:30:15,284 - 10 - training_embed.py - training - Epoch: [6][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6439 (306.3606)	
2019-01-08 14:30:15,516 - 10 - training_embed.py - training - Epoch: [6][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6669 (306.3514)	
2019-01-08 14:30:15,738 - 10 - training_embed.py - training - Epoch: [6][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4493 (306.3539)	
2019-01-08 14:30:15,961 - 10 - training_embed.py - training - Epoch: [6][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5091 (306.4128)	
2019-01-08 14:30:16,166 - 10 - training_embed.py - training - Epoch: [6][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2538 (306.3911)	
2019-01-08 14:30:16,390 - 10 - training_embed.py - training - Epoch: [6][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4282 (306.4125)	
2019-01-08 14:30:16,632 - 10 - training_embed.py - training - Epoch: [6][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.2995 (306.4153)	
2019-01-08 14:30:16,874 - 10 - training_embed.py - training - Epoch: [6][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.7725 (306.4240)	
2019-01-08 14:30:17,093 - 10 - training_embed.py - training - Epoch: [6][1100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 308.2234 (306.4310)	
2019-01-08 14:30:17,300 - 10 - training_embed.py - training - Epoch: [6][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.1916 (306.4358)	
2019-01-08 14:30:17,554 - 10 - training_embed.py - training - Epoch: [6][1300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.7191 (306.4654)	
2019-01-08 14:30:17,778 - 10 - training_embed.py - training - Epoch: [6][1400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.3428 (306.4561)	
2019-01-08 14:30:18,008 - 10 - training_embed.py - training - Epoch: [6][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2380 (306.4531)	
2019-01-08 14:30:18,216 - 10 - training_embed.py - training - Epoch: [6][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6815 (306.4396)	
2019-01-08 14:30:18,439 - 10 - training_embed.py - training - Epoch: [6][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.5939 (306.4426)	
2019-01-08 14:30:18,666 - 10 - training_embed.py - training - Epoch: [6][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5439 (306.4314)	
2019-01-08 14:30:18,879 - 10 - training_embed.py - training - Epoch: [6][1900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.3597 (306.4327)	
2019-01-08 14:30:19,134 - 10 - training_embed.py - training - Epoch: [6][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3637 (306.4605)	
2019-01-08 14:30:19,346 - 10 - training_embed.py - training - Epoch: [6][2100/2184]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 301.6505 (306.4547)	
2019-01-08 14:30:19,532 - 10 - training_embed.py - training - loss: 306.413617
2019-01-08 14:30:19,532 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 14:30:19,966 - 10 - training_embed.py - training - Epoch: [7][100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.1477 (306.0649)	
2019-01-08 14:30:20,195 - 10 - training_embed.py - training - Epoch: [7][200/2184]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 305.3499 (306.2326)	
2019-01-08 14:30:20,423 - 10 - training_embed.py - training - Epoch: [7][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3640 (306.2623)	
2019-01-08 14:30:20,652 - 10 - training_embed.py - training - Epoch: [7][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6442 (306.3161)	
2019-01-08 14:30:20,869 - 10 - training_embed.py - training - Epoch: [7][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8108 (306.3835)	
2019-01-08 14:30:21,103 - 10 - training_embed.py - training - Epoch: [7][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6463 (306.3686)	
2019-01-08 14:30:21,319 - 10 - training_embed.py - training - Epoch: [7][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.3900 (306.4276)	
2019-01-08 14:30:21,551 - 10 - training_embed.py - training - Epoch: [7][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4151 (306.4899)	
2019-01-08 14:30:21,770 - 10 - training_embed.py - training - Epoch: [7][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1964 (306.4766)	
2019-01-08 14:30:21,992 - 10 - training_embed.py - training - Epoch: [7][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.5174 (306.4605)	
2019-01-08 14:30:22,226 - 10 - training_embed.py - training - Epoch: [7][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7558 (306.4320)	
2019-01-08 14:30:22,450 - 10 - training_embed.py - training - Epoch: [7][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.4411 (306.4194)	
2019-01-08 14:30:22,670 - 10 - training_embed.py - training - Epoch: [7][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 314.2836 (306.4375)	
2019-01-08 14:30:22,897 - 10 - training_embed.py - training - Epoch: [7][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0573 (306.4082)	
2019-01-08 14:30:23,113 - 10 - training_embed.py - training - Epoch: [7][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.1170 (306.4011)	
2019-01-08 14:30:23,341 - 10 - training_embed.py - training - Epoch: [7][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4862 (306.4047)	
2019-01-08 14:30:23,576 - 10 - training_embed.py - training - Epoch: [7][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7488 (306.3999)	
2019-01-08 14:30:23,802 - 10 - training_embed.py - training - Epoch: [7][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0269 (306.3851)	
2019-01-08 14:30:24,018 - 10 - training_embed.py - training - Epoch: [7][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.0923 (306.3921)	
2019-01-08 14:30:24,232 - 10 - training_embed.py - training - Epoch: [7][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6270 (306.3831)	
2019-01-08 14:30:24,453 - 10 - training_embed.py - training - Epoch: [7][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8523 (306.3642)	
2019-01-08 14:30:24,643 - 10 - training_embed.py - training - loss: 306.329055
2019-01-08 14:30:24,644 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 14:30:25,122 - 10 - training_embed.py - training - Epoch: [8][100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.5242 (306.4116)	
2019-01-08 14:30:25,338 - 10 - training_embed.py - training - Epoch: [8][200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.3543 (306.4265)	
2019-01-08 14:30:25,564 - 10 - training_embed.py - training - Epoch: [8][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.2632 (306.3428)	
2019-01-08 14:30:25,780 - 10 - training_embed.py - training - Epoch: [8][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3253 (306.2857)	
2019-01-08 14:30:26,021 - 10 - training_embed.py - training - Epoch: [8][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6626 (306.2520)	
2019-01-08 14:30:26,254 - 10 - training_embed.py - training - Epoch: [8][600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.6255 (306.2574)	
2019-01-08 14:30:26,467 - 10 - training_embed.py - training - Epoch: [8][700/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 306.8354 (306.2457)	
2019-01-08 14:30:26,682 - 10 - training_embed.py - training - Epoch: [8][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1671 (306.2313)	
2019-01-08 14:30:26,894 - 10 - training_embed.py - training - Epoch: [8][900/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 303.6830 (306.2589)	
2019-01-08 14:30:27,134 - 10 - training_embed.py - training - Epoch: [8][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.3602 (306.2331)	
2019-01-08 14:30:27,353 - 10 - training_embed.py - training - Epoch: [8][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0529 (306.2248)	
2019-01-08 14:30:27,590 - 10 - training_embed.py - training - Epoch: [8][1200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.7063 (306.2212)	
2019-01-08 14:30:27,806 - 10 - training_embed.py - training - Epoch: [8][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6364 (306.2279)	
2019-01-08 14:30:28,029 - 10 - training_embed.py - training - Epoch: [8][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1506 (306.2395)	
2019-01-08 14:30:28,254 - 10 - training_embed.py - training - Epoch: [8][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1481 (306.2107)	
2019-01-08 14:30:28,475 - 10 - training_embed.py - training - Epoch: [8][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7723 (306.2142)	
2019-01-08 14:30:28,723 - 10 - training_embed.py - training - Epoch: [8][1700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.4374 (306.2262)	
2019-01-08 14:30:28,952 - 10 - training_embed.py - training - Epoch: [8][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6716 (306.2230)	
2019-01-08 14:30:29,171 - 10 - training_embed.py - training - Epoch: [8][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1241 (306.2664)	
2019-01-08 14:30:29,403 - 10 - training_embed.py - training - Epoch: [8][2000/2184]	Time 0.005 (0.002)	Data 0.002 (0.001)	Loss 305.3443 (306.2589)	
2019-01-08 14:30:29,619 - 10 - training_embed.py - training - Epoch: [8][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6216 (306.2618)	
2019-01-08 14:30:29,793 - 10 - training_embed.py - training - loss: 306.244108
2019-01-08 14:30:29,793 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 14:30:30,228 - 10 - training_embed.py - training - Epoch: [9][100/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 304.2390 (306.0633)	
2019-01-08 14:30:30,446 - 10 - training_embed.py - training - Epoch: [9][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.9505 (306.1922)	
2019-01-08 14:30:30,675 - 10 - training_embed.py - training - Epoch: [9][300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.4799 (306.1765)	
2019-01-08 14:30:30,899 - 10 - training_embed.py - training - Epoch: [9][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8668 (306.2095)	
2019-01-08 14:30:31,136 - 10 - training_embed.py - training - Epoch: [9][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.7849 (306.3211)	
2019-01-08 14:30:31,353 - 10 - training_embed.py - training - Epoch: [9][600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.2503 (306.2793)	
2019-01-08 14:30:31,566 - 10 - training_embed.py - training - Epoch: [9][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0170 (306.2660)	
2019-01-08 14:30:31,781 - 10 - training_embed.py - training - Epoch: [9][800/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 309.2556 (306.3081)	
2019-01-08 14:30:32,019 - 10 - training_embed.py - training - Epoch: [9][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0738 (306.2828)	
2019-01-08 14:30:32,239 - 10 - training_embed.py - training - Epoch: [9][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1461 (306.2837)	
2019-01-08 14:30:32,451 - 10 - training_embed.py - training - Epoch: [9][1100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.4468 (306.2672)	
2019-01-08 14:30:32,675 - 10 - training_embed.py - training - Epoch: [9][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.9426 (306.2408)	
2019-01-08 14:30:32,888 - 10 - training_embed.py - training - Epoch: [9][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0495 (306.2158)	
2019-01-08 14:30:33,117 - 10 - training_embed.py - training - Epoch: [9][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0522 (306.1907)	
2019-01-08 14:30:33,324 - 10 - training_embed.py - training - Epoch: [9][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.8620 (306.1827)	
2019-01-08 14:30:33,558 - 10 - training_embed.py - training - Epoch: [9][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.1322 (306.1929)	
2019-01-08 14:30:33,775 - 10 - training_embed.py - training - Epoch: [9][1700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.3715 (306.1914)	
2019-01-08 14:30:34,010 - 10 - training_embed.py - training - Epoch: [9][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1700 (306.1957)	
2019-01-08 14:30:34,237 - 10 - training_embed.py - training - Epoch: [9][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.2371 (306.1716)	
2019-01-08 14:30:34,460 - 10 - training_embed.py - training - Epoch: [9][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6875 (306.1688)	
2019-01-08 14:30:34,712 - 10 - training_embed.py - training - Epoch: [9][2100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 309.8098 (306.1619)	
2019-01-08 14:30:34,900 - 10 - training_embed.py - training - loss: 306.158714
2019-01-08 14:30:34,901 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 14:30:35,312 - 10 - training_embed.py - training - Epoch: [10][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0222 (305.8055)	
2019-01-08 14:30:35,532 - 10 - training_embed.py - training - Epoch: [10][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4501 (305.9209)	
2019-01-08 14:30:35,758 - 10 - training_embed.py - training - Epoch: [10][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5965 (305.9618)	
2019-01-08 14:30:35,995 - 10 - training_embed.py - training - Epoch: [10][400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.9311 (306.0297)	
2019-01-08 14:30:36,215 - 10 - training_embed.py - training - Epoch: [10][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.0947 (306.0203)	
2019-01-08 14:30:36,424 - 10 - training_embed.py - training - Epoch: [10][600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 303.0037 (306.0482)	
2019-01-08 14:30:36,647 - 10 - training_embed.py - training - Epoch: [10][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8780 (306.1018)	
2019-01-08 14:30:36,862 - 10 - training_embed.py - training - Epoch: [10][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9803 (306.1246)	
2019-01-08 14:30:37,089 - 10 - training_embed.py - training - Epoch: [10][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.4093 (306.1129)	
2019-01-08 14:30:37,300 - 10 - training_embed.py - training - Epoch: [10][1000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 303.2249 (306.0734)	
2019-01-08 14:30:37,517 - 10 - training_embed.py - training - Epoch: [10][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2120 (306.0702)	
2019-01-08 14:30:37,748 - 10 - training_embed.py - training - Epoch: [10][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3813 (306.0980)	
2019-01-08 14:30:37,974 - 10 - training_embed.py - training - Epoch: [10][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3176 (306.1187)	
2019-01-08 14:30:38,189 - 10 - training_embed.py - training - Epoch: [10][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6100 (306.1303)	
2019-01-08 14:30:38,432 - 10 - training_embed.py - training - Epoch: [10][1500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.1577 (306.1127)	
2019-01-08 14:30:38,661 - 10 - training_embed.py - training - Epoch: [10][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.9948 (306.1061)	
2019-01-08 14:30:38,891 - 10 - training_embed.py - training - Epoch: [10][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.7933 (306.1204)	
2019-01-08 14:30:39,131 - 10 - training_embed.py - training - Epoch: [10][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3172 (306.1182)	
2019-01-08 14:30:39,348 - 10 - training_embed.py - training - Epoch: [10][1900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 303.3996 (306.1159)	
2019-01-08 14:30:39,571 - 10 - training_embed.py - training - Epoch: [10][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1713 (306.1167)	
2019-01-08 14:30:39,798 - 10 - training_embed.py - training - Epoch: [10][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.0692 (306.1034)	
2019-01-08 14:30:39,983 - 10 - training_embed.py - training - loss: 306.072767
2019-01-08 14:30:39,983 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 14:30:40,413 - 10 - training_embed.py - training - Epoch: [11][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.7840 (306.1390)	
2019-01-08 14:30:40,637 - 10 - training_embed.py - training - Epoch: [11][200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.6522 (306.0430)	
2019-01-08 14:30:40,859 - 10 - training_embed.py - training - Epoch: [11][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5970 (305.9819)	
2019-01-08 14:30:41,069 - 10 - training_embed.py - training - Epoch: [11][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5035 (306.0210)	
2019-01-08 14:30:41,275 - 10 - training_embed.py - training - Epoch: [11][500/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 305.6280 (306.0572)	
2019-01-08 14:30:41,502 - 10 - training_embed.py - training - Epoch: [11][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1527 (306.0703)	
2019-01-08 14:30:41,723 - 10 - training_embed.py - training - Epoch: [11][700/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 304.4824 (306.0492)	
2019-01-08 14:30:41,957 - 10 - training_embed.py - training - Epoch: [11][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5745 (306.0682)	
2019-01-08 14:30:42,180 - 10 - training_embed.py - training - Epoch: [11][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.2530 (306.0590)	
2019-01-08 14:30:42,397 - 10 - training_embed.py - training - Epoch: [11][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2696 (306.0538)	
2019-01-08 14:30:42,635 - 10 - training_embed.py - training - Epoch: [11][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1759 (306.0617)	
2019-01-08 14:30:42,864 - 10 - training_embed.py - training - Epoch: [11][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7651 (306.0702)	
2019-01-08 14:30:43,084 - 10 - training_embed.py - training - Epoch: [11][1300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.1703 (306.0668)	
2019-01-08 14:30:43,300 - 10 - training_embed.py - training - Epoch: [11][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.5622 (306.0578)	
2019-01-08 14:30:43,516 - 10 - training_embed.py - training - Epoch: [11][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.2084 (306.0419)	
2019-01-08 14:30:43,731 - 10 - training_embed.py - training - Epoch: [11][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9346 (306.0404)	
2019-01-08 14:30:43,967 - 10 - training_embed.py - training - Epoch: [11][1700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.1970 (306.0454)	
2019-01-08 14:30:44,203 - 10 - training_embed.py - training - Epoch: [11][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.9807 (306.0610)	
2019-01-08 14:30:44,438 - 10 - training_embed.py - training - Epoch: [11][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7719 (306.0308)	
2019-01-08 14:30:44,646 - 10 - training_embed.py - training - Epoch: [11][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0886 (306.0233)	
2019-01-08 14:30:44,881 - 10 - training_embed.py - training - Epoch: [11][2100/2184]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 305.9009 (306.0300)	
2019-01-08 14:30:45,072 - 10 - training_embed.py - training - loss: 305.988258
2019-01-08 14:30:45,168 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 14:30:46,166 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 997.563 ms ~ 0.017 min ~ 0.998 sec
2019-01-08 14:30:46,957 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1788.502 ms ~ 0.030 min ~ 1.789 sec
2019-01-08 14:30:46,957 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 14:30:46,958 - 10 - corpus.py - subactivity_sampler - 0 / 163
2019-01-08 14:30:46,958 - 10 - corpus.py - subactivity_sampler - [74275. 84288. 74558. 85195. 79998. 86042. 74572.]
2019-01-08 14:32:03,758 - 10 - corpus.py - subactivity_sampler - 20 / 163
2019-01-08 14:32:03,758 - 10 - corpus.py - subactivity_sampler - [73784. 84813. 74277. 85065. 79789. 87478. 73722.]
2019-01-08 14:33:25,832 - 10 - corpus.py - subactivity_sampler - 40 / 163
2019-01-08 14:33:25,832 - 10 - corpus.py - subactivity_sampler - [73327. 85252. 74245. 84698. 80276. 88240. 72890.]
2019-01-08 14:34:40,609 - 10 - corpus.py - subactivity_sampler - 60 / 163
2019-01-08 14:34:40,609 - 10 - corpus.py - subactivity_sampler - [72437. 86630. 73364. 85124. 79181. 90434. 71758.]
2019-01-08 14:35:49,708 - 10 - corpus.py - subactivity_sampler - 80 / 163
2019-01-08 14:35:49,708 - 10 - corpus.py - subactivity_sampler - [72010. 88100. 73397. 84365. 78958. 90639. 71459.]
2019-01-08 14:37:01,932 - 10 - corpus.py - subactivity_sampler - 100 / 163
2019-01-08 14:37:01,932 - 10 - corpus.py - subactivity_sampler - [71820. 88225. 73889. 83268. 78179. 92462. 71085.]
2019-01-08 14:37:53,817 - 10 - corpus.py - subactivity_sampler - 120 / 163
2019-01-08 14:37:53,817 - 10 - corpus.py - subactivity_sampler - [71173. 88962. 73644. 83439. 77703. 94307. 69700.]
2019-01-08 14:38:54,998 - 10 - corpus.py - subactivity_sampler - 140 / 163
2019-01-08 14:38:54,998 - 10 - corpus.py - subactivity_sampler - [70675. 89661. 73297. 83334. 77193. 96122. 68646.]
2019-01-08 14:40:02,184 - 10 - corpus.py - subactivity_sampler - 160 / 163
2019-01-08 14:40:02,185 - 10 - corpus.py - subactivity_sampler - [70515. 90038. 72820. 82902. 76253. 97787. 68613.]
2019-01-08 14:40:16,114 - 10 - corpus.py - subactivity_sampler - [70514. 90031. 72824. 82836. 76201. 98052. 68470.]
2019-01-08 14:40:16,114 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 569156.626 ms ~ 9.486 min ~ 569.157 sec
2019-01-08 14:40:16,114 - 10 - corpus.py - ordering_sampler - .
2019-01-08 14:40:18,637 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 14:40:18,638 - 10 - corpus.py - ordering_sampler - inv_count_vec: [10. 19.  0. 27. 67.  9.]
2019-01-08 14:40:18,638 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 14:40:18,663 - 10 - corpus.py - rho_sampling - ['50.8143', '8.4779', '928.5380', '239.9023', '1.9343', '24.8556']
2019-01-08 14:40:18,663 - 10 - pipeline.py - baseline - Iteration 2
2019-01-08 14:40:18,832 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 14:40:18,842 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 14:40:18,842 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 14', '1: 35', '2: 18', '3: 4', '4: 34', '5: 32', '6: 33']
2019-01-08 14:40:18,867 - 10 - accuracy_class.py - mof_val - frames true: 131577	frames overall : 558928
2019-01-08 14:40:18,867 - 10 - corpus.py - accuracy_corpus - Action: salat
2019-01-08 14:40:18,867 - 10 - corpus.py - accuracy_corpus - MoF val: 0.23540956974780294
2019-01-08 14:40:18,867 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.23473327512667105
2019-01-08 14:40:18,867 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 23956
2019-01-08 14:40:18,867 - 10 - accuracy_class.py - mof_classes - label 4: 0.162984  1276 / 7829
2019-01-08 14:40:18,867 - 10 - accuracy_class.py - mof_classes - label 14: 0.995021  5995 / 6025
2019-01-08 14:40:18,867 - 10 - accuracy_class.py - mof_classes - label 18: 0.132083  345 / 2612
2019-01-08 14:40:18,867 - 10 - accuracy_class.py - mof_classes - label 32: 0.212830  65234 / 306508
2019-01-08 14:40:18,867 - 10 - accuracy_class.py - mof_classes - label 33: 0.387841  16498 / 42538
2019-01-08 14:40:18,867 - 10 - accuracy_class.py - mof_classes - label 34: 0.258174  42196 / 163440
2019-01-08 14:40:18,867 - 10 - accuracy_class.py - mof_classes - label 35: 0.005482  33 / 6020
2019-01-08 14:40:18,868 - 10 - accuracy_class.py - mof_classes - average class mof: 0.269302
2019-01-08 14:40:18,868 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 23956
2019-01-08 14:40:18,868 - 10 - accuracy_class.py - iou_classes - label 4: 0.014275  1276 / 89389
2019-01-08 14:40:18,868 - 10 - accuracy_class.py - iou_classes - label 14: 0.084982  5995 / 70544
2019-01-08 14:40:18,868 - 10 - accuracy_class.py - iou_classes - label 18: 0.004594  345 / 75091
2019-01-08 14:40:18,868 - 10 - accuracy_class.py - iou_classes - label 32: 0.192246  65234 / 339326
2019-01-08 14:40:18,868 - 10 - accuracy_class.py - iou_classes - label 33: 0.174564  16498 / 94510
2019-01-08 14:40:18,868 - 10 - accuracy_class.py - iou_classes - label 34: 0.213710  42196 / 197445
2019-01-08 14:40:18,868 - 10 - accuracy_class.py - iou_classes - label 35: 0.000344  33 / 96018
2019-01-08 14:40:18,868 - 10 - accuracy_class.py - iou_classes - average IoU: 0.097816
2019-01-08 14:40:18,868 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.085589
2019-01-08 14:40:27,825 - 10 - f1_score.py - f1 - f1 score: 0.289849
2019-01-08 14:40:27,847 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 9184.457 ms ~ 0.153 min ~ 9.184 sec
2019-01-08 14:40:27,847 - 10 - corpus.py - embedding_training - .
2019-01-08 14:40:27,848 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 14:40:27,848 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 14:40:27,848 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 14:40:31,868 - 10 - training_embed.py - training - create model
2019-01-08 14:40:31,868 - 10 - training_embed.py - training - epochs: 12
2019-01-08 14:40:31,868 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 14:40:32,256 - 10 - training_embed.py - training - Epoch: [0][100/2184]	Time 0.002 (0.004)	Data 0.001 (0.003)	Loss 308.4039 (307.4075)	
2019-01-08 14:40:32,462 - 10 - training_embed.py - training - Epoch: [0][200/2184]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 305.3113 (307.3598)	
2019-01-08 14:40:32,677 - 10 - training_embed.py - training - Epoch: [0][300/2184]	Time 0.005 (0.003)	Data 0.001 (0.002)	Loss 304.5649 (307.2321)	
2019-01-08 14:40:32,881 - 10 - training_embed.py - training - Epoch: [0][400/2184]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 307.7815 (307.2289)	
2019-01-08 14:40:33,109 - 10 - training_embed.py - training - Epoch: [0][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.4720 (307.1328)	
2019-01-08 14:40:33,333 - 10 - training_embed.py - training - Epoch: [0][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6789 (307.1579)	
2019-01-08 14:40:33,569 - 10 - training_embed.py - training - Epoch: [0][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0832 (307.2422)	
2019-01-08 14:40:33,781 - 10 - training_embed.py - training - Epoch: [0][800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.1140 (307.2332)	
2019-01-08 14:40:34,000 - 10 - training_embed.py - training - Epoch: [0][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7454 (307.2759)	
2019-01-08 14:40:34,236 - 10 - training_embed.py - training - Epoch: [0][1000/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 310.5098 (307.2810)	
2019-01-08 14:40:34,438 - 10 - training_embed.py - training - Epoch: [0][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2301 (307.2581)	
2019-01-08 14:40:34,666 - 10 - training_embed.py - training - Epoch: [0][1200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.4978 (307.2476)	
2019-01-08 14:40:34,878 - 10 - training_embed.py - training - Epoch: [0][1300/2184]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 309.0943 (307.2437)	
2019-01-08 14:40:35,089 - 10 - training_embed.py - training - Epoch: [0][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.3185 (307.2533)	
2019-01-08 14:40:35,327 - 10 - training_embed.py - training - Epoch: [0][1500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.0725 (307.2744)	
2019-01-08 14:40:35,567 - 10 - training_embed.py - training - Epoch: [0][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4250 (307.2789)	
2019-01-08 14:40:35,791 - 10 - training_embed.py - training - Epoch: [0][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1014 (307.2771)	
2019-01-08 14:40:36,001 - 10 - training_embed.py - training - Epoch: [0][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.7942 (307.2741)	
2019-01-08 14:40:36,209 - 10 - training_embed.py - training - Epoch: [0][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0774 (307.2630)	
2019-01-08 14:40:36,426 - 10 - training_embed.py - training - Epoch: [0][2000/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 302.9134 (307.2570)	
2019-01-08 14:40:36,668 - 10 - training_embed.py - training - Epoch: [0][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.2036 (307.2327)	
2019-01-08 14:40:36,839 - 10 - training_embed.py - training - loss: 307.195529
2019-01-08 14:40:36,840 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 14:40:37,285 - 10 - training_embed.py - training - Epoch: [1][100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.6853 (307.2108)	
2019-01-08 14:40:37,494 - 10 - training_embed.py - training - Epoch: [1][200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 309.9749 (307.0606)	
2019-01-08 14:40:37,722 - 10 - training_embed.py - training - Epoch: [1][300/2184]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 305.2493 (307.0346)	
2019-01-08 14:40:37,946 - 10 - training_embed.py - training - Epoch: [1][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.1449 (307.0152)	
2019-01-08 14:40:38,172 - 10 - training_embed.py - training - Epoch: [1][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5046 (307.0110)	
2019-01-08 14:40:38,388 - 10 - training_embed.py - training - Epoch: [1][600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.9238 (307.0113)	
2019-01-08 14:40:38,597 - 10 - training_embed.py - training - Epoch: [1][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6178 (307.0091)	
2019-01-08 14:40:38,812 - 10 - training_embed.py - training - Epoch: [1][800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.8271 (307.0266)	
2019-01-08 14:40:39,031 - 10 - training_embed.py - training - Epoch: [1][900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.4492 (307.0145)	
2019-01-08 14:40:39,250 - 10 - training_embed.py - training - Epoch: [1][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3641 (307.0476)	
2019-01-08 14:40:39,477 - 10 - training_embed.py - training - Epoch: [1][1100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.0886 (307.0373)	
2019-01-08 14:40:39,687 - 10 - training_embed.py - training - Epoch: [1][1200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.8604 (307.0759)	
2019-01-08 14:40:39,897 - 10 - training_embed.py - training - Epoch: [1][1300/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 303.8184 (307.0577)	
2019-01-08 14:40:40,125 - 10 - training_embed.py - training - Epoch: [1][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1537 (307.0659)	
2019-01-08 14:40:40,339 - 10 - training_embed.py - training - Epoch: [1][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7270 (307.0574)	
2019-01-08 14:40:40,568 - 10 - training_embed.py - training - Epoch: [1][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6446 (307.0750)	
2019-01-08 14:40:40,786 - 10 - training_embed.py - training - Epoch: [1][1700/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 306.4983 (307.0573)	
2019-01-08 14:40:40,999 - 10 - training_embed.py - training - Epoch: [1][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.9965 (307.0547)	
2019-01-08 14:40:41,213 - 10 - training_embed.py - training - Epoch: [1][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1375 (307.0704)	
2019-01-08 14:40:41,448 - 10 - training_embed.py - training - Epoch: [1][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3826 (307.0621)	
2019-01-08 14:40:41,671 - 10 - training_embed.py - training - Epoch: [1][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.7891 (307.0646)	
2019-01-08 14:40:41,858 - 10 - training_embed.py - training - loss: 307.033023
2019-01-08 14:40:41,858 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 14:40:42,295 - 10 - training_embed.py - training - Epoch: [2][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3870 (306.7942)	
2019-01-08 14:40:42,508 - 10 - training_embed.py - training - Epoch: [2][200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.6242 (306.8501)	
2019-01-08 14:40:42,728 - 10 - training_embed.py - training - Epoch: [2][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8925 (306.8128)	
2019-01-08 14:40:42,951 - 10 - training_embed.py - training - Epoch: [2][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0774 (306.8628)	
2019-01-08 14:40:43,157 - 10 - training_embed.py - training - Epoch: [2][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4484 (306.8862)	
2019-01-08 14:40:43,364 - 10 - training_embed.py - training - Epoch: [2][600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.1491 (306.9089)	
2019-01-08 14:40:43,593 - 10 - training_embed.py - training - Epoch: [2][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.7211 (306.9149)	
2019-01-08 14:40:43,824 - 10 - training_embed.py - training - Epoch: [2][800/2184]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 305.6623 (306.9096)	
2019-01-08 14:40:44,058 - 10 - training_embed.py - training - Epoch: [2][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.0968 (306.9330)	
2019-01-08 14:40:44,284 - 10 - training_embed.py - training - Epoch: [2][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.0524 (306.9327)	
2019-01-08 14:40:44,514 - 10 - training_embed.py - training - Epoch: [2][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.0749 (306.9306)	
2019-01-08 14:40:44,723 - 10 - training_embed.py - training - Epoch: [2][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3564 (306.9245)	
2019-01-08 14:40:44,946 - 10 - training_embed.py - training - Epoch: [2][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6514 (306.9463)	
2019-01-08 14:40:45,164 - 10 - training_embed.py - training - Epoch: [2][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3357 (306.9505)	
2019-01-08 14:40:45,390 - 10 - training_embed.py - training - Epoch: [2][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9737 (306.9438)	
2019-01-08 14:40:45,606 - 10 - training_embed.py - training - Epoch: [2][1600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.6517 (306.9113)	
2019-01-08 14:40:45,822 - 10 - training_embed.py - training - Epoch: [2][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7442 (306.9154)	
2019-01-08 14:40:46,036 - 10 - training_embed.py - training - Epoch: [2][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9995 (306.9156)	
2019-01-08 14:40:46,263 - 10 - training_embed.py - training - Epoch: [2][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8689 (306.9090)	
2019-01-08 14:40:46,467 - 10 - training_embed.py - training - Epoch: [2][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.5074 (306.9054)	
2019-01-08 14:40:46,714 - 10 - training_embed.py - training - Epoch: [2][2100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.2307 (306.9003)	
2019-01-08 14:40:46,895 - 10 - training_embed.py - training - loss: 306.870879
2019-01-08 14:40:46,895 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 14:40:47,326 - 10 - training_embed.py - training - Epoch: [3][100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.6552 (306.6213)	
2019-01-08 14:40:47,539 - 10 - training_embed.py - training - Epoch: [3][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.2782 (306.5346)	
2019-01-08 14:40:47,754 - 10 - training_embed.py - training - Epoch: [3][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1104 (306.6589)	
2019-01-08 14:40:47,973 - 10 - training_embed.py - training - Epoch: [3][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.8557 (306.6322)	
2019-01-08 14:40:48,185 - 10 - training_embed.py - training - Epoch: [3][500/2184]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 309.5127 (306.6599)	
2019-01-08 14:40:48,405 - 10 - training_embed.py - training - Epoch: [3][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0286 (306.7011)	
2019-01-08 14:40:48,631 - 10 - training_embed.py - training - Epoch: [3][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5434 (306.6296)	
2019-01-08 14:40:48,856 - 10 - training_embed.py - training - Epoch: [3][800/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.7386 (306.6985)	
2019-01-08 14:40:49,081 - 10 - training_embed.py - training - Epoch: [3][900/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 309.1069 (306.7294)	
2019-01-08 14:40:49,286 - 10 - training_embed.py - training - Epoch: [3][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3935 (306.7239)	
2019-01-08 14:40:49,505 - 10 - training_embed.py - training - Epoch: [3][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8586 (306.7201)	
2019-01-08 14:40:49,721 - 10 - training_embed.py - training - Epoch: [3][1200/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 312.3619 (306.7222)	
2019-01-08 14:40:49,950 - 10 - training_embed.py - training - Epoch: [3][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8694 (306.7166)	
2019-01-08 14:40:50,186 - 10 - training_embed.py - training - Epoch: [3][1400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.4925 (306.7105)	
2019-01-08 14:40:50,404 - 10 - training_embed.py - training - Epoch: [3][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4590 (306.6884)	
2019-01-08 14:40:50,624 - 10 - training_embed.py - training - Epoch: [3][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0380 (306.7049)	
2019-01-08 14:40:50,835 - 10 - training_embed.py - training - Epoch: [3][1700/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.2101 (306.7103)	
2019-01-08 14:40:51,067 - 10 - training_embed.py - training - Epoch: [3][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.1770 (306.7100)	
2019-01-08 14:40:51,282 - 10 - training_embed.py - training - Epoch: [3][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.8442 (306.7257)	
2019-01-08 14:40:51,517 - 10 - training_embed.py - training - Epoch: [3][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4771 (306.7375)	
2019-01-08 14:40:51,725 - 10 - training_embed.py - training - Epoch: [3][2100/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 305.4580 (306.7412)	
2019-01-08 14:40:51,911 - 10 - training_embed.py - training - loss: 306.708822
2019-01-08 14:40:51,911 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 14:40:52,358 - 10 - training_embed.py - training - Epoch: [4][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 312.1050 (306.5606)	
2019-01-08 14:40:52,591 - 10 - training_embed.py - training - Epoch: [4][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.6894 (306.5874)	
2019-01-08 14:40:52,805 - 10 - training_embed.py - training - Epoch: [4][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8280 (306.6965)	
2019-01-08 14:40:53,020 - 10 - training_embed.py - training - Epoch: [4][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0949 (306.6755)	
2019-01-08 14:40:53,231 - 10 - training_embed.py - training - Epoch: [4][500/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.0278 (306.6957)	
2019-01-08 14:40:53,474 - 10 - training_embed.py - training - Epoch: [4][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4449 (306.6842)	
2019-01-08 14:40:53,692 - 10 - training_embed.py - training - Epoch: [4][700/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 307.2729 (306.6749)	
2019-01-08 14:40:53,912 - 10 - training_embed.py - training - Epoch: [4][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1849 (306.6630)	
2019-01-08 14:40:54,122 - 10 - training_embed.py - training - Epoch: [4][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 301.5329 (306.6632)	
2019-01-08 14:40:54,345 - 10 - training_embed.py - training - Epoch: [4][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.3785 (306.6201)	
2019-01-08 14:40:54,571 - 10 - training_embed.py - training - Epoch: [4][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0359 (306.6088)	
2019-01-08 14:40:54,789 - 10 - training_embed.py - training - Epoch: [4][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9345 (306.6275)	
2019-01-08 14:40:55,014 - 10 - training_embed.py - training - Epoch: [4][1300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.2162 (306.6380)	
2019-01-08 14:40:55,228 - 10 - training_embed.py - training - Epoch: [4][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.0296 (306.6233)	
2019-01-08 14:40:55,449 - 10 - training_embed.py - training - Epoch: [4][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.2353 (306.6372)	
2019-01-08 14:40:55,656 - 10 - training_embed.py - training - Epoch: [4][1600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.8882 (306.6223)	
2019-01-08 14:40:55,874 - 10 - training_embed.py - training - Epoch: [4][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0395 (306.6068)	
2019-01-08 14:40:56,088 - 10 - training_embed.py - training - Epoch: [4][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1539 (306.5949)	
2019-01-08 14:40:56,322 - 10 - training_embed.py - training - Epoch: [4][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5800 (306.5884)	
2019-01-08 14:40:56,529 - 10 - training_embed.py - training - Epoch: [4][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.2592 (306.5766)	
2019-01-08 14:40:56,745 - 10 - training_embed.py - training - Epoch: [4][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5135 (306.5764)	
2019-01-08 14:40:56,913 - 10 - training_embed.py - training - loss: 306.547680
2019-01-08 14:40:56,913 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 14:40:57,361 - 10 - training_embed.py - training - Epoch: [5][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9500 (306.9285)	
2019-01-08 14:40:57,583 - 10 - training_embed.py - training - Epoch: [5][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.4661 (306.7186)	
2019-01-08 14:40:57,797 - 10 - training_embed.py - training - Epoch: [5][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4167 (306.5990)	
2019-01-08 14:40:58,002 - 10 - training_embed.py - training - Epoch: [5][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9789 (306.5995)	
2019-01-08 14:40:58,233 - 10 - training_embed.py - training - Epoch: [5][500/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.0296 (306.6311)	
2019-01-08 14:40:58,448 - 10 - training_embed.py - training - Epoch: [5][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7261 (306.6414)	
2019-01-08 14:40:58,673 - 10 - training_embed.py - training - Epoch: [5][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5391 (306.6084)	
2019-01-08 14:40:58,879 - 10 - training_embed.py - training - Epoch: [5][800/2184]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 302.9659 (306.5362)	
2019-01-08 14:40:59,095 - 10 - training_embed.py - training - Epoch: [5][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.8909 (306.4747)	
2019-01-08 14:40:59,306 - 10 - training_embed.py - training - Epoch: [5][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.0268 (306.4306)	
2019-01-08 14:40:59,544 - 10 - training_embed.py - training - Epoch: [5][1100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 309.7401 (306.4412)	
2019-01-08 14:40:59,763 - 10 - training_embed.py - training - Epoch: [5][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6161 (306.4753)	
2019-01-08 14:40:59,988 - 10 - training_embed.py - training - Epoch: [5][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3420 (306.4423)	
2019-01-08 14:41:00,224 - 10 - training_embed.py - training - Epoch: [5][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6271 (306.4586)	
2019-01-08 14:41:00,437 - 10 - training_embed.py - training - Epoch: [5][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3822 (306.4551)	
2019-01-08 14:41:00,674 - 10 - training_embed.py - training - Epoch: [5][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.3375 (306.4565)	
2019-01-08 14:41:00,894 - 10 - training_embed.py - training - Epoch: [5][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4479 (306.4566)	
2019-01-08 14:41:01,128 - 10 - training_embed.py - training - Epoch: [5][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7994 (306.4681)	
2019-01-08 14:41:01,346 - 10 - training_embed.py - training - Epoch: [5][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.8004 (306.4484)	
2019-01-08 14:41:01,553 - 10 - training_embed.py - training - Epoch: [5][2000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.8500 (306.4316)	
2019-01-08 14:41:01,788 - 10 - training_embed.py - training - Epoch: [5][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8192 (306.4201)	
2019-01-08 14:41:01,979 - 10 - training_embed.py - training - loss: 306.385660
2019-01-08 14:41:01,979 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 14:41:02,427 - 10 - training_embed.py - training - Epoch: [6][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.6631 (306.1914)	
2019-01-08 14:41:02,652 - 10 - training_embed.py - training - Epoch: [6][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9023 (306.3903)	
2019-01-08 14:41:02,878 - 10 - training_embed.py - training - Epoch: [6][300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.4054 (306.3332)	
2019-01-08 14:41:03,097 - 10 - training_embed.py - training - Epoch: [6][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.6482 (306.2691)	
2019-01-08 14:41:03,308 - 10 - training_embed.py - training - Epoch: [6][500/2184]	Time 0.003 (0.002)	Data 0.000 (0.001)	Loss 308.3261 (306.2326)	
2019-01-08 14:41:03,550 - 10 - training_embed.py - training - Epoch: [6][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4782 (306.2757)	
2019-01-08 14:41:03,771 - 10 - training_embed.py - training - Epoch: [6][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8643 (306.2812)	
2019-01-08 14:41:03,982 - 10 - training_embed.py - training - Epoch: [6][800/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 303.1312 (306.2725)	
2019-01-08 14:41:04,215 - 10 - training_embed.py - training - Epoch: [6][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0597 (306.2959)	
2019-01-08 14:41:04,429 - 10 - training_embed.py - training - Epoch: [6][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9048 (306.2714)	
2019-01-08 14:41:04,653 - 10 - training_embed.py - training - Epoch: [6][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4404 (306.2878)	
2019-01-08 14:41:04,855 - 10 - training_embed.py - training - Epoch: [6][1200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.3036 (306.2792)	
2019-01-08 14:41:05,073 - 10 - training_embed.py - training - Epoch: [6][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5179 (306.2993)	
2019-01-08 14:41:05,275 - 10 - training_embed.py - training - Epoch: [6][1400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 303.6855 (306.2755)	
2019-01-08 14:41:05,497 - 10 - training_embed.py - training - Epoch: [6][1500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.7784 (306.2776)	
2019-01-08 14:41:05,712 - 10 - training_embed.py - training - Epoch: [6][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3154 (306.2691)	
2019-01-08 14:41:05,932 - 10 - training_embed.py - training - Epoch: [6][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.1185 (306.2824)	
2019-01-08 14:41:06,154 - 10 - training_embed.py - training - Epoch: [6][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4214 (306.2778)	
2019-01-08 14:41:06,373 - 10 - training_embed.py - training - Epoch: [6][1900/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 303.4619 (306.2697)	
2019-01-08 14:41:06,608 - 10 - training_embed.py - training - Epoch: [6][2000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.0225 (306.2781)	
2019-01-08 14:41:06,819 - 10 - training_embed.py - training - Epoch: [6][2100/2184]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 301.0724 (306.2635)	
2019-01-08 14:41:07,008 - 10 - training_embed.py - training - loss: 306.222862
2019-01-08 14:41:07,008 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 14:41:07,446 - 10 - training_embed.py - training - Epoch: [7][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0045 (306.0567)	
2019-01-08 14:41:07,656 - 10 - training_embed.py - training - Epoch: [7][200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 302.3826 (305.9975)	
2019-01-08 14:41:07,888 - 10 - training_embed.py - training - Epoch: [7][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.4844 (306.0773)	
2019-01-08 14:41:08,108 - 10 - training_embed.py - training - Epoch: [7][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6352 (306.1478)	
2019-01-08 14:41:08,342 - 10 - training_embed.py - training - Epoch: [7][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4256 (306.2349)	
2019-01-08 14:41:08,561 - 10 - training_embed.py - training - Epoch: [7][600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.7572 (306.1692)	
2019-01-08 14:41:08,774 - 10 - training_embed.py - training - Epoch: [7][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9991 (306.2254)	
2019-01-08 14:41:08,998 - 10 - training_embed.py - training - Epoch: [7][800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 302.3080 (306.2662)	
2019-01-08 14:41:09,222 - 10 - training_embed.py - training - Epoch: [7][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8736 (306.2410)	
2019-01-08 14:41:09,455 - 10 - training_embed.py - training - Epoch: [7][1000/2184]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 305.3282 (306.2112)	
2019-01-08 14:41:09,674 - 10 - training_embed.py - training - Epoch: [7][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5731 (306.1850)	
2019-01-08 14:41:09,876 - 10 - training_embed.py - training - Epoch: [7][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6018 (306.1735)	
2019-01-08 14:41:10,088 - 10 - training_embed.py - training - Epoch: [7][1300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 311.7550 (306.1872)	
2019-01-08 14:41:10,318 - 10 - training_embed.py - training - Epoch: [7][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7647 (306.1498)	
2019-01-08 14:41:10,525 - 10 - training_embed.py - training - Epoch: [7][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.2106 (306.1542)	
2019-01-08 14:41:10,751 - 10 - training_embed.py - training - Epoch: [7][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3523 (306.1593)	
2019-01-08 14:41:10,966 - 10 - training_embed.py - training - Epoch: [7][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8127 (306.1464)	
2019-01-08 14:41:11,204 - 10 - training_embed.py - training - Epoch: [7][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1551 (306.1356)	
2019-01-08 14:41:11,439 - 10 - training_embed.py - training - Epoch: [7][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.0836 (306.1311)	
2019-01-08 14:41:11,652 - 10 - training_embed.py - training - Epoch: [7][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8585 (306.1305)	
2019-01-08 14:41:11,887 - 10 - training_embed.py - training - Epoch: [7][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6106 (306.1025)	
2019-01-08 14:41:12,063 - 10 - training_embed.py - training - loss: 306.060739
2019-01-08 14:41:12,063 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 14:41:12,502 - 10 - training_embed.py - training - Epoch: [8][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9649 (306.4371)	
2019-01-08 14:41:12,737 - 10 - training_embed.py - training - Epoch: [8][200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.4998 (306.2969)	
2019-01-08 14:41:12,948 - 10 - training_embed.py - training - Epoch: [8][300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.5904 (306.1416)	
2019-01-08 14:41:13,166 - 10 - training_embed.py - training - Epoch: [8][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1476 (306.0502)	
2019-01-08 14:41:13,382 - 10 - training_embed.py - training - Epoch: [8][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.1857 (305.9918)	
2019-01-08 14:41:13,601 - 10 - training_embed.py - training - Epoch: [8][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0071 (305.9782)	
2019-01-08 14:41:13,838 - 10 - training_embed.py - training - Epoch: [8][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4102 (305.9496)	
2019-01-08 14:41:14,053 - 10 - training_embed.py - training - Epoch: [8][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.7584 (305.9118)	
2019-01-08 14:41:14,285 - 10 - training_embed.py - training - Epoch: [8][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.7062 (305.9308)	
2019-01-08 14:41:14,510 - 10 - training_embed.py - training - Epoch: [8][1000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.9647 (305.9120)	
2019-01-08 14:41:14,724 - 10 - training_embed.py - training - Epoch: [8][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9784 (305.8970)	
2019-01-08 14:41:14,937 - 10 - training_embed.py - training - Epoch: [8][1200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.1765 (305.8996)	
2019-01-08 14:41:15,161 - 10 - training_embed.py - training - Epoch: [8][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.7401 (305.9117)	
2019-01-08 14:41:15,379 - 10 - training_embed.py - training - Epoch: [8][1400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.0336 (305.9094)	
2019-01-08 14:41:15,603 - 10 - training_embed.py - training - Epoch: [8][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5636 (305.8840)	
2019-01-08 14:41:15,815 - 10 - training_embed.py - training - Epoch: [8][1600/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 306.1748 (305.8810)	
2019-01-08 14:41:16,017 - 10 - training_embed.py - training - Epoch: [8][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9355 (305.8918)	
2019-01-08 14:41:16,242 - 10 - training_embed.py - training - Epoch: [8][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.0503 (305.8941)	
2019-01-08 14:41:16,459 - 10 - training_embed.py - training - Epoch: [8][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.0698 (305.9152)	
2019-01-08 14:41:16,700 - 10 - training_embed.py - training - Epoch: [8][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5390 (305.9193)	
2019-01-08 14:41:16,917 - 10 - training_embed.py - training - Epoch: [8][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8445 (305.9202)	
2019-01-08 14:41:17,101 - 10 - training_embed.py - training - loss: 305.899423
2019-01-08 14:41:17,101 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 14:41:17,553 - 10 - training_embed.py - training - Epoch: [9][100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.8782 (305.6204)	
2019-01-08 14:41:17,767 - 10 - training_embed.py - training - Epoch: [9][200/2184]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 307.7252 (305.7819)	
2019-01-08 14:41:17,991 - 10 - training_embed.py - training - Epoch: [9][300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.1629 (305.8165)	
2019-01-08 14:41:18,202 - 10 - training_embed.py - training - Epoch: [9][400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.7605 (305.8628)	
2019-01-08 14:41:18,413 - 10 - training_embed.py - training - Epoch: [9][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8262 (305.9911)	
2019-01-08 14:41:18,641 - 10 - training_embed.py - training - Epoch: [9][600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.8162 (305.9410)	
2019-01-08 14:41:18,861 - 10 - training_embed.py - training - Epoch: [9][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0206 (305.9067)	
2019-01-08 14:41:19,086 - 10 - training_embed.py - training - Epoch: [9][800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.2287 (305.8922)	
2019-01-08 14:41:19,302 - 10 - training_embed.py - training - Epoch: [9][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7753 (305.8628)	
2019-01-08 14:41:19,513 - 10 - training_embed.py - training - Epoch: [9][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.7479 (305.8692)	
2019-01-08 14:41:19,731 - 10 - training_embed.py - training - Epoch: [9][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.5171 (305.8448)	
2019-01-08 14:41:19,956 - 10 - training_embed.py - training - Epoch: [9][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.1267 (305.8112)	
2019-01-08 14:41:20,189 - 10 - training_embed.py - training - Epoch: [9][1300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.6103 (305.7903)	
2019-01-08 14:41:20,405 - 10 - training_embed.py - training - Epoch: [9][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.0862 (305.7691)	
2019-01-08 14:41:20,619 - 10 - training_embed.py - training - Epoch: [9][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6222 (305.7635)	
2019-01-08 14:41:20,840 - 10 - training_embed.py - training - Epoch: [9][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.0182 (305.7754)	
2019-01-08 14:41:21,077 - 10 - training_embed.py - training - Epoch: [9][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0504 (305.7723)	
2019-01-08 14:41:21,299 - 10 - training_embed.py - training - Epoch: [9][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1586 (305.7801)	
2019-01-08 14:41:21,508 - 10 - training_embed.py - training - Epoch: [9][1900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.3411 (305.7640)	
2019-01-08 14:41:21,725 - 10 - training_embed.py - training - Epoch: [9][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.4266 (305.7552)	
2019-01-08 14:41:21,952 - 10 - training_embed.py - training - Epoch: [9][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.9966 (305.7446)	
2019-01-08 14:41:22,141 - 10 - training_embed.py - training - loss: 305.736827
2019-01-08 14:41:22,141 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 14:41:22,602 - 10 - training_embed.py - training - Epoch: [10][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4396 (305.4751)	
2019-01-08 14:41:22,823 - 10 - training_embed.py - training - Epoch: [10][200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.2601 (305.5186)	
2019-01-08 14:41:23,027 - 10 - training_embed.py - training - Epoch: [10][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1278 (305.5664)	
2019-01-08 14:41:23,247 - 10 - training_embed.py - training - Epoch: [10][400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.0711 (305.6315)	
2019-01-08 14:41:23,463 - 10 - training_embed.py - training - Epoch: [10][500/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 300.2794 (305.5946)	
2019-01-08 14:41:23,678 - 10 - training_embed.py - training - Epoch: [10][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6357 (305.6234)	
2019-01-08 14:41:23,923 - 10 - training_embed.py - training - Epoch: [10][700/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.0884 (305.6602)	
2019-01-08 14:41:24,152 - 10 - training_embed.py - training - Epoch: [10][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.7767 (305.6735)	
2019-01-08 14:41:24,388 - 10 - training_embed.py - training - Epoch: [10][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.4392 (305.6327)	
2019-01-08 14:41:24,605 - 10 - training_embed.py - training - Epoch: [10][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.1185 (305.6042)	
2019-01-08 14:41:24,818 - 10 - training_embed.py - training - Epoch: [10][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6802 (305.6030)	
2019-01-08 14:41:25,045 - 10 - training_embed.py - training - Epoch: [10][1200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.8314 (305.6231)	
2019-01-08 14:41:25,253 - 10 - training_embed.py - training - Epoch: [10][1300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.3243 (305.6397)	
2019-01-08 14:41:25,461 - 10 - training_embed.py - training - Epoch: [10][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.6680 (305.6467)	
2019-01-08 14:41:25,690 - 10 - training_embed.py - training - Epoch: [10][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4999 (305.6325)	
2019-01-08 14:41:25,920 - 10 - training_embed.py - training - Epoch: [10][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.6020 (305.6255)	
2019-01-08 14:41:26,140 - 10 - training_embed.py - training - Epoch: [10][1700/2184]	Time 0.005 (0.002)	Data 0.002 (0.001)	Loss 301.1938 (305.6314)	
2019-01-08 14:41:26,364 - 10 - training_embed.py - training - Epoch: [10][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.8810 (305.6182)	
2019-01-08 14:41:26,584 - 10 - training_embed.py - training - Epoch: [10][1900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 303.1936 (305.6152)	
2019-01-08 14:41:26,790 - 10 - training_embed.py - training - Epoch: [10][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8550 (305.6123)	
2019-01-08 14:41:27,006 - 10 - training_embed.py - training - Epoch: [10][2100/2184]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 301.5020 (305.5965)	
2019-01-08 14:41:27,181 - 10 - training_embed.py - training - loss: 305.573801
2019-01-08 14:41:27,181 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 14:41:27,622 - 10 - training_embed.py - training - Epoch: [11][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2239 (305.5613)	
2019-01-08 14:41:27,835 - 10 - training_embed.py - training - Epoch: [11][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9325 (305.5894)	
2019-01-08 14:41:28,057 - 10 - training_embed.py - training - Epoch: [11][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4413 (305.5136)	
2019-01-08 14:41:28,283 - 10 - training_embed.py - training - Epoch: [11][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.2740 (305.4941)	
2019-01-08 14:41:28,504 - 10 - training_embed.py - training - Epoch: [11][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6711 (305.5521)	
2019-01-08 14:41:28,735 - 10 - training_embed.py - training - Epoch: [11][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.8505 (305.5230)	
2019-01-08 14:41:28,946 - 10 - training_embed.py - training - Epoch: [11][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.5334 (305.5343)	
2019-01-08 14:41:29,155 - 10 - training_embed.py - training - Epoch: [11][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6431 (305.5442)	
2019-01-08 14:41:29,379 - 10 - training_embed.py - training - Epoch: [11][900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 302.2269 (305.5276)	
2019-01-08 14:41:29,597 - 10 - training_embed.py - training - Epoch: [11][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4479 (305.4979)	
2019-01-08 14:41:29,814 - 10 - training_embed.py - training - Epoch: [11][1100/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 304.6683 (305.5317)	
2019-01-08 14:41:30,030 - 10 - training_embed.py - training - Epoch: [11][1200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.0121 (305.5316)	
2019-01-08 14:41:30,242 - 10 - training_embed.py - training - Epoch: [11][1300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.2642 (305.5193)	
2019-01-08 14:41:30,459 - 10 - training_embed.py - training - Epoch: [11][1400/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 308.1498 (305.5012)	
2019-01-08 14:41:30,680 - 10 - training_embed.py - training - Epoch: [11][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.9813 (305.4775)	
2019-01-08 14:41:30,909 - 10 - training_embed.py - training - Epoch: [11][1600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.6105 (305.4814)	
2019-01-08 14:41:31,149 - 10 - training_embed.py - training - Epoch: [11][1700/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.5977 (305.4857)	
2019-01-08 14:41:31,350 - 10 - training_embed.py - training - Epoch: [11][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5216 (305.4904)	
2019-01-08 14:41:31,569 - 10 - training_embed.py - training - Epoch: [11][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0055 (305.4590)	
2019-01-08 14:41:31,784 - 10 - training_embed.py - training - Epoch: [11][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.0404 (305.4474)	
2019-01-08 14:41:32,040 - 10 - training_embed.py - training - Epoch: [11][2100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.5512 (305.4543)	
2019-01-08 14:41:32,240 - 10 - training_embed.py - training - loss: 305.412071
2019-01-08 14:41:32,333 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 14:41:33,283 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 949.610 ms ~ 0.016 min ~ 0.950 sec
2019-01-08 14:41:34,027 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1694.139 ms ~ 0.028 min ~ 1.694 sec
2019-01-08 14:41:34,027 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 14:41:34,028 - 10 - corpus.py - subactivity_sampler - 0 / 163
2019-01-08 14:41:34,028 - 10 - corpus.py - subactivity_sampler - [70514. 90031. 72824. 82836. 76201. 98052. 68470.]
2019-01-08 14:42:51,125 - 10 - corpus.py - subactivity_sampler - 20 / 163
2019-01-08 14:42:51,126 - 10 - corpus.py - subactivity_sampler - [70302. 89769. 73428. 82040. 75884. 99584. 67921.]
2019-01-08 14:44:14,416 - 10 - corpus.py - subactivity_sampler - 40 / 163
2019-01-08 14:44:14,416 - 10 - corpus.py - subactivity_sampler - [ 70307.  89705.  73525.  81391.  75163. 100946.  67891.]
2019-01-08 14:45:29,685 - 10 - corpus.py - subactivity_sampler - 60 / 163
2019-01-08 14:45:29,685 - 10 - corpus.py - subactivity_sampler - [ 70190.  89827.  73390.  81879.  75305. 101091.  67246.]
2019-01-08 14:46:39,217 - 10 - corpus.py - subactivity_sampler - 80 / 163
2019-01-08 14:46:39,218 - 10 - corpus.py - subactivity_sampler - [ 70013.  89903.  74091.  80738.  75385. 101781.  67017.]
2019-01-08 14:47:53,393 - 10 - corpus.py - subactivity_sampler - 100 / 163
2019-01-08 14:47:53,393 - 10 - corpus.py - subactivity_sampler - [ 70012.  89694.  74303.  79859.  74841. 103692.  66527.]
2019-01-08 14:48:46,705 - 10 - corpus.py - subactivity_sampler - 120 / 163
2019-01-08 14:48:46,705 - 10 - corpus.py - subactivity_sampler - [ 69943.  89394.  74481.  79362.  73535. 106121.  66092.]
2019-01-08 14:49:48,396 - 10 - corpus.py - subactivity_sampler - 140 / 163
2019-01-08 14:49:48,397 - 10 - corpus.py - subactivity_sampler - [ 69723.  89813.  74364.  78866.  73584. 106691.  65887.]
2019-01-08 14:50:55,627 - 10 - corpus.py - subactivity_sampler - 160 / 163
2019-01-08 14:50:55,628 - 10 - corpus.py - subactivity_sampler - [ 69430.  89821.  75035.  77750.  73024. 108405.  65463.]
2019-01-08 14:51:09,723 - 10 - corpus.py - subactivity_sampler - [ 69435.  89796.  75085.  77642.  73023. 108468.  65479.]
2019-01-08 14:51:09,723 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 575695.984 ms ~ 9.595 min ~ 575.696 sec
2019-01-08 14:51:09,724 - 10 - corpus.py - ordering_sampler - .
2019-01-08 14:51:12,210 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 14:51:12,210 - 10 - corpus.py - ordering_sampler - inv_count_vec: [13. 51.  0.  3. 63. 10.]
2019-01-08 14:51:12,210 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 14:51:12,227 - 10 - corpus.py - rho_sampling - ['49.7318', '12.0377', '50.7165', '180.2452', '4.1385', '8.1478']
2019-01-08 14:51:12,227 - 10 - pipeline.py - baseline - Iteration 3
2019-01-08 14:51:12,410 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 14:51:12,421 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 14:51:12,421 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 14', '1: 35', '2: 18', '3: 4', '4: 34', '5: 32', '6: 33']
2019-01-08 14:51:12,441 - 10 - accuracy_class.py - mof_val - frames true: 138079	frames overall : 558928
2019-01-08 14:51:12,442 - 10 - corpus.py - accuracy_corpus - Action: salat
2019-01-08 14:51:12,442 - 10 - corpus.py - accuracy_corpus - MoF val: 0.24704255288695504
2019-01-08 14:51:12,442 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.24704255288695504
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 23956
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - mof_classes - label 4: 0.131307  1028 / 7829
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - mof_classes - label 14: 0.994855  5994 / 6025
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - mof_classes - label 18: 0.140505  367 / 2612
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - mof_classes - label 32: 0.236203  72398 / 306508
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - mof_classes - label 33: 0.382599  16275 / 42538
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - mof_classes - label 34: 0.256590  41937 / 163440
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - mof_classes - label 35: 0.013289  80 / 6020
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - mof_classes - average class mof: 0.269418
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 23956
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - iou_classes - label 4: 0.012174  1028 / 84443
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - iou_classes - label 14: 0.086287  5994 / 69466
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - iou_classes - label 18: 0.004746  367 / 77330
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - iou_classes - label 32: 0.211333  72398 / 342578
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - iou_classes - label 33: 0.177400  16275 / 91742
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - iou_classes - label 34: 0.215586  41937 / 194526
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - iou_classes - label 35: 0.000836  80 / 95736
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - iou_classes - average IoU: 0.101194
2019-01-08 14:51:12,442 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.088545
2019-01-08 14:51:21,459 - 10 - f1_score.py - f1 - f1 score: 0.299669
2019-01-08 14:51:21,493 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 9265.992 ms ~ 0.154 min ~ 9.266 sec
2019-01-08 14:51:21,494 - 10 - corpus.py - embedding_training - .
2019-01-08 14:51:21,494 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 14:51:21,494 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 14:51:21,494 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 14:51:25,334 - 10 - training_embed.py - training - create model
2019-01-08 14:51:25,334 - 10 - training_embed.py - training - epochs: 12
2019-01-08 14:51:25,334 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 14:51:25,704 - 10 - training_embed.py - training - Epoch: [0][100/2184]	Time 0.002 (0.004)	Data 0.001 (0.002)	Loss 308.2640 (307.6979)	
2019-01-08 14:51:25,911 - 10 - training_embed.py - training - Epoch: [0][200/2184]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 305.6169 (307.6380)	
2019-01-08 14:51:26,135 - 10 - training_embed.py - training - Epoch: [0][300/2184]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 305.2498 (307.5928)	
2019-01-08 14:51:26,357 - 10 - training_embed.py - training - Epoch: [0][400/2184]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 308.1353 (307.5858)	
2019-01-08 14:51:26,582 - 10 - training_embed.py - training - Epoch: [0][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.2049 (307.4656)	
2019-01-08 14:51:26,793 - 10 - training_embed.py - training - Epoch: [0][600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 309.4223 (307.4918)	
2019-01-08 14:51:27,008 - 10 - training_embed.py - training - Epoch: [0][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.2507 (307.5770)	
2019-01-08 14:51:27,228 - 10 - training_embed.py - training - Epoch: [0][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5662 (307.5959)	
2019-01-08 14:51:27,434 - 10 - training_embed.py - training - Epoch: [0][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.4579 (307.6458)	
2019-01-08 14:51:27,655 - 10 - training_embed.py - training - Epoch: [0][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.0128 (307.6344)	
2019-01-08 14:51:27,872 - 10 - training_embed.py - training - Epoch: [0][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3522 (307.6274)	
2019-01-08 14:51:28,086 - 10 - training_embed.py - training - Epoch: [0][1200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 308.7959 (307.6214)	
2019-01-08 14:51:28,303 - 10 - training_embed.py - training - Epoch: [0][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6129 (307.6192)	
2019-01-08 14:51:28,519 - 10 - training_embed.py - training - Epoch: [0][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8900 (307.6241)	
2019-01-08 14:51:28,778 - 10 - training_embed.py - training - Epoch: [0][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.5016 (307.6455)	
2019-01-08 14:51:28,988 - 10 - training_embed.py - training - Epoch: [0][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.5240 (307.6508)	
2019-01-08 14:51:29,231 - 10 - training_embed.py - training - Epoch: [0][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2339 (307.6532)	
2019-01-08 14:51:29,451 - 10 - training_embed.py - training - Epoch: [0][1800/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 306.5481 (307.6548)	
2019-01-08 14:51:29,720 - 10 - training_embed.py - training - Epoch: [0][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0641 (307.6525)	
2019-01-08 14:51:29,933 - 10 - training_embed.py - training - Epoch: [0][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.1367 (307.6497)	
2019-01-08 14:51:30,196 - 10 - training_embed.py - training - Epoch: [0][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.6147 (307.6284)	
2019-01-08 14:51:30,377 - 10 - training_embed.py - training - loss: 307.588170
2019-01-08 14:51:30,377 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 14:51:30,806 - 10 - training_embed.py - training - Epoch: [1][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1331 (307.6118)	
2019-01-08 14:51:31,004 - 10 - training_embed.py - training - Epoch: [1][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.5234 (307.4449)	
2019-01-08 14:51:31,212 - 10 - training_embed.py - training - Epoch: [1][300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.7209 (307.4901)	
2019-01-08 14:51:31,415 - 10 - training_embed.py - training - Epoch: [1][400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.4656 (307.4663)	
2019-01-08 14:51:31,642 - 10 - training_embed.py - training - Epoch: [1][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2141 (307.4629)	
2019-01-08 14:51:31,852 - 10 - training_embed.py - training - Epoch: [1][600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.6352 (307.4400)	
2019-01-08 14:51:32,076 - 10 - training_embed.py - training - Epoch: [1][700/2184]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 305.9570 (307.4275)	
2019-01-08 14:51:32,292 - 10 - training_embed.py - training - Epoch: [1][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8884 (307.4207)	
2019-01-08 14:51:32,496 - 10 - training_embed.py - training - Epoch: [1][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6715 (307.4066)	
2019-01-08 14:51:32,713 - 10 - training_embed.py - training - Epoch: [1][1000/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.5608 (307.4325)	
2019-01-08 14:51:32,923 - 10 - training_embed.py - training - Epoch: [1][1100/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 309.8875 (307.4327)	
2019-01-08 14:51:33,133 - 10 - training_embed.py - training - Epoch: [1][1200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 311.3927 (307.4568)	
2019-01-08 14:51:33,337 - 10 - training_embed.py - training - Epoch: [1][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2691 (307.4406)	
2019-01-08 14:51:33,556 - 10 - training_embed.py - training - Epoch: [1][1400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.7025 (307.4317)	
2019-01-08 14:51:33,781 - 10 - training_embed.py - training - Epoch: [1][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.7784 (307.4117)	
2019-01-08 14:51:33,993 - 10 - training_embed.py - training - Epoch: [1][1600/2184]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 305.8534 (307.4289)	
2019-01-08 14:51:34,207 - 10 - training_embed.py - training - Epoch: [1][1700/2184]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 307.0302 (307.4000)	
2019-01-08 14:51:34,417 - 10 - training_embed.py - training - Epoch: [1][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.5738 (307.3900)	
2019-01-08 14:51:34,645 - 10 - training_embed.py - training - Epoch: [1][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0839 (307.3971)	
2019-01-08 14:51:34,852 - 10 - training_embed.py - training - Epoch: [1][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1790 (307.3947)	
2019-01-08 14:51:35,058 - 10 - training_embed.py - training - Epoch: [1][2100/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 308.8653 (307.3925)	
2019-01-08 14:51:35,231 - 10 - training_embed.py - training - loss: 307.362054
2019-01-08 14:51:35,231 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 14:51:35,601 - 10 - training_embed.py - training - Epoch: [2][100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.8521 (307.1573)	
2019-01-08 14:51:35,814 - 10 - training_embed.py - training - Epoch: [2][200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.7617 (307.1617)	
2019-01-08 14:51:36,042 - 10 - training_embed.py - training - Epoch: [2][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8744 (307.1490)	
2019-01-08 14:51:36,252 - 10 - training_embed.py - training - Epoch: [2][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6696 (307.1376)	
2019-01-08 14:51:36,465 - 10 - training_embed.py - training - Epoch: [2][500/2184]	Time 0.006 (0.002)	Data 0.001 (0.001)	Loss 307.2316 (307.1446)	
2019-01-08 14:51:36,684 - 10 - training_embed.py - training - Epoch: [2][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3600 (307.1733)	
2019-01-08 14:51:36,889 - 10 - training_embed.py - training - Epoch: [2][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 311.1444 (307.1891)	
2019-01-08 14:51:37,108 - 10 - training_embed.py - training - Epoch: [2][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6020 (307.1887)	
2019-01-08 14:51:37,316 - 10 - training_embed.py - training - Epoch: [2][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.2814 (307.2340)	
2019-01-08 14:51:37,530 - 10 - training_embed.py - training - Epoch: [2][1000/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.5210 (307.2233)	
2019-01-08 14:51:37,739 - 10 - training_embed.py - training - Epoch: [2][1100/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.3141 (307.2195)	
2019-01-08 14:51:37,954 - 10 - training_embed.py - training - Epoch: [2][1200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.8627 (307.2122)	
2019-01-08 14:51:38,169 - 10 - training_embed.py - training - Epoch: [2][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0875 (307.2229)	
2019-01-08 14:51:38,382 - 10 - training_embed.py - training - Epoch: [2][1400/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 304.8768 (307.2230)	
2019-01-08 14:51:38,603 - 10 - training_embed.py - training - Epoch: [2][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2073 (307.2294)	
2019-01-08 14:51:38,840 - 10 - training_embed.py - training - Epoch: [2][1600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 309.1094 (307.2131)	
2019-01-08 14:51:39,058 - 10 - training_embed.py - training - Epoch: [2][1700/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.3080 (307.2119)	
2019-01-08 14:51:39,274 - 10 - training_embed.py - training - Epoch: [2][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1951 (307.2005)	
2019-01-08 14:51:39,492 - 10 - training_embed.py - training - Epoch: [2][1900/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.8018 (307.1931)	
2019-01-08 14:51:39,711 - 10 - training_embed.py - training - Epoch: [2][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 311.2031 (307.1879)	
2019-01-08 14:51:39,936 - 10 - training_embed.py - training - Epoch: [2][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0407 (307.1681)	
2019-01-08 14:51:40,111 - 10 - training_embed.py - training - loss: 307.136795
2019-01-08 14:51:40,112 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 14:51:40,531 - 10 - training_embed.py - training - Epoch: [3][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4845 (306.9105)	
2019-01-08 14:51:40,746 - 10 - training_embed.py - training - Epoch: [3][200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 303.1742 (306.8983)	
2019-01-08 14:51:40,951 - 10 - training_embed.py - training - Epoch: [3][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6811 (306.9787)	
2019-01-08 14:51:41,169 - 10 - training_embed.py - training - Epoch: [3][400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.2665 (306.9819)	
2019-01-08 14:51:41,375 - 10 - training_embed.py - training - Epoch: [3][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.4594 (306.9867)	
2019-01-08 14:51:41,576 - 10 - training_embed.py - training - Epoch: [3][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.9392 (306.9981)	
2019-01-08 14:51:41,795 - 10 - training_embed.py - training - Epoch: [3][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2132 (306.9355)	
2019-01-08 14:51:42,016 - 10 - training_embed.py - training - Epoch: [3][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1530 (306.9632)	
2019-01-08 14:51:42,240 - 10 - training_embed.py - training - Epoch: [3][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0799 (306.9687)	
2019-01-08 14:51:42,456 - 10 - training_embed.py - training - Epoch: [3][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.0627 (306.9626)	
2019-01-08 14:51:42,663 - 10 - training_embed.py - training - Epoch: [3][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2029 (306.9609)	
2019-01-08 14:51:42,883 - 10 - training_embed.py - training - Epoch: [3][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.5340 (306.9614)	
2019-01-08 14:51:43,098 - 10 - training_embed.py - training - Epoch: [3][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4221 (306.9551)	
2019-01-08 14:51:43,341 - 10 - training_embed.py - training - Epoch: [3][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6651 (306.9561)	
2019-01-08 14:51:43,549 - 10 - training_embed.py - training - Epoch: [3][1500/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 309.0526 (306.9335)	
2019-01-08 14:51:43,780 - 10 - training_embed.py - training - Epoch: [3][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.0412 (306.9348)	
2019-01-08 14:51:44,008 - 10 - training_embed.py - training - Epoch: [3][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9736 (306.9398)	
2019-01-08 14:51:44,234 - 10 - training_embed.py - training - Epoch: [3][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.3597 (306.9231)	
2019-01-08 14:51:44,464 - 10 - training_embed.py - training - Epoch: [3][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9205 (306.9375)	
2019-01-08 14:51:44,673 - 10 - training_embed.py - training - Epoch: [3][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.8045 (306.9454)	
2019-01-08 14:51:44,883 - 10 - training_embed.py - training - Epoch: [3][2100/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 306.4512 (306.9445)	
2019-01-08 14:51:45,062 - 10 - training_embed.py - training - loss: 306.911388
2019-01-08 14:51:45,065 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 14:51:45,491 - 10 - training_embed.py - training - Epoch: [4][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.8405 (306.7562)	
2019-01-08 14:51:45,697 - 10 - training_embed.py - training - Epoch: [4][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3641 (306.7722)	
2019-01-08 14:51:45,919 - 10 - training_embed.py - training - Epoch: [4][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.0896 (306.9556)	
2019-01-08 14:51:46,143 - 10 - training_embed.py - training - Epoch: [4][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6931 (306.8906)	
2019-01-08 14:51:46,354 - 10 - training_embed.py - training - Epoch: [4][500/2184]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 306.4374 (306.8824)	
2019-01-08 14:51:46,555 - 10 - training_embed.py - training - Epoch: [4][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6139 (306.8745)	
2019-01-08 14:51:46,779 - 10 - training_embed.py - training - Epoch: [4][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7104 (306.8501)	
2019-01-08 14:51:46,980 - 10 - training_embed.py - training - Epoch: [4][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2861 (306.8266)	
2019-01-08 14:51:47,198 - 10 - training_embed.py - training - Epoch: [4][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.6208 (306.8389)	
2019-01-08 14:51:47,422 - 10 - training_embed.py - training - Epoch: [4][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7058 (306.7887)	
2019-01-08 14:51:47,639 - 10 - training_embed.py - training - Epoch: [4][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5925 (306.7800)	
2019-01-08 14:51:47,847 - 10 - training_embed.py - training - Epoch: [4][1200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.4879 (306.7934)	
2019-01-08 14:51:48,069 - 10 - training_embed.py - training - Epoch: [4][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0187 (306.8007)	
2019-01-08 14:51:48,279 - 10 - training_embed.py - training - Epoch: [4][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4725 (306.7791)	
2019-01-08 14:51:48,506 - 10 - training_embed.py - training - Epoch: [4][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5751 (306.7811)	
2019-01-08 14:51:48,733 - 10 - training_embed.py - training - Epoch: [4][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7729 (306.7714)	
2019-01-08 14:51:48,955 - 10 - training_embed.py - training - Epoch: [4][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5444 (306.7578)	
2019-01-08 14:51:49,172 - 10 - training_embed.py - training - Epoch: [4][1800/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.5846 (306.7394)	
2019-01-08 14:51:49,380 - 10 - training_embed.py - training - Epoch: [4][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6562 (306.7374)	
2019-01-08 14:51:49,597 - 10 - training_embed.py - training - Epoch: [4][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.9506 (306.7229)	
2019-01-08 14:51:49,805 - 10 - training_embed.py - training - Epoch: [4][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.2170 (306.7149)	
2019-01-08 14:51:49,985 - 10 - training_embed.py - training - loss: 306.686955
2019-01-08 14:51:49,986 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 14:51:50,398 - 10 - training_embed.py - training - Epoch: [5][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1044 (306.9511)	
2019-01-08 14:51:50,616 - 10 - training_embed.py - training - Epoch: [5][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3616 (306.7711)	
2019-01-08 14:51:50,838 - 10 - training_embed.py - training - Epoch: [5][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.2170 (306.6426)	
2019-01-08 14:51:51,044 - 10 - training_embed.py - training - Epoch: [5][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1877 (306.6299)	
2019-01-08 14:51:51,267 - 10 - training_embed.py - training - Epoch: [5][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2186 (306.6998)	
2019-01-08 14:51:51,479 - 10 - training_embed.py - training - Epoch: [5][600/2184]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 308.3760 (306.7181)	
2019-01-08 14:51:51,696 - 10 - training_embed.py - training - Epoch: [5][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8284 (306.6625)	
2019-01-08 14:51:51,912 - 10 - training_embed.py - training - Epoch: [5][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8517 (306.5883)	
2019-01-08 14:51:52,119 - 10 - training_embed.py - training - Epoch: [5][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.4228 (306.5398)	
2019-01-08 14:51:52,331 - 10 - training_embed.py - training - Epoch: [5][1000/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 306.7320 (306.5144)	
2019-01-08 14:51:52,548 - 10 - training_embed.py - training - Epoch: [5][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6673 (306.5270)	
2019-01-08 14:51:52,768 - 10 - training_embed.py - training - Epoch: [5][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8622 (306.5483)	
2019-01-08 14:51:52,976 - 10 - training_embed.py - training - Epoch: [5][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0508 (306.5114)	
2019-01-08 14:51:53,188 - 10 - training_embed.py - training - Epoch: [5][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2361 (306.5332)	
2019-01-08 14:51:53,402 - 10 - training_embed.py - training - Epoch: [5][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.1755 (306.5310)	
2019-01-08 14:51:53,603 - 10 - training_embed.py - training - Epoch: [5][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5419 (306.5302)	
2019-01-08 14:51:53,824 - 10 - training_embed.py - training - Epoch: [5][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4799 (306.5271)	
2019-01-08 14:51:54,041 - 10 - training_embed.py - training - Epoch: [5][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.5770 (306.5415)	
2019-01-08 14:51:54,252 - 10 - training_embed.py - training - Epoch: [5][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.2244 (306.5215)	
2019-01-08 14:51:54,476 - 10 - training_embed.py - training - Epoch: [5][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2846 (306.5011)	
2019-01-08 14:51:54,693 - 10 - training_embed.py - training - Epoch: [5][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2782 (306.4923)	
2019-01-08 14:51:54,873 - 10 - training_embed.py - training - loss: 306.461622
2019-01-08 14:51:54,874 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 14:51:55,278 - 10 - training_embed.py - training - Epoch: [6][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.4295 (306.0449)	
2019-01-08 14:51:55,502 - 10 - training_embed.py - training - Epoch: [6][200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.8029 (306.4124)	
2019-01-08 14:51:55,722 - 10 - training_embed.py - training - Epoch: [6][300/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 308.8337 (306.3269)	
2019-01-08 14:51:55,946 - 10 - training_embed.py - training - Epoch: [6][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.5599 (306.2861)	
2019-01-08 14:51:56,179 - 10 - training_embed.py - training - Epoch: [6][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8052 (306.2547)	
2019-01-08 14:51:56,414 - 10 - training_embed.py - training - Epoch: [6][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3152 (306.2970)	
2019-01-08 14:51:56,636 - 10 - training_embed.py - training - Epoch: [6][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4725 (306.3003)	
2019-01-08 14:51:56,848 - 10 - training_embed.py - training - Epoch: [6][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.2657 (306.2735)	
2019-01-08 14:51:57,058 - 10 - training_embed.py - training - Epoch: [6][900/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 306.6818 (306.2899)	
2019-01-08 14:51:57,304 - 10 - training_embed.py - training - Epoch: [6][1000/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 301.4825 (306.2829)	
2019-01-08 14:51:57,533 - 10 - training_embed.py - training - Epoch: [6][1100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.7635 (306.2900)	
2019-01-08 14:51:57,753 - 10 - training_embed.py - training - Epoch: [6][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6169 (306.2816)	
2019-01-08 14:51:57,969 - 10 - training_embed.py - training - Epoch: [6][1300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.0189 (306.3117)	
2019-01-08 14:51:58,190 - 10 - training_embed.py - training - Epoch: [6][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1052 (306.2918)	
2019-01-08 14:51:58,420 - 10 - training_embed.py - training - Epoch: [6][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1970 (306.2982)	
2019-01-08 14:51:58,633 - 10 - training_embed.py - training - Epoch: [6][1600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.2939 (306.2822)	
2019-01-08 14:51:58,845 - 10 - training_embed.py - training - Epoch: [6][1700/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.2761 (306.2955)	
2019-01-08 14:51:59,044 - 10 - training_embed.py - training - Epoch: [6][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4573 (306.2968)	
2019-01-08 14:51:59,253 - 10 - training_embed.py - training - Epoch: [6][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5257 (306.2900)	
2019-01-08 14:51:59,470 - 10 - training_embed.py - training - Epoch: [6][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.2566 (306.2984)	
2019-01-08 14:51:59,682 - 10 - training_embed.py - training - Epoch: [6][2100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 300.3247 (306.2835)	
2019-01-08 14:51:59,876 - 10 - training_embed.py - training - loss: 306.235862
2019-01-08 14:51:59,876 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 14:52:00,365 - 10 - training_embed.py - training - Epoch: [7][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6880 (305.9729)	
2019-01-08 14:52:00,598 - 10 - training_embed.py - training - Epoch: [7][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 301.4078 (305.9951)	
2019-01-08 14:52:00,834 - 10 - training_embed.py - training - Epoch: [7][300/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 308.8771 (306.0887)	
2019-01-08 14:52:01,054 - 10 - training_embed.py - training - Epoch: [7][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6204 (306.1079)	
2019-01-08 14:52:01,305 - 10 - training_embed.py - training - Epoch: [7][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.1427 (306.1912)	
2019-01-08 14:52:01,588 - 10 - training_embed.py - training - Epoch: [7][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1394 (306.1474)	
2019-01-08 14:52:01,821 - 10 - training_embed.py - training - Epoch: [7][700/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 308.3058 (306.1827)	
2019-01-08 14:52:02,093 - 10 - training_embed.py - training - Epoch: [7][800/2184]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 303.9519 (306.2208)	
2019-01-08 14:52:02,325 - 10 - training_embed.py - training - Epoch: [7][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.7450 (306.2000)	
2019-01-08 14:52:02,547 - 10 - training_embed.py - training - Epoch: [7][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6277 (306.1510)	
2019-01-08 14:52:02,779 - 10 - training_embed.py - training - Epoch: [7][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9309 (306.1429)	
2019-01-08 14:52:02,984 - 10 - training_embed.py - training - Epoch: [7][1200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.9597 (306.1273)	
2019-01-08 14:52:03,206 - 10 - training_embed.py - training - Epoch: [7][1300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 311.6091 (306.1300)	
2019-01-08 14:52:03,430 - 10 - training_embed.py - training - Epoch: [7][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2879 (306.0955)	
2019-01-08 14:52:03,662 - 10 - training_embed.py - training - Epoch: [7][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8620 (306.0994)	
2019-01-08 14:52:03,897 - 10 - training_embed.py - training - Epoch: [7][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.2457 (306.1023)	
2019-01-08 14:52:04,126 - 10 - training_embed.py - training - Epoch: [7][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9050 (306.0919)	
2019-01-08 14:52:04,347 - 10 - training_embed.py - training - Epoch: [7][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9676 (306.0798)	
2019-01-08 14:52:04,556 - 10 - training_embed.py - training - Epoch: [7][1900/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 303.7755 (306.0714)	
2019-01-08 14:52:04,774 - 10 - training_embed.py - training - Epoch: [7][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.7983 (306.0692)	
2019-01-08 14:52:04,990 - 10 - training_embed.py - training - Epoch: [7][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6252 (306.0560)	
2019-01-08 14:52:05,163 - 10 - training_embed.py - training - loss: 306.010455
2019-01-08 14:52:05,163 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 14:52:05,586 - 10 - training_embed.py - training - Epoch: [8][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5685 (306.2873)	
2019-01-08 14:52:05,803 - 10 - training_embed.py - training - Epoch: [8][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0117 (306.0989)	
2019-01-08 14:52:06,011 - 10 - training_embed.py - training - Epoch: [8][300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.1884 (305.9273)	
2019-01-08 14:52:06,239 - 10 - training_embed.py - training - Epoch: [8][400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.9150 (305.8887)	
2019-01-08 14:52:06,458 - 10 - training_embed.py - training - Epoch: [8][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1350 (305.8779)	
2019-01-08 14:52:06,693 - 10 - training_embed.py - training - Epoch: [8][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4003 (305.8643)	
2019-01-08 14:52:06,915 - 10 - training_embed.py - training - Epoch: [8][700/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.0149 (305.8362)	
2019-01-08 14:52:07,139 - 10 - training_embed.py - training - Epoch: [8][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8916 (305.8076)	
2019-01-08 14:52:07,359 - 10 - training_embed.py - training - Epoch: [8][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 302.5732 (305.8308)	
2019-01-08 14:52:07,589 - 10 - training_embed.py - training - Epoch: [8][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5794 (305.8077)	
2019-01-08 14:52:07,826 - 10 - training_embed.py - training - Epoch: [8][1100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.3703 (305.7987)	
2019-01-08 14:52:08,063 - 10 - training_embed.py - training - Epoch: [8][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2092 (305.8218)	
2019-01-08 14:52:08,282 - 10 - training_embed.py - training - Epoch: [8][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5964 (305.8286)	
2019-01-08 14:52:08,505 - 10 - training_embed.py - training - Epoch: [8][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.5362 (305.8282)	
2019-01-08 14:52:08,746 - 10 - training_embed.py - training - Epoch: [8][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.5049 (305.7898)	
2019-01-08 14:52:08,973 - 10 - training_embed.py - training - Epoch: [8][1600/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.2744 (305.7786)	
2019-01-08 14:52:09,193 - 10 - training_embed.py - training - Epoch: [8][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8895 (305.7789)	
2019-01-08 14:52:09,395 - 10 - training_embed.py - training - Epoch: [8][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.1843 (305.7860)	
2019-01-08 14:52:09,618 - 10 - training_embed.py - training - Epoch: [8][1900/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 306.2943 (305.8055)	
2019-01-08 14:52:09,836 - 10 - training_embed.py - training - Epoch: [8][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9542 (305.8125)	
2019-01-08 14:52:10,053 - 10 - training_embed.py - training - Epoch: [8][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3734 (305.8122)	
2019-01-08 14:52:10,231 - 10 - training_embed.py - training - loss: 305.786276
2019-01-08 14:52:10,231 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 14:52:10,691 - 10 - training_embed.py - training - Epoch: [9][100/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 306.0215 (305.4017)	
2019-01-08 14:52:10,906 - 10 - training_embed.py - training - Epoch: [9][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.7999 (305.5401)	
2019-01-08 14:52:11,174 - 10 - training_embed.py - training - Epoch: [9][300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.6766 (305.6524)	
2019-01-08 14:52:11,413 - 10 - training_embed.py - training - Epoch: [9][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4148 (305.6591)	
2019-01-08 14:52:11,654 - 10 - training_embed.py - training - Epoch: [9][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7061 (305.7848)	
2019-01-08 14:52:11,877 - 10 - training_embed.py - training - Epoch: [9][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4115 (305.7462)	
2019-01-08 14:52:12,103 - 10 - training_embed.py - training - Epoch: [9][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8005 (305.7307)	
2019-01-08 14:52:12,390 - 10 - training_embed.py - training - Epoch: [9][800/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.2886 (305.7233)	
2019-01-08 14:52:12,610 - 10 - training_embed.py - training - Epoch: [9][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7514 (305.7077)	
2019-01-08 14:52:12,880 - 10 - training_embed.py - training - Epoch: [9][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8253 (305.7218)	
2019-01-08 14:52:13,114 - 10 - training_embed.py - training - Epoch: [9][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1288 (305.6988)	
2019-01-08 14:52:13,354 - 10 - training_embed.py - training - Epoch: [9][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.9855 (305.6535)	
2019-01-08 14:52:13,569 - 10 - training_embed.py - training - Epoch: [9][1300/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 306.0646 (305.6283)	
2019-01-08 14:52:13,782 - 10 - training_embed.py - training - Epoch: [9][1400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 303.0786 (305.6056)	
2019-01-08 14:52:14,006 - 10 - training_embed.py - training - Epoch: [9][1500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.0208 (305.5948)	
2019-01-08 14:52:14,245 - 10 - training_embed.py - training - Epoch: [9][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3760 (305.6126)	
2019-01-08 14:52:14,499 - 10 - training_embed.py - training - Epoch: [9][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.9189 (305.6083)	
2019-01-08 14:52:14,731 - 10 - training_embed.py - training - Epoch: [9][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2058 (305.6166)	
2019-01-08 14:52:14,953 - 10 - training_embed.py - training - Epoch: [9][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.4111 (305.5905)	
2019-01-08 14:52:15,182 - 10 - training_embed.py - training - Epoch: [9][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5206 (305.5842)	
2019-01-08 14:52:15,415 - 10 - training_embed.py - training - Epoch: [9][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.0190 (305.5701)	
2019-01-08 14:52:15,614 - 10 - training_embed.py - training - loss: 305.561267
2019-01-08 14:52:15,614 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 14:52:16,079 - 10 - training_embed.py - training - Epoch: [10][100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.6289 (305.1637)	
2019-01-08 14:52:16,318 - 10 - training_embed.py - training - Epoch: [10][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.7333 (305.2233)	
2019-01-08 14:52:16,556 - 10 - training_embed.py - training - Epoch: [10][300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.4503 (305.2764)	
2019-01-08 14:52:16,784 - 10 - training_embed.py - training - Epoch: [10][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9697 (305.3525)	
2019-01-08 14:52:17,014 - 10 - training_embed.py - training - Epoch: [10][500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 300.8196 (305.3432)	
2019-01-08 14:52:17,235 - 10 - training_embed.py - training - Epoch: [10][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0412 (305.3878)	
2019-01-08 14:52:17,443 - 10 - training_embed.py - training - Epoch: [10][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.4842 (305.4208)	
2019-01-08 14:52:17,671 - 10 - training_embed.py - training - Epoch: [10][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.2834 (305.4721)	
2019-01-08 14:52:17,896 - 10 - training_embed.py - training - Epoch: [10][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.7300 (305.4043)	
2019-01-08 14:52:18,118 - 10 - training_embed.py - training - Epoch: [10][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6143 (305.3941)	
2019-01-08 14:52:18,338 - 10 - training_embed.py - training - Epoch: [10][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5905 (305.3861)	
2019-01-08 14:52:18,562 - 10 - training_embed.py - training - Epoch: [10][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5752 (305.3952)	
2019-01-08 14:52:18,778 - 10 - training_embed.py - training - Epoch: [10][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1710 (305.4089)	
2019-01-08 14:52:19,004 - 10 - training_embed.py - training - Epoch: [10][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.5638 (305.4334)	
2019-01-08 14:52:19,241 - 10 - training_embed.py - training - Epoch: [10][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6120 (305.4123)	
2019-01-08 14:52:19,466 - 10 - training_embed.py - training - Epoch: [10][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.4572 (305.3990)	
2019-01-08 14:52:19,710 - 10 - training_embed.py - training - Epoch: [10][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.3175 (305.3987)	
2019-01-08 14:52:19,945 - 10 - training_embed.py - training - Epoch: [10][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.2599 (305.3918)	
2019-01-08 14:52:20,173 - 10 - training_embed.py - training - Epoch: [10][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6525 (305.3848)	
2019-01-08 14:52:20,385 - 10 - training_embed.py - training - Epoch: [10][2000/2184]	Time 0.007 (0.002)	Data 0.001 (0.001)	Loss 306.9685 (305.3778)	
2019-01-08 14:52:20,620 - 10 - training_embed.py - training - Epoch: [10][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.9378 (305.3597)	
2019-01-08 14:52:20,816 - 10 - training_embed.py - training - loss: 305.335505
2019-01-08 14:52:20,816 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 14:52:21,282 - 10 - training_embed.py - training - Epoch: [11][100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.6314 (305.1354)	
2019-01-08 14:52:21,504 - 10 - training_embed.py - training - Epoch: [11][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.3186 (305.1984)	
2019-01-08 14:52:21,724 - 10 - training_embed.py - training - Epoch: [11][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5509 (305.1261)	
2019-01-08 14:52:21,957 - 10 - training_embed.py - training - Epoch: [11][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8180 (305.0916)	
2019-01-08 14:52:22,217 - 10 - training_embed.py - training - Epoch: [11][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 302.2280 (305.1888)	
2019-01-08 14:52:22,447 - 10 - training_embed.py - training - Epoch: [11][600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.7729 (305.1681)	
2019-01-08 14:52:22,684 - 10 - training_embed.py - training - Epoch: [11][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1834 (305.1690)	
2019-01-08 14:52:22,908 - 10 - training_embed.py - training - Epoch: [11][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.2787 (305.1798)	
2019-01-08 14:52:23,142 - 10 - training_embed.py - training - Epoch: [11][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 301.7259 (305.1735)	
2019-01-08 14:52:23,379 - 10 - training_embed.py - training - Epoch: [11][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7871 (305.1542)	
2019-01-08 14:52:23,625 - 10 - training_embed.py - training - Epoch: [11][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7850 (305.1879)	
2019-01-08 14:52:23,868 - 10 - training_embed.py - training - Epoch: [11][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3709 (305.2045)	
2019-01-08 14:52:24,087 - 10 - training_embed.py - training - Epoch: [11][1300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.8839 (305.2000)	
2019-01-08 14:52:24,319 - 10 - training_embed.py - training - Epoch: [11][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.1160 (305.1973)	
2019-01-08 14:52:24,553 - 10 - training_embed.py - training - Epoch: [11][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.3557 (305.1828)	
2019-01-08 14:52:24,775 - 10 - training_embed.py - training - Epoch: [11][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.2824 (305.1724)	
2019-01-08 14:52:25,015 - 10 - training_embed.py - training - Epoch: [11][1700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.0128 (305.1785)	
2019-01-08 14:52:25,255 - 10 - training_embed.py - training - Epoch: [11][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8309 (305.1862)	
2019-01-08 14:52:25,479 - 10 - training_embed.py - training - Epoch: [11][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4718 (305.1615)	
2019-01-08 14:52:25,709 - 10 - training_embed.py - training - Epoch: [11][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.6030 (305.1475)	
2019-01-08 14:52:25,940 - 10 - training_embed.py - training - Epoch: [11][2100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.2963 (305.1560)	
2019-01-08 14:52:26,120 - 10 - training_embed.py - training - loss: 305.111167
2019-01-08 14:52:26,201 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 14:52:27,157 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 955.101 ms ~ 0.016 min ~ 0.955 sec
2019-01-08 14:52:27,890 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1688.251 ms ~ 0.028 min ~ 1.688 sec
2019-01-08 14:52:27,890 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 14:52:27,890 - 10 - corpus.py - subactivity_sampler - 0 / 163
2019-01-08 14:52:27,891 - 10 - corpus.py - subactivity_sampler - [ 69435.  89796.  75085.  77642.  73023. 108468.  65479.]
2019-01-08 14:53:42,843 - 10 - corpus.py - subactivity_sampler - 20 / 163
2019-01-08 14:53:42,843 - 10 - corpus.py - subactivity_sampler - [ 69415.  89666.  75342.  77191.  72531. 109864.  64919.]
2019-01-08 14:55:03,091 - 10 - corpus.py - subactivity_sampler - 40 / 163
2019-01-08 14:55:03,091 - 10 - corpus.py - subactivity_sampler - [ 69350.  89506.  75597.  75789.  72517. 111536.  64633.]
2019-01-08 14:56:16,191 - 10 - corpus.py - subactivity_sampler - 60 / 163
2019-01-08 14:56:16,192 - 10 - corpus.py - subactivity_sampler - [ 69233.  89786.  75777.  74967.  72594. 113797.  62774.]
2019-01-08 14:57:25,392 - 10 - corpus.py - subactivity_sampler - 80 / 163
2019-01-08 14:57:25,392 - 10 - corpus.py - subactivity_sampler - [ 69047.  89893.  76337.  73908.  71914. 115672.  62157.]
2019-01-08 14:58:37,014 - 10 - corpus.py - subactivity_sampler - 100 / 163
2019-01-08 14:58:37,015 - 10 - corpus.py - subactivity_sampler - [ 69096.  89911.  76827.  72388.  71460. 117977.  61269.]
2019-01-08 14:59:28,651 - 10 - corpus.py - subactivity_sampler - 120 / 163
2019-01-08 14:59:28,651 - 10 - corpus.py - subactivity_sampler - [ 69073.  89826.  76968.  71892.  71535. 118727.  60907.]
2019-01-08 15:00:29,828 - 10 - corpus.py - subactivity_sampler - 140 / 163
2019-01-08 15:00:29,829 - 10 - corpus.py - subactivity_sampler - [ 68932.  89955.  77847.  70769.  71801. 119637.  59987.]
2019-01-08 15:01:36,693 - 10 - corpus.py - subactivity_sampler - 160 / 163
2019-01-08 15:01:36,694 - 10 - corpus.py - subactivity_sampler - [ 68770.  89911.  78521.  69047.  72158. 120577.  59944.]
2019-01-08 15:01:50,863 - 10 - corpus.py - subactivity_sampler - [ 68632.  89813.  78821.  68688.  72148. 121134.  59692.]
2019-01-08 15:01:50,863 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 562973.446 ms ~ 9.383 min ~ 562.973 sec
2019-01-08 15:01:50,863 - 10 - corpus.py - ordering_sampler - .
2019-01-08 15:01:53,278 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 15:01:53,278 - 10 - corpus.py - ordering_sampler - inv_count_vec: [15. 51. 13.  3. 61. 15.]
2019-01-08 15:01:53,278 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 15:01:53,295 - 10 - corpus.py - rho_sampling - ['49.1829', '11.6779', '37.2100', '185.6735', '3.6679', '11.8239']
2019-01-08 15:01:53,295 - 10 - pipeline.py - baseline - Iteration 4
2019-01-08 15:01:53,457 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 15:01:53,466 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 15:01:53,467 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 14', '1: 35', '2: 18', '3: 4', '4: 34', '5: 32', '6: 33']
2019-01-08 15:01:53,486 - 10 - accuracy_class.py - mof_val - frames true: 144045	frames overall : 558928
2019-01-08 15:01:53,486 - 10 - corpus.py - accuracy_corpus - Action: salat
2019-01-08 15:01:53,486 - 10 - corpus.py - accuracy_corpus - MoF val: 0.25771655740989896
2019-01-08 15:01:53,486 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.25771655740989896
2019-01-08 15:01:53,486 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 23956
2019-01-08 15:01:53,486 - 10 - accuracy_class.py - mof_classes - label 4: 0.115724  906 / 7829
2019-01-08 15:01:53,486 - 10 - accuracy_class.py - mof_classes - label 14: 0.985228  5936 / 6025
2019-01-08 15:01:53,486 - 10 - accuracy_class.py - mof_classes - label 18: 0.142037  371 / 2612
2019-01-08 15:01:53,486 - 10 - accuracy_class.py - mof_classes - label 32: 0.262147  80350 / 306508
2019-01-08 15:01:53,486 - 10 - accuracy_class.py - mof_classes - label 33: 0.347619  14787 / 42538
2019-01-08 15:01:53,486 - 10 - accuracy_class.py - mof_classes - label 34: 0.254619  41615 / 163440
2019-01-08 15:01:53,486 - 10 - accuracy_class.py - mof_classes - label 35: 0.013289  80 / 6020
2019-01-08 15:01:53,486 - 10 - accuracy_class.py - mof_classes - average class mof: 0.265083
2019-01-08 15:01:53,486 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 23956
2019-01-08 15:01:53,487 - 10 - accuracy_class.py - iou_classes - label 4: 0.011982  906 / 75611
2019-01-08 15:01:53,487 - 10 - accuracy_class.py - iou_classes - label 14: 0.086378  5936 / 68721
2019-01-08 15:01:53,487 - 10 - accuracy_class.py - iou_classes - label 18: 0.004577  371 / 81062
2019-01-08 15:01:53,487 - 10 - accuracy_class.py - iou_classes - label 32: 0.231362  80350 / 347292
2019-01-08 15:01:53,487 - 10 - accuracy_class.py - iou_classes - label 33: 0.169104  14787 / 87443
2019-01-08 15:01:53,487 - 10 - accuracy_class.py - iou_classes - label 34: 0.214540  41615 / 193973
2019-01-08 15:01:53,487 - 10 - accuracy_class.py - iou_classes - label 35: 0.000835  80 / 95753
2019-01-08 15:01:53,487 - 10 - accuracy_class.py - iou_classes - average IoU: 0.102683
2019-01-08 15:01:53,487 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.089847
2019-01-08 15:02:02,980 - 10 - f1_score.py - f1 - f1 score: 0.296193
2019-01-08 15:02:03,002 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 9707.254 ms ~ 0.162 min ~ 9.707 sec
2019-01-08 15:02:03,003 - 10 - corpus.py - embedding_training - .
2019-01-08 15:02:03,003 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 15:02:03,004 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 15:02:03,004 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 15:02:06,864 - 10 - training_embed.py - training - create model
2019-01-08 15:02:06,865 - 10 - training_embed.py - training - epochs: 12
2019-01-08 15:02:06,865 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 15:02:07,238 - 10 - training_embed.py - training - Epoch: [0][100/2184]	Time 0.002 (0.004)	Data 0.001 (0.003)	Loss 308.8139 (308.5615)	
2019-01-08 15:02:07,446 - 10 - training_embed.py - training - Epoch: [0][200/2184]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 306.8544 (308.4954)	
2019-01-08 15:02:07,654 - 10 - training_embed.py - training - Epoch: [0][300/2184]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 305.5346 (308.4236)	
2019-01-08 15:02:07,876 - 10 - training_embed.py - training - Epoch: [0][400/2184]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.6590 (308.4364)	
2019-01-08 15:02:08,098 - 10 - training_embed.py - training - Epoch: [0][500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.8448 (308.3646)	
2019-01-08 15:02:08,318 - 10 - training_embed.py - training - Epoch: [0][600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 309.5085 (308.3999)	
2019-01-08 15:02:08,526 - 10 - training_embed.py - training - Epoch: [0][700/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.8454 (308.4681)	
2019-01-08 15:02:08,744 - 10 - training_embed.py - training - Epoch: [0][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.4643 (308.4524)	
2019-01-08 15:02:08,959 - 10 - training_embed.py - training - Epoch: [0][900/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 310.4133 (308.4769)	
2019-01-08 15:02:09,165 - 10 - training_embed.py - training - Epoch: [0][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.8255 (308.4731)	
2019-01-08 15:02:09,388 - 10 - training_embed.py - training - Epoch: [0][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6826 (308.4610)	
2019-01-08 15:02:09,591 - 10 - training_embed.py - training - Epoch: [0][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8029 (308.4567)	
2019-01-08 15:02:09,809 - 10 - training_embed.py - training - Epoch: [0][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8793 (308.4549)	
2019-01-08 15:02:10,026 - 10 - training_embed.py - training - Epoch: [0][1400/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 307.4736 (308.4516)	
2019-01-08 15:02:10,238 - 10 - training_embed.py - training - Epoch: [0][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.9323 (308.4546)	
2019-01-08 15:02:10,455 - 10 - training_embed.py - training - Epoch: [0][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.0155 (308.4580)	
2019-01-08 15:02:10,678 - 10 - training_embed.py - training - Epoch: [0][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8799 (308.4633)	
2019-01-08 15:02:10,913 - 10 - training_embed.py - training - Epoch: [0][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.3117 (308.4645)	
2019-01-08 15:02:11,131 - 10 - training_embed.py - training - Epoch: [0][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2272 (308.4624)	
2019-01-08 15:02:11,352 - 10 - training_embed.py - training - Epoch: [0][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.3535 (308.4609)	
2019-01-08 15:02:11,576 - 10 - training_embed.py - training - Epoch: [0][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3306 (308.4366)	
2019-01-08 15:02:11,761 - 10 - training_embed.py - training - loss: 308.388983
2019-01-08 15:02:11,762 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 15:02:12,187 - 10 - training_embed.py - training - Epoch: [1][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7910 (308.3702)	
2019-01-08 15:02:12,393 - 10 - training_embed.py - training - Epoch: [1][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 311.7526 (308.1590)	
2019-01-08 15:02:12,615 - 10 - training_embed.py - training - Epoch: [1][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0098 (308.2232)	
2019-01-08 15:02:12,835 - 10 - training_embed.py - training - Epoch: [1][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.9893 (308.1911)	
2019-01-08 15:02:13,051 - 10 - training_embed.py - training - Epoch: [1][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9817 (308.2157)	
2019-01-08 15:02:13,274 - 10 - training_embed.py - training - Epoch: [1][600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.8611 (308.2208)	
2019-01-08 15:02:13,505 - 10 - training_embed.py - training - Epoch: [1][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3875 (308.2202)	
2019-01-08 15:02:13,738 - 10 - training_embed.py - training - Epoch: [1][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2603 (308.2161)	
2019-01-08 15:02:13,947 - 10 - training_embed.py - training - Epoch: [1][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4145 (308.2117)	
2019-01-08 15:02:14,168 - 10 - training_embed.py - training - Epoch: [1][1000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 309.5774 (308.2283)	
2019-01-08 15:02:14,408 - 10 - training_embed.py - training - Epoch: [1][1100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 311.1570 (308.2079)	
2019-01-08 15:02:14,632 - 10 - training_embed.py - training - Epoch: [1][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 313.6529 (308.2223)	
2019-01-08 15:02:14,859 - 10 - training_embed.py - training - Epoch: [1][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8676 (308.2011)	
2019-01-08 15:02:15,082 - 10 - training_embed.py - training - Epoch: [1][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.7302 (308.1928)	
2019-01-08 15:02:15,308 - 10 - training_embed.py - training - Epoch: [1][1500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.6352 (308.1633)	
2019-01-08 15:02:15,517 - 10 - training_embed.py - training - Epoch: [1][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1421 (308.1698)	
2019-01-08 15:02:15,730 - 10 - training_embed.py - training - Epoch: [1][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2586 (308.1402)	
2019-01-08 15:02:15,948 - 10 - training_embed.py - training - Epoch: [1][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.0983 (308.1259)	
2019-01-08 15:02:16,156 - 10 - training_embed.py - training - Epoch: [1][1900/2184]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 306.4416 (308.1301)	
2019-01-08 15:02:16,379 - 10 - training_embed.py - training - Epoch: [1][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.7111 (308.1283)	
2019-01-08 15:02:16,594 - 10 - training_embed.py - training - Epoch: [1][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0710 (308.1212)	
2019-01-08 15:02:16,784 - 10 - training_embed.py - training - loss: 308.086869
2019-01-08 15:02:16,784 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 15:02:17,212 - 10 - training_embed.py - training - Epoch: [2][100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.8452 (307.9507)	
2019-01-08 15:02:17,438 - 10 - training_embed.py - training - Epoch: [2][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.2731 (308.0189)	
2019-01-08 15:02:17,648 - 10 - training_embed.py - training - Epoch: [2][300/2184]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 309.2081 (307.9461)	
2019-01-08 15:02:17,864 - 10 - training_embed.py - training - Epoch: [2][400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.8036 (307.9063)	
2019-01-08 15:02:18,067 - 10 - training_embed.py - training - Epoch: [2][500/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 308.5690 (307.8890)	
2019-01-08 15:02:18,289 - 10 - training_embed.py - training - Epoch: [2][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3755 (307.8818)	
2019-01-08 15:02:18,504 - 10 - training_embed.py - training - Epoch: [2][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.9178 (307.9011)	
2019-01-08 15:02:18,721 - 10 - training_embed.py - training - Epoch: [2][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8658 (307.8857)	
2019-01-08 15:02:18,944 - 10 - training_embed.py - training - Epoch: [2][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.3743 (307.9368)	
2019-01-08 15:02:19,161 - 10 - training_embed.py - training - Epoch: [2][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.9138 (307.9088)	
2019-01-08 15:02:19,382 - 10 - training_embed.py - training - Epoch: [2][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2255 (307.8939)	
2019-01-08 15:02:19,610 - 10 - training_embed.py - training - Epoch: [2][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2939 (307.8736)	
2019-01-08 15:02:19,825 - 10 - training_embed.py - training - Epoch: [2][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6280 (307.8875)	
2019-01-08 15:02:20,057 - 10 - training_embed.py - training - Epoch: [2][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.2858 (307.8909)	
2019-01-08 15:02:20,278 - 10 - training_embed.py - training - Epoch: [2][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.1666 (307.8906)	
2019-01-08 15:02:20,491 - 10 - training_embed.py - training - Epoch: [2][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5966 (307.8662)	
2019-01-08 15:02:20,714 - 10 - training_embed.py - training - Epoch: [2][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8438 (307.8627)	
2019-01-08 15:02:20,927 - 10 - training_embed.py - training - Epoch: [2][1800/2184]	Time 0.005 (0.002)	Data 0.002 (0.001)	Loss 308.0461 (307.8519)	
2019-01-08 15:02:21,133 - 10 - training_embed.py - training - Epoch: [2][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.2008 (307.8431)	
2019-01-08 15:02:21,349 - 10 - training_embed.py - training - Epoch: [2][2000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 311.7376 (307.8392)	
2019-01-08 15:02:21,570 - 10 - training_embed.py - training - Epoch: [2][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6499 (307.8159)	
2019-01-08 15:02:21,744 - 10 - training_embed.py - training - loss: 307.786485
2019-01-08 15:02:21,744 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 15:02:22,163 - 10 - training_embed.py - training - Epoch: [3][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1432 (307.5742)	
2019-01-08 15:02:22,371 - 10 - training_embed.py - training - Epoch: [3][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.0388 (307.5582)	
2019-01-08 15:02:22,581 - 10 - training_embed.py - training - Epoch: [3][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1484 (307.6715)	
2019-01-08 15:02:22,795 - 10 - training_embed.py - training - Epoch: [3][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3802 (307.6322)	
2019-01-08 15:02:23,006 - 10 - training_embed.py - training - Epoch: [3][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.4023 (307.6183)	
2019-01-08 15:02:23,226 - 10 - training_embed.py - training - Epoch: [3][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.1358 (307.6411)	
2019-01-08 15:02:23,456 - 10 - training_embed.py - training - Epoch: [3][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.6454 (307.5578)	
2019-01-08 15:02:23,675 - 10 - training_embed.py - training - Epoch: [3][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.2396 (307.5991)	
2019-01-08 15:02:23,882 - 10 - training_embed.py - training - Epoch: [3][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.7246 (307.5877)	
2019-01-08 15:02:24,111 - 10 - training_embed.py - training - Epoch: [3][1000/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 306.6479 (307.5659)	
2019-01-08 15:02:24,335 - 10 - training_embed.py - training - Epoch: [3][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5863 (307.5630)	
2019-01-08 15:02:24,546 - 10 - training_embed.py - training - Epoch: [3][1200/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 311.5234 (307.5599)	
2019-01-08 15:02:24,780 - 10 - training_embed.py - training - Epoch: [3][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.1773 (307.5624)	
2019-01-08 15:02:24,990 - 10 - training_embed.py - training - Epoch: [3][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3128 (307.5626)	
2019-01-08 15:02:25,211 - 10 - training_embed.py - training - Epoch: [3][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.2904 (307.5442)	
2019-01-08 15:02:25,421 - 10 - training_embed.py - training - Epoch: [3][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.1729 (307.5409)	
2019-01-08 15:02:25,646 - 10 - training_embed.py - training - Epoch: [3][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8691 (307.5418)	
2019-01-08 15:02:25,853 - 10 - training_embed.py - training - Epoch: [3][1800/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 303.4601 (307.5247)	
2019-01-08 15:02:26,066 - 10 - training_embed.py - training - Epoch: [3][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.7838 (307.5354)	
2019-01-08 15:02:26,277 - 10 - training_embed.py - training - Epoch: [3][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.6726 (307.5433)	
2019-01-08 15:02:26,485 - 10 - training_embed.py - training - Epoch: [3][2100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.8141 (307.5234)	
2019-01-08 15:02:26,663 - 10 - training_embed.py - training - loss: 307.486025
2019-01-08 15:02:26,664 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 15:02:27,102 - 10 - training_embed.py - training - Epoch: [4][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.2722 (307.3000)	
2019-01-08 15:02:27,308 - 10 - training_embed.py - training - Epoch: [4][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.0231 (307.3289)	
2019-01-08 15:02:27,511 - 10 - training_embed.py - training - Epoch: [4][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.4649 (307.4930)	
2019-01-08 15:02:27,739 - 10 - training_embed.py - training - Epoch: [4][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9120 (307.4424)	
2019-01-08 15:02:27,956 - 10 - training_embed.py - training - Epoch: [4][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4011 (307.4459)	
2019-01-08 15:02:28,164 - 10 - training_embed.py - training - Epoch: [4][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2520 (307.4196)	
2019-01-08 15:02:28,378 - 10 - training_embed.py - training - Epoch: [4][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.9419 (307.3914)	
2019-01-08 15:02:28,590 - 10 - training_embed.py - training - Epoch: [4][800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.9949 (307.3613)	
2019-01-08 15:02:28,808 - 10 - training_embed.py - training - Epoch: [4][900/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 302.0133 (307.3655)	
2019-01-08 15:02:29,031 - 10 - training_embed.py - training - Epoch: [4][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.9271 (307.3084)	
2019-01-08 15:02:29,247 - 10 - training_embed.py - training - Epoch: [4][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0194 (307.3016)	
2019-01-08 15:02:29,448 - 10 - training_embed.py - training - Epoch: [4][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8811 (307.3174)	
2019-01-08 15:02:29,664 - 10 - training_embed.py - training - Epoch: [4][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7682 (307.3132)	
2019-01-08 15:02:29,870 - 10 - training_embed.py - training - Epoch: [4][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2776 (307.2945)	
2019-01-08 15:02:30,092 - 10 - training_embed.py - training - Epoch: [4][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2946 (307.2998)	
2019-01-08 15:02:30,306 - 10 - training_embed.py - training - Epoch: [4][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.7454 (307.2815)	
2019-01-08 15:02:30,528 - 10 - training_embed.py - training - Epoch: [4][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.5378 (307.2524)	
2019-01-08 15:02:30,749 - 10 - training_embed.py - training - Epoch: [4][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.0928 (307.2418)	
2019-01-08 15:02:30,982 - 10 - training_embed.py - training - Epoch: [4][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8985 (307.2499)	
2019-01-08 15:02:31,210 - 10 - training_embed.py - training - Epoch: [4][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.0175 (307.2302)	
2019-01-08 15:02:31,436 - 10 - training_embed.py - training - Epoch: [4][2100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.3893 (307.2179)	
2019-01-08 15:02:31,614 - 10 - training_embed.py - training - loss: 307.186327
2019-01-08 15:02:31,617 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 15:02:32,044 - 10 - training_embed.py - training - Epoch: [5][100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.8163 (307.4166)	
2019-01-08 15:02:32,248 - 10 - training_embed.py - training - Epoch: [5][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.9252 (307.2609)	
2019-01-08 15:02:32,473 - 10 - training_embed.py - training - Epoch: [5][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9037 (307.0896)	
2019-01-08 15:02:32,693 - 10 - training_embed.py - training - Epoch: [5][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6428 (307.0766)	
2019-01-08 15:02:32,907 - 10 - training_embed.py - training - Epoch: [5][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.9110 (307.1601)	
2019-01-08 15:02:33,107 - 10 - training_embed.py - training - Epoch: [5][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.4400 (307.1482)	
2019-01-08 15:02:33,319 - 10 - training_embed.py - training - Epoch: [5][700/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.8752 (307.0983)	
2019-01-08 15:02:33,540 - 10 - training_embed.py - training - Epoch: [5][800/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 304.0435 (307.0270)	
2019-01-08 15:02:33,756 - 10 - training_embed.py - training - Epoch: [5][900/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 305.0905 (306.9920)	
2019-01-08 15:02:33,963 - 10 - training_embed.py - training - Epoch: [5][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8612 (306.9648)	
2019-01-08 15:02:34,178 - 10 - training_embed.py - training - Epoch: [5][1100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.6854 (306.9860)	
2019-01-08 15:02:34,394 - 10 - training_embed.py - training - Epoch: [5][1200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.1594 (306.9961)	
2019-01-08 15:02:34,611 - 10 - training_embed.py - training - Epoch: [5][1300/2184]	Time 0.008 (0.002)	Data 0.007 (0.001)	Loss 306.8453 (306.9600)	
2019-01-08 15:02:34,820 - 10 - training_embed.py - training - Epoch: [5][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2264 (306.9707)	
2019-01-08 15:02:35,028 - 10 - training_embed.py - training - Epoch: [5][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.2595 (306.9585)	
2019-01-08 15:02:35,238 - 10 - training_embed.py - training - Epoch: [5][1600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.1119 (306.9629)	
2019-01-08 15:02:35,442 - 10 - training_embed.py - training - Epoch: [5][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9938 (306.9507)	
2019-01-08 15:02:35,689 - 10 - training_embed.py - training - Epoch: [5][1800/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.1946 (306.9650)	
2019-01-08 15:02:35,897 - 10 - training_embed.py - training - Epoch: [5][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.6840 (306.9462)	
2019-01-08 15:02:36,124 - 10 - training_embed.py - training - Epoch: [5][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7501 (306.9273)	
2019-01-08 15:02:36,344 - 10 - training_embed.py - training - Epoch: [5][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8483 (306.9163)	
2019-01-08 15:02:36,531 - 10 - training_embed.py - training - loss: 306.886255
2019-01-08 15:02:36,531 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 15:02:36,949 - 10 - training_embed.py - training - Epoch: [6][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.1484 (306.5063)	
2019-01-08 15:02:37,161 - 10 - training_embed.py - training - Epoch: [6][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.5551 (306.6902)	
2019-01-08 15:02:37,377 - 10 - training_embed.py - training - Epoch: [6][300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.0240 (306.6495)	
2019-01-08 15:02:37,599 - 10 - training_embed.py - training - Epoch: [6][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.5375 (306.6701)	
2019-01-08 15:02:37,817 - 10 - training_embed.py - training - Epoch: [6][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9756 (306.6106)	
2019-01-08 15:02:38,029 - 10 - training_embed.py - training - Epoch: [6][600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.8040 (306.6486)	
2019-01-08 15:02:38,237 - 10 - training_embed.py - training - Epoch: [6][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.3429 (306.6794)	
2019-01-08 15:02:38,449 - 10 - training_embed.py - training - Epoch: [6][800/2184]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 303.9250 (306.6734)	
2019-01-08 15:02:38,666 - 10 - training_embed.py - training - Epoch: [6][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3068 (306.6804)	
2019-01-08 15:02:38,870 - 10 - training_embed.py - training - Epoch: [6][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 300.9431 (306.6863)	
2019-01-08 15:02:39,074 - 10 - training_embed.py - training - Epoch: [6][1100/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 308.3217 (306.6865)	
2019-01-08 15:02:39,287 - 10 - training_embed.py - training - Epoch: [6][1200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.5740 (306.6763)	
2019-01-08 15:02:39,498 - 10 - training_embed.py - training - Epoch: [6][1300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.7186 (306.7035)	
2019-01-08 15:02:39,712 - 10 - training_embed.py - training - Epoch: [6][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.0999 (306.6868)	
2019-01-08 15:02:39,927 - 10 - training_embed.py - training - Epoch: [6][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7940 (306.6809)	
2019-01-08 15:02:40,149 - 10 - training_embed.py - training - Epoch: [6][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7121 (306.6669)	
2019-01-08 15:02:40,356 - 10 - training_embed.py - training - Epoch: [6][1700/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.0927 (306.6704)	
2019-01-08 15:02:40,576 - 10 - training_embed.py - training - Epoch: [6][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.7166 (306.6656)	
2019-01-08 15:02:40,784 - 10 - training_embed.py - training - Epoch: [6][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5717 (306.6546)	
2019-01-08 15:02:41,000 - 10 - training_embed.py - training - Epoch: [6][2000/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.4548 (306.6487)	
2019-01-08 15:02:41,216 - 10 - training_embed.py - training - Epoch: [6][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 300.7768 (306.6383)	
2019-01-08 15:02:41,383 - 10 - training_embed.py - training - loss: 306.586702
2019-01-08 15:02:41,384 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 15:02:41,806 - 10 - training_embed.py - training - Epoch: [7][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2426 (306.3477)	
2019-01-08 15:02:42,022 - 10 - training_embed.py - training - Epoch: [7][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.1525 (306.2410)	
2019-01-08 15:02:42,229 - 10 - training_embed.py - training - Epoch: [7][300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.4899 (306.3547)	
2019-01-08 15:02:42,450 - 10 - training_embed.py - training - Epoch: [7][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6342 (306.3460)	
2019-01-08 15:02:42,680 - 10 - training_embed.py - training - Epoch: [7][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.0535 (306.4134)	
2019-01-08 15:02:42,911 - 10 - training_embed.py - training - Epoch: [7][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8715 (306.3937)	
2019-01-08 15:02:43,129 - 10 - training_embed.py - training - Epoch: [7][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6295 (306.4274)	
2019-01-08 15:02:43,341 - 10 - training_embed.py - training - Epoch: [7][800/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 305.3946 (306.4755)	
2019-01-08 15:02:43,547 - 10 - training_embed.py - training - Epoch: [7][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8251 (306.4808)	
2019-01-08 15:02:43,758 - 10 - training_embed.py - training - Epoch: [7][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8106 (306.4263)	
2019-01-08 15:02:43,981 - 10 - training_embed.py - training - Epoch: [7][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9603 (306.4068)	
2019-01-08 15:02:44,194 - 10 - training_embed.py - training - Epoch: [7][1200/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 305.7669 (306.3915)	
2019-01-08 15:02:44,412 - 10 - training_embed.py - training - Epoch: [7][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.9337 (306.4013)	
2019-01-08 15:02:44,631 - 10 - training_embed.py - training - Epoch: [7][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4386 (306.3657)	
2019-01-08 15:02:44,852 - 10 - training_embed.py - training - Epoch: [7][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.0698 (306.3705)	
2019-01-08 15:02:45,062 - 10 - training_embed.py - training - Epoch: [7][1600/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 305.6459 (306.3819)	
2019-01-08 15:02:45,276 - 10 - training_embed.py - training - Epoch: [7][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3024 (306.3677)	
2019-01-08 15:02:45,495 - 10 - training_embed.py - training - Epoch: [7][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9506 (306.3562)	
2019-01-08 15:02:45,709 - 10 - training_embed.py - training - Epoch: [7][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2339 (306.3470)	
2019-01-08 15:02:45,938 - 10 - training_embed.py - training - Epoch: [7][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9563 (306.3485)	
2019-01-08 15:02:46,156 - 10 - training_embed.py - training - Epoch: [7][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7074 (306.3365)	
2019-01-08 15:02:46,333 - 10 - training_embed.py - training - loss: 306.287161
2019-01-08 15:02:46,333 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 15:02:46,764 - 10 - training_embed.py - training - Epoch: [8][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0481 (306.4131)	
2019-01-08 15:02:46,967 - 10 - training_embed.py - training - Epoch: [8][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9103 (306.3249)	
2019-01-08 15:02:47,179 - 10 - training_embed.py - training - Epoch: [8][300/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 305.0351 (306.1942)	
2019-01-08 15:02:47,396 - 10 - training_embed.py - training - Epoch: [8][400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.6524 (306.1316)	
2019-01-08 15:02:47,628 - 10 - training_embed.py - training - Epoch: [8][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7710 (306.1489)	
2019-01-08 15:02:47,856 - 10 - training_embed.py - training - Epoch: [8][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2023 (306.1203)	
2019-01-08 15:02:48,083 - 10 - training_embed.py - training - Epoch: [8][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7572 (306.0708)	
2019-01-08 15:02:48,302 - 10 - training_embed.py - training - Epoch: [8][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5275 (306.0274)	
2019-01-08 15:02:48,506 - 10 - training_embed.py - training - Epoch: [8][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.0522 (306.0602)	
2019-01-08 15:02:48,724 - 10 - training_embed.py - training - Epoch: [8][1000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.8497 (306.0288)	
2019-01-08 15:02:48,932 - 10 - training_embed.py - training - Epoch: [8][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4396 (306.0276)	
2019-01-08 15:02:49,140 - 10 - training_embed.py - training - Epoch: [8][1200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.0571 (306.0565)	
2019-01-08 15:02:49,360 - 10 - training_embed.py - training - Epoch: [8][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5777 (306.0428)	
2019-01-08 15:02:49,580 - 10 - training_embed.py - training - Epoch: [8][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5784 (306.0469)	
2019-01-08 15:02:49,787 - 10 - training_embed.py - training - Epoch: [8][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.6340 (306.0101)	
2019-01-08 15:02:50,003 - 10 - training_embed.py - training - Epoch: [8][1600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.3626 (306.0110)	
2019-01-08 15:02:50,218 - 10 - training_embed.py - training - Epoch: [8][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1297 (306.0110)	
2019-01-08 15:02:50,419 - 10 - training_embed.py - training - Epoch: [8][1800/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.6258 (306.0120)	
2019-01-08 15:02:50,645 - 10 - training_embed.py - training - Epoch: [8][1900/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.0297 (306.0239)	
2019-01-08 15:02:50,867 - 10 - training_embed.py - training - Epoch: [8][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0521 (306.0211)	
2019-01-08 15:02:51,085 - 10 - training_embed.py - training - Epoch: [8][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.2289 (306.0179)	
2019-01-08 15:02:51,262 - 10 - training_embed.py - training - loss: 305.988770
2019-01-08 15:02:51,262 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 15:02:51,687 - 10 - training_embed.py - training - Epoch: [9][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3475 (305.5546)	
2019-01-08 15:02:51,906 - 10 - training_embed.py - training - Epoch: [9][200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.7620 (305.7430)	
2019-01-08 15:02:52,113 - 10 - training_embed.py - training - Epoch: [9][300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.2873 (305.8205)	
2019-01-08 15:02:52,328 - 10 - training_embed.py - training - Epoch: [9][400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.5964 (305.8626)	
2019-01-08 15:02:52,535 - 10 - training_embed.py - training - Epoch: [9][500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.1845 (305.9185)	
2019-01-08 15:02:52,764 - 10 - training_embed.py - training - Epoch: [9][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9012 (305.8765)	
2019-01-08 15:02:52,972 - 10 - training_embed.py - training - Epoch: [9][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.7224 (305.8809)	
2019-01-08 15:02:53,180 - 10 - training_embed.py - training - Epoch: [9][800/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 307.6328 (305.8791)	
2019-01-08 15:02:53,388 - 10 - training_embed.py - training - Epoch: [9][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4813 (305.8705)	
2019-01-08 15:02:53,604 - 10 - training_embed.py - training - Epoch: [9][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3990 (305.8721)	
2019-01-08 15:02:53,819 - 10 - training_embed.py - training - Epoch: [9][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5551 (305.8505)	
2019-01-08 15:02:54,034 - 10 - training_embed.py - training - Epoch: [9][1200/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 309.8439 (305.8022)	
2019-01-08 15:02:54,250 - 10 - training_embed.py - training - Epoch: [9][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4715 (305.7772)	
2019-01-08 15:02:54,456 - 10 - training_embed.py - training - Epoch: [9][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.5081 (305.7511)	
2019-01-08 15:02:54,670 - 10 - training_embed.py - training - Epoch: [9][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.3424 (305.7250)	
2019-01-08 15:02:54,881 - 10 - training_embed.py - training - Epoch: [9][1600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.4178 (305.7533)	
2019-01-08 15:02:55,107 - 10 - training_embed.py - training - Epoch: [9][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.2626 (305.7511)	
2019-01-08 15:02:55,332 - 10 - training_embed.py - training - Epoch: [9][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7508 (305.7510)	
2019-01-08 15:02:55,554 - 10 - training_embed.py - training - Epoch: [9][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.1097 (305.7248)	
2019-01-08 15:02:55,772 - 10 - training_embed.py - training - Epoch: [9][2000/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 304.1371 (305.7145)	
2019-01-08 15:02:55,987 - 10 - training_embed.py - training - Epoch: [9][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6881 (305.6969)	
2019-01-08 15:02:56,167 - 10 - training_embed.py - training - loss: 305.689789
2019-01-08 15:02:56,168 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 15:02:56,602 - 10 - training_embed.py - training - Epoch: [10][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8411 (305.1039)	
2019-01-08 15:02:56,823 - 10 - training_embed.py - training - Epoch: [10][200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.3405 (305.2523)	
2019-01-08 15:02:57,035 - 10 - training_embed.py - training - Epoch: [10][300/2184]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 306.0857 (305.2913)	
2019-01-08 15:02:57,251 - 10 - training_embed.py - training - Epoch: [10][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1402 (305.3935)	
2019-01-08 15:02:57,466 - 10 - training_embed.py - training - Epoch: [10][500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 301.4004 (305.4000)	
2019-01-08 15:02:57,684 - 10 - training_embed.py - training - Epoch: [10][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.1089 (305.4566)	
2019-01-08 15:02:57,896 - 10 - training_embed.py - training - Epoch: [10][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.0761 (305.4730)	
2019-01-08 15:02:58,127 - 10 - training_embed.py - training - Epoch: [10][800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.0169 (305.5392)	
2019-01-08 15:02:58,346 - 10 - training_embed.py - training - Epoch: [10][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.2895 (305.4871)	
2019-01-08 15:02:58,559 - 10 - training_embed.py - training - Epoch: [10][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.2125 (305.4730)	
2019-01-08 15:02:58,776 - 10 - training_embed.py - training - Epoch: [10][1100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.9708 (305.4603)	
2019-01-08 15:02:58,990 - 10 - training_embed.py - training - Epoch: [10][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4686 (305.4791)	
2019-01-08 15:02:59,209 - 10 - training_embed.py - training - Epoch: [10][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7050 (305.4880)	
2019-01-08 15:02:59,445 - 10 - training_embed.py - training - Epoch: [10][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.7097 (305.5105)	
2019-01-08 15:02:59,664 - 10 - training_embed.py - training - Epoch: [10][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1118 (305.4935)	
2019-01-08 15:02:59,875 - 10 - training_embed.py - training - Epoch: [10][1600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.0657 (305.4759)	
2019-01-08 15:03:00,101 - 10 - training_embed.py - training - Epoch: [10][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.3284 (305.4821)	
2019-01-08 15:03:00,332 - 10 - training_embed.py - training - Epoch: [10][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.7237 (305.4651)	
2019-01-08 15:03:00,545 - 10 - training_embed.py - training - Epoch: [10][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.0396 (305.4515)	
2019-01-08 15:03:00,774 - 10 - training_embed.py - training - Epoch: [10][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7740 (305.4338)	
2019-01-08 15:03:00,999 - 10 - training_embed.py - training - Epoch: [10][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.8475 (305.4192)	
2019-01-08 15:03:01,181 - 10 - training_embed.py - training - loss: 305.390573
2019-01-08 15:03:01,181 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 15:03:01,632 - 10 - training_embed.py - training - Epoch: [11][100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 309.0055 (305.2176)	
2019-01-08 15:03:01,851 - 10 - training_embed.py - training - Epoch: [11][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8810 (305.2152)	
2019-01-08 15:03:02,055 - 10 - training_embed.py - training - Epoch: [11][300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.4013 (305.1110)	
2019-01-08 15:03:02,270 - 10 - training_embed.py - training - Epoch: [11][400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 303.3511 (305.0750)	
2019-01-08 15:03:02,472 - 10 - training_embed.py - training - Epoch: [11][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.6432 (305.1692)	
2019-01-08 15:03:02,691 - 10 - training_embed.py - training - Epoch: [11][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0777 (305.1378)	
2019-01-08 15:03:02,906 - 10 - training_embed.py - training - Epoch: [11][700/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.8423 (305.1699)	
2019-01-08 15:03:03,126 - 10 - training_embed.py - training - Epoch: [11][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0473 (305.1802)	
2019-01-08 15:03:03,336 - 10 - training_embed.py - training - Epoch: [11][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 301.4531 (305.1794)	
2019-01-08 15:03:03,566 - 10 - training_embed.py - training - Epoch: [11][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7062 (305.1540)	
2019-01-08 15:03:03,794 - 10 - training_embed.py - training - Epoch: [11][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6982 (305.1989)	
2019-01-08 15:03:04,020 - 10 - training_embed.py - training - Epoch: [11][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1918 (305.2079)	
2019-01-08 15:03:04,256 - 10 - training_embed.py - training - Epoch: [11][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4018 (305.1908)	
2019-01-08 15:03:04,485 - 10 - training_embed.py - training - Epoch: [11][1400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.7808 (305.1868)	
2019-01-08 15:03:04,693 - 10 - training_embed.py - training - Epoch: [11][1500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 303.4231 (305.1690)	
2019-01-08 15:03:04,903 - 10 - training_embed.py - training - Epoch: [11][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.1402 (305.1592)	
2019-01-08 15:03:05,112 - 10 - training_embed.py - training - Epoch: [11][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4484 (305.1645)	
2019-01-08 15:03:05,329 - 10 - training_embed.py - training - Epoch: [11][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.8556 (305.1801)	
2019-01-08 15:03:05,547 - 10 - training_embed.py - training - Epoch: [11][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.4049 (305.1559)	
2019-01-08 15:03:05,759 - 10 - training_embed.py - training - Epoch: [11][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.3451 (305.1462)	
2019-01-08 15:03:05,984 - 10 - training_embed.py - training - Epoch: [11][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.1851 (305.1451)	
2019-01-08 15:03:06,165 - 10 - training_embed.py - training - loss: 305.092777
2019-01-08 15:03:06,251 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 15:03:07,187 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 936.254 ms ~ 0.016 min ~ 0.936 sec
2019-01-08 15:03:07,921 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1669.964 ms ~ 0.028 min ~ 1.670 sec
2019-01-08 15:03:07,921 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 15:03:07,921 - 10 - corpus.py - subactivity_sampler - 0 / 163
2019-01-08 15:03:07,922 - 10 - corpus.py - subactivity_sampler - [ 68632.  89813.  78821.  68688.  72148. 121134.  59692.]
2019-01-08 15:04:22,417 - 10 - corpus.py - subactivity_sampler - 20 / 163
2019-01-08 15:04:22,417 - 10 - corpus.py - subactivity_sampler - [ 68606.  89848.  79210.  67651.  72122. 121817.  59674.]
2019-01-08 15:05:42,207 - 10 - corpus.py - subactivity_sampler - 40 / 163
2019-01-08 15:05:42,208 - 10 - corpus.py - subactivity_sampler - [ 68356.  90012.  79429.  66483.  72229. 123111.  59308.]
2019-01-08 15:06:54,991 - 10 - corpus.py - subactivity_sampler - 60 / 163
2019-01-08 15:06:54,992 - 10 - corpus.py - subactivity_sampler - [ 68324.  88934.  79543.  65589.  73794. 123587.  59157.]
2019-01-08 15:08:03,388 - 10 - corpus.py - subactivity_sampler - 80 / 163
2019-01-08 15:08:03,388 - 10 - corpus.py - subactivity_sampler - [ 68260.  89046.  80162.  65184.  73693. 124870.  57713.]
2019-01-08 15:09:15,413 - 10 - corpus.py - subactivity_sampler - 100 / 163
2019-01-08 15:09:15,413 - 10 - corpus.py - subactivity_sampler - [ 68251.  88843.  80543.  64209.  73700. 126099.  57283.]
2019-01-08 15:10:06,035 - 10 - corpus.py - subactivity_sampler - 120 / 163
2019-01-08 15:10:06,036 - 10 - corpus.py - subactivity_sampler - [ 68158.  88846.  81740.  63042.  73532. 126616.  56994.]
2019-01-08 15:11:05,073 - 10 - corpus.py - subactivity_sampler - 140 / 163
2019-01-08 15:11:05,073 - 10 - corpus.py - subactivity_sampler - [ 68143.  89045.  82597.  62137.  73316. 127014.  56676.]
2019-01-08 15:12:10,362 - 10 - corpus.py - subactivity_sampler - 160 / 163
2019-01-08 15:12:10,362 - 10 - corpus.py - subactivity_sampler - [ 68091.  88860.  82900.  61315.  72912. 128699.  56151.]
2019-01-08 15:12:23,909 - 10 - corpus.py - subactivity_sampler - [ 68086.  88859.  83207.  61018.  72848. 128887.  56023.]
2019-01-08 15:12:23,909 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 555988.663 ms ~ 9.266 min ~ 555.989 sec
2019-01-08 15:12:23,909 - 10 - corpus.py - ordering_sampler - .
2019-01-08 15:12:26,359 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 15:12:26,360 - 10 - corpus.py - ordering_sampler - inv_count_vec: [19. 62. 24.  3. 61. 17.]
2019-01-08 15:12:26,360 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 15:12:26,383 - 10 - corpus.py - rho_sampling - ['44.5273', '11.8557', '27.9876', '284.0574', '4.2103', '9.3430']
2019-01-08 15:12:26,383 - 10 - pipeline.py - baseline - Iteration 5
2019-01-08 15:12:26,548 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 15:12:26,558 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 15:12:26,558 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 14', '1: 35', '2: 18', '3: 4', '4: 34', '5: 32', '6: 33']
2019-01-08 15:12:26,583 - 10 - accuracy_class.py - mof_val - frames true: 148843	frames overall : 558928
2019-01-08 15:12:26,583 - 10 - corpus.py - accuracy_corpus - Action: salat
2019-01-08 15:12:26,583 - 10 - corpus.py - accuracy_corpus - MoF val: 0.2663008473363295
2019-01-08 15:12:26,583 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.2663008473363295
2019-01-08 15:12:26,583 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 23956
2019-01-08 15:12:26,583 - 10 - accuracy_class.py - mof_classes - label 4: 0.126325  989 / 7829
2019-01-08 15:12:26,583 - 10 - accuracy_class.py - mof_classes - label 14: 0.984896  5934 / 6025
2019-01-08 15:12:26,583 - 10 - accuracy_class.py - mof_classes - label 18: 0.142037  371 / 2612
2019-01-08 15:12:26,583 - 10 - accuracy_class.py - mof_classes - label 32: 0.280270  85905 / 306508
2019-01-08 15:12:26,583 - 10 - accuracy_class.py - mof_classes - label 33: 0.334336  14222 / 42538
2019-01-08 15:12:26,584 - 10 - accuracy_class.py - mof_classes - label 34: 0.252949  41342 / 163440
2019-01-08 15:12:26,584 - 10 - accuracy_class.py - mof_classes - label 35: 0.013289  80 / 6020
2019-01-08 15:12:26,584 - 10 - accuracy_class.py - mof_classes - average class mof: 0.266763
2019-01-08 15:12:26,584 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 23956
2019-01-08 15:12:26,584 - 10 - accuracy_class.py - iou_classes - label 4: 0.014575  989 / 67858
2019-01-08 15:12:26,584 - 10 - accuracy_class.py - iou_classes - label 14: 0.087038  5934 / 68177
2019-01-08 15:12:26,584 - 10 - accuracy_class.py - iou_classes - label 18: 0.004342  371 / 85448
2019-01-08 15:12:26,584 - 10 - accuracy_class.py - iou_classes - label 32: 0.245801  85905 / 349490
2019-01-08 15:12:26,584 - 10 - accuracy_class.py - iou_classes - label 33: 0.168629  14222 / 84339
2019-01-08 15:12:26,584 - 10 - accuracy_class.py - iou_classes - label 34: 0.212069  41342 / 194946
2019-01-08 15:12:26,584 - 10 - accuracy_class.py - iou_classes - label 35: 0.000844  80 / 94799
2019-01-08 15:12:26,584 - 10 - accuracy_class.py - iou_classes - average IoU: 0.104757
2019-01-08 15:12:26,584 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.091662
2019-01-08 15:12:35,498 - 10 - f1_score.py - f1 - f1 score: 0.302906
2019-01-08 15:12:35,519 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 9136.082 ms ~ 0.152 min ~ 9.136 sec
2019-01-08 15:12:35,520 - 10 - corpus.py - embedding_training - .
2019-01-08 15:12:35,520 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 15:12:35,520 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 15:12:35,520 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 15:12:39,365 - 10 - training_embed.py - training - create model
2019-01-08 15:12:39,365 - 10 - training_embed.py - training - epochs: 12
2019-01-08 15:12:39,366 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 15:12:39,747 - 10 - training_embed.py - training - Epoch: [0][100/2184]	Time 0.003 (0.004)	Data 0.002 (0.003)	Loss 309.8279 (309.3234)	
2019-01-08 15:12:39,973 - 10 - training_embed.py - training - Epoch: [0][200/2184]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 307.4010 (309.2281)	
2019-01-08 15:12:40,185 - 10 - training_embed.py - training - Epoch: [0][300/2184]	Time 0.004 (0.003)	Data 0.003 (0.002)	Loss 306.9222 (309.1072)	
2019-01-08 15:12:40,403 - 10 - training_embed.py - training - Epoch: [0][400/2184]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 310.5052 (309.0821)	
2019-01-08 15:12:40,611 - 10 - training_embed.py - training - Epoch: [0][500/2184]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 309.2477 (308.9863)	
2019-01-08 15:12:40,823 - 10 - training_embed.py - training - Epoch: [0][600/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 309.6631 (309.0141)	
2019-01-08 15:12:41,038 - 10 - training_embed.py - training - Epoch: [0][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4079 (309.0609)	
2019-01-08 15:12:41,248 - 10 - training_embed.py - training - Epoch: [0][800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.7140 (309.0376)	
2019-01-08 15:12:41,463 - 10 - training_embed.py - training - Epoch: [0][900/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 311.7514 (309.0578)	
2019-01-08 15:12:41,686 - 10 - training_embed.py - training - Epoch: [0][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 313.8239 (309.0578)	
2019-01-08 15:12:41,916 - 10 - training_embed.py - training - Epoch: [0][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.4708 (309.0247)	
2019-01-08 15:12:42,127 - 10 - training_embed.py - training - Epoch: [0][1200/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 310.1320 (309.0172)	
2019-01-08 15:12:42,357 - 10 - training_embed.py - training - Epoch: [0][1300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 308.5795 (309.0021)	
2019-01-08 15:12:42,582 - 10 - training_embed.py - training - Epoch: [0][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1715 (308.9944)	
2019-01-08 15:12:42,804 - 10 - training_embed.py - training - Epoch: [0][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0615 (308.9982)	
2019-01-08 15:12:43,023 - 10 - training_embed.py - training - Epoch: [0][1600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 310.8015 (309.0076)	
2019-01-08 15:12:43,240 - 10 - training_embed.py - training - Epoch: [0][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8201 (309.0105)	
2019-01-08 15:12:43,458 - 10 - training_embed.py - training - Epoch: [0][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.6073 (309.0082)	
2019-01-08 15:12:43,679 - 10 - training_embed.py - training - Epoch: [0][1900/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 308.8274 (309.0049)	
2019-01-08 15:12:43,889 - 10 - training_embed.py - training - Epoch: [0][2000/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.5774 (309.0055)	
2019-01-08 15:12:44,100 - 10 - training_embed.py - training - Epoch: [0][2100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.6748 (308.9903)	
2019-01-08 15:12:44,277 - 10 - training_embed.py - training - loss: 308.951066
2019-01-08 15:12:44,278 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 15:12:44,716 - 10 - training_embed.py - training - Epoch: [1][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6452 (308.8100)	
2019-01-08 15:12:44,930 - 10 - training_embed.py - training - Epoch: [1][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 312.0799 (308.5452)	
2019-01-08 15:12:45,146 - 10 - training_embed.py - training - Epoch: [1][300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.3423 (308.6568)	
2019-01-08 15:12:45,358 - 10 - training_embed.py - training - Epoch: [1][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.2856 (308.6557)	
2019-01-08 15:12:45,583 - 10 - training_embed.py - training - Epoch: [1][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1660 (308.6879)	
2019-01-08 15:12:45,800 - 10 - training_embed.py - training - Epoch: [1][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3012 (308.7210)	
2019-01-08 15:12:46,017 - 10 - training_embed.py - training - Epoch: [1][700/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.9689 (308.6912)	
2019-01-08 15:12:46,237 - 10 - training_embed.py - training - Epoch: [1][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4846 (308.6886)	
2019-01-08 15:12:46,445 - 10 - training_embed.py - training - Epoch: [1][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.5758 (308.6938)	
2019-01-08 15:12:46,661 - 10 - training_embed.py - training - Epoch: [1][1000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 310.0715 (308.7139)	
2019-01-08 15:12:46,894 - 10 - training_embed.py - training - Epoch: [1][1100/2184]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 310.2380 (308.6958)	
2019-01-08 15:12:47,095 - 10 - training_embed.py - training - Epoch: [1][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 313.4004 (308.7186)	
2019-01-08 15:12:47,329 - 10 - training_embed.py - training - Epoch: [1][1300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 308.0388 (308.6915)	
2019-01-08 15:12:47,556 - 10 - training_embed.py - training - Epoch: [1][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.7407 (308.6831)	
2019-01-08 15:12:47,776 - 10 - training_embed.py - training - Epoch: [1][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5358 (308.6614)	
2019-01-08 15:12:47,987 - 10 - training_embed.py - training - Epoch: [1][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9357 (308.6759)	
2019-01-08 15:12:48,189 - 10 - training_embed.py - training - Epoch: [1][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1661 (308.6599)	
2019-01-08 15:12:48,412 - 10 - training_embed.py - training - Epoch: [1][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.7461 (308.6487)	
2019-01-08 15:12:48,633 - 10 - training_embed.py - training - Epoch: [1][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0573 (308.6486)	
2019-01-08 15:12:48,845 - 10 - training_embed.py - training - Epoch: [1][2000/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 309.9394 (308.6394)	
2019-01-08 15:12:49,057 - 10 - training_embed.py - training - Epoch: [1][2100/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.7138 (308.6323)	
2019-01-08 15:12:49,238 - 10 - training_embed.py - training - loss: 308.595170
2019-01-08 15:12:49,238 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 15:12:49,662 - 10 - training_embed.py - training - Epoch: [2][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4035 (308.3236)	
2019-01-08 15:12:49,878 - 10 - training_embed.py - training - Epoch: [2][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.0993 (308.5448)	
2019-01-08 15:12:50,102 - 10 - training_embed.py - training - Epoch: [2][300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 310.0630 (308.5161)	
2019-01-08 15:12:50,320 - 10 - training_embed.py - training - Epoch: [2][400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.6184 (308.4544)	
2019-01-08 15:12:50,538 - 10 - training_embed.py - training - Epoch: [2][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.8272 (308.4166)	
2019-01-08 15:12:50,764 - 10 - training_embed.py - training - Epoch: [2][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.4558 (308.3929)	
2019-01-08 15:12:50,969 - 10 - training_embed.py - training - Epoch: [2][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.7711 (308.3985)	
2019-01-08 15:12:51,183 - 10 - training_embed.py - training - Epoch: [2][800/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.1237 (308.3714)	
2019-01-08 15:12:51,401 - 10 - training_embed.py - training - Epoch: [2][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.8983 (308.4108)	
2019-01-08 15:12:51,626 - 10 - training_embed.py - training - Epoch: [2][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6958 (308.3797)	
2019-01-08 15:12:51,845 - 10 - training_embed.py - training - Epoch: [2][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5491 (308.3593)	
2019-01-08 15:12:52,071 - 10 - training_embed.py - training - Epoch: [2][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4941 (308.3416)	
2019-01-08 15:12:52,300 - 10 - training_embed.py - training - Epoch: [2][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6042 (308.3539)	
2019-01-08 15:12:52,517 - 10 - training_embed.py - training - Epoch: [2][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6578 (308.3670)	
2019-01-08 15:12:52,733 - 10 - training_embed.py - training - Epoch: [2][1500/2184]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 310.8123 (308.3610)	
2019-01-08 15:12:52,939 - 10 - training_embed.py - training - Epoch: [2][1600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 310.3633 (308.3389)	
2019-01-08 15:12:53,157 - 10 - training_embed.py - training - Epoch: [2][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4045 (308.3389)	
2019-01-08 15:12:53,361 - 10 - training_embed.py - training - Epoch: [2][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4449 (308.3217)	
2019-01-08 15:12:53,577 - 10 - training_embed.py - training - Epoch: [2][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4169 (308.3135)	
2019-01-08 15:12:53,798 - 10 - training_embed.py - training - Epoch: [2][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.8891 (308.3047)	
2019-01-08 15:12:54,015 - 10 - training_embed.py - training - Epoch: [2][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6211 (308.2841)	
2019-01-08 15:12:54,205 - 10 - training_embed.py - training - loss: 308.241009
2019-01-08 15:12:54,206 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 15:12:54,625 - 10 - training_embed.py - training - Epoch: [3][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6710 (308.0418)	
2019-01-08 15:12:54,840 - 10 - training_embed.py - training - Epoch: [3][200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.1783 (307.9695)	
2019-01-08 15:12:55,051 - 10 - training_embed.py - training - Epoch: [3][300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.4865 (308.0492)	
2019-01-08 15:12:55,274 - 10 - training_embed.py - training - Epoch: [3][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4212 (308.0632)	
2019-01-08 15:12:55,498 - 10 - training_embed.py - training - Epoch: [3][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 312.4417 (308.0593)	
2019-01-08 15:12:55,721 - 10 - training_embed.py - training - Epoch: [3][600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.4914 (308.0749)	
2019-01-08 15:12:55,941 - 10 - training_embed.py - training - Epoch: [3][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8121 (308.0072)	
2019-01-08 15:12:56,164 - 10 - training_embed.py - training - Epoch: [3][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4149 (308.0150)	
2019-01-08 15:12:56,383 - 10 - training_embed.py - training - Epoch: [3][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.9922 (307.9749)	
2019-01-08 15:12:56,594 - 10 - training_embed.py - training - Epoch: [3][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.7470 (307.9531)	
2019-01-08 15:12:56,810 - 10 - training_embed.py - training - Epoch: [3][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5413 (307.9495)	
2019-01-08 15:12:57,016 - 10 - training_embed.py - training - Epoch: [3][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 312.6448 (307.9642)	
2019-01-08 15:12:57,238 - 10 - training_embed.py - training - Epoch: [3][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.6989 (307.9534)	
2019-01-08 15:12:57,448 - 10 - training_embed.py - training - Epoch: [3][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.2240 (307.9638)	
2019-01-08 15:12:57,660 - 10 - training_embed.py - training - Epoch: [3][1500/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 309.7067 (307.9457)	
2019-01-08 15:12:57,879 - 10 - training_embed.py - training - Epoch: [3][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.8287 (307.9400)	
2019-01-08 15:12:58,103 - 10 - training_embed.py - training - Epoch: [3][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8008 (307.9346)	
2019-01-08 15:12:58,328 - 10 - training_embed.py - training - Epoch: [3][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8791 (307.9120)	
2019-01-08 15:12:58,539 - 10 - training_embed.py - training - Epoch: [3][1900/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 306.7315 (307.9308)	
2019-01-08 15:12:58,765 - 10 - training_embed.py - training - Epoch: [3][2000/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.4144 (307.9405)	
2019-01-08 15:12:58,982 - 10 - training_embed.py - training - Epoch: [3][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2819 (307.9201)	
2019-01-08 15:12:59,159 - 10 - training_embed.py - training - loss: 307.887131
2019-01-08 15:12:59,160 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 15:12:59,587 - 10 - training_embed.py - training - Epoch: [4][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.6110 (307.6912)	
2019-01-08 15:12:59,809 - 10 - training_embed.py - training - Epoch: [4][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7346 (307.7288)	
2019-01-08 15:13:00,037 - 10 - training_embed.py - training - Epoch: [4][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.1411 (307.8902)	
2019-01-08 15:13:00,256 - 10 - training_embed.py - training - Epoch: [4][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.8437 (307.8965)	
2019-01-08 15:13:00,461 - 10 - training_embed.py - training - Epoch: [4][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7964 (307.8865)	
2019-01-08 15:13:00,681 - 10 - training_embed.py - training - Epoch: [4][600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.7079 (307.8378)	
2019-01-08 15:13:00,891 - 10 - training_embed.py - training - Epoch: [4][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.1249 (307.8101)	
2019-01-08 15:13:01,115 - 10 - training_embed.py - training - Epoch: [4][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1666 (307.7524)	
2019-01-08 15:13:01,339 - 10 - training_embed.py - training - Epoch: [4][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.0778 (307.7489)	
2019-01-08 15:13:01,542 - 10 - training_embed.py - training - Epoch: [4][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.4518 (307.6895)	
2019-01-08 15:13:01,766 - 10 - training_embed.py - training - Epoch: [4][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5398 (307.6826)	
2019-01-08 15:13:01,978 - 10 - training_embed.py - training - Epoch: [4][1200/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.2937 (307.7065)	
2019-01-08 15:13:02,192 - 10 - training_embed.py - training - Epoch: [4][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3243 (307.6999)	
2019-01-08 15:13:02,410 - 10 - training_embed.py - training - Epoch: [4][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1491 (307.6628)	
2019-01-08 15:13:02,628 - 10 - training_embed.py - training - Epoch: [4][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8108 (307.6712)	
2019-01-08 15:13:02,843 - 10 - training_embed.py - training - Epoch: [4][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0946 (307.6594)	
2019-01-08 15:13:03,050 - 10 - training_embed.py - training - Epoch: [4][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6666 (307.6291)	
2019-01-08 15:13:03,267 - 10 - training_embed.py - training - Epoch: [4][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2388 (307.6200)	
2019-01-08 15:13:03,488 - 10 - training_embed.py - training - Epoch: [4][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.1068 (307.6263)	
2019-01-08 15:13:03,704 - 10 - training_embed.py - training - Epoch: [4][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9839 (307.6016)	
2019-01-08 15:13:03,929 - 10 - training_embed.py - training - Epoch: [4][2100/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 307.7397 (307.5810)	
2019-01-08 15:13:04,120 - 10 - training_embed.py - training - loss: 307.534194
2019-01-08 15:13:04,120 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 15:13:04,546 - 10 - training_embed.py - training - Epoch: [5][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6848 (307.7451)	
2019-01-08 15:13:04,765 - 10 - training_embed.py - training - Epoch: [5][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9284 (307.5426)	
2019-01-08 15:13:04,971 - 10 - training_embed.py - training - Epoch: [5][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0597 (307.4148)	
2019-01-08 15:13:05,183 - 10 - training_embed.py - training - Epoch: [5][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4329 (307.4273)	
2019-01-08 15:13:05,400 - 10 - training_embed.py - training - Epoch: [5][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5049 (307.5177)	
2019-01-08 15:13:05,620 - 10 - training_embed.py - training - Epoch: [5][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5500 (307.4961)	
2019-01-08 15:13:05,829 - 10 - training_embed.py - training - Epoch: [5][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.5228 (307.4180)	
2019-01-08 15:13:06,049 - 10 - training_embed.py - training - Epoch: [5][800/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 304.8953 (307.3461)	
2019-01-08 15:13:06,266 - 10 - training_embed.py - training - Epoch: [5][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.1270 (307.3076)	
2019-01-08 15:13:06,487 - 10 - training_embed.py - training - Epoch: [5][1000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.9996 (307.2731)	
2019-01-08 15:13:06,711 - 10 - training_embed.py - training - Epoch: [5][1100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.2969 (307.2817)	
2019-01-08 15:13:06,926 - 10 - training_embed.py - training - Epoch: [5][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0868 (307.2927)	
2019-01-08 15:13:07,152 - 10 - training_embed.py - training - Epoch: [5][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5135 (307.2662)	
2019-01-08 15:13:07,371 - 10 - training_embed.py - training - Epoch: [5][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1423 (307.2727)	
2019-01-08 15:13:07,577 - 10 - training_embed.py - training - Epoch: [5][1500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.6600 (307.2815)	
2019-01-08 15:13:07,806 - 10 - training_embed.py - training - Epoch: [5][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.0073 (307.2814)	
2019-01-08 15:13:08,016 - 10 - training_embed.py - training - Epoch: [5][1700/2184]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 309.0349 (307.2602)	
2019-01-08 15:13:08,230 - 10 - training_embed.py - training - Epoch: [5][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9725 (307.2627)	
2019-01-08 15:13:08,453 - 10 - training_embed.py - training - Epoch: [5][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5780 (307.2432)	
2019-01-08 15:13:08,681 - 10 - training_embed.py - training - Epoch: [5][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.1131 (307.2304)	
2019-01-08 15:13:08,897 - 10 - training_embed.py - training - Epoch: [5][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0363 (307.2179)	
2019-01-08 15:13:09,071 - 10 - training_embed.py - training - loss: 307.181827
2019-01-08 15:13:09,071 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 15:13:09,511 - 10 - training_embed.py - training - Epoch: [6][100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.1533 (306.7301)	
2019-01-08 15:13:09,721 - 10 - training_embed.py - training - Epoch: [6][200/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 306.1040 (306.9822)	
2019-01-08 15:13:09,936 - 10 - training_embed.py - training - Epoch: [6][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.1153 (306.9667)	
2019-01-08 15:13:10,142 - 10 - training_embed.py - training - Epoch: [6][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8073 (306.9718)	
2019-01-08 15:13:10,370 - 10 - training_embed.py - training - Epoch: [6][500/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.2631 (306.8938)	
2019-01-08 15:13:10,574 - 10 - training_embed.py - training - Epoch: [6][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6055 (306.9130)	
2019-01-08 15:13:10,808 - 10 - training_embed.py - training - Epoch: [6][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2860 (306.9446)	
2019-01-08 15:13:11,018 - 10 - training_embed.py - training - Epoch: [6][800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.3984 (306.9334)	
2019-01-08 15:13:11,243 - 10 - training_embed.py - training - Epoch: [6][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6288 (306.9548)	
2019-01-08 15:13:11,451 - 10 - training_embed.py - training - Epoch: [6][1000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 301.3407 (306.9433)	
2019-01-08 15:13:11,671 - 10 - training_embed.py - training - Epoch: [6][1100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.0880 (306.9425)	
2019-01-08 15:13:11,899 - 10 - training_embed.py - training - Epoch: [6][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5141 (306.9384)	
2019-01-08 15:13:12,114 - 10 - training_embed.py - training - Epoch: [6][1300/2184]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 306.0909 (306.9641)	
2019-01-08 15:13:12,345 - 10 - training_embed.py - training - Epoch: [6][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8196 (306.9346)	
2019-01-08 15:13:12,550 - 10 - training_embed.py - training - Epoch: [6][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8552 (306.9284)	
2019-01-08 15:13:12,779 - 10 - training_embed.py - training - Epoch: [6][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3954 (306.9202)	
2019-01-08 15:13:12,985 - 10 - training_embed.py - training - Epoch: [6][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5973 (306.9218)	
2019-01-08 15:13:13,205 - 10 - training_embed.py - training - Epoch: [6][1800/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.9373 (306.9115)	
2019-01-08 15:13:13,435 - 10 - training_embed.py - training - Epoch: [6][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7488 (306.8949)	
2019-01-08 15:13:13,654 - 10 - training_embed.py - training - Epoch: [6][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.1382 (306.8913)	
2019-01-08 15:13:13,871 - 10 - training_embed.py - training - Epoch: [6][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.6486 (306.8838)	
2019-01-08 15:13:14,046 - 10 - training_embed.py - training - loss: 306.829694
2019-01-08 15:13:14,046 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 15:13:14,440 - 10 - training_embed.py - training - Epoch: [7][100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.0752 (306.4604)	
2019-01-08 15:13:14,661 - 10 - training_embed.py - training - Epoch: [7][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 300.4368 (306.3601)	
2019-01-08 15:13:14,875 - 10 - training_embed.py - training - Epoch: [7][300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 311.7565 (306.5083)	
2019-01-08 15:13:15,090 - 10 - training_embed.py - training - Epoch: [7][400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.2143 (306.5320)	
2019-01-08 15:13:15,303 - 10 - training_embed.py - training - Epoch: [7][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5821 (306.6407)	
2019-01-08 15:13:15,531 - 10 - training_embed.py - training - Epoch: [7][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5234 (306.6151)	
2019-01-08 15:13:15,745 - 10 - training_embed.py - training - Epoch: [7][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.3746 (306.6424)	
2019-01-08 15:13:15,959 - 10 - training_embed.py - training - Epoch: [7][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5406 (306.7151)	
2019-01-08 15:13:16,176 - 10 - training_embed.py - training - Epoch: [7][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8715 (306.7003)	
2019-01-08 15:13:16,401 - 10 - training_embed.py - training - Epoch: [7][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.6328 (306.6206)	
2019-01-08 15:13:16,621 - 10 - training_embed.py - training - Epoch: [7][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7111 (306.6141)	
2019-01-08 15:13:16,838 - 10 - training_embed.py - training - Epoch: [7][1200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.4076 (306.6029)	
2019-01-08 15:13:17,057 - 10 - training_embed.py - training - Epoch: [7][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6776 (306.6084)	
2019-01-08 15:13:17,267 - 10 - training_embed.py - training - Epoch: [7][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.0975 (306.5845)	
2019-01-08 15:13:17,475 - 10 - training_embed.py - training - Epoch: [7][1500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.2161 (306.5876)	
2019-01-08 15:13:17,690 - 10 - training_embed.py - training - Epoch: [7][1600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.1528 (306.5933)	
2019-01-08 15:13:17,917 - 10 - training_embed.py - training - Epoch: [7][1700/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.5634 (306.5735)	
2019-01-08 15:13:18,134 - 10 - training_embed.py - training - Epoch: [7][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5656 (306.5693)	
2019-01-08 15:13:18,347 - 10 - training_embed.py - training - Epoch: [7][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6997 (306.5529)	
2019-01-08 15:13:18,579 - 10 - training_embed.py - training - Epoch: [7][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0085 (306.5445)	
2019-01-08 15:13:18,800 - 10 - training_embed.py - training - Epoch: [7][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2604 (306.5241)	
2019-01-08 15:13:18,973 - 10 - training_embed.py - training - loss: 306.477875
2019-01-08 15:13:18,973 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 15:13:19,393 - 10 - training_embed.py - training - Epoch: [8][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7084 (306.4879)	
2019-01-08 15:13:19,607 - 10 - training_embed.py - training - Epoch: [8][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6541 (306.4449)	
2019-01-08 15:13:19,830 - 10 - training_embed.py - training - Epoch: [8][300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.0403 (306.3291)	
2019-01-08 15:13:20,045 - 10 - training_embed.py - training - Epoch: [8][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2443 (306.2565)	
2019-01-08 15:13:20,252 - 10 - training_embed.py - training - Epoch: [8][500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.3293 (306.2736)	
2019-01-08 15:13:20,468 - 10 - training_embed.py - training - Epoch: [8][600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.5641 (306.2464)	
2019-01-08 15:13:20,676 - 10 - training_embed.py - training - Epoch: [8][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.8878 (306.2215)	
2019-01-08 15:13:20,911 - 10 - training_embed.py - training - Epoch: [8][800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.2246 (306.1963)	
2019-01-08 15:13:21,135 - 10 - training_embed.py - training - Epoch: [8][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3967 (306.2454)	
2019-01-08 15:13:21,350 - 10 - training_embed.py - training - Epoch: [8][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1438 (306.2139)	
2019-01-08 15:13:21,559 - 10 - training_embed.py - training - Epoch: [8][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0194 (306.2040)	
2019-01-08 15:13:21,787 - 10 - training_embed.py - training - Epoch: [8][1200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.7169 (306.2331)	
2019-01-08 15:13:22,007 - 10 - training_embed.py - training - Epoch: [8][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2595 (306.2070)	
2019-01-08 15:13:22,228 - 10 - training_embed.py - training - Epoch: [8][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6075 (306.2118)	
2019-01-08 15:13:22,456 - 10 - training_embed.py - training - Epoch: [8][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.7696 (306.1699)	
2019-01-08 15:13:22,666 - 10 - training_embed.py - training - Epoch: [8][1600/2184]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 308.4352 (306.1709)	
2019-01-08 15:13:22,883 - 10 - training_embed.py - training - Epoch: [8][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9974 (306.1644)	
2019-01-08 15:13:23,106 - 10 - training_embed.py - training - Epoch: [8][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.4345 (306.1624)	
2019-01-08 15:13:23,326 - 10 - training_embed.py - training - Epoch: [8][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.4111 (306.1661)	
2019-01-08 15:13:23,535 - 10 - training_embed.py - training - Epoch: [8][2000/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.5577 (306.1652)	
2019-01-08 15:13:23,754 - 10 - training_embed.py - training - Epoch: [8][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.4882 (306.1605)	
2019-01-08 15:13:23,931 - 10 - training_embed.py - training - loss: 306.127296
2019-01-08 15:13:23,931 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 15:13:24,354 - 10 - training_embed.py - training - Epoch: [9][100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.7613 (305.5187)	
2019-01-08 15:13:24,559 - 10 - training_embed.py - training - Epoch: [9][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.6163 (305.7826)	
2019-01-08 15:13:24,775 - 10 - training_embed.py - training - Epoch: [9][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.4930 (305.8337)	
2019-01-08 15:13:24,997 - 10 - training_embed.py - training - Epoch: [9][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1611 (305.8680)	
2019-01-08 15:13:25,214 - 10 - training_embed.py - training - Epoch: [9][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2810 (305.9069)	
2019-01-08 15:13:25,430 - 10 - training_embed.py - training - Epoch: [9][600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.5547 (305.8828)	
2019-01-08 15:13:25,632 - 10 - training_embed.py - training - Epoch: [9][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.1590 (305.8893)	
2019-01-08 15:13:25,848 - 10 - training_embed.py - training - Epoch: [9][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6436 (305.9013)	
2019-01-08 15:13:26,072 - 10 - training_embed.py - training - Epoch: [9][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5928 (305.9045)	
2019-01-08 15:13:26,300 - 10 - training_embed.py - training - Epoch: [9][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.2202 (305.9116)	
2019-01-08 15:13:26,526 - 10 - training_embed.py - training - Epoch: [9][1100/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.3400 (305.9066)	
2019-01-08 15:13:26,748 - 10 - training_embed.py - training - Epoch: [9][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.3639 (305.8567)	
2019-01-08 15:13:26,983 - 10 - training_embed.py - training - Epoch: [9][1300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.6674 (305.8321)	
2019-01-08 15:13:27,201 - 10 - training_embed.py - training - Epoch: [9][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.5220 (305.8091)	
2019-01-08 15:13:27,408 - 10 - training_embed.py - training - Epoch: [9][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.1183 (305.7982)	
2019-01-08 15:13:27,612 - 10 - training_embed.py - training - Epoch: [9][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3471 (305.8294)	
2019-01-08 15:13:27,827 - 10 - training_embed.py - training - Epoch: [9][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.5033 (305.8321)	
2019-01-08 15:13:28,049 - 10 - training_embed.py - training - Epoch: [9][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0649 (305.8372)	
2019-01-08 15:13:28,253 - 10 - training_embed.py - training - Epoch: [9][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.7659 (305.8163)	
2019-01-08 15:13:28,484 - 10 - training_embed.py - training - Epoch: [9][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3038 (305.8052)	
2019-01-08 15:13:28,707 - 10 - training_embed.py - training - Epoch: [9][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.7692 (305.7875)	
2019-01-08 15:13:28,885 - 10 - training_embed.py - training - loss: 305.776531
2019-01-08 15:13:28,885 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 15:13:29,334 - 10 - training_embed.py - training - Epoch: [10][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2816 (305.1397)	
2019-01-08 15:13:29,546 - 10 - training_embed.py - training - Epoch: [10][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6898 (305.3543)	
2019-01-08 15:13:29,759 - 10 - training_embed.py - training - Epoch: [10][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8472 (305.4160)	
2019-01-08 15:13:29,977 - 10 - training_embed.py - training - Epoch: [10][400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.3938 (305.5053)	
2019-01-08 15:13:30,195 - 10 - training_embed.py - training - Epoch: [10][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.8767 (305.4786)	
2019-01-08 15:13:30,411 - 10 - training_embed.py - training - Epoch: [10][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6489 (305.5302)	
2019-01-08 15:13:30,610 - 10 - training_embed.py - training - Epoch: [10][700/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.1982 (305.5338)	
2019-01-08 15:13:30,827 - 10 - training_embed.py - training - Epoch: [10][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.7832 (305.5990)	
2019-01-08 15:13:31,041 - 10 - training_embed.py - training - Epoch: [10][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 302.9619 (305.5567)	
2019-01-08 15:13:31,262 - 10 - training_embed.py - training - Epoch: [10][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.7401 (305.5452)	
2019-01-08 15:13:31,479 - 10 - training_embed.py - training - Epoch: [10][1100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.5184 (305.5286)	
2019-01-08 15:13:31,695 - 10 - training_embed.py - training - Epoch: [10][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0370 (305.5419)	
2019-01-08 15:13:31,909 - 10 - training_embed.py - training - Epoch: [10][1300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.5730 (305.5430)	
2019-01-08 15:13:32,126 - 10 - training_embed.py - training - Epoch: [10][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8206 (305.5577)	
2019-01-08 15:13:32,337 - 10 - training_embed.py - training - Epoch: [10][1500/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 304.6057 (305.5386)	
2019-01-08 15:13:32,552 - 10 - training_embed.py - training - Epoch: [10][1600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.7137 (305.5237)	
2019-01-08 15:13:32,778 - 10 - training_embed.py - training - Epoch: [10][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 301.8549 (305.5275)	
2019-01-08 15:13:32,993 - 10 - training_embed.py - training - Epoch: [10][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8193 (305.5017)	
2019-01-08 15:13:33,210 - 10 - training_embed.py - training - Epoch: [10][1900/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 306.5149 (305.4840)	
2019-01-08 15:13:33,428 - 10 - training_embed.py - training - Epoch: [10][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1141 (305.4693)	
2019-01-08 15:13:33,648 - 10 - training_embed.py - training - Epoch: [10][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.3702 (305.4458)	
2019-01-08 15:13:33,828 - 10 - training_embed.py - training - loss: 305.425116
2019-01-08 15:13:33,829 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 15:13:34,254 - 10 - training_embed.py - training - Epoch: [11][100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.3651 (305.3251)	
2019-01-08 15:13:34,463 - 10 - training_embed.py - training - Epoch: [11][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.9471 (305.2390)	
2019-01-08 15:13:34,693 - 10 - training_embed.py - training - Epoch: [11][300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.8170 (305.1224)	
2019-01-08 15:13:34,920 - 10 - training_embed.py - training - Epoch: [11][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.0467 (305.1049)	
2019-01-08 15:13:35,140 - 10 - training_embed.py - training - Epoch: [11][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6254 (305.2145)	
2019-01-08 15:13:35,359 - 10 - training_embed.py - training - Epoch: [11][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3472 (305.1794)	
2019-01-08 15:13:35,574 - 10 - training_embed.py - training - Epoch: [11][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0520 (305.2056)	
2019-01-08 15:13:35,787 - 10 - training_embed.py - training - Epoch: [11][800/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 304.0257 (305.1876)	
2019-01-08 15:13:36,011 - 10 - training_embed.py - training - Epoch: [11][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 300.0077 (305.1984)	
2019-01-08 15:13:36,232 - 10 - training_embed.py - training - Epoch: [11][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.0126 (305.1715)	
2019-01-08 15:13:36,445 - 10 - training_embed.py - training - Epoch: [11][1100/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.0122 (305.1980)	
2019-01-08 15:13:36,661 - 10 - training_embed.py - training - Epoch: [11][1200/2184]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 305.5688 (305.2098)	
2019-01-08 15:13:36,887 - 10 - training_embed.py - training - Epoch: [11][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9659 (305.1955)	
2019-01-08 15:13:37,114 - 10 - training_embed.py - training - Epoch: [11][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6155 (305.1912)	
2019-01-08 15:13:37,331 - 10 - training_embed.py - training - Epoch: [11][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3011 (305.1674)	
2019-01-08 15:13:37,554 - 10 - training_embed.py - training - Epoch: [11][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.3123 (305.1476)	
2019-01-08 15:13:37,772 - 10 - training_embed.py - training - Epoch: [11][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6647 (305.1448)	
2019-01-08 15:13:37,989 - 10 - training_embed.py - training - Epoch: [11][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.6546 (305.1590)	
2019-01-08 15:13:38,202 - 10 - training_embed.py - training - Epoch: [11][1900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 302.3598 (305.1386)	
2019-01-08 15:13:38,433 - 10 - training_embed.py - training - Epoch: [11][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.1646 (305.1280)	
2019-01-08 15:13:38,637 - 10 - training_embed.py - training - Epoch: [11][2100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.6183 (305.1263)	
2019-01-08 15:13:38,812 - 10 - training_embed.py - training - loss: 305.075502
2019-01-08 15:13:38,903 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 15:13:39,891 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 987.890 ms ~ 0.016 min ~ 0.988 sec
2019-01-08 15:13:40,642 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1738.985 ms ~ 0.029 min ~ 1.739 sec
2019-01-08 15:13:40,643 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 15:13:40,643 - 10 - corpus.py - subactivity_sampler - 0 / 163
2019-01-08 15:13:40,643 - 10 - corpus.py - subactivity_sampler - [ 68086.  88859.  83207.  61018.  72848. 128887.  56023.]
2019-01-08 15:14:55,625 - 10 - corpus.py - subactivity_sampler - 20 / 163
2019-01-08 15:14:55,625 - 10 - corpus.py - subactivity_sampler - [ 67931.  88998.  83359.  60398.  72677. 129762.  55803.]
2019-01-08 15:16:15,559 - 10 - corpus.py - subactivity_sampler - 40 / 163
2019-01-08 15:16:15,559 - 10 - corpus.py - subactivity_sampler - [ 68015.  89153.  83727.  59333.  72182. 131421.  55097.]
2019-01-08 15:17:28,640 - 10 - corpus.py - subactivity_sampler - 60 / 163
2019-01-08 15:17:28,641 - 10 - corpus.py - subactivity_sampler - [ 67474.  89822.  83931.  58716.  72249. 131694.  55042.]
2019-01-08 15:18:35,647 - 10 - corpus.py - subactivity_sampler - 80 / 163
2019-01-08 15:18:35,648 - 10 - corpus.py - subactivity_sampler - [ 67480.  89747.  84766.  57761.  72271. 132436.  54467.]
2019-01-08 15:19:46,226 - 10 - corpus.py - subactivity_sampler - 100 / 163
2019-01-08 15:19:46,226 - 10 - corpus.py - subactivity_sampler - [ 67276.  89890.  84943.  57191.  72138. 133745.  53745.]
2019-01-08 15:20:36,504 - 10 - corpus.py - subactivity_sampler - 120 / 163
2019-01-08 15:20:36,504 - 10 - corpus.py - subactivity_sampler - [ 66829.  89983.  85298.  57118.  72118. 134069.  53513.]
2019-01-08 15:21:36,174 - 10 - corpus.py - subactivity_sampler - 140 / 163
2019-01-08 15:21:36,174 - 10 - corpus.py - subactivity_sampler - [ 66804.  90478.  85194.  56894.  71749. 134570.  53239.]
2019-01-08 15:22:41,477 - 10 - corpus.py - subactivity_sampler - 160 / 163
2019-01-08 15:22:41,477 - 10 - corpus.py - subactivity_sampler - [ 66755.  90901.  85372.  55994.  71718. 135033.  53155.]
2019-01-08 15:22:55,028 - 10 - corpus.py - subactivity_sampler - [ 66733.  90867.  85482.  55795.  71652. 135246.  53153.]
2019-01-08 15:22:55,029 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 554386.008 ms ~ 9.240 min ~ 554.386 sec
2019-01-08 15:22:55,029 - 10 - corpus.py - ordering_sampler - .
2019-01-08 15:22:58,543 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 15:22:58,543 - 10 - corpus.py - ordering_sampler - inv_count_vec: [17. 63. 34.  3. 59. 18.]
2019-01-08 15:22:58,543 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 15:22:58,560 - 10 - corpus.py - rho_sampling - ['49.9193', '7.8282', '14.9901', '180.9795', '3.7398', '76.8826']
2019-01-08 15:22:58,560 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 15:22:58,728 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 15:22:58,738 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 15:22:58,738 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 14', '1: 34', '2: 18', '3: 4', '4: 35', '5: 32', '6: 33']
2019-01-08 15:22:58,757 - 10 - accuracy_class.py - mof_val - frames true: 153782	frames overall : 558928
2019-01-08 15:22:58,757 - 10 - corpus.py - accuracy_corpus - Action: salat
2019-01-08 15:22:58,758 - 10 - corpus.py - accuracy_corpus - MoF val: 0.2751374058912776
2019-01-08 15:22:58,758 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.27281331405834025
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 23956
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - mof_classes - label 4: 0.124792  977 / 7829
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - mof_classes - label 14: 0.984896  5934 / 6025
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - mof_classes - label 18: 0.142420  372 / 2612
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - mof_classes - label 32: 0.295620  90610 / 306508
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - mof_classes - label 33: 0.321289  13667 / 42538
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - mof_classes - label 34: 0.255140  41700 / 163440
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - mof_classes - label 35: 0.086711  522 / 6020
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - mof_classes - average class mof: 0.276359
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 23956
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - iou_classes - label 4: 0.015595  977 / 62647
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - iou_classes - label 14: 0.088800  5934 / 66824
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - iou_classes - label 18: 0.004241  372 / 87722
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - iou_classes - label 32: 0.258042  90610 / 351144
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - iou_classes - label 33: 0.166622  13667 / 82024
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - iou_classes - label 34: 0.196137  41700 / 212607
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - iou_classes - label 35: 0.006766  522 / 77150
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - iou_classes - average IoU: 0.105172
2019-01-08 15:22:58,758 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.092025
2019-01-08 15:23:07,684 - 10 - f1_score.py - f1 - f1 score: 0.307350
2019-01-08 15:23:07,705 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 9144.727 ms ~ 0.152 min ~ 9.145 sec
2019-01-08 15:23:07,705 - 10 - corpus.py - embedding_training - .
2019-01-08 15:23:07,705 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 15:23:07,705 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 15:23:07,705 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 15:23:11,569 - 10 - training_embed.py - training - create model
2019-01-08 15:23:11,569 - 10 - training_embed.py - training - epochs: 12
2019-01-08 15:23:11,570 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 15:23:11,950 - 10 - training_embed.py - training - Epoch: [0][100/2184]	Time 0.003 (0.004)	Data 0.002 (0.003)	Loss 310.6521 (309.8301)	
2019-01-08 15:23:12,177 - 10 - training_embed.py - training - Epoch: [0][200/2184]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 308.5766 (309.6709)	
2019-01-08 15:23:12,388 - 10 - training_embed.py - training - Epoch: [0][300/2184]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 306.8164 (309.6308)	
2019-01-08 15:23:12,609 - 10 - training_embed.py - training - Epoch: [0][400/2184]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 309.8845 (309.6040)	
2019-01-08 15:23:12,819 - 10 - training_embed.py - training - Epoch: [0][500/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 310.4069 (309.5366)	
2019-01-08 15:23:13,024 - 10 - training_embed.py - training - Epoch: [0][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.6717 (309.5533)	
2019-01-08 15:23:13,248 - 10 - training_embed.py - training - Epoch: [0][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4498 (309.5996)	
2019-01-08 15:23:13,455 - 10 - training_embed.py - training - Epoch: [0][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5569 (309.5718)	
2019-01-08 15:23:13,665 - 10 - training_embed.py - training - Epoch: [0][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 312.2332 (309.5878)	
2019-01-08 15:23:13,883 - 10 - training_embed.py - training - Epoch: [0][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 314.3219 (309.5859)	
2019-01-08 15:23:14,106 - 10 - training_embed.py - training - Epoch: [0][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4749 (309.5667)	
2019-01-08 15:23:14,315 - 10 - training_embed.py - training - Epoch: [0][1200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.5202 (309.5639)	
2019-01-08 15:23:14,533 - 10 - training_embed.py - training - Epoch: [0][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1952 (309.5414)	
2019-01-08 15:23:14,761 - 10 - training_embed.py - training - Epoch: [0][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.1524 (309.5308)	
2019-01-08 15:23:14,986 - 10 - training_embed.py - training - Epoch: [0][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.2733 (309.5344)	
2019-01-08 15:23:15,207 - 10 - training_embed.py - training - Epoch: [0][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.2138 (309.5355)	
2019-01-08 15:23:15,420 - 10 - training_embed.py - training - Epoch: [0][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4519 (309.5424)	
2019-01-08 15:23:15,631 - 10 - training_embed.py - training - Epoch: [0][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.1530 (309.5386)	
2019-01-08 15:23:15,849 - 10 - training_embed.py - training - Epoch: [0][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.5247 (309.5330)	
2019-01-08 15:23:16,068 - 10 - training_embed.py - training - Epoch: [0][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3735 (309.5281)	
2019-01-08 15:23:16,284 - 10 - training_embed.py - training - Epoch: [0][2100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.2011 (309.5064)	
2019-01-08 15:23:16,475 - 10 - training_embed.py - training - loss: 309.463453
2019-01-08 15:23:16,475 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 15:23:16,917 - 10 - training_embed.py - training - Epoch: [1][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6509 (309.3002)	
2019-01-08 15:23:17,135 - 10 - training_embed.py - training - Epoch: [1][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 312.3088 (309.0585)	
2019-01-08 15:23:17,345 - 10 - training_embed.py - training - Epoch: [1][300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.5449 (309.1373)	
2019-01-08 15:23:17,560 - 10 - training_embed.py - training - Epoch: [1][400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 309.1185 (309.1386)	
2019-01-08 15:23:17,785 - 10 - training_embed.py - training - Epoch: [1][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.2277 (309.1631)	
2019-01-08 15:23:18,017 - 10 - training_embed.py - training - Epoch: [1][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.4579 (309.2049)	
2019-01-08 15:23:18,275 - 10 - training_embed.py - training - Epoch: [1][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.6252 (309.1940)	
2019-01-08 15:23:18,597 - 10 - training_embed.py - training - Epoch: [1][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.6017 (309.1958)	
2019-01-08 15:23:18,905 - 10 - training_embed.py - training - Epoch: [1][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.9193 (309.2120)	
2019-01-08 15:23:19,125 - 10 - training_embed.py - training - Epoch: [1][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.0579 (309.2195)	
2019-01-08 15:23:19,354 - 10 - training_embed.py - training - Epoch: [1][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.9160 (309.2075)	
2019-01-08 15:23:19,604 - 10 - training_embed.py - training - Epoch: [1][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 314.2600 (309.2292)	
2019-01-08 15:23:19,842 - 10 - training_embed.py - training - Epoch: [1][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8163 (309.2040)	
2019-01-08 15:23:20,067 - 10 - training_embed.py - training - Epoch: [1][1400/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 310.9182 (309.1906)	
2019-01-08 15:23:20,288 - 10 - training_embed.py - training - Epoch: [1][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8600 (309.1650)	
2019-01-08 15:23:20,508 - 10 - training_embed.py - training - Epoch: [1][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.4239 (309.1716)	
2019-01-08 15:23:20,741 - 10 - training_embed.py - training - Epoch: [1][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.1690 (309.1595)	
2019-01-08 15:23:20,996 - 10 - training_embed.py - training - Epoch: [1][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.0636 (309.1341)	
2019-01-08 15:23:21,216 - 10 - training_embed.py - training - Epoch: [1][1900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.3182 (309.1340)	
2019-01-08 15:23:21,431 - 10 - training_embed.py - training - Epoch: [1][2000/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 309.3918 (309.1125)	
2019-01-08 15:23:21,670 - 10 - training_embed.py - training - Epoch: [1][2100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.5836 (309.0947)	
2019-01-08 15:23:21,851 - 10 - training_embed.py - training - loss: 309.053877
2019-01-08 15:23:21,851 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 15:23:22,292 - 10 - training_embed.py - training - Epoch: [2][100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 309.3353 (308.7886)	
2019-01-08 15:23:22,512 - 10 - training_embed.py - training - Epoch: [2][200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.1921 (308.9168)	
2019-01-08 15:23:22,737 - 10 - training_embed.py - training - Epoch: [2][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.8773 (308.9154)	
2019-01-08 15:23:22,993 - 10 - training_embed.py - training - Epoch: [2][400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.7837 (308.8698)	
2019-01-08 15:23:23,255 - 10 - training_embed.py - training - Epoch: [2][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.4336 (308.8440)	
2019-01-08 15:23:23,478 - 10 - training_embed.py - training - Epoch: [2][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8526 (308.8194)	
2019-01-08 15:23:23,733 - 10 - training_embed.py - training - Epoch: [2][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4676 (308.7999)	
2019-01-08 15:23:23,973 - 10 - training_embed.py - training - Epoch: [2][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3309 (308.7791)	
2019-01-08 15:23:24,222 - 10 - training_embed.py - training - Epoch: [2][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.2727 (308.8107)	
2019-01-08 15:23:24,442 - 10 - training_embed.py - training - Epoch: [2][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.9247 (308.7866)	
2019-01-08 15:23:24,666 - 10 - training_embed.py - training - Epoch: [2][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9880 (308.7758)	
2019-01-08 15:23:24,940 - 10 - training_embed.py - training - Epoch: [2][1200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 310.1577 (308.7622)	
2019-01-08 15:23:25,304 - 10 - training_embed.py - training - Epoch: [2][1300/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.0343 (308.7748)	
2019-01-08 15:23:25,632 - 10 - training_embed.py - training - Epoch: [2][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5029 (308.7838)	
2019-01-08 15:23:25,851 - 10 - training_embed.py - training - Epoch: [2][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.7452 (308.7742)	
2019-01-08 15:23:26,083 - 10 - training_embed.py - training - Epoch: [2][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.2756 (308.7514)	
2019-01-08 15:23:26,313 - 10 - training_embed.py - training - Epoch: [2][1700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.3738 (308.7491)	
2019-01-08 15:23:26,530 - 10 - training_embed.py - training - Epoch: [2][1800/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.4530 (308.7264)	
2019-01-08 15:23:26,760 - 10 - training_embed.py - training - Epoch: [2][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1533 (308.7212)	
2019-01-08 15:23:26,968 - 10 - training_embed.py - training - Epoch: [2][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 311.5509 (308.7070)	
2019-01-08 15:23:27,181 - 10 - training_embed.py - training - Epoch: [2][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8746 (308.6873)	
2019-01-08 15:23:27,367 - 10 - training_embed.py - training - loss: 308.646739
2019-01-08 15:23:27,367 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 15:23:27,803 - 10 - training_embed.py - training - Epoch: [3][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3390 (308.3957)	
2019-01-08 15:23:28,008 - 10 - training_embed.py - training - Epoch: [3][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.5793 (308.3177)	
2019-01-08 15:23:28,234 - 10 - training_embed.py - training - Epoch: [3][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6561 (308.4044)	
2019-01-08 15:23:28,458 - 10 - training_embed.py - training - Epoch: [3][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3942 (308.4214)	
2019-01-08 15:23:28,675 - 10 - training_embed.py - training - Epoch: [3][500/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 312.3063 (308.3989)	
2019-01-08 15:23:28,892 - 10 - training_embed.py - training - Epoch: [3][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0286 (308.4018)	
2019-01-08 15:23:29,108 - 10 - training_embed.py - training - Epoch: [3][700/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.2103 (308.3426)	
2019-01-08 15:23:29,336 - 10 - training_embed.py - training - Epoch: [3][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.9862 (308.3504)	
2019-01-08 15:23:29,550 - 10 - training_embed.py - training - Epoch: [3][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 312.3898 (308.3180)	
2019-01-08 15:23:29,777 - 10 - training_embed.py - training - Epoch: [3][1000/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 308.6048 (308.2940)	
2019-01-08 15:23:29,996 - 10 - training_embed.py - training - Epoch: [3][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7827 (308.2901)	
2019-01-08 15:23:30,216 - 10 - training_embed.py - training - Epoch: [3][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 312.7968 (308.3004)	
2019-01-08 15:23:30,439 - 10 - training_embed.py - training - Epoch: [3][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.7413 (308.2986)	
2019-01-08 15:23:30,668 - 10 - training_embed.py - training - Epoch: [3][1400/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.6558 (308.3164)	
2019-01-08 15:23:30,887 - 10 - training_embed.py - training - Epoch: [3][1500/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 310.2213 (308.3004)	
2019-01-08 15:23:31,100 - 10 - training_embed.py - training - Epoch: [3][1600/2184]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 309.4569 (308.2993)	
2019-01-08 15:23:31,311 - 10 - training_embed.py - training - Epoch: [3][1700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.6711 (308.3019)	
2019-01-08 15:23:31,521 - 10 - training_embed.py - training - Epoch: [3][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.6942 (308.2820)	
2019-01-08 15:23:31,734 - 10 - training_embed.py - training - Epoch: [3][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2723 (308.2941)	
2019-01-08 15:23:31,951 - 10 - training_embed.py - training - Epoch: [3][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6544 (308.3022)	
2019-01-08 15:23:32,189 - 10 - training_embed.py - training - Epoch: [3][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4033 (308.2776)	
2019-01-08 15:23:32,380 - 10 - training_embed.py - training - loss: 308.239358
2019-01-08 15:23:32,380 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 15:23:32,809 - 10 - training_embed.py - training - Epoch: [4][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.7263 (308.0453)	
2019-01-08 15:23:33,028 - 10 - training_embed.py - training - Epoch: [4][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.4107 (308.0723)	
2019-01-08 15:23:33,242 - 10 - training_embed.py - training - Epoch: [4][300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 311.3839 (308.2585)	
2019-01-08 15:23:33,476 - 10 - training_embed.py - training - Epoch: [4][400/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 305.6387 (308.2162)	
2019-01-08 15:23:33,695 - 10 - training_embed.py - training - Epoch: [4][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.3194 (308.1943)	
2019-01-08 15:23:33,919 - 10 - training_embed.py - training - Epoch: [4][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.4963 (308.1535)	
2019-01-08 15:23:34,138 - 10 - training_embed.py - training - Epoch: [4][700/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 307.9313 (308.1351)	
2019-01-08 15:23:34,358 - 10 - training_embed.py - training - Epoch: [4][800/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.0698 (308.0575)	
2019-01-08 15:23:34,561 - 10 - training_embed.py - training - Epoch: [4][900/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 302.4374 (308.0416)	
2019-01-08 15:23:34,785 - 10 - training_embed.py - training - Epoch: [4][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.2379 (307.9943)	
2019-01-08 15:23:34,995 - 10 - training_embed.py - training - Epoch: [4][1100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.9438 (307.9756)	
2019-01-08 15:23:35,215 - 10 - training_embed.py - training - Epoch: [4][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8051 (308.0061)	
2019-01-08 15:23:35,422 - 10 - training_embed.py - training - Epoch: [4][1300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.4548 (308.0043)	
2019-01-08 15:23:35,645 - 10 - training_embed.py - training - Epoch: [4][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2482 (307.9757)	
2019-01-08 15:23:35,874 - 10 - training_embed.py - training - Epoch: [4][1500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1344 (307.9764)	
2019-01-08 15:23:36,091 - 10 - training_embed.py - training - Epoch: [4][1600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 306.9918 (307.9735)	
2019-01-08 15:23:36,297 - 10 - training_embed.py - training - Epoch: [4][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3704 (307.9432)	
2019-01-08 15:23:36,509 - 10 - training_embed.py - training - Epoch: [4][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.6866 (307.9350)	
2019-01-08 15:23:36,727 - 10 - training_embed.py - training - Epoch: [4][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3226 (307.9376)	
2019-01-08 15:23:36,938 - 10 - training_embed.py - training - Epoch: [4][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.0777 (307.9048)	
2019-01-08 15:23:37,151 - 10 - training_embed.py - training - Epoch: [4][2100/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 306.8530 (307.8834)	
2019-01-08 15:23:37,321 - 10 - training_embed.py - training - loss: 307.833751
2019-01-08 15:23:37,322 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 15:23:37,753 - 10 - training_embed.py - training - Epoch: [5][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6488 (307.9723)	
2019-01-08 15:23:37,976 - 10 - training_embed.py - training - Epoch: [5][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.2444 (307.7606)	
2019-01-08 15:23:38,187 - 10 - training_embed.py - training - Epoch: [5][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0797 (307.6302)	
2019-01-08 15:23:38,410 - 10 - training_embed.py - training - Epoch: [5][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.3395 (307.6144)	
2019-01-08 15:23:38,630 - 10 - training_embed.py - training - Epoch: [5][500/2184]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 306.3147 (307.7109)	
2019-01-08 15:23:38,848 - 10 - training_embed.py - training - Epoch: [5][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3944 (307.7100)	
2019-01-08 15:23:39,058 - 10 - training_embed.py - training - Epoch: [5][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.9859 (307.6491)	
2019-01-08 15:23:39,284 - 10 - training_embed.py - training - Epoch: [5][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3011 (307.5799)	
2019-01-08 15:23:39,502 - 10 - training_embed.py - training - Epoch: [5][900/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 305.4010 (307.5453)	
2019-01-08 15:23:39,711 - 10 - training_embed.py - training - Epoch: [5][1000/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.0172 (307.5271)	
2019-01-08 15:23:39,929 - 10 - training_embed.py - training - Epoch: [5][1100/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.8491 (307.5404)	
2019-01-08 15:23:40,136 - 10 - training_embed.py - training - Epoch: [5][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8294 (307.5439)	
2019-01-08 15:23:40,355 - 10 - training_embed.py - training - Epoch: [5][1300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.1335 (307.5253)	
2019-01-08 15:23:40,573 - 10 - training_embed.py - training - Epoch: [5][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.1026 (307.5190)	
2019-01-08 15:23:40,789 - 10 - training_embed.py - training - Epoch: [5][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.1555 (307.5231)	
2019-01-08 15:23:41,008 - 10 - training_embed.py - training - Epoch: [5][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2448 (307.5277)	
2019-01-08 15:23:41,228 - 10 - training_embed.py - training - Epoch: [5][1700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.1554 (307.5105)	
2019-01-08 15:23:41,439 - 10 - training_embed.py - training - Epoch: [5][1800/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.8672 (307.5105)	
2019-01-08 15:23:41,662 - 10 - training_embed.py - training - Epoch: [5][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5777 (307.4899)	
2019-01-08 15:23:41,869 - 10 - training_embed.py - training - Epoch: [5][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.8237 (307.4834)	
2019-01-08 15:23:42,085 - 10 - training_embed.py - training - Epoch: [5][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.3195 (307.4638)	
2019-01-08 15:23:42,265 - 10 - training_embed.py - training - loss: 307.428919
2019-01-08 15:23:42,265 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 15:23:42,699 - 10 - training_embed.py - training - Epoch: [6][100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.7403 (306.8246)	
2019-01-08 15:23:42,911 - 10 - training_embed.py - training - Epoch: [6][200/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.4552 (307.1377)	
2019-01-08 15:23:43,138 - 10 - training_embed.py - training - Epoch: [6][300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 311.4240 (307.1302)	
2019-01-08 15:23:43,349 - 10 - training_embed.py - training - Epoch: [6][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4464 (307.1676)	
2019-01-08 15:23:43,569 - 10 - training_embed.py - training - Epoch: [6][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.4296 (307.1244)	
2019-01-08 15:23:43,778 - 10 - training_embed.py - training - Epoch: [6][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.4206 (307.1396)	
2019-01-08 15:23:43,994 - 10 - training_embed.py - training - Epoch: [6][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.3552 (307.1679)	
2019-01-08 15:23:44,214 - 10 - training_embed.py - training - Epoch: [6][800/2184]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 304.9061 (307.1468)	
2019-01-08 15:23:44,435 - 10 - training_embed.py - training - Epoch: [6][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8272 (307.1565)	
2019-01-08 15:23:44,657 - 10 - training_embed.py - training - Epoch: [6][1000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 300.3568 (307.1483)	
2019-01-08 15:23:44,860 - 10 - training_embed.py - training - Epoch: [6][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.0220 (307.1445)	
2019-01-08 15:23:45,079 - 10 - training_embed.py - training - Epoch: [6][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2331 (307.1516)	
2019-01-08 15:23:45,305 - 10 - training_embed.py - training - Epoch: [6][1300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.2523 (307.1744)	
2019-01-08 15:23:45,516 - 10 - training_embed.py - training - Epoch: [6][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3922 (307.1451)	
2019-01-08 15:23:45,734 - 10 - training_embed.py - training - Epoch: [6][1500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.5968 (307.1449)	
2019-01-08 15:23:45,961 - 10 - training_embed.py - training - Epoch: [6][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5912 (307.1221)	
2019-01-08 15:23:46,178 - 10 - training_embed.py - training - Epoch: [6][1700/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 307.0982 (307.1239)	
2019-01-08 15:23:46,397 - 10 - training_embed.py - training - Epoch: [6][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.9325 (307.1055)	
2019-01-08 15:23:46,611 - 10 - training_embed.py - training - Epoch: [6][1900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.3861 (307.0884)	
2019-01-08 15:23:46,836 - 10 - training_embed.py - training - Epoch: [6][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.3730 (307.0788)	
2019-01-08 15:23:47,050 - 10 - training_embed.py - training - Epoch: [6][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 300.0858 (307.0740)	
2019-01-08 15:23:47,229 - 10 - training_embed.py - training - loss: 307.024755
2019-01-08 15:23:47,229 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 15:23:47,665 - 10 - training_embed.py - training - Epoch: [7][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.1234 (306.6184)	
2019-01-08 15:23:47,881 - 10 - training_embed.py - training - Epoch: [7][200/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 301.9852 (306.4551)	
2019-01-08 15:23:48,097 - 10 - training_embed.py - training - Epoch: [7][300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 313.5471 (306.6009)	
2019-01-08 15:23:48,312 - 10 - training_embed.py - training - Epoch: [7][400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.9920 (306.6307)	
2019-01-08 15:23:48,534 - 10 - training_embed.py - training - Epoch: [7][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 309.8109 (306.7251)	
2019-01-08 15:23:48,742 - 10 - training_embed.py - training - Epoch: [7][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.1431 (306.6947)	
2019-01-08 15:23:48,971 - 10 - training_embed.py - training - Epoch: [7][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.3753 (306.7277)	
2019-01-08 15:23:49,192 - 10 - training_embed.py - training - Epoch: [7][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.1374 (306.8119)	
2019-01-08 15:23:49,420 - 10 - training_embed.py - training - Epoch: [7][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.2391 (306.8043)	
2019-01-08 15:23:49,674 - 10 - training_embed.py - training - Epoch: [7][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.8336 (306.7346)	
2019-01-08 15:23:49,929 - 10 - training_embed.py - training - Epoch: [7][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.4477 (306.7184)	
2019-01-08 15:23:50,154 - 10 - training_embed.py - training - Epoch: [7][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.6912 (306.7133)	
2019-01-08 15:23:50,389 - 10 - training_embed.py - training - Epoch: [7][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.3642 (306.7188)	
2019-01-08 15:23:50,627 - 10 - training_embed.py - training - Epoch: [7][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3705 (306.6975)	
2019-01-08 15:23:50,842 - 10 - training_embed.py - training - Epoch: [7][1500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.7200 (306.7002)	
2019-01-08 15:23:51,180 - 10 - training_embed.py - training - Epoch: [7][1600/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.4438 (306.7061)	
2019-01-08 15:23:51,443 - 10 - training_embed.py - training - Epoch: [7][1700/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 307.7862 (306.6935)	
2019-01-08 15:23:51,699 - 10 - training_embed.py - training - Epoch: [7][1800/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.9117 (306.6950)	
2019-01-08 15:23:51,973 - 10 - training_embed.py - training - Epoch: [7][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3180 (306.6794)	
2019-01-08 15:23:52,235 - 10 - training_embed.py - training - Epoch: [7][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.8812 (306.6751)	
2019-01-08 15:23:52,497 - 10 - training_embed.py - training - Epoch: [7][2100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 307.0444 (306.6634)	
2019-01-08 15:23:52,689 - 10 - training_embed.py - training - loss: 306.620672
2019-01-08 15:23:52,689 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 15:23:53,187 - 10 - training_embed.py - training - Epoch: [8][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6087 (306.5441)	
2019-01-08 15:23:53,450 - 10 - training_embed.py - training - Epoch: [8][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6984 (306.5079)	
2019-01-08 15:23:53,693 - 10 - training_embed.py - training - Epoch: [8][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.8929 (306.3865)	
2019-01-08 15:23:53,929 - 10 - training_embed.py - training - Epoch: [8][400/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 305.2123 (306.3270)	
2019-01-08 15:23:54,247 - 10 - training_embed.py - training - Epoch: [8][500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.1550 (306.3514)	
2019-01-08 15:23:54,493 - 10 - training_embed.py - training - Epoch: [8][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.9556 (306.3052)	
2019-01-08 15:23:54,720 - 10 - training_embed.py - training - Epoch: [8][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.0352 (306.3018)	
2019-01-08 15:23:54,947 - 10 - training_embed.py - training - Epoch: [8][800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.8830 (306.2875)	
2019-01-08 15:23:55,186 - 10 - training_embed.py - training - Epoch: [8][900/2184]	Time 0.007 (0.002)	Data 0.005 (0.001)	Loss 304.2801 (306.3383)	
2019-01-08 15:23:55,441 - 10 - training_embed.py - training - Epoch: [8][1000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6469 (306.3106)	
2019-01-08 15:23:55,674 - 10 - training_embed.py - training - Epoch: [8][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.3655 (306.2915)	
2019-01-08 15:23:55,884 - 10 - training_embed.py - training - Epoch: [8][1200/2184]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 304.7523 (306.3122)	
2019-01-08 15:23:56,092 - 10 - training_embed.py - training - Epoch: [8][1300/2184]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 308.9766 (306.2936)	
2019-01-08 15:23:56,330 - 10 - training_embed.py - training - Epoch: [8][1400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.7861 (306.2961)	
2019-01-08 15:23:56,571 - 10 - training_embed.py - training - Epoch: [8][1500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 303.4291 (306.2563)	
2019-01-08 15:23:56,808 - 10 - training_embed.py - training - Epoch: [8][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 309.8834 (306.2611)	
2019-01-08 15:23:57,029 - 10 - training_embed.py - training - Epoch: [8][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.3792 (306.2584)	
2019-01-08 15:23:57,271 - 10 - training_embed.py - training - Epoch: [8][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.7718 (306.2595)	
2019-01-08 15:23:57,568 - 10 - training_embed.py - training - Epoch: [8][1900/2184]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 305.2939 (306.2603)	
2019-01-08 15:23:57,802 - 10 - training_embed.py - training - Epoch: [8][2000/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.6124 (306.2545)	
2019-01-08 15:23:58,033 - 10 - training_embed.py - training - Epoch: [8][2100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.1256 (306.2515)	
2019-01-08 15:23:58,218 - 10 - training_embed.py - training - loss: 306.218051
2019-01-08 15:23:58,218 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 15:23:58,651 - 10 - training_embed.py - training - Epoch: [9][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.2524 (305.7513)	
2019-01-08 15:23:58,858 - 10 - training_embed.py - training - Epoch: [9][200/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.1944 (305.9314)	
2019-01-08 15:23:59,085 - 10 - training_embed.py - training - Epoch: [9][300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.3747 (305.9301)	
2019-01-08 15:23:59,315 - 10 - training_embed.py - training - Epoch: [9][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5077 (305.9323)	
2019-01-08 15:23:59,541 - 10 - training_embed.py - training - Epoch: [9][500/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 308.8281 (305.9673)	
2019-01-08 15:23:59,756 - 10 - training_embed.py - training - Epoch: [9][600/2184]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 304.9778 (305.9381)	
2019-01-08 15:23:59,965 - 10 - training_embed.py - training - Epoch: [9][700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.7916 (305.9327)	
2019-01-08 15:24:00,195 - 10 - training_embed.py - training - Epoch: [9][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7401 (305.9591)	
2019-01-08 15:24:00,422 - 10 - training_embed.py - training - Epoch: [9][900/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.9277 (305.9546)	
2019-01-08 15:24:00,649 - 10 - training_embed.py - training - Epoch: [9][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.3210 (305.9605)	
2019-01-08 15:24:00,894 - 10 - training_embed.py - training - Epoch: [9][1100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9416 (305.9594)	
2019-01-08 15:24:01,125 - 10 - training_embed.py - training - Epoch: [9][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 310.5530 (305.9256)	
2019-01-08 15:24:01,344 - 10 - training_embed.py - training - Epoch: [9][1300/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.3028 (305.9042)	
2019-01-08 15:24:01,552 - 10 - training_embed.py - training - Epoch: [9][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.0271 (305.8800)	
2019-01-08 15:24:01,765 - 10 - training_embed.py - training - Epoch: [9][1500/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.7570 (305.8552)	
2019-01-08 15:24:01,989 - 10 - training_embed.py - training - Epoch: [9][1600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.5143 (305.8779)	
2019-01-08 15:24:02,223 - 10 - training_embed.py - training - Epoch: [9][1700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 303.7599 (305.8768)	
2019-01-08 15:24:02,487 - 10 - training_embed.py - training - Epoch: [9][1800/2184]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 304.4901 (305.8728)	
2019-01-08 15:24:02,727 - 10 - training_embed.py - training - Epoch: [9][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.6620 (305.8585)	
2019-01-08 15:24:02,956 - 10 - training_embed.py - training - Epoch: [9][2000/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 303.7704 (305.8467)	
2019-01-08 15:24:03,171 - 10 - training_embed.py - training - Epoch: [9][2100/2184]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 310.5109 (305.8249)	
2019-01-08 15:24:03,357 - 10 - training_embed.py - training - loss: 305.815535
2019-01-08 15:24:03,357 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 15:24:03,794 - 10 - training_embed.py - training - Epoch: [10][100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 307.4240 (305.1633)	
2019-01-08 15:24:04,007 - 10 - training_embed.py - training - Epoch: [10][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.4896 (305.3141)	
2019-01-08 15:24:04,256 - 10 - training_embed.py - training - Epoch: [10][300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.2775 (305.4054)	
2019-01-08 15:24:04,478 - 10 - training_embed.py - training - Epoch: [10][400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.2532 (305.5049)	
2019-01-08 15:24:04,690 - 10 - training_embed.py - training - Epoch: [10][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 304.5414 (305.4764)	
2019-01-08 15:24:04,924 - 10 - training_embed.py - training - Epoch: [10][600/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 305.0039 (305.5030)	
2019-01-08 15:24:05,195 - 10 - training_embed.py - training - Epoch: [10][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 306.0335 (305.5098)	
2019-01-08 15:24:05,495 - 10 - training_embed.py - training - Epoch: [10][800/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 303.7743 (305.5684)	
2019-01-08 15:24:05,828 - 10 - training_embed.py - training - Epoch: [10][900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.7798 (305.5183)	
2019-01-08 15:24:06,123 - 10 - training_embed.py - training - Epoch: [10][1000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 307.5381 (305.5051)	
2019-01-08 15:24:06,380 - 10 - training_embed.py - training - Epoch: [10][1100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.0565 (305.4981)	
2019-01-08 15:24:06,657 - 10 - training_embed.py - training - Epoch: [10][1200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.5898 (305.5040)	
2019-01-08 15:24:06,924 - 10 - training_embed.py - training - Epoch: [10][1300/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.0699 (305.5150)	
2019-01-08 15:24:07,201 - 10 - training_embed.py - training - Epoch: [10][1400/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 304.7550 (305.5357)	
2019-01-08 15:24:07,580 - 10 - training_embed.py - training - Epoch: [10][1500/2184]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 305.2988 (305.5170)	
2019-01-08 15:24:07,948 - 10 - training_embed.py - training - Epoch: [10][1600/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 304.4384 (305.5022)	
2019-01-08 15:24:08,278 - 10 - training_embed.py - training - Epoch: [10][1700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 301.7258 (305.5118)	
2019-01-08 15:24:08,556 - 10 - training_embed.py - training - Epoch: [10][1800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.9138 (305.4878)	
2019-01-08 15:24:08,794 - 10 - training_embed.py - training - Epoch: [10][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 305.3611 (305.4661)	
2019-01-08 15:24:09,030 - 10 - training_embed.py - training - Epoch: [10][2000/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 305.3613 (305.4538)	
2019-01-08 15:24:09,248 - 10 - training_embed.py - training - Epoch: [10][2100/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 303.5950 (305.4330)	
2019-01-08 15:24:09,423 - 10 - training_embed.py - training - loss: 305.412303
2019-01-08 15:24:09,423 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 15:24:09,849 - 10 - training_embed.py - training - Epoch: [11][100/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.4893 (305.3523)	
2019-01-08 15:24:10,067 - 10 - training_embed.py - training - Epoch: [11][200/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.8638 (305.2119)	
2019-01-08 15:24:10,278 - 10 - training_embed.py - training - Epoch: [11][300/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 308.6053 (305.0969)	
2019-01-08 15:24:10,492 - 10 - training_embed.py - training - Epoch: [11][400/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 301.4524 (305.1086)	
2019-01-08 15:24:10,718 - 10 - training_embed.py - training - Epoch: [11][500/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.9896 (305.2011)	
2019-01-08 15:24:10,946 - 10 - training_embed.py - training - Epoch: [11][600/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 307.2359 (305.1763)	
2019-01-08 15:24:11,165 - 10 - training_embed.py - training - Epoch: [11][700/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 304.4996 (305.1691)	
2019-01-08 15:24:11,389 - 10 - training_embed.py - training - Epoch: [11][800/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 303.0543 (305.1562)	
2019-01-08 15:24:11,612 - 10 - training_embed.py - training - Epoch: [11][900/2184]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 299.8971 (305.1688)	
2019-01-08 15:24:11,832 - 10 - training_embed.py - training - Epoch: [11][1000/2184]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 306.0745 (305.1362)	
2019-01-08 15:24:12,042 - 10 - training_embed.py - training - Epoch: [11][1100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 306.9309 (305.1649)	
2019-01-08 15:24:12,254 - 10 - training_embed.py - training - Epoch: [11][1200/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.5553 (305.1710)	
2019-01-08 15:24:12,465 - 10 - training_embed.py - training - Epoch: [11][1300/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.0384 (305.1520)	
2019-01-08 15:24:12,688 - 10 - training_embed.py - training - Epoch: [11][1400/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 308.7528 (305.1493)	
2019-01-08 15:24:12,894 - 10 - training_embed.py - training - Epoch: [11][1500/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.3102 (305.1207)	
2019-01-08 15:24:13,118 - 10 - training_embed.py - training - Epoch: [11][1600/2184]	Time 0.007 (0.002)	Data 0.006 (0.001)	Loss 302.3613 (305.0980)	
2019-01-08 15:24:13,328 - 10 - training_embed.py - training - Epoch: [11][1700/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 306.6828 (305.0956)	
2019-01-08 15:24:13,558 - 10 - training_embed.py - training - Epoch: [11][1800/2184]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 302.8094 (305.1029)	
2019-01-08 15:24:13,782 - 10 - training_embed.py - training - Epoch: [11][1900/2184]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 302.7926 (305.0822)	
2019-01-08 15:24:14,004 - 10 - training_embed.py - training - Epoch: [11][2000/2184]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 305.7903 (305.0682)	
2019-01-08 15:24:14,218 - 10 - training_embed.py - training - Epoch: [11][2100/2184]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 304.0666 (305.0628)	
2019-01-08 15:24:14,406 - 10 - training_embed.py - training - loss: 305.011043
2019-01-08 15:24:14,503 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 15:24:15,596 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1092.278 ms ~ 0.018 min ~ 1.092 sec
2019-01-08 15:24:16,340 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 1836.969 ms ~ 0.031 min ~ 1.837 sec
2019-01-08 15:24:16,340 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 15:24:16,341 - 10 - corpus.py - subactivity_sampler - 0 / 163
2019-01-08 15:24:16,341 - 10 - corpus.py - subactivity_sampler - [ 66733.  90867.  85482.  55795.  71652. 135246.  53153.]
2019-01-08 15:25:31,156 - 10 - corpus.py - subactivity_sampler - 20 / 163
2019-01-08 15:25:31,156 - 10 - corpus.py - subactivity_sampler - [ 66636.  91150.  86105.  54905.  71448. 135740.  52944.]
2019-01-08 15:26:50,795 - 10 - corpus.py - subactivity_sampler - 40 / 163
2019-01-08 15:26:50,796 - 10 - corpus.py - subactivity_sampler - [ 66672.  91241.  86409.  54151.  71367. 136395.  52693.]
2019-01-08 15:28:04,838 - 10 - corpus.py - subactivity_sampler - 60 / 163
2019-01-08 15:28:04,838 - 10 - corpus.py - subactivity_sampler - [ 66572.  91318.  86743.  53124.  71421. 137280.  52470.]
2019-01-08 15:29:14,455 - 10 - corpus.py - subactivity_sampler - 80 / 163
2019-01-08 15:29:14,455 - 10 - corpus.py - subactivity_sampler - [ 66736.  91870.  86273.  52415.  71751. 137508.  52375.]
2019-01-08 15:30:26,856 - 10 - corpus.py - subactivity_sampler - 100 / 163
2019-01-08 15:30:26,857 - 10 - corpus.py - subactivity_sampler - [ 66709.  91940.  86069.  52035.  71672. 138224.  52279.]
2019-01-08 15:31:18,111 - 10 - corpus.py - subactivity_sampler - 120 / 163
2019-01-08 15:31:18,111 - 10 - corpus.py - subactivity_sampler - [ 66660.  92473.  86349.  51252.  71668. 138264.  52262.]
2019-01-08 15:32:18,028 - 10 - corpus.py - subactivity_sampler - 140 / 163
2019-01-08 15:32:18,028 - 10 - corpus.py - subactivity_sampler - [ 66653.  92542.  86537.  50630.  71586. 138819.  52161.]
2019-01-08 15:33:24,260 - 10 - corpus.py - subactivity_sampler - 160 / 163
2019-01-08 15:33:24,261 - 10 - corpus.py - subactivity_sampler - [ 66486.  92532.  86852.  50009.  71772. 139126.  52151.]
2019-01-08 15:33:38,110 - 10 - corpus.py - subactivity_sampler - [ 66488.  92530.  86855.  49916.  71772. 139216.  52151.]
2019-01-08 15:33:38,110 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 561769.529 ms ~ 9.363 min ~ 561.770 sec
2019-01-08 15:33:38,110 - 10 - corpus.py - ordering_sampler - .
2019-01-08 15:33:41,641 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 15:33:41,641 - 10 - corpus.py - ordering_sampler - inv_count_vec: [17. 62. 40.  3. 57. 14.]
2019-01-08 15:33:41,641 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 15:33:41,659 - 10 - corpus.py - rho_sampling - ['47.2971', '11.9491', '22.4388', '95.5409', '3.5348', '2.8096']
2019-01-08 15:33:41,659 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 15:33:41,846 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 15:33:41,860 - 10 - accuracy_class.py - mof - # gt_labels: 8   # pr_labels: 7
2019-01-08 15:33:41,860 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 14', '1: 34', '2: 18', '3: 4', '4: 35', '5: 32', '6: 33']
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - mof_val - frames true: 157133	frames overall : 558928
2019-01-08 15:33:41,883 - 10 - corpus.py - accuracy_corpus - Action: salat
2019-01-08 15:33:41,883 - 10 - corpus.py - accuracy_corpus - MoF val: 0.281132811381788
2019-01-08 15:33:41,883 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.281132811381788
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 23956
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - mof_classes - label 4: 0.112914  884 / 7829
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - mof_classes - label 14: 0.984896  5934 / 6025
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - mof_classes - label 18: 0.141271  369 / 2612
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - mof_classes - label 32: 0.304005  93180 / 306508
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - mof_classes - label 33: 0.318280  13539 / 42538
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - mof_classes - label 34: 0.261393  42722 / 163440
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - mof_classes - label 35: 0.083887  505 / 6020
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - mof_classes - average class mof: 0.275831
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 23956
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - iou_classes - label 4: 0.015547  884 / 56861
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - iou_classes - label 14: 0.089127  5934 / 66579
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - iou_classes - label 18: 0.004142  369 / 89098
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - iou_classes - label 32: 0.264307  93180 / 352544
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - iou_classes - label 33: 0.166839  13539 / 81150
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - iou_classes - label 34: 0.200340  42722 / 213248
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - iou_classes - label 35: 0.006534  505 / 77287
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - iou_classes - average IoU: 0.106691
2019-01-08 15:33:41,883 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.093354
2019-01-08 15:33:50,980 - 10 - f1_score.py - f1 - f1 score: 0.308772
2019-01-08 15:33:51,001 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 9341.732 ms ~ 0.156 min ~ 9.342 sec
2019-01-08 15:33:51,022 - 10 - utils.py - wrap - <function baseline at 0x7f3991d24e18> took 4524717.316 ms ~ 75.412 min ~ 4524.717 sec
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - batch_size: 256
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - bg: False
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - bg_trh: 85
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - data: /media/data/kukleva/lab/Breakfast/feat/s1
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - data_type: 2
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - dataset: bf
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - dataset_root: /media/data/kukleva/lab/Breakfast
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - embed_dim: 30
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - epochs: 12
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - ext: txt
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - feature_dim: 64
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - full: True
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - gmm: 1
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - gmms: one
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - gt: /media/data/kukleva/lab/Breakfast/feat/s1/groundTruth
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - gt_training: False
2019-01-08 15:33:51,023 - 10 - utils.py - update_opt_str - log: DEBUG
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - log_str: slim.mallow._pancake_!bg_data2_bf_embed30_epochs12_full_gmm1_one_!gt_lr1e-10_!lr_ordering_reg0.1_!viterbi_!zeros_
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - lr: 1e-10
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - lr_adj: False
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - momentum: 0.9
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - num_workers: 4
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - ordering: True
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - prefix: slim.mallow.
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - reg_cov: 0.1
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - resume: False
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - resume_str: 
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - save_embed_feat: False
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - save_likelihood: False
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - save_model: False
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - seed: 0
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - subaction: pancake
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - vis: False
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - viterbi: False
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - weight_decay: 0.0001
2019-01-08 15:33:51,024 - 10 - utils.py - update_opt_str - zeros: False
2019-01-08 15:33:51,044 - 10 - utils.py - wrap - <function GroundTruth.load_gt at 0x7f39322b36a8> took 19.244 ms ~ 0.000 min ~ 0.019 sec
2019-01-08 15:33:51,188 - 10 - corpus.py - __init__ - pancake  subactions: 13
2019-01-08 15:33:51,189 - 10 - corpus.py - _init_videos - .
2019-01-08 15:34:24,962 - 10 - corpus.py - _init_videos - gt statistic: Counter({29: 433642, 27: 168353, 28: 75620, 26: 54534, 11: 52421, 17: 39063, 30: 36684, 2: 36524, 14: 20628, 10: 11025, 31: 5514, 4: 2910, 16: 207})
2019-01-08 15:34:24,962 - 10 - corpus.py - _update_fg_mask - .
2019-01-08 15:34:25,035 - 10 - corpus.py - __init__ - min: -38.754585  max: 41.215057  avg: 0.072707
2019-01-08 15:34:25,035 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 15:34:25,100 - 10 - corpus.py - rho_sampling - ['64.0767', '103.1747', '929.0535', '90.9104', '17.1809', '163.7961', '52.0887', '55.1381', '160.2581', '9.1577', '13.0916', '457.2870']
2019-01-08 15:34:25,100 - 10 - pipeline.py - baseline - Iteration 0
2019-01-08 15:34:25,509 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 15:34:25,528 - 10 - accuracy_class.py - mof - # gt_labels: 14   # pr_labels: 13
2019-01-08 15:34:25,528 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 11', '1: 26', '2: 2', '3: 27', '4: 10', '5: 17', '6: 28', '7: 31', '8: 16', '9: 4', '10: 29', '11: 14', '12: 30']
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_val - frames true: 244935	frames overall : 937125
2019-01-08 15:34:25,576 - 10 - corpus.py - accuracy_corpus - Action: pancake
2019-01-08 15:34:25,576 - 10 - corpus.py - accuracy_corpus - MoF val: 0.2613685474189676
2019-01-08 15:34:25,576 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.0
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35402
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 2: 0.273163  9977 / 36524
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1774
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 10: 0.229569  2531 / 11025
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 11: 0.746612  34271 / 45902
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 14: 0.357248  6314 / 17674
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 207
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 17: 0.263362  10264 / 38973
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 26: 0.450552  24351 / 54047
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 27: 0.305168  51376 / 168353
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 28: 0.222839  13960 / 62646
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 29: 0.155672  67023 / 430540
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 30: 0.871216  24868 / 28544
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - label 31: 0.000000  0 / 5514
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - mof_classes - average class mof: 0.276814
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35402
2019-01-08 15:34:25,576 - 10 - accuracy_class.py - iou_classes - label 2: 0.101095  9977 / 98689
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 73823
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - label 10: 0.031399  2531 / 80607
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - label 11: 0.409001  34271 / 83792
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - label 14: 0.075720  6314 / 83386
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 72267
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - label 17: 0.101817  10264 / 100808
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - label 26: 0.239103  24351 / 101843
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - label 27: 0.271680  51376 / 189105
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - label 28: 0.115592  13960 / 120770
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - label 29: 0.153881  67023 / 435551
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - label 30: 0.328559  24868 / 75688
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - label 31: 0.000000  0 / 77584
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - average IoU: 0.140604
2019-01-08 15:34:25,577 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.130561
2019-01-08 15:34:40,657 - 10 - f1_score.py - f1 - f1 score: 0.296833
2019-01-08 15:34:40,693 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 15592.389 ms ~ 0.260 min ~ 15.592 sec
2019-01-08 15:34:40,693 - 10 - corpus.py - embedding_training - .
2019-01-08 15:34:40,693 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 15:34:40,693 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 15:34:40,693 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 15:34:46,323 - 10 - training_embed.py - training - create model
2019-01-08 15:34:46,324 - 10 - training_embed.py - training - epochs: 12
2019-01-08 15:34:46,324 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 15:34:46,712 - 10 - training_embed.py - training - Epoch: [0][100/3661]	Time 0.002 (0.004)	Data 0.001 (0.003)	Loss 607.7307 (612.6144)	
2019-01-08 15:34:46,935 - 10 - training_embed.py - training - Epoch: [0][200/3661]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 614.4505 (612.6029)	
2019-01-08 15:34:47,143 - 10 - training_embed.py - training - Epoch: [0][300/3661]	Time 0.003 (0.003)	Data 0.002 (0.002)	Loss 609.5339 (612.6701)	
2019-01-08 15:34:47,376 - 10 - training_embed.py - training - Epoch: [0][400/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 612.2440 (612.8064)	
2019-01-08 15:34:47,592 - 10 - training_embed.py - training - Epoch: [0][500/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 601.5809 (612.8493)	
2019-01-08 15:34:47,815 - 10 - training_embed.py - training - Epoch: [0][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.9324 (612.9259)	
2019-01-08 15:34:48,045 - 10 - training_embed.py - training - Epoch: [0][700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 610.5829 (612.9020)	
2019-01-08 15:34:48,257 - 10 - training_embed.py - training - Epoch: [0][800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 617.7084 (612.9577)	
2019-01-08 15:34:48,469 - 10 - training_embed.py - training - Epoch: [0][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.8513 (612.9862)	
2019-01-08 15:34:48,695 - 10 - training_embed.py - training - Epoch: [0][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.5165 (613.0295)	
2019-01-08 15:34:48,903 - 10 - training_embed.py - training - Epoch: [0][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.4626 (612.9834)	
2019-01-08 15:34:49,110 - 10 - training_embed.py - training - Epoch: [0][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.9171 (612.9856)	
2019-01-08 15:34:49,329 - 10 - training_embed.py - training - Epoch: [0][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.0228 (612.9872)	
2019-01-08 15:34:49,548 - 10 - training_embed.py - training - Epoch: [0][1400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.4550 (612.9787)	
2019-01-08 15:34:49,761 - 10 - training_embed.py - training - Epoch: [0][1500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.3721 (612.9679)	
2019-01-08 15:34:49,969 - 10 - training_embed.py - training - Epoch: [0][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8717 (612.9494)	
2019-01-08 15:34:50,189 - 10 - training_embed.py - training - Epoch: [0][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.8497 (612.9779)	
2019-01-08 15:34:50,418 - 10 - training_embed.py - training - Epoch: [0][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.9001 (612.9812)	
2019-01-08 15:34:50,625 - 10 - training_embed.py - training - Epoch: [0][1900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.1299 (612.9617)	
2019-01-08 15:34:50,834 - 10 - training_embed.py - training - Epoch: [0][2000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 618.4678 (612.9547)	
2019-01-08 15:34:51,041 - 10 - training_embed.py - training - Epoch: [0][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.6033 (612.9527)	
2019-01-08 15:34:51,262 - 10 - training_embed.py - training - Epoch: [0][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.6937 (612.9486)	
2019-01-08 15:34:51,465 - 10 - training_embed.py - training - Epoch: [0][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.6424 (612.9597)	
2019-01-08 15:34:51,672 - 10 - training_embed.py - training - Epoch: [0][2400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.3624 (612.9706)	
2019-01-08 15:34:51,879 - 10 - training_embed.py - training - Epoch: [0][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.6038 (612.9601)	
2019-01-08 15:34:52,106 - 10 - training_embed.py - training - Epoch: [0][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9169 (612.9692)	
2019-01-08 15:34:52,323 - 10 - training_embed.py - training - Epoch: [0][2700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 623.6124 (612.9749)	
2019-01-08 15:34:52,547 - 10 - training_embed.py - training - Epoch: [0][2800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 611.4978 (612.9825)	
2019-01-08 15:34:52,769 - 10 - training_embed.py - training - Epoch: [0][2900/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 615.7420 (612.9659)	
2019-01-08 15:34:52,983 - 10 - training_embed.py - training - Epoch: [0][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.2369 (612.9927)	
2019-01-08 15:34:53,203 - 10 - training_embed.py - training - Epoch: [0][3100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 616.3763 (613.0012)	
2019-01-08 15:34:53,416 - 10 - training_embed.py - training - Epoch: [0][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.8625 (613.0184)	
2019-01-08 15:34:53,630 - 10 - training_embed.py - training - Epoch: [0][3300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 617.5770 (613.0044)	
2019-01-08 15:34:53,868 - 10 - training_embed.py - training - Epoch: [0][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.5511 (612.9837)	
2019-01-08 15:34:54,071 - 10 - training_embed.py - training - Epoch: [0][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.7255 (612.9906)	
2019-01-08 15:34:54,280 - 10 - training_embed.py - training - Epoch: [0][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.1843 (612.9858)	
2019-01-08 15:34:54,413 - 10 - training_embed.py - training - loss: 612.936818
2019-01-08 15:34:54,413 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 15:34:54,866 - 10 - training_embed.py - training - Epoch: [1][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.5877 (613.1962)	
2019-01-08 15:34:55,086 - 10 - training_embed.py - training - Epoch: [1][200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 598.4432 (613.0844)	
2019-01-08 15:34:55,295 - 10 - training_embed.py - training - Epoch: [1][300/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 617.1833 (613.2367)	
2019-01-08 15:34:55,505 - 10 - training_embed.py - training - Epoch: [1][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.3364 (613.0142)	
2019-01-08 15:34:55,722 - 10 - training_embed.py - training - Epoch: [1][500/3661]	Time 0.009 (0.002)	Data 0.007 (0.001)	Loss 610.3674 (612.9244)	
2019-01-08 15:34:55,944 - 10 - training_embed.py - training - Epoch: [1][600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 616.3123 (612.9208)	
2019-01-08 15:34:56,147 - 10 - training_embed.py - training - Epoch: [1][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.1059 (612.9150)	
2019-01-08 15:34:56,355 - 10 - training_embed.py - training - Epoch: [1][800/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 609.7588 (612.9172)	
2019-01-08 15:34:56,574 - 10 - training_embed.py - training - Epoch: [1][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.6708 (612.9325)	
2019-01-08 15:34:56,802 - 10 - training_embed.py - training - Epoch: [1][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.5986 (612.9524)	
2019-01-08 15:34:57,006 - 10 - training_embed.py - training - Epoch: [1][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.0547 (613.0026)	
2019-01-08 15:34:57,211 - 10 - training_embed.py - training - Epoch: [1][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.3753 (612.9311)	
2019-01-08 15:34:57,448 - 10 - training_embed.py - training - Epoch: [1][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.8300 (612.8902)	
2019-01-08 15:34:57,666 - 10 - training_embed.py - training - Epoch: [1][1400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 616.0222 (612.9288)	
2019-01-08 15:34:57,871 - 10 - training_embed.py - training - Epoch: [1][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.4193 (612.9258)	
2019-01-08 15:34:58,083 - 10 - training_embed.py - training - Epoch: [1][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.7759 (612.8871)	
2019-01-08 15:34:58,291 - 10 - training_embed.py - training - Epoch: [1][1700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 613.1700 (612.8542)	
2019-01-08 15:34:58,504 - 10 - training_embed.py - training - Epoch: [1][1800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.7986 (612.8717)	
2019-01-08 15:34:58,717 - 10 - training_embed.py - training - Epoch: [1][1900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.7932 (612.8366)	
2019-01-08 15:34:58,935 - 10 - training_embed.py - training - Epoch: [1][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.0284 (612.8335)	
2019-01-08 15:34:59,141 - 10 - training_embed.py - training - Epoch: [1][2100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.7593 (612.8729)	
2019-01-08 15:34:59,346 - 10 - training_embed.py - training - Epoch: [1][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.4095 (612.8316)	
2019-01-08 15:34:59,570 - 10 - training_embed.py - training - Epoch: [1][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.1587 (612.8160)	
2019-01-08 15:34:59,784 - 10 - training_embed.py - training - Epoch: [1][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.4334 (612.8102)	
2019-01-08 15:35:00,013 - 10 - training_embed.py - training - Epoch: [1][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0234 (612.8149)	
2019-01-08 15:35:00,227 - 10 - training_embed.py - training - Epoch: [1][2600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.3802 (612.8181)	
2019-01-08 15:35:00,438 - 10 - training_embed.py - training - Epoch: [1][2700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 613.2484 (612.8085)	
2019-01-08 15:35:00,658 - 10 - training_embed.py - training - Epoch: [1][2800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.6912 (612.7922)	
2019-01-08 15:35:00,881 - 10 - training_embed.py - training - Epoch: [1][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.2354 (612.7916)	
2019-01-08 15:35:01,106 - 10 - training_embed.py - training - Epoch: [1][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.5894 (612.7715)	
2019-01-08 15:35:01,327 - 10 - training_embed.py - training - Epoch: [1][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.0245 (612.7434)	
2019-01-08 15:35:01,551 - 10 - training_embed.py - training - Epoch: [1][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.8948 (612.7449)	
2019-01-08 15:35:01,778 - 10 - training_embed.py - training - Epoch: [1][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.8771 (612.7403)	
2019-01-08 15:35:01,994 - 10 - training_embed.py - training - Epoch: [1][3400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.2255 (612.7409)	
2019-01-08 15:35:02,226 - 10 - training_embed.py - training - Epoch: [1][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.6400 (612.7413)	
2019-01-08 15:35:02,442 - 10 - training_embed.py - training - Epoch: [1][3600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 615.8621 (612.7287)	
2019-01-08 15:35:02,570 - 10 - training_embed.py - training - loss: 612.688778
2019-01-08 15:35:02,570 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 15:35:03,040 - 10 - training_embed.py - training - Epoch: [2][100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.1987 (613.3591)	
2019-01-08 15:35:03,253 - 10 - training_embed.py - training - Epoch: [2][200/3661]	Time 0.005 (0.002)	Data 0.002 (0.001)	Loss 608.6241 (612.9275)	
2019-01-08 15:35:03,474 - 10 - training_embed.py - training - Epoch: [2][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.6589 (612.7620)	
2019-01-08 15:35:03,697 - 10 - training_embed.py - training - Epoch: [2][400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 612.2639 (612.9029)	
2019-01-08 15:35:03,918 - 10 - training_embed.py - training - Epoch: [2][500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 611.3273 (612.7090)	
2019-01-08 15:35:04,145 - 10 - training_embed.py - training - Epoch: [2][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.2645 (612.7471)	
2019-01-08 15:35:04,356 - 10 - training_embed.py - training - Epoch: [2][700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.2750 (612.8010)	
2019-01-08 15:35:04,580 - 10 - training_embed.py - training - Epoch: [2][800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 618.6107 (612.7391)	
2019-01-08 15:35:04,796 - 10 - training_embed.py - training - Epoch: [2][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.2041 (612.7016)	
2019-01-08 15:35:05,019 - 10 - training_embed.py - training - Epoch: [2][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.6715 (612.6129)	
2019-01-08 15:35:05,228 - 10 - training_embed.py - training - Epoch: [2][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.6544 (612.5687)	
2019-01-08 15:35:05,446 - 10 - training_embed.py - training - Epoch: [2][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.0955 (612.5901)	
2019-01-08 15:35:05,669 - 10 - training_embed.py - training - Epoch: [2][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.1638 (612.6283)	
2019-01-08 15:35:05,893 - 10 - training_embed.py - training - Epoch: [2][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.6869 (612.5873)	
2019-01-08 15:35:06,107 - 10 - training_embed.py - training - Epoch: [2][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.1574 (612.6109)	
2019-01-08 15:35:06,327 - 10 - training_embed.py - training - Epoch: [2][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.4568 (612.5892)	
2019-01-08 15:35:06,560 - 10 - training_embed.py - training - Epoch: [2][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.9290 (612.6274)	
2019-01-08 15:35:06,768 - 10 - training_embed.py - training - Epoch: [2][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.6592 (612.6127)	
2019-01-08 15:35:06,989 - 10 - training_embed.py - training - Epoch: [2][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6163 (612.6010)	
2019-01-08 15:35:07,207 - 10 - training_embed.py - training - Epoch: [2][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.1298 (612.5394)	
2019-01-08 15:35:07,421 - 10 - training_embed.py - training - Epoch: [2][2100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.4890 (612.5224)	
2019-01-08 15:35:07,653 - 10 - training_embed.py - training - Epoch: [2][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.4826 (612.4874)	
2019-01-08 15:35:07,886 - 10 - training_embed.py - training - Epoch: [2][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.2563 (612.4850)	
2019-01-08 15:35:08,108 - 10 - training_embed.py - training - Epoch: [2][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.8677 (612.5008)	
2019-01-08 15:35:08,327 - 10 - training_embed.py - training - Epoch: [2][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 599.5654 (612.5042)	
2019-01-08 15:35:08,528 - 10 - training_embed.py - training - Epoch: [2][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.9689 (612.5140)	
2019-01-08 15:35:08,744 - 10 - training_embed.py - training - Epoch: [2][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.7104 (612.5181)	
2019-01-08 15:35:08,959 - 10 - training_embed.py - training - Epoch: [2][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 619.0026 (612.5125)	
2019-01-08 15:35:09,165 - 10 - training_embed.py - training - Epoch: [2][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.8583 (612.4963)	
2019-01-08 15:35:09,389 - 10 - training_embed.py - training - Epoch: [2][3000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.8734 (612.4896)	
2019-01-08 15:35:09,615 - 10 - training_embed.py - training - Epoch: [2][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.7062 (612.4834)	
2019-01-08 15:35:09,846 - 10 - training_embed.py - training - Epoch: [2][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.3450 (612.4869)	
2019-01-08 15:35:10,073 - 10 - training_embed.py - training - Epoch: [2][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.7368 (612.4933)	
2019-01-08 15:35:10,280 - 10 - training_embed.py - training - Epoch: [2][3400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 604.6837 (612.4949)	
2019-01-08 15:35:10,487 - 10 - training_embed.py - training - Epoch: [2][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.6036 (612.4802)	
2019-01-08 15:35:10,713 - 10 - training_embed.py - training - Epoch: [2][3600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 610.9382 (612.4819)	
2019-01-08 15:35:10,841 - 10 - training_embed.py - training - loss: 612.440705
2019-01-08 15:35:10,842 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 15:35:11,303 - 10 - training_embed.py - training - Epoch: [3][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.6719 (611.8858)	
2019-01-08 15:35:11,512 - 10 - training_embed.py - training - Epoch: [3][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.3206 (611.9498)	
2019-01-08 15:35:11,731 - 10 - training_embed.py - training - Epoch: [3][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.5133 (612.1744)	
2019-01-08 15:35:11,955 - 10 - training_embed.py - training - Epoch: [3][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.7138 (612.2841)	
2019-01-08 15:35:12,157 - 10 - training_embed.py - training - Epoch: [3][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.1657 (612.2482)	
2019-01-08 15:35:12,369 - 10 - training_embed.py - training - Epoch: [3][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6834 (612.2229)	
2019-01-08 15:35:12,602 - 10 - training_embed.py - training - Epoch: [3][700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.5405 (612.1890)	
2019-01-08 15:35:12,813 - 10 - training_embed.py - training - Epoch: [3][800/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 619.1320 (612.2210)	
2019-01-08 15:35:13,033 - 10 - training_embed.py - training - Epoch: [3][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2028 (612.2800)	
2019-01-08 15:35:13,239 - 10 - training_embed.py - training - Epoch: [3][1000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.6169 (612.2575)	
2019-01-08 15:35:13,467 - 10 - training_embed.py - training - Epoch: [3][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 616.3087 (612.2263)	
2019-01-08 15:35:13,697 - 10 - training_embed.py - training - Epoch: [3][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.8569 (612.1895)	
2019-01-08 15:35:13,925 - 10 - training_embed.py - training - Epoch: [3][1300/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 609.2238 (612.2068)	
2019-01-08 15:35:14,130 - 10 - training_embed.py - training - Epoch: [3][1400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.8564 (612.1631)	
2019-01-08 15:35:14,353 - 10 - training_embed.py - training - Epoch: [3][1500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 604.3228 (612.1418)	
2019-01-08 15:35:14,569 - 10 - training_embed.py - training - Epoch: [3][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.0626 (612.1794)	
2019-01-08 15:35:14,792 - 10 - training_embed.py - training - Epoch: [3][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8909 (612.1752)	
2019-01-08 15:35:15,030 - 10 - training_embed.py - training - Epoch: [3][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.4357 (612.2050)	
2019-01-08 15:35:15,260 - 10 - training_embed.py - training - Epoch: [3][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.5446 (612.2203)	
2019-01-08 15:35:15,465 - 10 - training_embed.py - training - Epoch: [3][2000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.3782 (612.2275)	
2019-01-08 15:35:15,704 - 10 - training_embed.py - training - Epoch: [3][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 619.4011 (612.2473)	
2019-01-08 15:35:15,924 - 10 - training_embed.py - training - Epoch: [3][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.8057 (612.2458)	
2019-01-08 15:35:16,137 - 10 - training_embed.py - training - Epoch: [3][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.4407 (612.2418)	
2019-01-08 15:35:16,360 - 10 - training_embed.py - training - Epoch: [3][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.7005 (612.2417)	
2019-01-08 15:35:16,582 - 10 - training_embed.py - training - Epoch: [3][2500/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 605.4006 (612.2551)	
2019-01-08 15:35:16,801 - 10 - training_embed.py - training - Epoch: [3][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.3299 (612.2672)	
2019-01-08 15:35:17,007 - 10 - training_embed.py - training - Epoch: [3][2700/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 605.9245 (612.2569)	
2019-01-08 15:35:17,231 - 10 - training_embed.py - training - Epoch: [3][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.1400 (612.2615)	
2019-01-08 15:35:17,467 - 10 - training_embed.py - training - Epoch: [3][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9216 (612.2462)	
2019-01-08 15:35:17,696 - 10 - training_embed.py - training - Epoch: [3][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.8631 (612.2240)	
2019-01-08 15:35:17,911 - 10 - training_embed.py - training - Epoch: [3][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.5370 (612.1931)	
2019-01-08 15:35:18,142 - 10 - training_embed.py - training - Epoch: [3][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.8139 (612.1903)	
2019-01-08 15:35:18,375 - 10 - training_embed.py - training - Epoch: [3][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3359 (612.1827)	
2019-01-08 15:35:18,588 - 10 - training_embed.py - training - Epoch: [3][3400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.9103 (612.1744)	
2019-01-08 15:35:18,803 - 10 - training_embed.py - training - Epoch: [3][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.2553 (612.1963)	
2019-01-08 15:35:19,029 - 10 - training_embed.py - training - Epoch: [3][3600/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 617.2166 (612.2162)	
2019-01-08 15:35:19,156 - 10 - training_embed.py - training - loss: 612.191156
2019-01-08 15:35:19,157 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 15:35:19,621 - 10 - training_embed.py - training - Epoch: [4][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.1281 (612.1098)	
2019-01-08 15:35:19,825 - 10 - training_embed.py - training - Epoch: [4][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.1621 (612.0888)	
2019-01-08 15:35:20,036 - 10 - training_embed.py - training - Epoch: [4][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.7850 (612.3613)	
2019-01-08 15:35:20,253 - 10 - training_embed.py - training - Epoch: [4][400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.8665 (612.5200)	
2019-01-08 15:35:20,460 - 10 - training_embed.py - training - Epoch: [4][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.6128 (612.4572)	
2019-01-08 15:35:20,682 - 10 - training_embed.py - training - Epoch: [4][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.8372 (612.5307)	
2019-01-08 15:35:20,918 - 10 - training_embed.py - training - Epoch: [4][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.5151 (612.4705)	
2019-01-08 15:35:21,130 - 10 - training_embed.py - training - Epoch: [4][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.1314 (612.4423)	
2019-01-08 15:35:21,358 - 10 - training_embed.py - training - Epoch: [4][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.7943 (612.4115)	
2019-01-08 15:35:21,579 - 10 - training_embed.py - training - Epoch: [4][1000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 600.7866 (612.4194)	
2019-01-08 15:35:21,793 - 10 - training_embed.py - training - Epoch: [4][1100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.7244 (612.3878)	
2019-01-08 15:35:22,011 - 10 - training_embed.py - training - Epoch: [4][1200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 610.5324 (612.3122)	
2019-01-08 15:35:22,230 - 10 - training_embed.py - training - Epoch: [4][1300/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 614.5291 (612.2570)	
2019-01-08 15:35:22,454 - 10 - training_embed.py - training - Epoch: [4][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9718 (612.2773)	
2019-01-08 15:35:22,670 - 10 - training_embed.py - training - Epoch: [4][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.5967 (612.2114)	
2019-01-08 15:35:22,892 - 10 - training_embed.py - training - Epoch: [4][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.1462 (612.1687)	
2019-01-08 15:35:23,124 - 10 - training_embed.py - training - Epoch: [4][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.0320 (612.1616)	
2019-01-08 15:35:23,346 - 10 - training_embed.py - training - Epoch: [4][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.6358 (612.1622)	
2019-01-08 15:35:23,553 - 10 - training_embed.py - training - Epoch: [4][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.5017 (612.1766)	
2019-01-08 15:35:23,789 - 10 - training_embed.py - training - Epoch: [4][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.4858 (612.1678)	
2019-01-08 15:35:24,006 - 10 - training_embed.py - training - Epoch: [4][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.2310 (612.1956)	
2019-01-08 15:35:24,219 - 10 - training_embed.py - training - Epoch: [4][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.5525 (612.1701)	
2019-01-08 15:35:24,450 - 10 - training_embed.py - training - Epoch: [4][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.6295 (612.1248)	
2019-01-08 15:35:24,676 - 10 - training_embed.py - training - Epoch: [4][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.0202 (612.1010)	
2019-01-08 15:35:24,887 - 10 - training_embed.py - training - Epoch: [4][2500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 610.5755 (612.1020)	
2019-01-08 15:35:25,096 - 10 - training_embed.py - training - Epoch: [4][2600/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 608.4884 (612.0745)	
2019-01-08 15:35:25,324 - 10 - training_embed.py - training - Epoch: [4][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.2385 (612.0499)	
2019-01-08 15:35:25,550 - 10 - training_embed.py - training - Epoch: [4][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.2960 (612.0597)	
2019-01-08 15:35:25,768 - 10 - training_embed.py - training - Epoch: [4][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.8734 (612.0537)	
2019-01-08 15:35:25,977 - 10 - training_embed.py - training - Epoch: [4][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.1627 (612.0365)	
2019-01-08 15:35:26,193 - 10 - training_embed.py - training - Epoch: [4][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.1507 (612.0226)	
2019-01-08 15:35:26,413 - 10 - training_embed.py - training - Epoch: [4][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0215 (612.0214)	
2019-01-08 15:35:26,632 - 10 - training_embed.py - training - Epoch: [4][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.0680 (612.0055)	
2019-01-08 15:35:26,860 - 10 - training_embed.py - training - Epoch: [4][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.3983 (611.9798)	
2019-01-08 15:35:27,090 - 10 - training_embed.py - training - Epoch: [4][3500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.4451 (611.9885)	
2019-01-08 15:35:27,319 - 10 - training_embed.py - training - Epoch: [4][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0245 (611.9819)	
2019-01-08 15:35:27,457 - 10 - training_embed.py - training - loss: 611.940513
2019-01-08 15:35:27,458 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 15:35:27,898 - 10 - training_embed.py - training - Epoch: [5][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9839 (611.4614)	
2019-01-08 15:35:28,109 - 10 - training_embed.py - training - Epoch: [5][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.3554 (612.1828)	
2019-01-08 15:35:28,331 - 10 - training_embed.py - training - Epoch: [5][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.3411 (612.2349)	
2019-01-08 15:35:28,539 - 10 - training_embed.py - training - Epoch: [5][400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.5181 (612.0526)	
2019-01-08 15:35:28,764 - 10 - training_embed.py - training - Epoch: [5][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.3779 (611.9173)	
2019-01-08 15:35:28,988 - 10 - training_embed.py - training - Epoch: [5][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.1802 (611.8246)	
2019-01-08 15:35:29,196 - 10 - training_embed.py - training - Epoch: [5][700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.1695 (611.7959)	
2019-01-08 15:35:29,425 - 10 - training_embed.py - training - Epoch: [5][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.4066 (611.7317)	
2019-01-08 15:35:29,632 - 10 - training_embed.py - training - Epoch: [5][900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 605.4581 (611.7526)	
2019-01-08 15:35:29,854 - 10 - training_embed.py - training - Epoch: [5][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.5984 (611.7276)	
2019-01-08 15:35:30,067 - 10 - training_embed.py - training - Epoch: [5][1100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.2565 (611.7174)	
2019-01-08 15:35:30,304 - 10 - training_embed.py - training - Epoch: [5][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.5366 (611.7199)	
2019-01-08 15:35:30,513 - 10 - training_embed.py - training - Epoch: [5][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.8889 (611.7850)	
2019-01-08 15:35:30,732 - 10 - training_embed.py - training - Epoch: [5][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.5254 (611.7261)	
2019-01-08 15:35:30,936 - 10 - training_embed.py - training - Epoch: [5][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5391 (611.7319)	
2019-01-08 15:35:31,148 - 10 - training_embed.py - training - Epoch: [5][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.4227 (611.7128)	
2019-01-08 15:35:31,366 - 10 - training_embed.py - training - Epoch: [5][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.6585 (611.6875)	
2019-01-08 15:35:31,589 - 10 - training_embed.py - training - Epoch: [5][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.1286 (611.6993)	
2019-01-08 15:35:31,792 - 10 - training_embed.py - training - Epoch: [5][1900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 615.2610 (611.6573)	
2019-01-08 15:35:32,014 - 10 - training_embed.py - training - Epoch: [5][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2974 (611.6571)	
2019-01-08 15:35:32,229 - 10 - training_embed.py - training - Epoch: [5][2100/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 612.1885 (611.6580)	
2019-01-08 15:35:32,449 - 10 - training_embed.py - training - Epoch: [5][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.7028 (611.6850)	
2019-01-08 15:35:32,665 - 10 - training_embed.py - training - Epoch: [5][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.8480 (611.7142)	
2019-01-08 15:35:32,879 - 10 - training_embed.py - training - Epoch: [5][2400/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 612.6205 (611.7074)	
2019-01-08 15:35:33,106 - 10 - training_embed.py - training - Epoch: [5][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2811 (611.7104)	
2019-01-08 15:35:33,336 - 10 - training_embed.py - training - Epoch: [5][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.8580 (611.7172)	
2019-01-08 15:35:33,568 - 10 - training_embed.py - training - Epoch: [5][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5645 (611.7129)	
2019-01-08 15:35:33,788 - 10 - training_embed.py - training - Epoch: [5][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.3629 (611.7095)	
2019-01-08 15:35:34,004 - 10 - training_embed.py - training - Epoch: [5][2900/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 613.0040 (611.7072)	
2019-01-08 15:35:34,222 - 10 - training_embed.py - training - Epoch: [5][3000/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 610.6335 (611.7239)	
2019-01-08 15:35:34,444 - 10 - training_embed.py - training - Epoch: [5][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.0975 (611.7327)	
2019-01-08 15:35:34,653 - 10 - training_embed.py - training - Epoch: [5][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.0551 (611.7256)	
2019-01-08 15:35:34,870 - 10 - training_embed.py - training - Epoch: [5][3300/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 622.5417 (611.7214)	
2019-01-08 15:35:35,103 - 10 - training_embed.py - training - Epoch: [5][3400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.1577 (611.7281)	
2019-01-08 15:35:35,312 - 10 - training_embed.py - training - Epoch: [5][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.4603 (611.7356)	
2019-01-08 15:35:35,527 - 10 - training_embed.py - training - Epoch: [5][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.2868 (611.7399)	
2019-01-08 15:35:35,659 - 10 - training_embed.py - training - loss: 611.688760
2019-01-08 15:35:35,659 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 15:35:36,073 - 10 - training_embed.py - training - Epoch: [6][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.1099 (611.4897)	
2019-01-08 15:35:36,299 - 10 - training_embed.py - training - Epoch: [6][200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.2109 (611.5712)	
2019-01-08 15:35:36,529 - 10 - training_embed.py - training - Epoch: [6][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.7021 (611.4434)	
2019-01-08 15:35:36,763 - 10 - training_embed.py - training - Epoch: [6][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.7797 (611.5076)	
2019-01-08 15:35:36,989 - 10 - training_embed.py - training - Epoch: [6][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.3477 (611.6625)	
2019-01-08 15:35:37,201 - 10 - training_embed.py - training - Epoch: [6][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.7059 (611.6070)	
2019-01-08 15:35:37,411 - 10 - training_embed.py - training - Epoch: [6][700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.4776 (611.7020)	
2019-01-08 15:35:37,633 - 10 - training_embed.py - training - Epoch: [6][800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.7962 (611.7430)	
2019-01-08 15:35:37,853 - 10 - training_embed.py - training - Epoch: [6][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.8210 (611.6804)	
2019-01-08 15:35:38,080 - 10 - training_embed.py - training - Epoch: [6][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.8135 (611.6381)	
2019-01-08 15:35:38,286 - 10 - training_embed.py - training - Epoch: [6][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.6887 (611.6803)	
2019-01-08 15:35:38,506 - 10 - training_embed.py - training - Epoch: [6][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3488 (611.6621)	
2019-01-08 15:35:38,710 - 10 - training_embed.py - training - Epoch: [6][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.6548 (611.6879)	
2019-01-08 15:35:38,924 - 10 - training_embed.py - training - Epoch: [6][1400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.5421 (611.7241)	
2019-01-08 15:35:39,138 - 10 - training_embed.py - training - Epoch: [6][1500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.2710 (611.7211)	
2019-01-08 15:35:39,361 - 10 - training_embed.py - training - Epoch: [6][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.5836 (611.6900)	
2019-01-08 15:35:39,584 - 10 - training_embed.py - training - Epoch: [6][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.2946 (611.6919)	
2019-01-08 15:35:39,796 - 10 - training_embed.py - training - Epoch: [6][1800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.4123 (611.6536)	
2019-01-08 15:35:40,019 - 10 - training_embed.py - training - Epoch: [6][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.5458 (611.6270)	
2019-01-08 15:35:40,228 - 10 - training_embed.py - training - Epoch: [6][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.2391 (611.6045)	
2019-01-08 15:35:40,452 - 10 - training_embed.py - training - Epoch: [6][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.2162 (611.5598)	
2019-01-08 15:35:40,669 - 10 - training_embed.py - training - Epoch: [6][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.4837 (611.5303)	
2019-01-08 15:35:40,899 - 10 - training_embed.py - training - Epoch: [6][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2310 (611.5404)	
2019-01-08 15:35:41,120 - 10 - training_embed.py - training - Epoch: [6][2400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 607.6868 (611.5313)	
2019-01-08 15:35:41,334 - 10 - training_embed.py - training - Epoch: [6][2500/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 607.9105 (611.5375)	
2019-01-08 15:35:41,544 - 10 - training_embed.py - training - Epoch: [6][2600/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 607.8354 (611.5364)	
2019-01-08 15:35:41,764 - 10 - training_embed.py - training - Epoch: [6][2700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.5581 (611.5428)	
2019-01-08 15:35:41,992 - 10 - training_embed.py - training - Epoch: [6][2800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.3895 (611.5311)	
2019-01-08 15:35:42,212 - 10 - training_embed.py - training - Epoch: [6][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.2388 (611.5362)	
2019-01-08 15:35:42,441 - 10 - training_embed.py - training - Epoch: [6][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.6454 (611.5076)	
2019-01-08 15:35:42,654 - 10 - training_embed.py - training - Epoch: [6][3100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 619.7976 (611.4993)	
2019-01-08 15:35:42,866 - 10 - training_embed.py - training - Epoch: [6][3200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.6713 (611.5135)	
2019-01-08 15:35:43,086 - 10 - training_embed.py - training - Epoch: [6][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.8219 (611.5273)	
2019-01-08 15:35:43,308 - 10 - training_embed.py - training - Epoch: [6][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.1655 (611.5228)	
2019-01-08 15:35:43,520 - 10 - training_embed.py - training - Epoch: [6][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.6372 (611.5031)	
2019-01-08 15:35:43,733 - 10 - training_embed.py - training - Epoch: [6][3600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.6017 (611.4847)	
2019-01-08 15:35:43,867 - 10 - training_embed.py - training - loss: 611.436300
2019-01-08 15:35:43,868 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 15:35:44,327 - 10 - training_embed.py - training - Epoch: [7][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.3958 (611.3267)	
2019-01-08 15:35:44,556 - 10 - training_embed.py - training - Epoch: [7][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.7665 (611.2261)	
2019-01-08 15:35:44,785 - 10 - training_embed.py - training - Epoch: [7][300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.4165 (611.4749)	
2019-01-08 15:35:44,997 - 10 - training_embed.py - training - Epoch: [7][400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 606.2750 (611.4497)	
2019-01-08 15:35:45,237 - 10 - training_embed.py - training - Epoch: [7][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.3933 (611.3536)	
2019-01-08 15:35:45,450 - 10 - training_embed.py - training - Epoch: [7][600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 611.1702 (611.2599)	
2019-01-08 15:35:45,672 - 10 - training_embed.py - training - Epoch: [7][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.4199 (611.1893)	
2019-01-08 15:35:45,878 - 10 - training_embed.py - training - Epoch: [7][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.9886 (611.2450)	
2019-01-08 15:35:46,100 - 10 - training_embed.py - training - Epoch: [7][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.4483 (611.1896)	
2019-01-08 15:35:46,323 - 10 - training_embed.py - training - Epoch: [7][1000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 611.8394 (611.2255)	
2019-01-08 15:35:46,556 - 10 - training_embed.py - training - Epoch: [7][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.6287 (611.3280)	
2019-01-08 15:35:46,772 - 10 - training_embed.py - training - Epoch: [7][1200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.6574 (611.3171)	
2019-01-08 15:35:46,983 - 10 - training_embed.py - training - Epoch: [7][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.2407 (611.3295)	
2019-01-08 15:35:47,210 - 10 - training_embed.py - training - Epoch: [7][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.7004 (611.2983)	
2019-01-08 15:35:47,417 - 10 - training_embed.py - training - Epoch: [7][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.2095 (611.2432)	
2019-01-08 15:35:47,642 - 10 - training_embed.py - training - Epoch: [7][1600/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 607.5617 (611.2252)	
2019-01-08 15:35:47,868 - 10 - training_embed.py - training - Epoch: [7][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.5651 (611.2833)	
2019-01-08 15:35:48,085 - 10 - training_embed.py - training - Epoch: [7][1800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.7928 (611.2866)	
2019-01-08 15:35:48,316 - 10 - training_embed.py - training - Epoch: [7][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.3796 (611.2294)	
2019-01-08 15:35:48,527 - 10 - training_embed.py - training - Epoch: [7][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.2979 (611.2088)	
2019-01-08 15:35:48,739 - 10 - training_embed.py - training - Epoch: [7][2100/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 609.5596 (611.2069)	
2019-01-08 15:35:48,951 - 10 - training_embed.py - training - Epoch: [7][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.8803 (611.2171)	
2019-01-08 15:35:49,156 - 10 - training_embed.py - training - Epoch: [7][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.5754 (611.2374)	
2019-01-08 15:35:49,375 - 10 - training_embed.py - training - Epoch: [7][2400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.3826 (611.2744)	
2019-01-08 15:35:49,587 - 10 - training_embed.py - training - Epoch: [7][2500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.6926 (611.2853)	
2019-01-08 15:35:49,813 - 10 - training_embed.py - training - Epoch: [7][2600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.0142 (611.2741)	
2019-01-08 15:35:50,047 - 10 - training_embed.py - training - Epoch: [7][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.0528 (611.2538)	
2019-01-08 15:35:50,260 - 10 - training_embed.py - training - Epoch: [7][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.8752 (611.2628)	
2019-01-08 15:35:50,478 - 10 - training_embed.py - training - Epoch: [7][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3028 (611.2543)	
2019-01-08 15:35:50,684 - 10 - training_embed.py - training - Epoch: [7][3000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.3657 (611.2681)	
2019-01-08 15:35:50,892 - 10 - training_embed.py - training - Epoch: [7][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.0697 (611.2421)	
2019-01-08 15:35:51,101 - 10 - training_embed.py - training - Epoch: [7][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.9251 (611.2387)	
2019-01-08 15:35:51,332 - 10 - training_embed.py - training - Epoch: [7][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.0186 (611.2200)	
2019-01-08 15:35:51,541 - 10 - training_embed.py - training - Epoch: [7][3400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.9058 (611.2265)	
2019-01-08 15:35:51,750 - 10 - training_embed.py - training - Epoch: [7][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3272 (611.2152)	
2019-01-08 15:35:51,966 - 10 - training_embed.py - training - Epoch: [7][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 600.0161 (611.2243)	
2019-01-08 15:35:52,101 - 10 - training_embed.py - training - loss: 611.181839
2019-01-08 15:35:52,101 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 15:35:52,555 - 10 - training_embed.py - training - Epoch: [8][100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.1702 (610.9658)	
2019-01-08 15:35:52,774 - 10 - training_embed.py - training - Epoch: [8][200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.2592 (610.9197)	
2019-01-08 15:35:53,007 - 10 - training_embed.py - training - Epoch: [8][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.7429 (610.9056)	
2019-01-08 15:35:53,226 - 10 - training_embed.py - training - Epoch: [8][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.0952 (611.0050)	
2019-01-08 15:35:53,450 - 10 - training_embed.py - training - Epoch: [8][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 619.7628 (610.8882)	
2019-01-08 15:35:53,671 - 10 - training_embed.py - training - Epoch: [8][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.8221 (610.9523)	
2019-01-08 15:35:53,885 - 10 - training_embed.py - training - Epoch: [8][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.3837 (610.9706)	
2019-01-08 15:35:54,102 - 10 - training_embed.py - training - Epoch: [8][800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.7104 (611.0227)	
2019-01-08 15:35:54,326 - 10 - training_embed.py - training - Epoch: [8][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.9722 (610.9939)	
2019-01-08 15:35:54,551 - 10 - training_embed.py - training - Epoch: [8][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.6964 (611.0384)	
2019-01-08 15:35:54,769 - 10 - training_embed.py - training - Epoch: [8][1100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 611.9672 (611.0420)	
2019-01-08 15:35:54,996 - 10 - training_embed.py - training - Epoch: [8][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.3124 (611.1481)	
2019-01-08 15:35:55,222 - 10 - training_embed.py - training - Epoch: [8][1300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 612.2009 (611.0888)	
2019-01-08 15:35:55,431 - 10 - training_embed.py - training - Epoch: [8][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.1744 (611.1025)	
2019-01-08 15:35:55,643 - 10 - training_embed.py - training - Epoch: [8][1500/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 611.5829 (611.0995)	
2019-01-08 15:35:55,865 - 10 - training_embed.py - training - Epoch: [8][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.6650 (611.1453)	
2019-01-08 15:35:56,082 - 10 - training_embed.py - training - Epoch: [8][1700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 605.5732 (611.1526)	
2019-01-08 15:35:56,312 - 10 - training_embed.py - training - Epoch: [8][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.7282 (611.1522)	
2019-01-08 15:35:56,529 - 10 - training_embed.py - training - Epoch: [8][1900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.5674 (611.1318)	
2019-01-08 15:35:56,733 - 10 - training_embed.py - training - Epoch: [8][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2646 (611.1364)	
2019-01-08 15:35:56,948 - 10 - training_embed.py - training - Epoch: [8][2100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.3488 (611.1833)	
2019-01-08 15:35:57,164 - 10 - training_embed.py - training - Epoch: [8][2200/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 608.9368 (611.1589)	
2019-01-08 15:35:57,387 - 10 - training_embed.py - training - Epoch: [8][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.5546 (611.1353)	
2019-01-08 15:35:57,611 - 10 - training_embed.py - training - Epoch: [8][2400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.6031 (611.1253)	
2019-01-08 15:35:57,823 - 10 - training_embed.py - training - Epoch: [8][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.2330 (611.1159)	
2019-01-08 15:35:58,033 - 10 - training_embed.py - training - Epoch: [8][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.4042 (611.1218)	
2019-01-08 15:35:58,255 - 10 - training_embed.py - training - Epoch: [8][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.8735 (611.1055)	
2019-01-08 15:35:58,489 - 10 - training_embed.py - training - Epoch: [8][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.7946 (611.0629)	
2019-01-08 15:35:58,714 - 10 - training_embed.py - training - Epoch: [8][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.7387 (611.0374)	
2019-01-08 15:35:58,941 - 10 - training_embed.py - training - Epoch: [8][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6491 (611.0118)	
2019-01-08 15:35:59,162 - 10 - training_embed.py - training - Epoch: [8][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.2044 (610.9990)	
2019-01-08 15:35:59,369 - 10 - training_embed.py - training - Epoch: [8][3200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.8414 (610.9827)	
2019-01-08 15:35:59,591 - 10 - training_embed.py - training - Epoch: [8][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.1766 (611.0000)	
2019-01-08 15:35:59,802 - 10 - training_embed.py - training - Epoch: [8][3400/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 608.8755 (610.9778)	
2019-01-08 15:36:00,010 - 10 - training_embed.py - training - Epoch: [8][3500/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 615.5439 (610.9706)	
2019-01-08 15:36:00,225 - 10 - training_embed.py - training - Epoch: [8][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.7195 (610.9684)	
2019-01-08 15:36:00,354 - 10 - training_embed.py - training - loss: 610.926565
2019-01-08 15:36:00,354 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 15:36:00,810 - 10 - training_embed.py - training - Epoch: [9][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.5282 (610.3387)	
2019-01-08 15:36:01,030 - 10 - training_embed.py - training - Epoch: [9][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.4510 (610.8603)	
2019-01-08 15:36:01,243 - 10 - training_embed.py - training - Epoch: [9][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.0103 (610.9533)	
2019-01-08 15:36:01,465 - 10 - training_embed.py - training - Epoch: [9][400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 617.5949 (610.8280)	
2019-01-08 15:36:01,689 - 10 - training_embed.py - training - Epoch: [9][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.4047 (610.9162)	
2019-01-08 15:36:01,913 - 10 - training_embed.py - training - Epoch: [9][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.1144 (610.9618)	
2019-01-08 15:36:02,123 - 10 - training_embed.py - training - Epoch: [9][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.9318 (610.9001)	
2019-01-08 15:36:02,354 - 10 - training_embed.py - training - Epoch: [9][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.6399 (610.8020)	
2019-01-08 15:36:02,565 - 10 - training_embed.py - training - Epoch: [9][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.8315 (610.7718)	
2019-01-08 15:36:02,800 - 10 - training_embed.py - training - Epoch: [9][1000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.1701 (610.8054)	
2019-01-08 15:36:03,006 - 10 - training_embed.py - training - Epoch: [9][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.1355 (610.7574)	
2019-01-08 15:36:03,227 - 10 - training_embed.py - training - Epoch: [9][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0573 (610.7073)	
2019-01-08 15:36:03,441 - 10 - training_embed.py - training - Epoch: [9][1300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 616.8471 (610.7069)	
2019-01-08 15:36:03,660 - 10 - training_embed.py - training - Epoch: [9][1400/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 609.2831 (610.7583)	
2019-01-08 15:36:03,874 - 10 - training_embed.py - training - Epoch: [9][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.7128 (610.7514)	
2019-01-08 15:36:04,099 - 10 - training_embed.py - training - Epoch: [9][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.3549 (610.7578)	
2019-01-08 15:36:04,322 - 10 - training_embed.py - training - Epoch: [9][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2707 (610.7311)	
2019-01-08 15:36:04,544 - 10 - training_embed.py - training - Epoch: [9][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.7375 (610.7998)	
2019-01-08 15:36:04,749 - 10 - training_embed.py - training - Epoch: [9][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.1898 (610.7858)	
2019-01-08 15:36:04,983 - 10 - training_embed.py - training - Epoch: [9][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.1135 (610.7276)	
2019-01-08 15:36:05,191 - 10 - training_embed.py - training - Epoch: [9][2100/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 609.7852 (610.7279)	
2019-01-08 15:36:05,410 - 10 - training_embed.py - training - Epoch: [9][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.0665 (610.7572)	
2019-01-08 15:36:05,610 - 10 - training_embed.py - training - Epoch: [9][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.8470 (610.7418)	
2019-01-08 15:36:05,835 - 10 - training_embed.py - training - Epoch: [9][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.9373 (610.7289)	
2019-01-08 15:36:06,054 - 10 - training_embed.py - training - Epoch: [9][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.4480 (610.7215)	
2019-01-08 15:36:06,290 - 10 - training_embed.py - training - Epoch: [9][2600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.5693 (610.7367)	
2019-01-08 15:36:06,517 - 10 - training_embed.py - training - Epoch: [9][2700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 598.9324 (610.7406)	
2019-01-08 15:36:06,727 - 10 - training_embed.py - training - Epoch: [9][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.5151 (610.7559)	
2019-01-08 15:36:06,953 - 10 - training_embed.py - training - Epoch: [9][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.9118 (610.7363)	
2019-01-08 15:36:07,171 - 10 - training_embed.py - training - Epoch: [9][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.3729 (610.7543)	
2019-01-08 15:36:07,393 - 10 - training_embed.py - training - Epoch: [9][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.8008 (610.7561)	
2019-01-08 15:36:07,611 - 10 - training_embed.py - training - Epoch: [9][3200/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 613.2297 (610.7453)	
2019-01-08 15:36:07,829 - 10 - training_embed.py - training - Epoch: [9][3300/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 603.3445 (610.7272)	
2019-01-08 15:36:08,065 - 10 - training_embed.py - training - Epoch: [9][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.0540 (610.7065)	
2019-01-08 15:36:08,286 - 10 - training_embed.py - training - Epoch: [9][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.5737 (610.7032)	
2019-01-08 15:36:08,494 - 10 - training_embed.py - training - Epoch: [9][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.3994 (610.7023)	
2019-01-08 15:36:08,624 - 10 - training_embed.py - training - loss: 610.670043
2019-01-08 15:36:08,624 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 15:36:09,089 - 10 - training_embed.py - training - Epoch: [10][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.2792 (610.5874)	
2019-01-08 15:36:09,293 - 10 - training_embed.py - training - Epoch: [10][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.3887 (610.9295)	
2019-01-08 15:36:09,500 - 10 - training_embed.py - training - Epoch: [10][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.1998 (610.6716)	
2019-01-08 15:36:09,713 - 10 - training_embed.py - training - Epoch: [10][400/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 611.1813 (610.6165)	
2019-01-08 15:36:09,934 - 10 - training_embed.py - training - Epoch: [10][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3481 (610.5650)	
2019-01-08 15:36:10,150 - 10 - training_embed.py - training - Epoch: [10][600/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 612.1130 (610.5075)	
2019-01-08 15:36:10,360 - 10 - training_embed.py - training - Epoch: [10][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.7001 (610.4400)	
2019-01-08 15:36:10,575 - 10 - training_embed.py - training - Epoch: [10][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.4365 (610.5389)	
2019-01-08 15:36:10,790 - 10 - training_embed.py - training - Epoch: [10][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.5215 (610.5221)	
2019-01-08 15:36:11,020 - 10 - training_embed.py - training - Epoch: [10][1000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.5734 (610.4807)	
2019-01-08 15:36:11,235 - 10 - training_embed.py - training - Epoch: [10][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.3885 (610.5106)	
2019-01-08 15:36:11,452 - 10 - training_embed.py - training - Epoch: [10][1200/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 617.0426 (610.5588)	
2019-01-08 15:36:11,678 - 10 - training_embed.py - training - Epoch: [10][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.3481 (610.5187)	
2019-01-08 15:36:11,884 - 10 - training_embed.py - training - Epoch: [10][1400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.9532 (610.5538)	
2019-01-08 15:36:12,097 - 10 - training_embed.py - training - Epoch: [10][1500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.6846 (610.6237)	
2019-01-08 15:36:12,313 - 10 - training_embed.py - training - Epoch: [10][1600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 611.6012 (610.5903)	
2019-01-08 15:36:12,534 - 10 - training_embed.py - training - Epoch: [10][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.0151 (610.5710)	
2019-01-08 15:36:12,756 - 10 - training_embed.py - training - Epoch: [10][1800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 615.3607 (610.5836)	
2019-01-08 15:36:12,966 - 10 - training_embed.py - training - Epoch: [10][1900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.0952 (610.6130)	
2019-01-08 15:36:13,178 - 10 - training_embed.py - training - Epoch: [10][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.5988 (610.5784)	
2019-01-08 15:36:13,395 - 10 - training_embed.py - training - Epoch: [10][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.0108 (610.5793)	
2019-01-08 15:36:13,614 - 10 - training_embed.py - training - Epoch: [10][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.2652 (610.5624)	
2019-01-08 15:36:13,825 - 10 - training_embed.py - training - Epoch: [10][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.2093 (610.5402)	
2019-01-08 15:36:14,050 - 10 - training_embed.py - training - Epoch: [10][2400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.5942 (610.5270)	
2019-01-08 15:36:14,274 - 10 - training_embed.py - training - Epoch: [10][2500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 606.3328 (610.5212)	
2019-01-08 15:36:14,489 - 10 - training_embed.py - training - Epoch: [10][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6811 (610.4960)	
2019-01-08 15:36:14,698 - 10 - training_embed.py - training - Epoch: [10][2700/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 609.1705 (610.5033)	
2019-01-08 15:36:14,920 - 10 - training_embed.py - training - Epoch: [10][2800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 608.2892 (610.5020)	
2019-01-08 15:36:15,128 - 10 - training_embed.py - training - Epoch: [10][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.7993 (610.5037)	
2019-01-08 15:36:15,341 - 10 - training_embed.py - training - Epoch: [10][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.2354 (610.5028)	
2019-01-08 15:36:15,555 - 10 - training_embed.py - training - Epoch: [10][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.2576 (610.4870)	
2019-01-08 15:36:15,788 - 10 - training_embed.py - training - Epoch: [10][3200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.4481 (610.4990)	
2019-01-08 15:36:16,008 - 10 - training_embed.py - training - Epoch: [10][3300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 616.5981 (610.4834)	
2019-01-08 15:36:16,233 - 10 - training_embed.py - training - Epoch: [10][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.0000 (610.4701)	
2019-01-08 15:36:16,441 - 10 - training_embed.py - training - Epoch: [10][3500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.3262 (610.4659)	
2019-01-08 15:36:16,656 - 10 - training_embed.py - training - Epoch: [10][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.0714 (610.4651)	
2019-01-08 15:36:16,787 - 10 - training_embed.py - training - loss: 610.412501
2019-01-08 15:36:16,787 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 15:36:17,243 - 10 - training_embed.py - training - Epoch: [11][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.7889 (610.4943)	
2019-01-08 15:36:17,470 - 10 - training_embed.py - training - Epoch: [11][200/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 614.2415 (610.1702)	
2019-01-08 15:36:17,692 - 10 - training_embed.py - training - Epoch: [11][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.6463 (610.4840)	
2019-01-08 15:36:17,908 - 10 - training_embed.py - training - Epoch: [11][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6693 (610.4798)	
2019-01-08 15:36:18,112 - 10 - training_embed.py - training - Epoch: [11][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.5580 (610.5030)	
2019-01-08 15:36:18,329 - 10 - training_embed.py - training - Epoch: [11][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.4876 (610.4033)	
2019-01-08 15:36:18,564 - 10 - training_embed.py - training - Epoch: [11][700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.2313 (610.4399)	
2019-01-08 15:36:18,781 - 10 - training_embed.py - training - Epoch: [11][800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.6577 (610.4367)	
2019-01-08 15:36:18,992 - 10 - training_embed.py - training - Epoch: [11][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.1388 (610.4548)	
2019-01-08 15:36:19,212 - 10 - training_embed.py - training - Epoch: [11][1000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.4321 (610.3971)	
2019-01-08 15:36:19,434 - 10 - training_embed.py - training - Epoch: [11][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.1925 (610.3524)	
2019-01-08 15:36:19,649 - 10 - training_embed.py - training - Epoch: [11][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.9186 (610.3746)	
2019-01-08 15:36:19,874 - 10 - training_embed.py - training - Epoch: [11][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6199 (610.3618)	
2019-01-08 15:36:20,099 - 10 - training_embed.py - training - Epoch: [11][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0512 (610.2788)	
2019-01-08 15:36:20,307 - 10 - training_embed.py - training - Epoch: [11][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.3663 (610.2867)	
2019-01-08 15:36:20,527 - 10 - training_embed.py - training - Epoch: [11][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.4584 (610.3096)	
2019-01-08 15:36:20,756 - 10 - training_embed.py - training - Epoch: [11][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.4904 (610.3179)	
2019-01-08 15:36:20,980 - 10 - training_embed.py - training - Epoch: [11][1800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.3016 (610.2730)	
2019-01-08 15:36:21,190 - 10 - training_embed.py - training - Epoch: [11][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.2462 (610.2901)	
2019-01-08 15:36:21,415 - 10 - training_embed.py - training - Epoch: [11][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.4213 (610.2674)	
2019-01-08 15:36:21,642 - 10 - training_embed.py - training - Epoch: [11][2100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.9765 (610.2624)	
2019-01-08 15:36:21,852 - 10 - training_embed.py - training - Epoch: [11][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.8891 (610.2699)	
2019-01-08 15:36:22,074 - 10 - training_embed.py - training - Epoch: [11][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.3179 (610.2765)	
2019-01-08 15:36:22,298 - 10 - training_embed.py - training - Epoch: [11][2400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.0111 (610.2890)	
2019-01-08 15:36:22,523 - 10 - training_embed.py - training - Epoch: [11][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.2228 (610.2817)	
2019-01-08 15:36:22,728 - 10 - training_embed.py - training - Epoch: [11][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.6753 (610.2776)	
2019-01-08 15:36:22,949 - 10 - training_embed.py - training - Epoch: [11][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3773 (610.2446)	
2019-01-08 15:36:23,182 - 10 - training_embed.py - training - Epoch: [11][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.8926 (610.2436)	
2019-01-08 15:36:23,396 - 10 - training_embed.py - training - Epoch: [11][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.6507 (610.2204)	
2019-01-08 15:36:23,603 - 10 - training_embed.py - training - Epoch: [11][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.2365 (610.2171)	
2019-01-08 15:36:23,831 - 10 - training_embed.py - training - Epoch: [11][3100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.7596 (610.2227)	
2019-01-08 15:36:24,052 - 10 - training_embed.py - training - Epoch: [11][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.7960 (610.2135)	
2019-01-08 15:36:24,274 - 10 - training_embed.py - training - Epoch: [11][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.5255 (610.2133)	
2019-01-08 15:36:24,480 - 10 - training_embed.py - training - Epoch: [11][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9569 (610.2077)	
2019-01-08 15:36:24,689 - 10 - training_embed.py - training - Epoch: [11][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.9525 (610.2098)	
2019-01-08 15:36:24,925 - 10 - training_embed.py - training - Epoch: [11][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.1376 (610.2022)	
2019-01-08 15:36:25,055 - 10 - training_embed.py - training - loss: 610.152636
2019-01-08 15:36:25,202 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 15:36:26,704 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1501.005 ms ~ 0.025 min ~ 1.501 sec
2019-01-08 15:36:28,706 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 3504.013 ms ~ 0.058 min ~ 3.504 sec
2019-01-08 15:36:28,706 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 15:36:28,707 - 10 - corpus.py - subactivity_sampler - 0 / 157
2019-01-08 15:36:28,707 - 10 - corpus.py - subactivity_sampler - [72161. 72147. 72142. 72128. 72113. 72099. 72084. 72070. 72060. 72049.
 72034. 72026. 72012.]
2019-01-08 15:41:20,628 - 10 - corpus.py - subactivity_sampler - 20 / 157
2019-01-08 15:41:20,629 - 10 - corpus.py - subactivity_sampler - [73886. 70260. 72647. 71350. 71749. 72241. 72023. 72673. 72238. 71709.
 71551. 72717. 72081.]
2019-01-08 15:45:43,324 - 10 - corpus.py - subactivity_sampler - 40 / 157
2019-01-08 15:45:43,324 - 10 - corpus.py - subactivity_sampler - [76203. 67397. 72819. 71973. 70410. 72530. 71886. 73906. 71698. 72069.
 70391. 74297. 71546.]
2019-01-08 15:50:46,690 - 10 - corpus.py - subactivity_sampler - 60 / 157
2019-01-08 15:50:46,690 - 10 - corpus.py - subactivity_sampler - [76993. 65404. 73436. 73356. 69253. 72076. 72786. 73812. 71453. 72404.
 69076. 75938. 71138.]
2019-01-08 15:55:36,659 - 10 - corpus.py - subactivity_sampler - 80 / 157
2019-01-08 15:55:36,660 - 10 - corpus.py - subactivity_sampler - [79422. 62511. 73503. 73967. 68636. 71962. 72480. 74817. 70977. 73083.
 67840. 77325. 70602.]
2019-01-08 16:01:03,081 - 10 - corpus.py - subactivity_sampler - 100 / 157
2019-01-08 16:01:03,082 - 10 - corpus.py - subactivity_sampler - [81045. 60504. 73709. 74602. 67650. 71313. 72872. 76219. 70322. 73416.
 65882. 79371. 70220.]
2019-01-08 16:04:42,326 - 10 - corpus.py - subactivity_sampler - 120 / 157
2019-01-08 16:04:42,327 - 10 - corpus.py - subactivity_sampler - [83621. 57475. 73955. 75047. 67828. 70425. 72810. 77364. 69616. 73347.
 63404. 82348. 69885.]
2019-01-08 16:10:36,850 - 10 - corpus.py - subactivity_sampler - 140 / 157
2019-01-08 16:10:36,850 - 10 - corpus.py - subactivity_sampler - [86943. 53820. 74085. 76588. 65886. 69986. 73213. 78612. 68858. 74131.
 60813. 85272. 68918.]
2019-01-08 16:15:24,887 - 10 - corpus.py - subactivity_sampler - [88922. 49942. 75642. 78682. 63584. 69538. 73427. 80530. 67395. 74296.
 58295. 89439. 67433.]
2019-01-08 16:15:24,887 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 2336180.550 ms ~ 38.936 min ~ 2336.181 sec
2019-01-08 16:15:24,887 - 10 - corpus.py - ordering_sampler - .
2019-01-08 16:15:35,995 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 16:15:35,996 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 7.  5.  0.  5. 88.  0.  5.  2.  0. 16. 73.  0.]
2019-01-08 16:15:35,996 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 16:15:36,046 - 10 - corpus.py - rho_sampling - ['56.5430', '33.0668', '928.1069', '81.2588', '5.4705', '162.1174', '210.0225', '45.2289', '959.9816', '134.7234', '27.2117', '272.9075']
2019-01-08 16:15:36,047 - 10 - pipeline.py - baseline - Iteration 1
2019-01-08 16:15:36,462 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 16:15:36,481 - 10 - accuracy_class.py - mof - # gt_labels: 14   # pr_labels: 13
2019-01-08 16:15:36,482 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 11', '1: 26', '2: 2', '3: 27', '4: 10', '5: 17', '6: 28', '7: 14', '8: 31', '9: 16', '10: 4', '11: 29', '12: 30']
2019-01-08 16:15:36,529 - 10 - accuracy_class.py - mof_val - frames true: 258237	frames overall : 937125
2019-01-08 16:15:36,529 - 10 - corpus.py - accuracy_corpus - Action: pancake
2019-01-08 16:15:36,529 - 10 - corpus.py - accuracy_corpus - MoF val: 0.275563025210084
2019-01-08 16:15:36,529 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.2520048019207683
2019-01-08 16:15:36,529 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35402
2019-01-08 16:15:36,529 - 10 - accuracy_class.py - mof_classes - label 2: 0.270398  9876 / 36524
2019-01-08 16:15:36,529 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1774
2019-01-08 16:15:36,529 - 10 - accuracy_class.py - mof_classes - label 10: 0.141043  1555 / 11025
2019-01-08 16:15:36,529 - 10 - accuracy_class.py - mof_classes - label 11: 0.806762  37032 / 45902
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - mof_classes - label 14: 0.135397  2393 / 17674
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 207
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - mof_classes - label 17: 0.311010  12121 / 38973
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - mof_classes - label 26: 0.305271  16499 / 54047
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - mof_classes - label 27: 0.363195  61145 / 168353
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - mof_classes - label 28: 0.259282  16243 / 62646
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - mof_classes - label 29: 0.179642  77343 / 430540
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - mof_classes - label 30: 0.841858  24030 / 28544
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - mof_classes - label 31: 0.000000  0 / 5514
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - mof_classes - average class mof: 0.258133
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35402
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 2: 0.096549  9876 / 102290
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 60069
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 10: 0.021286  1555 / 73054
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 11: 0.378681  37032 / 97792
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 14: 0.024976  2393 / 95811
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 74503
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 17: 0.125750  12121 / 96390
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 26: 0.188582  16499 / 87490
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 27: 0.328931  61145 / 185890
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 28: 0.135550  16243 / 119830
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 29: 0.174733  77343 / 442636
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 30: 0.333996  24030 / 71947
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - label 31: 0.000000  0 / 72909
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - average IoU: 0.139156
2019-01-08 16:15:36,530 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.129217
2019-01-08 16:15:51,783 - 10 - f1_score.py - f1 - f1 score: 0.291964
2019-01-08 16:15:51,815 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 15767.996 ms ~ 0.263 min ~ 15.768 sec
2019-01-08 16:15:51,815 - 10 - corpus.py - embedding_training - .
2019-01-08 16:15:51,815 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 16:15:51,815 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 16:15:51,815 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 16:15:57,545 - 10 - training_embed.py - training - create model
2019-01-08 16:15:57,546 - 10 - training_embed.py - training - epochs: 12
2019-01-08 16:15:57,546 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 16:15:57,968 - 10 - training_embed.py - training - Epoch: [0][100/3661]	Time 0.002 (0.004)	Data 0.001 (0.003)	Loss 613.4148 (613.5062)	
2019-01-08 16:15:58,191 - 10 - training_embed.py - training - Epoch: [0][200/3661]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 607.6631 (613.4078)	
2019-01-08 16:15:58,416 - 10 - training_embed.py - training - Epoch: [0][300/3661]	Time 0.001 (0.003)	Data 0.000 (0.002)	Loss 610.2602 (613.5026)	
2019-01-08 16:15:58,624 - 10 - training_embed.py - training - Epoch: [0][400/3661]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 617.1186 (613.5575)	
2019-01-08 16:15:58,847 - 10 - training_embed.py - training - Epoch: [0][500/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 604.6469 (613.6027)	
2019-01-08 16:15:59,080 - 10 - training_embed.py - training - Epoch: [0][600/3661]	Time 0.004 (0.003)	Data 0.002 (0.001)	Loss 613.8014 (613.7323)	
2019-01-08 16:15:59,333 - 10 - training_embed.py - training - Epoch: [0][700/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 614.2372 (613.7399)	
2019-01-08 16:15:59,565 - 10 - training_embed.py - training - Epoch: [0][800/3661]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 619.6215 (613.7700)	
2019-01-08 16:15:59,806 - 10 - training_embed.py - training - Epoch: [0][900/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 609.8352 (613.8031)	
2019-01-08 16:16:00,050 - 10 - training_embed.py - training - Epoch: [0][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.6195 (613.7874)	
2019-01-08 16:16:00,270 - 10 - training_embed.py - training - Epoch: [0][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.1750 (613.7211)	
2019-01-08 16:16:00,501 - 10 - training_embed.py - training - Epoch: [0][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 621.1088 (613.7230)	
2019-01-08 16:16:00,715 - 10 - training_embed.py - training - Epoch: [0][1300/3661]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 614.0688 (613.7786)	
2019-01-08 16:16:00,938 - 10 - training_embed.py - training - Epoch: [0][1400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.7950 (613.7697)	
2019-01-08 16:16:01,161 - 10 - training_embed.py - training - Epoch: [0][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 619.3793 (613.7781)	
2019-01-08 16:16:01,368 - 10 - training_embed.py - training - Epoch: [0][1600/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 613.3178 (613.7590)	
2019-01-08 16:16:01,578 - 10 - training_embed.py - training - Epoch: [0][1700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.6819 (613.7722)	
2019-01-08 16:16:01,789 - 10 - training_embed.py - training - Epoch: [0][1800/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 610.8888 (613.7591)	
2019-01-08 16:16:02,011 - 10 - training_embed.py - training - Epoch: [0][1900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 606.8793 (613.7223)	
2019-01-08 16:16:02,222 - 10 - training_embed.py - training - Epoch: [0][2000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 618.7109 (613.7085)	
2019-01-08 16:16:02,450 - 10 - training_embed.py - training - Epoch: [0][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9069 (613.7001)	
2019-01-08 16:16:02,659 - 10 - training_embed.py - training - Epoch: [0][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.0401 (613.7077)	
2019-01-08 16:16:02,874 - 10 - training_embed.py - training - Epoch: [0][2300/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 609.0447 (613.7203)	
2019-01-08 16:16:03,087 - 10 - training_embed.py - training - Epoch: [0][2400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.2367 (613.7221)	
2019-01-08 16:16:03,322 - 10 - training_embed.py - training - Epoch: [0][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.5573 (613.7246)	
2019-01-08 16:16:03,533 - 10 - training_embed.py - training - Epoch: [0][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.1249 (613.7332)	
2019-01-08 16:16:03,739 - 10 - training_embed.py - training - Epoch: [0][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 622.3594 (613.7392)	
2019-01-08 16:16:03,969 - 10 - training_embed.py - training - Epoch: [0][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0136 (613.7471)	
2019-01-08 16:16:04,170 - 10 - training_embed.py - training - Epoch: [0][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.2081 (613.7105)	
2019-01-08 16:16:04,393 - 10 - training_embed.py - training - Epoch: [0][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.6874 (613.7254)	
2019-01-08 16:16:04,598 - 10 - training_embed.py - training - Epoch: [0][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.3577 (613.7162)	
2019-01-08 16:16:04,826 - 10 - training_embed.py - training - Epoch: [0][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.4291 (613.7293)	
2019-01-08 16:16:05,035 - 10 - training_embed.py - training - Epoch: [0][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.4291 (613.7253)	
2019-01-08 16:16:05,260 - 10 - training_embed.py - training - Epoch: [0][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.1226 (613.7096)	
2019-01-08 16:16:05,482 - 10 - training_embed.py - training - Epoch: [0][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.9641 (613.7074)	
2019-01-08 16:16:05,710 - 10 - training_embed.py - training - Epoch: [0][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.7465 (613.7097)	
2019-01-08 16:16:05,865 - 10 - training_embed.py - training - loss: 613.655560
2019-01-08 16:16:05,865 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 16:16:06,343 - 10 - training_embed.py - training - Epoch: [1][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6251 (613.8245)	
2019-01-08 16:16:06,584 - 10 - training_embed.py - training - Epoch: [1][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.6570 (613.8324)	
2019-01-08 16:16:06,800 - 10 - training_embed.py - training - Epoch: [1][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.1365 (613.8745)	
2019-01-08 16:16:07,026 - 10 - training_embed.py - training - Epoch: [1][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.7423 (613.6204)	
2019-01-08 16:16:07,251 - 10 - training_embed.py - training - Epoch: [1][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.1092 (613.5712)	
2019-01-08 16:16:07,473 - 10 - training_embed.py - training - Epoch: [1][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.2399 (613.5774)	
2019-01-08 16:16:07,688 - 10 - training_embed.py - training - Epoch: [1][700/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 611.1996 (613.5706)	
2019-01-08 16:16:07,901 - 10 - training_embed.py - training - Epoch: [1][800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 608.7424 (613.5432)	
2019-01-08 16:16:08,115 - 10 - training_embed.py - training - Epoch: [1][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 616.1023 (613.4707)	
2019-01-08 16:16:08,326 - 10 - training_embed.py - training - Epoch: [1][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.7680 (613.5081)	
2019-01-08 16:16:08,547 - 10 - training_embed.py - training - Epoch: [1][1100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 612.6510 (613.5608)	
2019-01-08 16:16:08,752 - 10 - training_embed.py - training - Epoch: [1][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.3494 (613.5054)	
2019-01-08 16:16:08,968 - 10 - training_embed.py - training - Epoch: [1][1300/3661]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 620.6202 (613.4191)	
2019-01-08 16:16:09,184 - 10 - training_embed.py - training - Epoch: [1][1400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 615.8121 (613.4193)	
2019-01-08 16:16:09,408 - 10 - training_embed.py - training - Epoch: [1][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.6909 (613.4461)	
2019-01-08 16:16:09,616 - 10 - training_embed.py - training - Epoch: [1][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.4582 (613.4052)	
2019-01-08 16:16:09,824 - 10 - training_embed.py - training - Epoch: [1][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.9333 (613.3745)	
2019-01-08 16:16:10,048 - 10 - training_embed.py - training - Epoch: [1][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.0156 (613.3925)	
2019-01-08 16:16:10,254 - 10 - training_embed.py - training - Epoch: [1][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.7493 (613.3733)	
2019-01-08 16:16:10,475 - 10 - training_embed.py - training - Epoch: [1][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.1438 (613.3696)	
2019-01-08 16:16:10,680 - 10 - training_embed.py - training - Epoch: [1][2100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.7339 (613.4098)	
2019-01-08 16:16:10,908 - 10 - training_embed.py - training - Epoch: [1][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.3428 (613.3673)	
2019-01-08 16:16:11,123 - 10 - training_embed.py - training - Epoch: [1][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.3204 (613.3344)	
2019-01-08 16:16:11,345 - 10 - training_embed.py - training - Epoch: [1][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6827 (613.3276)	
2019-01-08 16:16:11,572 - 10 - training_embed.py - training - Epoch: [1][2500/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 615.8214 (613.3207)	
2019-01-08 16:16:11,789 - 10 - training_embed.py - training - Epoch: [1][2600/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 613.9057 (613.2936)	
2019-01-08 16:16:12,020 - 10 - training_embed.py - training - Epoch: [1][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.8996 (613.2837)	
2019-01-08 16:16:12,236 - 10 - training_embed.py - training - Epoch: [1][2800/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 613.8801 (613.2787)	
2019-01-08 16:16:12,460 - 10 - training_embed.py - training - Epoch: [1][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.3076 (613.2757)	
2019-01-08 16:16:12,669 - 10 - training_embed.py - training - Epoch: [1][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.2811 (613.2461)	
2019-01-08 16:16:12,892 - 10 - training_embed.py - training - Epoch: [1][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.7495 (613.2205)	
2019-01-08 16:16:13,113 - 10 - training_embed.py - training - Epoch: [1][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.5960 (613.2155)	
2019-01-08 16:16:13,320 - 10 - training_embed.py - training - Epoch: [1][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.6116 (613.2036)	
2019-01-08 16:16:13,537 - 10 - training_embed.py - training - Epoch: [1][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.0188 (613.2114)	
2019-01-08 16:16:13,755 - 10 - training_embed.py - training - Epoch: [1][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 619.9566 (613.2323)	
2019-01-08 16:16:13,978 - 10 - training_embed.py - training - Epoch: [1][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.5568 (613.2318)	
2019-01-08 16:16:14,113 - 10 - training_embed.py - training - loss: 613.198071
2019-01-08 16:16:14,114 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 16:16:14,507 - 10 - training_embed.py - training - Epoch: [2][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.0831 (613.2702)	
2019-01-08 16:16:14,717 - 10 - training_embed.py - training - Epoch: [2][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.7887 (613.0193)	
2019-01-08 16:16:14,922 - 10 - training_embed.py - training - Epoch: [2][300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.5829 (612.8980)	
2019-01-08 16:16:15,131 - 10 - training_embed.py - training - Epoch: [2][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.7256 (613.0027)	
2019-01-08 16:16:15,351 - 10 - training_embed.py - training - Epoch: [2][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.0152 (612.8994)	
2019-01-08 16:16:15,565 - 10 - training_embed.py - training - Epoch: [2][600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.3836 (612.9163)	
2019-01-08 16:16:15,781 - 10 - training_embed.py - training - Epoch: [2][700/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 613.5092 (612.9753)	
2019-01-08 16:16:15,996 - 10 - training_embed.py - training - Epoch: [2][800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 617.0126 (612.9512)	
2019-01-08 16:16:16,206 - 10 - training_embed.py - training - Epoch: [2][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.7347 (612.9490)	
2019-01-08 16:16:16,428 - 10 - training_embed.py - training - Epoch: [2][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.0144 (612.8388)	
2019-01-08 16:16:16,645 - 10 - training_embed.py - training - Epoch: [2][1100/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 616.1934 (612.8064)	
2019-01-08 16:16:16,892 - 10 - training_embed.py - training - Epoch: [2][1200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.9233 (612.8389)	
2019-01-08 16:16:17,109 - 10 - training_embed.py - training - Epoch: [2][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6227 (612.8339)	
2019-01-08 16:16:17,353 - 10 - training_embed.py - training - Epoch: [2][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.7057 (612.8128)	
2019-01-08 16:16:17,638 - 10 - training_embed.py - training - Epoch: [2][1500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 598.8831 (612.8371)	
2019-01-08 16:16:17,887 - 10 - training_embed.py - training - Epoch: [2][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.5359 (612.8015)	
2019-01-08 16:16:18,114 - 10 - training_embed.py - training - Epoch: [2][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3040 (612.8326)	
2019-01-08 16:16:18,342 - 10 - training_embed.py - training - Epoch: [2][1800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 612.9910 (612.8398)	
2019-01-08 16:16:18,625 - 10 - training_embed.py - training - Epoch: [2][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2822 (612.8232)	
2019-01-08 16:16:18,868 - 10 - training_embed.py - training - Epoch: [2][2000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 613.0049 (612.7875)	
2019-01-08 16:16:19,149 - 10 - training_embed.py - training - Epoch: [2][2100/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 606.9169 (612.7928)	
2019-01-08 16:16:19,425 - 10 - training_embed.py - training - Epoch: [2][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.5593 (612.7526)	
2019-01-08 16:16:19,701 - 10 - training_embed.py - training - Epoch: [2][2300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 603.5026 (612.7376)	
2019-01-08 16:16:19,943 - 10 - training_embed.py - training - Epoch: [2][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.3273 (612.7464)	
2019-01-08 16:16:20,163 - 10 - training_embed.py - training - Epoch: [2][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.7859 (612.7673)	
2019-01-08 16:16:20,398 - 10 - training_embed.py - training - Epoch: [2][2600/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 603.9998 (612.7787)	
2019-01-08 16:16:20,672 - 10 - training_embed.py - training - Epoch: [2][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.7796 (612.7731)	
2019-01-08 16:16:20,888 - 10 - training_embed.py - training - Epoch: [2][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.7175 (612.7814)	
2019-01-08 16:16:21,120 - 10 - training_embed.py - training - Epoch: [2][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.5103 (612.7756)	
2019-01-08 16:16:21,361 - 10 - training_embed.py - training - Epoch: [2][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3052 (612.7694)	
2019-01-08 16:16:21,593 - 10 - training_embed.py - training - Epoch: [2][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.2817 (612.7688)	
2019-01-08 16:16:21,829 - 10 - training_embed.py - training - Epoch: [2][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.3742 (612.7735)	
2019-01-08 16:16:22,067 - 10 - training_embed.py - training - Epoch: [2][3300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 621.1590 (612.8023)	
2019-01-08 16:16:22,272 - 10 - training_embed.py - training - Epoch: [2][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.6411 (612.8015)	
2019-01-08 16:16:22,503 - 10 - training_embed.py - training - Epoch: [2][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.6904 (612.7917)	
2019-01-08 16:16:22,738 - 10 - training_embed.py - training - Epoch: [2][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.6943 (612.7839)	
2019-01-08 16:16:22,866 - 10 - training_embed.py - training - loss: 612.739669
2019-01-08 16:16:22,866 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 16:16:23,343 - 10 - training_embed.py - training - Epoch: [3][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.9214 (612.3046)	
2019-01-08 16:16:23,568 - 10 - training_embed.py - training - Epoch: [3][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.5443 (612.3759)	
2019-01-08 16:16:23,786 - 10 - training_embed.py - training - Epoch: [3][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5176 (612.4056)	
2019-01-08 16:16:24,014 - 10 - training_embed.py - training - Epoch: [3][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.2793 (612.5237)	
2019-01-08 16:16:24,224 - 10 - training_embed.py - training - Epoch: [3][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.6230 (612.4531)	
2019-01-08 16:16:24,447 - 10 - training_embed.py - training - Epoch: [3][600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.4952 (612.3727)	
2019-01-08 16:16:24,674 - 10 - training_embed.py - training - Epoch: [3][700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.1434 (612.4004)	
2019-01-08 16:16:24,892 - 10 - training_embed.py - training - Epoch: [3][800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 612.9279 (612.4504)	
2019-01-08 16:16:25,140 - 10 - training_embed.py - training - Epoch: [3][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3049 (612.4956)	
2019-01-08 16:16:25,362 - 10 - training_embed.py - training - Epoch: [3][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.1107 (612.4529)	
2019-01-08 16:16:25,584 - 10 - training_embed.py - training - Epoch: [3][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 619.6440 (612.4598)	
2019-01-08 16:16:25,803 - 10 - training_embed.py - training - Epoch: [3][1200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 615.9892 (612.4233)	
2019-01-08 16:16:26,035 - 10 - training_embed.py - training - Epoch: [3][1300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.4518 (612.4582)	
2019-01-08 16:16:26,268 - 10 - training_embed.py - training - Epoch: [3][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.5651 (612.4232)	
2019-01-08 16:16:26,481 - 10 - training_embed.py - training - Epoch: [3][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.3616 (612.4041)	
2019-01-08 16:16:26,700 - 10 - training_embed.py - training - Epoch: [3][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.0273 (612.4304)	
2019-01-08 16:16:26,918 - 10 - training_embed.py - training - Epoch: [3][1700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.9348 (612.4463)	
2019-01-08 16:16:27,127 - 10 - training_embed.py - training - Epoch: [3][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.2474 (612.4643)	
2019-01-08 16:16:27,346 - 10 - training_embed.py - training - Epoch: [3][1900/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 612.3373 (612.4626)	
2019-01-08 16:16:27,587 - 10 - training_embed.py - training - Epoch: [3][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.1312 (612.4488)	
2019-01-08 16:16:27,809 - 10 - training_embed.py - training - Epoch: [3][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 621.8992 (612.4453)	
2019-01-08 16:16:28,020 - 10 - training_embed.py - training - Epoch: [3][2200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.7882 (612.4166)	
2019-01-08 16:16:28,253 - 10 - training_embed.py - training - Epoch: [3][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.4526 (612.3932)	
2019-01-08 16:16:28,475 - 10 - training_embed.py - training - Epoch: [3][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.7172 (612.3851)	
2019-01-08 16:16:28,740 - 10 - training_embed.py - training - Epoch: [3][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.4685 (612.3769)	
2019-01-08 16:16:28,998 - 10 - training_embed.py - training - Epoch: [3][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.3835 (612.3767)	
2019-01-08 16:16:29,250 - 10 - training_embed.py - training - Epoch: [3][2700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.7933 (612.3718)	
2019-01-08 16:16:29,523 - 10 - training_embed.py - training - Epoch: [3][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.1910 (612.3794)	
2019-01-08 16:16:29,766 - 10 - training_embed.py - training - Epoch: [3][2900/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 608.3898 (612.3590)	
2019-01-08 16:16:30,055 - 10 - training_embed.py - training - Epoch: [3][3000/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 606.4818 (612.3150)	
2019-01-08 16:16:30,332 - 10 - training_embed.py - training - Epoch: [3][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.8024 (612.2912)	
2019-01-08 16:16:30,580 - 10 - training_embed.py - training - Epoch: [3][3200/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 607.8959 (612.2790)	
2019-01-08 16:16:30,825 - 10 - training_embed.py - training - Epoch: [3][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.6866 (612.2715)	
2019-01-08 16:16:31,052 - 10 - training_embed.py - training - Epoch: [3][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.4245 (612.2611)	
2019-01-08 16:16:31,270 - 10 - training_embed.py - training - Epoch: [3][3500/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 603.9488 (612.2870)	
2019-01-08 16:16:31,506 - 10 - training_embed.py - training - Epoch: [3][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.1459 (612.3025)	
2019-01-08 16:16:31,654 - 10 - training_embed.py - training - loss: 612.280612
2019-01-08 16:16:31,655 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 16:16:32,131 - 10 - training_embed.py - training - Epoch: [4][100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 616.5206 (611.7888)	
2019-01-08 16:16:32,335 - 10 - training_embed.py - training - Epoch: [4][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.0167 (612.0019)	
2019-01-08 16:16:32,549 - 10 - training_embed.py - training - Epoch: [4][300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 615.4304 (612.2423)	
2019-01-08 16:16:32,774 - 10 - training_embed.py - training - Epoch: [4][400/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 609.8846 (612.4008)	
2019-01-08 16:16:33,026 - 10 - training_embed.py - training - Epoch: [4][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.0528 (612.2559)	
2019-01-08 16:16:33,263 - 10 - training_embed.py - training - Epoch: [4][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.1649 (612.3208)	
2019-01-08 16:16:33,530 - 10 - training_embed.py - training - Epoch: [4][700/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 610.6636 (612.2385)	
2019-01-08 16:16:33,766 - 10 - training_embed.py - training - Epoch: [4][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.4816 (612.1804)	
2019-01-08 16:16:33,980 - 10 - training_embed.py - training - Epoch: [4][900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.1109 (612.1164)	
2019-01-08 16:16:34,205 - 10 - training_embed.py - training - Epoch: [4][1000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 603.7776 (612.1311)	
2019-01-08 16:16:34,424 - 10 - training_embed.py - training - Epoch: [4][1100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 617.0262 (612.1157)	
2019-01-08 16:16:34,637 - 10 - training_embed.py - training - Epoch: [4][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.9415 (612.0717)	
2019-01-08 16:16:34,851 - 10 - training_embed.py - training - Epoch: [4][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0762 (612.0203)	
2019-01-08 16:16:35,054 - 10 - training_embed.py - training - Epoch: [4][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.1861 (612.0309)	
2019-01-08 16:16:35,273 - 10 - training_embed.py - training - Epoch: [4][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.8591 (611.9688)	
2019-01-08 16:16:35,490 - 10 - training_embed.py - training - Epoch: [4][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.2840 (611.9554)	
2019-01-08 16:16:35,705 - 10 - training_embed.py - training - Epoch: [4][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.1213 (611.9671)	
2019-01-08 16:16:35,931 - 10 - training_embed.py - training - Epoch: [4][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.4099 (611.9659)	
2019-01-08 16:16:36,149 - 10 - training_embed.py - training - Epoch: [4][1900/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 614.0798 (611.9728)	
2019-01-08 16:16:36,376 - 10 - training_embed.py - training - Epoch: [4][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2783 (611.9753)	
2019-01-08 16:16:36,626 - 10 - training_embed.py - training - Epoch: [4][2100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.7693 (612.0070)	
2019-01-08 16:16:36,856 - 10 - training_embed.py - training - Epoch: [4][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.9299 (611.9759)	
2019-01-08 16:16:37,079 - 10 - training_embed.py - training - Epoch: [4][2300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.1511 (611.9210)	
2019-01-08 16:16:37,314 - 10 - training_embed.py - training - Epoch: [4][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6923 (611.9146)	
2019-01-08 16:16:37,548 - 10 - training_embed.py - training - Epoch: [4][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3233 (611.9134)	
2019-01-08 16:16:37,786 - 10 - training_embed.py - training - Epoch: [4][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3502 (611.8919)	
2019-01-08 16:16:38,010 - 10 - training_embed.py - training - Epoch: [4][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.3684 (611.8741)	
2019-01-08 16:16:38,259 - 10 - training_embed.py - training - Epoch: [4][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.1948 (611.8847)	
2019-01-08 16:16:38,476 - 10 - training_embed.py - training - Epoch: [4][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.5259 (611.8862)	
2019-01-08 16:16:38,703 - 10 - training_embed.py - training - Epoch: [4][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.4524 (611.8747)	
2019-01-08 16:16:38,916 - 10 - training_embed.py - training - Epoch: [4][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0253 (611.8618)	
2019-01-08 16:16:39,135 - 10 - training_embed.py - training - Epoch: [4][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8525 (611.8730)	
2019-01-08 16:16:39,355 - 10 - training_embed.py - training - Epoch: [4][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.2401 (611.8605)	
2019-01-08 16:16:39,571 - 10 - training_embed.py - training - Epoch: [4][3400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.4428 (611.8476)	
2019-01-08 16:16:39,794 - 10 - training_embed.py - training - Epoch: [4][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.1866 (611.8670)	
2019-01-08 16:16:40,009 - 10 - training_embed.py - training - Epoch: [4][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.6836 (611.8636)	
2019-01-08 16:16:40,144 - 10 - training_embed.py - training - loss: 611.819420
2019-01-08 16:16:40,145 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 16:16:40,608 - 10 - training_embed.py - training - Epoch: [5][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.2191 (611.3056)	
2019-01-08 16:16:40,825 - 10 - training_embed.py - training - Epoch: [5][200/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 618.1279 (611.8877)	
2019-01-08 16:16:41,025 - 10 - training_embed.py - training - Epoch: [5][300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.9747 (611.9251)	
2019-01-08 16:16:41,256 - 10 - training_embed.py - training - Epoch: [5][400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.1263 (611.9189)	
2019-01-08 16:16:41,488 - 10 - training_embed.py - training - Epoch: [5][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.4877 (611.7733)	
2019-01-08 16:16:41,705 - 10 - training_embed.py - training - Epoch: [5][600/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 615.6053 (611.6450)	
2019-01-08 16:16:41,925 - 10 - training_embed.py - training - Epoch: [5][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.1362 (611.6405)	
2019-01-08 16:16:42,138 - 10 - training_embed.py - training - Epoch: [5][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.6286 (611.5074)	
2019-01-08 16:16:42,374 - 10 - training_embed.py - training - Epoch: [5][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.6486 (611.5477)	
2019-01-08 16:16:42,600 - 10 - training_embed.py - training - Epoch: [5][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.1099 (611.5198)	
2019-01-08 16:16:42,815 - 10 - training_embed.py - training - Epoch: [5][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.9216 (611.5209)	
2019-01-08 16:16:43,036 - 10 - training_embed.py - training - Epoch: [5][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.2077 (611.4891)	
2019-01-08 16:16:43,247 - 10 - training_embed.py - training - Epoch: [5][1300/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 617.7337 (611.5204)	
2019-01-08 16:16:43,467 - 10 - training_embed.py - training - Epoch: [5][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.4631 (611.4739)	
2019-01-08 16:16:43,684 - 10 - training_embed.py - training - Epoch: [5][1500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 617.3860 (611.4966)	
2019-01-08 16:16:43,925 - 10 - training_embed.py - training - Epoch: [5][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 616.4804 (611.4917)	
2019-01-08 16:16:44,153 - 10 - training_embed.py - training - Epoch: [5][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.8768 (611.4510)	
2019-01-08 16:16:44,378 - 10 - training_embed.py - training - Epoch: [5][1800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.9536 (611.4804)	
2019-01-08 16:16:44,600 - 10 - training_embed.py - training - Epoch: [5][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.7363 (611.4335)	
2019-01-08 16:16:44,818 - 10 - training_embed.py - training - Epoch: [5][2000/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 606.0217 (611.4465)	
2019-01-08 16:16:45,051 - 10 - training_embed.py - training - Epoch: [5][2100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 611.2571 (611.4333)	
2019-01-08 16:16:45,284 - 10 - training_embed.py - training - Epoch: [5][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.4833 (611.4487)	
2019-01-08 16:16:45,496 - 10 - training_embed.py - training - Epoch: [5][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.3858 (611.4638)	
2019-01-08 16:16:45,711 - 10 - training_embed.py - training - Epoch: [5][2400/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 608.7333 (611.4560)	
2019-01-08 16:16:45,935 - 10 - training_embed.py - training - Epoch: [5][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.4288 (611.4519)	
2019-01-08 16:16:46,147 - 10 - training_embed.py - training - Epoch: [5][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.2441 (611.4406)	
2019-01-08 16:16:46,367 - 10 - training_embed.py - training - Epoch: [5][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.3273 (611.4313)	
2019-01-08 16:16:46,575 - 10 - training_embed.py - training - Epoch: [5][2800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 617.8809 (611.4361)	
2019-01-08 16:16:46,805 - 10 - training_embed.py - training - Epoch: [5][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.7994 (611.4404)	
2019-01-08 16:16:47,022 - 10 - training_embed.py - training - Epoch: [5][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.4679 (611.4370)	
2019-01-08 16:16:47,243 - 10 - training_embed.py - training - Epoch: [5][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.2017 (611.4341)	
2019-01-08 16:16:47,470 - 10 - training_embed.py - training - Epoch: [5][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.3917 (611.4161)	
2019-01-08 16:16:47,692 - 10 - training_embed.py - training - Epoch: [5][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.3886 (611.4026)	
2019-01-08 16:16:47,922 - 10 - training_embed.py - training - Epoch: [5][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.5681 (611.3975)	
2019-01-08 16:16:48,130 - 10 - training_embed.py - training - Epoch: [5][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.8283 (611.4115)	
2019-01-08 16:16:48,355 - 10 - training_embed.py - training - Epoch: [5][3600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.0167 (611.3916)	
2019-01-08 16:16:48,493 - 10 - training_embed.py - training - loss: 611.356601
2019-01-08 16:16:48,493 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 16:16:48,944 - 10 - training_embed.py - training - Epoch: [6][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2676 (610.5395)	
2019-01-08 16:16:49,156 - 10 - training_embed.py - training - Epoch: [6][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.7357 (610.7780)	
2019-01-08 16:16:49,377 - 10 - training_embed.py - training - Epoch: [6][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.4244 (610.7995)	
2019-01-08 16:16:49,607 - 10 - training_embed.py - training - Epoch: [6][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.1880 (610.8894)	
2019-01-08 16:16:49,825 - 10 - training_embed.py - training - Epoch: [6][500/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 613.1755 (611.0668)	
2019-01-08 16:16:50,042 - 10 - training_embed.py - training - Epoch: [6][600/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 607.2145 (611.0357)	
2019-01-08 16:16:50,265 - 10 - training_embed.py - training - Epoch: [6][700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 612.3870 (611.1372)	
2019-01-08 16:16:50,484 - 10 - training_embed.py - training - Epoch: [6][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.2230 (611.1691)	
2019-01-08 16:16:50,720 - 10 - training_embed.py - training - Epoch: [6][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.7728 (611.1424)	
2019-01-08 16:16:50,953 - 10 - training_embed.py - training - Epoch: [6][1000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.4666 (611.1359)	
2019-01-08 16:16:51,188 - 10 - training_embed.py - training - Epoch: [6][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.4780 (611.1728)	
2019-01-08 16:16:51,417 - 10 - training_embed.py - training - Epoch: [6][1200/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 615.7493 (611.1335)	
2019-01-08 16:16:51,637 - 10 - training_embed.py - training - Epoch: [6][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.0719 (611.1404)	
2019-01-08 16:16:51,842 - 10 - training_embed.py - training - Epoch: [6][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.0970 (611.1862)	
2019-01-08 16:16:52,061 - 10 - training_embed.py - training - Epoch: [6][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.0295 (611.1829)	
2019-01-08 16:16:52,265 - 10 - training_embed.py - training - Epoch: [6][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.8313 (611.1222)	
2019-01-08 16:16:52,495 - 10 - training_embed.py - training - Epoch: [6][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.9602 (611.1349)	
2019-01-08 16:16:52,718 - 10 - training_embed.py - training - Epoch: [6][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.3981 (611.0907)	
2019-01-08 16:16:52,952 - 10 - training_embed.py - training - Epoch: [6][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.9294 (611.0733)	
2019-01-08 16:16:53,166 - 10 - training_embed.py - training - Epoch: [6][2000/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 613.8201 (611.0461)	
2019-01-08 16:16:53,397 - 10 - training_embed.py - training - Epoch: [6][2100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 611.3470 (610.9938)	
2019-01-08 16:16:53,616 - 10 - training_embed.py - training - Epoch: [6][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.8848 (610.9705)	
2019-01-08 16:16:53,844 - 10 - training_embed.py - training - Epoch: [6][2300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 611.5145 (610.9771)	
2019-01-08 16:16:54,064 - 10 - training_embed.py - training - Epoch: [6][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.6105 (610.9667)	
2019-01-08 16:16:54,307 - 10 - training_embed.py - training - Epoch: [6][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.6295 (610.9756)	
2019-01-08 16:16:54,542 - 10 - training_embed.py - training - Epoch: [6][2600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.2816 (610.9728)	
2019-01-08 16:16:54,743 - 10 - training_embed.py - training - Epoch: [6][2700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 615.0126 (610.9676)	
2019-01-08 16:16:54,963 - 10 - training_embed.py - training - Epoch: [6][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.7090 (610.9723)	
2019-01-08 16:16:55,173 - 10 - training_embed.py - training - Epoch: [6][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.8749 (610.9672)	
2019-01-08 16:16:55,390 - 10 - training_embed.py - training - Epoch: [6][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.5957 (610.9612)	
2019-01-08 16:16:55,607 - 10 - training_embed.py - training - Epoch: [6][3100/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 620.5279 (610.9505)	
2019-01-08 16:16:55,836 - 10 - training_embed.py - training - Epoch: [6][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.2198 (610.9574)	
2019-01-08 16:16:56,068 - 10 - training_embed.py - training - Epoch: [6][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.9476 (610.9709)	
2019-01-08 16:16:56,292 - 10 - training_embed.py - training - Epoch: [6][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3723 (610.9636)	
2019-01-08 16:16:56,515 - 10 - training_embed.py - training - Epoch: [6][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.7667 (610.9542)	
2019-01-08 16:16:56,734 - 10 - training_embed.py - training - Epoch: [6][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.9265 (610.9366)	
2019-01-08 16:16:56,879 - 10 - training_embed.py - training - loss: 610.892549
2019-01-08 16:16:56,881 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 16:16:57,400 - 10 - training_embed.py - training - Epoch: [7][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3624 (610.9088)	
2019-01-08 16:16:57,613 - 10 - training_embed.py - training - Epoch: [7][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.1113 (610.6233)	
2019-01-08 16:16:57,832 - 10 - training_embed.py - training - Epoch: [7][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.9961 (610.8288)	
2019-01-08 16:16:58,047 - 10 - training_embed.py - training - Epoch: [7][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.2827 (610.8450)	
2019-01-08 16:16:58,263 - 10 - training_embed.py - training - Epoch: [7][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.8283 (610.7066)	
2019-01-08 16:16:58,479 - 10 - training_embed.py - training - Epoch: [7][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.7526 (610.5838)	
2019-01-08 16:16:58,698 - 10 - training_embed.py - training - Epoch: [7][700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 605.2772 (610.5380)	
2019-01-08 16:16:58,909 - 10 - training_embed.py - training - Epoch: [7][800/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 602.6129 (610.5835)	
2019-01-08 16:16:59,120 - 10 - training_embed.py - training - Epoch: [7][900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.2909 (610.5056)	
2019-01-08 16:16:59,341 - 10 - training_embed.py - training - Epoch: [7][1000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.8149 (610.4742)	
2019-01-08 16:16:59,567 - 10 - training_embed.py - training - Epoch: [7][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.8973 (610.5049)	
2019-01-08 16:16:59,775 - 10 - training_embed.py - training - Epoch: [7][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.4642 (610.4354)	
2019-01-08 16:16:59,991 - 10 - training_embed.py - training - Epoch: [7][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.8620 (610.5052)	
2019-01-08 16:17:00,209 - 10 - training_embed.py - training - Epoch: [7][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.8995 (610.4790)	
2019-01-08 16:17:00,421 - 10 - training_embed.py - training - Epoch: [7][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.7977 (610.4266)	
2019-01-08 16:17:00,635 - 10 - training_embed.py - training - Epoch: [7][1600/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 605.1631 (610.4260)	
2019-01-08 16:17:00,861 - 10 - training_embed.py - training - Epoch: [7][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 616.9400 (610.5046)	
2019-01-08 16:17:01,086 - 10 - training_embed.py - training - Epoch: [7][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2023 (610.5238)	
2019-01-08 16:17:01,317 - 10 - training_embed.py - training - Epoch: [7][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.7073 (610.5049)	
2019-01-08 16:17:01,539 - 10 - training_embed.py - training - Epoch: [7][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.6999 (610.4844)	
2019-01-08 16:17:01,749 - 10 - training_embed.py - training - Epoch: [7][2100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.5659 (610.4902)	
2019-01-08 16:17:01,977 - 10 - training_embed.py - training - Epoch: [7][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5891 (610.5154)	
2019-01-08 16:17:02,186 - 10 - training_embed.py - training - Epoch: [7][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.9879 (610.5286)	
2019-01-08 16:17:02,406 - 10 - training_embed.py - training - Epoch: [7][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.8392 (610.5443)	
2019-01-08 16:17:02,631 - 10 - training_embed.py - training - Epoch: [7][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.3054 (610.5469)	
2019-01-08 16:17:02,854 - 10 - training_embed.py - training - Epoch: [7][2600/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 609.1605 (610.5349)	
2019-01-08 16:17:03,068 - 10 - training_embed.py - training - Epoch: [7][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.7278 (610.5213)	
2019-01-08 16:17:03,274 - 10 - training_embed.py - training - Epoch: [7][2800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.6587 (610.5353)	
2019-01-08 16:17:03,498 - 10 - training_embed.py - training - Epoch: [7][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.4595 (610.5276)	
2019-01-08 16:17:03,705 - 10 - training_embed.py - training - Epoch: [7][3000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 608.9992 (610.5441)	
2019-01-08 16:17:03,926 - 10 - training_embed.py - training - Epoch: [7][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.4238 (610.5154)	
2019-01-08 16:17:04,151 - 10 - training_embed.py - training - Epoch: [7][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.9999 (610.4947)	
2019-01-08 16:17:04,369 - 10 - training_embed.py - training - Epoch: [7][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.4805 (610.4866)	
2019-01-08 16:17:04,593 - 10 - training_embed.py - training - Epoch: [7][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.5456 (610.4830)	
2019-01-08 16:17:04,802 - 10 - training_embed.py - training - Epoch: [7][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.3548 (610.4649)	
2019-01-08 16:17:05,023 - 10 - training_embed.py - training - Epoch: [7][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.1748 (610.4720)	
2019-01-08 16:17:05,158 - 10 - training_embed.py - training - loss: 610.425209
2019-01-08 16:17:05,159 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 16:17:05,639 - 10 - training_embed.py - training - Epoch: [8][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0258 (610.3115)	
2019-01-08 16:17:05,850 - 10 - training_embed.py - training - Epoch: [8][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.1380 (610.2097)	
2019-01-08 16:17:06,076 - 10 - training_embed.py - training - Epoch: [8][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5922 (610.1449)	
2019-01-08 16:17:06,284 - 10 - training_embed.py - training - Epoch: [8][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.5096 (610.2497)	
2019-01-08 16:17:06,497 - 10 - training_embed.py - training - Epoch: [8][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.6340 (610.1400)	
2019-01-08 16:17:06,719 - 10 - training_embed.py - training - Epoch: [8][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.8965 (610.2108)	
2019-01-08 16:17:06,939 - 10 - training_embed.py - training - Epoch: [8][700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.7440 (610.1790)	
2019-01-08 16:17:07,159 - 10 - training_embed.py - training - Epoch: [8][800/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 605.3612 (610.2047)	
2019-01-08 16:17:07,381 - 10 - training_embed.py - training - Epoch: [8][900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 608.9898 (610.2046)	
2019-01-08 16:17:07,608 - 10 - training_embed.py - training - Epoch: [8][1000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.9830 (610.2115)	
2019-01-08 16:17:07,823 - 10 - training_embed.py - training - Epoch: [8][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.1460 (610.1874)	
2019-01-08 16:17:08,033 - 10 - training_embed.py - training - Epoch: [8][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.2151 (610.2723)	
2019-01-08 16:17:08,251 - 10 - training_embed.py - training - Epoch: [8][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.1015 (610.2115)	
2019-01-08 16:17:08,492 - 10 - training_embed.py - training - Epoch: [8][1400/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 609.2994 (610.2064)	
2019-01-08 16:17:08,721 - 10 - training_embed.py - training - Epoch: [8][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8815 (610.1651)	
2019-01-08 16:17:08,957 - 10 - training_embed.py - training - Epoch: [8][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.7812 (610.1675)	
2019-01-08 16:17:09,204 - 10 - training_embed.py - training - Epoch: [8][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.3114 (610.1549)	
2019-01-08 16:17:09,463 - 10 - training_embed.py - training - Epoch: [8][1800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 605.6033 (610.1657)	
2019-01-08 16:17:09,699 - 10 - training_embed.py - training - Epoch: [8][1900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.2560 (610.1529)	
2019-01-08 16:17:09,919 - 10 - training_embed.py - training - Epoch: [8][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3693 (610.1749)	
2019-01-08 16:17:10,149 - 10 - training_embed.py - training - Epoch: [8][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.4152 (610.2041)	
2019-01-08 16:17:10,427 - 10 - training_embed.py - training - Epoch: [8][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.1828 (610.1803)	
2019-01-08 16:17:10,744 - 10 - training_embed.py - training - Epoch: [8][2300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.9187 (610.1740)	
2019-01-08 16:17:11,261 - 10 - training_embed.py - training - Epoch: [8][2400/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 613.7663 (610.1664)	
2019-01-08 16:17:11,714 - 10 - training_embed.py - training - Epoch: [8][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.6199 (610.1611)	
2019-01-08 16:17:12,161 - 10 - training_embed.py - training - Epoch: [8][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.3979 (610.1596)	
2019-01-08 16:17:12,414 - 10 - training_embed.py - training - Epoch: [8][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.1097 (610.1307)	
2019-01-08 16:17:12,629 - 10 - training_embed.py - training - Epoch: [8][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.3288 (610.1032)	
2019-01-08 16:17:12,918 - 10 - training_embed.py - training - Epoch: [8][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.8015 (610.0786)	
2019-01-08 16:17:13,160 - 10 - training_embed.py - training - Epoch: [8][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.1920 (610.0658)	
2019-01-08 16:17:13,390 - 10 - training_embed.py - training - Epoch: [8][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.3717 (610.0526)	
2019-01-08 16:17:13,612 - 10 - training_embed.py - training - Epoch: [8][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.3474 (610.0342)	
2019-01-08 16:17:13,877 - 10 - training_embed.py - training - Epoch: [8][3300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.4642 (610.0433)	
2019-01-08 16:17:14,104 - 10 - training_embed.py - training - Epoch: [8][3400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.5364 (610.0279)	
2019-01-08 16:17:14,367 - 10 - training_embed.py - training - Epoch: [8][3500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 613.1823 (610.0010)	
2019-01-08 16:17:14,603 - 10 - training_embed.py - training - Epoch: [8][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.7256 (610.0030)	
2019-01-08 16:17:14,744 - 10 - training_embed.py - training - loss: 609.956212
2019-01-08 16:17:14,744 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 16:17:15,225 - 10 - training_embed.py - training - Epoch: [9][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3912 (609.3666)	
2019-01-08 16:17:15,484 - 10 - training_embed.py - training - Epoch: [9][200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.5430 (609.9090)	
2019-01-08 16:17:15,714 - 10 - training_embed.py - training - Epoch: [9][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.9977 (610.1160)	
2019-01-08 16:17:15,971 - 10 - training_embed.py - training - Epoch: [9][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.0345 (610.0182)	
2019-01-08 16:17:16,222 - 10 - training_embed.py - training - Epoch: [9][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.0901 (610.0217)	
2019-01-08 16:17:16,450 - 10 - training_embed.py - training - Epoch: [9][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.6286 (609.9761)	
2019-01-08 16:17:16,706 - 10 - training_embed.py - training - Epoch: [9][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.0028 (609.8922)	
2019-01-08 16:17:16,948 - 10 - training_embed.py - training - Epoch: [9][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.4857 (609.8486)	
2019-01-08 16:17:17,184 - 10 - training_embed.py - training - Epoch: [9][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.2585 (609.7511)	
2019-01-08 16:17:17,421 - 10 - training_embed.py - training - Epoch: [9][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.7465 (609.7351)	
2019-01-08 16:17:17,698 - 10 - training_embed.py - training - Epoch: [9][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.3052 (609.7031)	
2019-01-08 16:17:17,944 - 10 - training_embed.py - training - Epoch: [9][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.1728 (609.6699)	
2019-01-08 16:17:18,176 - 10 - training_embed.py - training - Epoch: [9][1300/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 610.3840 (609.6465)	
2019-01-08 16:17:18,435 - 10 - training_embed.py - training - Epoch: [9][1400/3661]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 606.4457 (609.6681)	
2019-01-08 16:17:18,657 - 10 - training_embed.py - training - Epoch: [9][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.7277 (609.6443)	
2019-01-08 16:17:18,895 - 10 - training_embed.py - training - Epoch: [9][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.3688 (609.6417)	
2019-01-08 16:17:19,106 - 10 - training_embed.py - training - Epoch: [9][1700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 610.2684 (609.6180)	
2019-01-08 16:17:19,336 - 10 - training_embed.py - training - Epoch: [9][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2639 (609.6867)	
2019-01-08 16:17:19,564 - 10 - training_embed.py - training - Epoch: [9][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6973 (609.6839)	
2019-01-08 16:17:19,785 - 10 - training_embed.py - training - Epoch: [9][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.9412 (609.6272)	
2019-01-08 16:17:20,018 - 10 - training_embed.py - training - Epoch: [9][2100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.2601 (609.6256)	
2019-01-08 16:17:20,260 - 10 - training_embed.py - training - Epoch: [9][2200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.1153 (609.6581)	
2019-01-08 16:17:20,481 - 10 - training_embed.py - training - Epoch: [9][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 599.9123 (609.6380)	
2019-01-08 16:17:20,703 - 10 - training_embed.py - training - Epoch: [9][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.9797 (609.6386)	
2019-01-08 16:17:20,986 - 10 - training_embed.py - training - Epoch: [9][2500/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 608.7257 (609.6272)	
2019-01-08 16:17:21,285 - 10 - training_embed.py - training - Epoch: [9][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.6432 (609.6265)	
2019-01-08 16:17:21,519 - 10 - training_embed.py - training - Epoch: [9][2700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 604.5482 (609.6107)	
2019-01-08 16:17:21,731 - 10 - training_embed.py - training - Epoch: [9][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.5092 (609.6286)	
2019-01-08 16:17:21,961 - 10 - training_embed.py - training - Epoch: [9][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.0278 (609.6143)	
2019-01-08 16:17:22,203 - 10 - training_embed.py - training - Epoch: [9][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.3440 (609.6223)	
2019-01-08 16:17:22,450 - 10 - training_embed.py - training - Epoch: [9][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.7192 (609.6133)	
2019-01-08 16:17:22,688 - 10 - training_embed.py - training - Epoch: [9][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.6976 (609.5903)	
2019-01-08 16:17:22,954 - 10 - training_embed.py - training - Epoch: [9][3300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.4167 (609.5771)	
2019-01-08 16:17:23,220 - 10 - training_embed.py - training - Epoch: [9][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.9347 (609.5520)	
2019-01-08 16:17:23,482 - 10 - training_embed.py - training - Epoch: [9][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.7879 (609.5395)	
2019-01-08 16:17:23,723 - 10 - training_embed.py - training - Epoch: [9][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3843 (609.5225)	
2019-01-08 16:17:23,863 - 10 - training_embed.py - training - loss: 609.483895
2019-01-08 16:17:23,863 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 16:17:24,345 - 10 - training_embed.py - training - Epoch: [10][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.5707 (609.2591)	
2019-01-08 16:17:24,586 - 10 - training_embed.py - training - Epoch: [10][200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.6526 (609.7283)	
2019-01-08 16:17:24,858 - 10 - training_embed.py - training - Epoch: [10][300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 606.9626 (609.5689)	
2019-01-08 16:17:25,099 - 10 - training_embed.py - training - Epoch: [10][400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.7191 (609.5201)	
2019-01-08 16:17:25,343 - 10 - training_embed.py - training - Epoch: [10][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.6162 (609.4145)	
2019-01-08 16:17:25,562 - 10 - training_embed.py - training - Epoch: [10][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.4659 (609.3299)	
2019-01-08 16:17:25,823 - 10 - training_embed.py - training - Epoch: [10][700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 602.6266 (609.2182)	
2019-01-08 16:17:26,091 - 10 - training_embed.py - training - Epoch: [10][800/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 607.9981 (609.2405)	
2019-01-08 16:17:26,359 - 10 - training_embed.py - training - Epoch: [10][900/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 610.5280 (609.2176)	
2019-01-08 16:17:26,615 - 10 - training_embed.py - training - Epoch: [10][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.2859 (609.1717)	
2019-01-08 16:17:26,843 - 10 - training_embed.py - training - Epoch: [10][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.9730 (609.1801)	
2019-01-08 16:17:27,064 - 10 - training_embed.py - training - Epoch: [10][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.4707 (609.1885)	
2019-01-08 16:17:27,292 - 10 - training_embed.py - training - Epoch: [10][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.9249 (609.1583)	
2019-01-08 16:17:27,506 - 10 - training_embed.py - training - Epoch: [10][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.3892 (609.1654)	
2019-01-08 16:17:27,755 - 10 - training_embed.py - training - Epoch: [10][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.4514 (609.2372)	
2019-01-08 16:17:28,001 - 10 - training_embed.py - training - Epoch: [10][1600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.7098 (609.1809)	
2019-01-08 16:17:28,240 - 10 - training_embed.py - training - Epoch: [10][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.7551 (609.1765)	
2019-01-08 16:17:28,463 - 10 - training_embed.py - training - Epoch: [10][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.3561 (609.1897)	
2019-01-08 16:17:28,675 - 10 - training_embed.py - training - Epoch: [10][1900/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 609.1584 (609.2303)	
2019-01-08 16:17:28,921 - 10 - training_embed.py - training - Epoch: [10][2000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.3621 (609.1954)	
2019-01-08 16:17:29,146 - 10 - training_embed.py - training - Epoch: [10][2100/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 607.2452 (609.1652)	
2019-01-08 16:17:29,404 - 10 - training_embed.py - training - Epoch: [10][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.1286 (609.1426)	
2019-01-08 16:17:29,657 - 10 - training_embed.py - training - Epoch: [10][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3894 (609.1348)	
2019-01-08 16:17:29,904 - 10 - training_embed.py - training - Epoch: [10][2400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.9218 (609.1269)	
2019-01-08 16:17:30,123 - 10 - training_embed.py - training - Epoch: [10][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.3640 (609.1384)	
2019-01-08 16:17:30,349 - 10 - training_embed.py - training - Epoch: [10][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.9893 (609.1077)	
2019-01-08 16:17:30,576 - 10 - training_embed.py - training - Epoch: [10][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.4080 (609.1209)	
2019-01-08 16:17:30,797 - 10 - training_embed.py - training - Epoch: [10][2800/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 606.1210 (609.1095)	
2019-01-08 16:17:31,083 - 10 - training_embed.py - training - Epoch: [10][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.5533 (609.1066)	
2019-01-08 16:17:31,337 - 10 - training_embed.py - training - Epoch: [10][3000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 616.7411 (609.1057)	
2019-01-08 16:17:31,673 - 10 - training_embed.py - training - Epoch: [10][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.9342 (609.0745)	
2019-01-08 16:17:31,975 - 10 - training_embed.py - training - Epoch: [10][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.2656 (609.0858)	
2019-01-08 16:17:32,215 - 10 - training_embed.py - training - Epoch: [10][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.5731 (609.0762)	
2019-01-08 16:17:32,451 - 10 - training_embed.py - training - Epoch: [10][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.5788 (609.0609)	
2019-01-08 16:17:32,688 - 10 - training_embed.py - training - Epoch: [10][3500/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 608.5316 (609.0706)	
2019-01-08 16:17:32,923 - 10 - training_embed.py - training - Epoch: [10][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.7043 (609.0629)	
2019-01-08 16:17:33,061 - 10 - training_embed.py - training - loss: 609.009499
2019-01-08 16:17:33,061 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 16:17:33,574 - 10 - training_embed.py - training - Epoch: [11][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.0735 (608.8794)	
2019-01-08 16:17:33,813 - 10 - training_embed.py - training - Epoch: [11][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.4407 (608.8181)	
2019-01-08 16:17:34,058 - 10 - training_embed.py - training - Epoch: [11][300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.7313 (609.2344)	
2019-01-08 16:17:34,360 - 10 - training_embed.py - training - Epoch: [11][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6590 (609.1998)	
2019-01-08 16:17:34,623 - 10 - training_embed.py - training - Epoch: [11][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.0085 (609.0958)	
2019-01-08 16:17:34,881 - 10 - training_embed.py - training - Epoch: [11][600/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 603.1579 (608.9754)	
2019-01-08 16:17:35,213 - 10 - training_embed.py - training - Epoch: [11][700/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 607.9388 (608.9969)	
2019-01-08 16:17:35,473 - 10 - training_embed.py - training - Epoch: [11][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.6204 (608.9393)	
2019-01-08 16:17:35,768 - 10 - training_embed.py - training - Epoch: [11][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.3439 (608.9006)	
2019-01-08 16:17:36,048 - 10 - training_embed.py - training - Epoch: [11][1000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 599.2003 (608.8072)	
2019-01-08 16:17:36,292 - 10 - training_embed.py - training - Epoch: [11][1100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 605.1929 (608.7556)	
2019-01-08 16:17:36,513 - 10 - training_embed.py - training - Epoch: [11][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 599.9003 (608.7510)	
2019-01-08 16:17:36,792 - 10 - training_embed.py - training - Epoch: [11][1300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.9037 (608.7580)	
2019-01-08 16:17:37,050 - 10 - training_embed.py - training - Epoch: [11][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.7097 (608.6834)	
2019-01-08 16:17:37,291 - 10 - training_embed.py - training - Epoch: [11][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.2166 (608.6863)	
2019-01-08 16:17:37,548 - 10 - training_embed.py - training - Epoch: [11][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.1491 (608.7217)	
2019-01-08 16:17:37,790 - 10 - training_embed.py - training - Epoch: [11][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.2913 (608.7041)	
2019-01-08 16:17:38,014 - 10 - training_embed.py - training - Epoch: [11][1800/3661]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 604.6652 (608.6848)	
2019-01-08 16:17:38,220 - 10 - training_embed.py - training - Epoch: [11][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.7393 (608.6874)	
2019-01-08 16:17:38,438 - 10 - training_embed.py - training - Epoch: [11][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.7549 (608.6669)	
2019-01-08 16:17:38,663 - 10 - training_embed.py - training - Epoch: [11][2100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 604.5456 (608.6543)	
2019-01-08 16:17:38,882 - 10 - training_embed.py - training - Epoch: [11][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.4444 (608.6284)	
2019-01-08 16:17:39,122 - 10 - training_embed.py - training - Epoch: [11][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.1201 (608.6390)	
2019-01-08 16:17:39,329 - 10 - training_embed.py - training - Epoch: [11][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.9429 (608.6471)	
2019-01-08 16:17:39,546 - 10 - training_embed.py - training - Epoch: [11][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.8323 (608.6599)	
2019-01-08 16:17:39,751 - 10 - training_embed.py - training - Epoch: [11][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.2062 (608.6623)	
2019-01-08 16:17:39,974 - 10 - training_embed.py - training - Epoch: [11][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.5814 (608.6474)	
2019-01-08 16:17:40,197 - 10 - training_embed.py - training - Epoch: [11][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.6577 (608.6314)	
2019-01-08 16:17:40,421 - 10 - training_embed.py - training - Epoch: [11][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.7671 (608.6067)	
2019-01-08 16:17:40,644 - 10 - training_embed.py - training - Epoch: [11][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.0456 (608.5825)	
2019-01-08 16:17:40,864 - 10 - training_embed.py - training - Epoch: [11][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.8629 (608.5873)	
2019-01-08 16:17:41,086 - 10 - training_embed.py - training - Epoch: [11][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.0309 (608.5841)	
2019-01-08 16:17:41,300 - 10 - training_embed.py - training - Epoch: [11][3300/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 609.8669 (608.6017)	
2019-01-08 16:17:41,520 - 10 - training_embed.py - training - Epoch: [11][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3732 (608.5921)	
2019-01-08 16:17:41,756 - 10 - training_embed.py - training - Epoch: [11][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.2896 (608.5796)	
2019-01-08 16:17:41,974 - 10 - training_embed.py - training - Epoch: [11][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.9399 (608.5810)	
2019-01-08 16:17:42,105 - 10 - training_embed.py - training - loss: 608.530753
2019-01-08 16:17:42,254 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 16:17:44,168 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1914.042 ms ~ 0.032 min ~ 1.914 sec
2019-01-08 16:17:46,179 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 3925.759 ms ~ 0.065 min ~ 3.926 sec
2019-01-08 16:17:46,179 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 16:17:46,180 - 10 - corpus.py - subactivity_sampler - 0 / 157
2019-01-08 16:17:46,180 - 10 - corpus.py - subactivity_sampler - [88922. 49942. 75642. 78682. 63584. 69538. 73427. 80530. 67395. 74296.
 58295. 89439. 67433.]
2019-01-08 16:22:51,766 - 10 - corpus.py - subactivity_sampler - 20 / 157
2019-01-08 16:22:51,767 - 10 - corpus.py - subactivity_sampler - [90668. 48263. 75091. 79505. 62713. 69949. 72998. 81819. 66520. 74047.
 53803. 94271. 67478.]
2019-01-08 16:27:25,763 - 10 - corpus.py - subactivity_sampler - 40 / 157
2019-01-08 16:27:25,763 - 10 - corpus.py - subactivity_sampler - [92052. 46742. 74472. 80603. 61581. 69758. 72699. 83231. 65569. 74612.
 50707. 97626. 67473.]
2019-01-08 16:32:34,404 - 10 - corpus.py - subactivity_sampler - 60 / 157
2019-01-08 16:32:34,404 - 10 - corpus.py - subactivity_sampler - [ 93726.  44997.  74024.  82471.  60033.  69367.  72413.  85539.  63937.
  74479.  45809. 102992.  67338.]
2019-01-08 16:37:28,923 - 10 - corpus.py - subactivity_sampler - 80 / 157
2019-01-08 16:37:28,924 - 10 - corpus.py - subactivity_sampler - [ 95413.  42860.  74213.  83152.  59325.  68681.  72078.  87210.  62332.
  75173.  41481. 108376.  66831.]
2019-01-08 16:42:54,243 - 10 - corpus.py - subactivity_sampler - 100 / 157
2019-01-08 16:42:54,243 - 10 - corpus.py - subactivity_sampler - [ 97099.  40735.  73894.  83987.  58043.  69193.  71649.  88722.  60338.
  76871.  36905. 113354.  66335.]
2019-01-08 16:46:32,486 - 10 - corpus.py - subactivity_sampler - 120 / 157
2019-01-08 16:46:32,487 - 10 - corpus.py - subactivity_sampler - [ 99926.  38328.  72658.  85224.  57363.  68874.  71285.  90114.  59581.
  77250.  33797. 116802.  65923.]
2019-01-08 16:52:21,658 - 10 - corpus.py - subactivity_sampler - 140 / 157
2019-01-08 16:52:21,659 - 10 - corpus.py - subactivity_sampler - [101579.  36459.  72359.  86358.  56313.  68296.  71087.  91518.  57996.
  78370.  29436. 121946.  65408.]
2019-01-08 16:57:01,868 - 10 - corpus.py - subactivity_sampler - [102679.  34908.  72220.  87415.  55011.  68221.  70275.  95238.  55085.
  78979.  25527. 127025.  64542.]
2019-01-08 16:57:01,868 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 2355688.841 ms ~ 39.261 min ~ 2355.689 sec
2019-01-08 16:57:01,868 - 10 - corpus.py - ordering_sampler - .
2019-01-08 16:57:12,374 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 16:57:12,377 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 14.  32.   0.  20. 128.   1.   3.  14.   0.   5.  39.   0.]
2019-01-08 16:57:12,377 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 16:57:12,428 - 10 - corpus.py - rho_sampling - ['52.0883', '6.3825', '927.9707', '72.1177', '2.1094', '161.2849', '2.7501', '10.8978', '940.5364', '157.8523', '51.4538', '575.5360']
2019-01-08 16:57:12,428 - 10 - pipeline.py - baseline - Iteration 2
2019-01-08 16:57:12,839 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 16:57:12,859 - 10 - accuracy_class.py - mof - # gt_labels: 14   # pr_labels: 13
2019-01-08 16:57:12,859 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 11', '1: 2', '2: 26', '3: 27', '4: 10', '5: 17', '6: 28', '7: 14', '8: 31', '9: 16', '10: 4', '11: 29', '12: 30']
2019-01-08 16:57:12,907 - 10 - accuracy_class.py - mof_val - frames true: 292864	frames overall : 937125
2019-01-08 16:57:12,907 - 10 - corpus.py - accuracy_corpus - Action: pancake
2019-01-08 16:57:12,907 - 10 - corpus.py - accuracy_corpus - MoF val: 0.31251327197545686
2019-01-08 16:57:12,907 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.31064479124983324
2019-01-08 16:57:12,907 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35402
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - label 2: 0.359627  13135 / 36524
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1774
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - label 10: 0.078186  862 / 11025
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - label 11: 0.830661  38129 / 45902
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - label 14: 0.127985  2262 / 17674
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 207
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - label 17: 0.320222  12480 / 38973
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - label 26: 0.122153  6602 / 54047
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - label 27: 0.416654  70145 / 168353
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - label 28: 0.270919  16972 / 62646
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - label 29: 0.252959  108909 / 430540
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - label 30: 0.818666  23368 / 28544
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - label 31: 0.000000  0 / 5514
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - mof_classes - average class mof: 0.257002
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35402
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 2: 0.225312  13135 / 58297
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 27301
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 10: 0.013226  862 / 65174
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 11: 0.345209  38129 / 110452
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 14: 0.020443  2262 / 110650
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 79186
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 17: 0.131765  12480 / 94714
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 26: 0.055171  6602 / 119665
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 27: 0.377890  70145 / 185623
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 28: 0.146375  16972 / 115949
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 29: 0.242745  108909 / 448656
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 30: 0.335179  23368 / 69718
2019-01-08 16:57:12,908 - 10 - accuracy_class.py - iou_classes - label 31: 0.000000  0 / 60599
2019-01-08 16:57:12,909 - 10 - accuracy_class.py - iou_classes - average IoU: 0.145639
2019-01-08 16:57:12,909 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.135237
2019-01-08 16:57:28,494 - 10 - f1_score.py - f1 - f1 score: 0.296038
2019-01-08 16:57:28,528 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 16099.686 ms ~ 0.268 min ~ 16.100 sec
2019-01-08 16:57:28,528 - 10 - corpus.py - embedding_training - .
2019-01-08 16:57:28,528 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 16:57:28,528 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 16:57:28,528 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 16:57:33,977 - 10 - training_embed.py - training - create model
2019-01-08 16:57:33,977 - 10 - training_embed.py - training - epochs: 12
2019-01-08 16:57:33,978 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 16:57:34,409 - 10 - training_embed.py - training - Epoch: [0][100/3661]	Time 0.002 (0.004)	Data 0.001 (0.003)	Loss 615.3359 (615.4953)	
2019-01-08 16:57:34,620 - 10 - training_embed.py - training - Epoch: [0][200/3661]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 610.5223 (615.3722)	
2019-01-08 16:57:34,839 - 10 - training_embed.py - training - Epoch: [0][300/3661]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 616.5883 (615.4509)	
2019-01-08 16:57:35,064 - 10 - training_embed.py - training - Epoch: [0][400/3661]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 617.8127 (615.5166)	
2019-01-08 16:57:35,273 - 10 - training_embed.py - training - Epoch: [0][500/3661]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 606.8273 (615.5130)	
2019-01-08 16:57:35,482 - 10 - training_embed.py - training - Epoch: [0][600/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 616.7007 (615.6621)	
2019-01-08 16:57:35,695 - 10 - training_embed.py - training - Epoch: [0][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.4854 (615.7045)	
2019-01-08 16:57:35,896 - 10 - training_embed.py - training - Epoch: [0][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.5911 (615.6956)	
2019-01-08 16:57:36,100 - 10 - training_embed.py - training - Epoch: [0][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.1545 (615.6717)	
2019-01-08 16:57:36,303 - 10 - training_embed.py - training - Epoch: [0][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.2291 (615.6674)	
2019-01-08 16:57:36,508 - 10 - training_embed.py - training - Epoch: [0][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.7653 (615.6224)	
2019-01-08 16:57:36,730 - 10 - training_embed.py - training - Epoch: [0][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.8063 (615.5991)	
2019-01-08 16:57:36,948 - 10 - training_embed.py - training - Epoch: [0][1300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 619.0131 (615.6889)	
2019-01-08 16:57:37,155 - 10 - training_embed.py - training - Epoch: [0][1400/3661]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 610.9185 (615.6764)	
2019-01-08 16:57:37,360 - 10 - training_embed.py - training - Epoch: [0][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.5069 (615.6445)	
2019-01-08 16:57:37,570 - 10 - training_embed.py - training - Epoch: [0][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.9405 (615.6115)	
2019-01-08 16:57:37,779 - 10 - training_embed.py - training - Epoch: [0][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.3774 (615.6095)	
2019-01-08 16:57:37,985 - 10 - training_embed.py - training - Epoch: [0][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.4874 (615.5722)	
2019-01-08 16:57:38,198 - 10 - training_embed.py - training - Epoch: [0][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.6390 (615.5296)	
2019-01-08 16:57:38,414 - 10 - training_embed.py - training - Epoch: [0][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 624.8015 (615.5089)	
2019-01-08 16:57:38,639 - 10 - training_embed.py - training - Epoch: [0][2100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.4637 (615.5085)	
2019-01-08 16:57:38,841 - 10 - training_embed.py - training - Epoch: [0][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.2507 (615.5081)	
2019-01-08 16:57:39,054 - 10 - training_embed.py - training - Epoch: [0][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.7254 (615.5164)	
2019-01-08 16:57:39,257 - 10 - training_embed.py - training - Epoch: [0][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.7081 (615.4983)	
2019-01-08 16:57:39,475 - 10 - training_embed.py - training - Epoch: [0][2500/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 616.6378 (615.4905)	
2019-01-08 16:57:39,696 - 10 - training_embed.py - training - Epoch: [0][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.5623 (615.4806)	
2019-01-08 16:57:39,910 - 10 - training_embed.py - training - Epoch: [0][2700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 622.0013 (615.4885)	
2019-01-08 16:57:40,128 - 10 - training_embed.py - training - Epoch: [0][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9494 (615.4898)	
2019-01-08 16:57:40,340 - 10 - training_embed.py - training - Epoch: [0][2900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.8817 (615.4512)	
2019-01-08 16:57:40,561 - 10 - training_embed.py - training - Epoch: [0][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.9335 (615.4627)	
2019-01-08 16:57:40,767 - 10 - training_embed.py - training - Epoch: [0][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.0049 (615.4425)	
2019-01-08 16:57:40,967 - 10 - training_embed.py - training - Epoch: [0][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5673 (615.4558)	
2019-01-08 16:57:41,177 - 10 - training_embed.py - training - Epoch: [0][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.5208 (615.4465)	
2019-01-08 16:57:41,398 - 10 - training_embed.py - training - Epoch: [0][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.7432 (615.4326)	
2019-01-08 16:57:41,627 - 10 - training_embed.py - training - Epoch: [0][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.6917 (615.4262)	
2019-01-08 16:57:41,845 - 10 - training_embed.py - training - Epoch: [0][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.0062 (615.4091)	
2019-01-08 16:57:41,971 - 10 - training_embed.py - training - loss: 615.347207
2019-01-08 16:57:41,973 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 16:57:42,406 - 10 - training_embed.py - training - Epoch: [1][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.1330 (615.3480)	
2019-01-08 16:57:42,615 - 10 - training_embed.py - training - Epoch: [1][200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.8636 (615.3507)	
2019-01-08 16:57:42,823 - 10 - training_embed.py - training - Epoch: [1][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.6982 (615.3634)	
2019-01-08 16:57:43,032 - 10 - training_embed.py - training - Epoch: [1][400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 615.1920 (615.1178)	
2019-01-08 16:57:43,244 - 10 - training_embed.py - training - Epoch: [1][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 618.8902 (615.0641)	
2019-01-08 16:57:43,449 - 10 - training_embed.py - training - Epoch: [1][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.3219 (615.0050)	
2019-01-08 16:57:43,678 - 10 - training_embed.py - training - Epoch: [1][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.8347 (614.9727)	
2019-01-08 16:57:43,883 - 10 - training_embed.py - training - Epoch: [1][800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.2976 (614.9663)	
2019-01-08 16:57:44,107 - 10 - training_embed.py - training - Epoch: [1][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.6172 (614.8409)	
2019-01-08 16:57:44,330 - 10 - training_embed.py - training - Epoch: [1][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.7579 (614.9109)	
2019-01-08 16:57:44,530 - 10 - training_embed.py - training - Epoch: [1][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.7474 (614.9388)	
2019-01-08 16:57:44,751 - 10 - training_embed.py - training - Epoch: [1][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 617.4509 (614.8772)	
2019-01-08 16:57:44,970 - 10 - training_embed.py - training - Epoch: [1][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.0124 (614.8290)	
2019-01-08 16:57:45,171 - 10 - training_embed.py - training - Epoch: [1][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.5404 (614.8275)	
2019-01-08 16:57:45,374 - 10 - training_embed.py - training - Epoch: [1][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.0408 (614.8193)	
2019-01-08 16:57:45,582 - 10 - training_embed.py - training - Epoch: [1][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.7606 (614.8130)	
2019-01-08 16:57:45,801 - 10 - training_embed.py - training - Epoch: [1][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8634 (614.7713)	
2019-01-08 16:57:46,022 - 10 - training_embed.py - training - Epoch: [1][1800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.6179 (614.7686)	
2019-01-08 16:57:46,232 - 10 - training_embed.py - training - Epoch: [1][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.1998 (614.7757)	
2019-01-08 16:57:46,435 - 10 - training_embed.py - training - Epoch: [1][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.9335 (614.7560)	
2019-01-08 16:57:46,644 - 10 - training_embed.py - training - Epoch: [1][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9239 (614.7781)	
2019-01-08 16:57:46,869 - 10 - training_embed.py - training - Epoch: [1][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.9077 (614.7344)	
2019-01-08 16:57:47,073 - 10 - training_embed.py - training - Epoch: [1][2300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 616.4996 (614.7012)	
2019-01-08 16:57:47,293 - 10 - training_embed.py - training - Epoch: [1][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.9743 (614.6890)	
2019-01-08 16:57:47,498 - 10 - training_embed.py - training - Epoch: [1][2500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 617.5350 (614.6889)	
2019-01-08 16:57:47,717 - 10 - training_embed.py - training - Epoch: [1][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.7866 (614.6688)	
2019-01-08 16:57:47,940 - 10 - training_embed.py - training - Epoch: [1][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 621.7274 (614.6404)	
2019-01-08 16:57:48,141 - 10 - training_embed.py - training - Epoch: [1][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 616.0283 (614.6282)	
2019-01-08 16:57:48,350 - 10 - training_embed.py - training - Epoch: [1][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.0942 (614.6246)	
2019-01-08 16:57:48,563 - 10 - training_embed.py - training - Epoch: [1][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.9716 (614.5903)	
2019-01-08 16:57:48,773 - 10 - training_embed.py - training - Epoch: [1][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6739 (614.5602)	
2019-01-08 16:57:48,978 - 10 - training_embed.py - training - Epoch: [1][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.1176 (614.5561)	
2019-01-08 16:57:49,188 - 10 - training_embed.py - training - Epoch: [1][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.4459 (614.5400)	
2019-01-08 16:57:49,390 - 10 - training_embed.py - training - Epoch: [1][3400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 608.9648 (614.5322)	
2019-01-08 16:57:49,618 - 10 - training_embed.py - training - Epoch: [1][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.9528 (614.5481)	
2019-01-08 16:57:49,833 - 10 - training_embed.py - training - Epoch: [1][3600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 620.8454 (614.5341)	
2019-01-08 16:57:49,964 - 10 - training_embed.py - training - loss: 614.498254
2019-01-08 16:57:49,965 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 16:57:50,436 - 10 - training_embed.py - training - Epoch: [2][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.9732 (613.8324)	
2019-01-08 16:57:50,640 - 10 - training_embed.py - training - Epoch: [2][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.1280 (613.9221)	
2019-01-08 16:57:50,852 - 10 - training_embed.py - training - Epoch: [2][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.4240 (613.7991)	
2019-01-08 16:57:51,060 - 10 - training_embed.py - training - Epoch: [2][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.1635 (613.9285)	
2019-01-08 16:57:51,274 - 10 - training_embed.py - training - Epoch: [2][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3221 (613.8147)	
2019-01-08 16:57:51,484 - 10 - training_embed.py - training - Epoch: [2][600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 618.4672 (613.8385)	
2019-01-08 16:57:51,688 - 10 - training_embed.py - training - Epoch: [2][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.6979 (613.8687)	
2019-01-08 16:57:51,900 - 10 - training_embed.py - training - Epoch: [2][800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 617.6085 (613.8710)	
2019-01-08 16:57:52,130 - 10 - training_embed.py - training - Epoch: [2][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0519 (613.8950)	
2019-01-08 16:57:52,346 - 10 - training_embed.py - training - Epoch: [2][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.5249 (613.8195)	
2019-01-08 16:57:52,557 - 10 - training_embed.py - training - Epoch: [2][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 620.4257 (613.7900)	
2019-01-08 16:57:52,780 - 10 - training_embed.py - training - Epoch: [2][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.1386 (613.8337)	
2019-01-08 16:57:52,983 - 10 - training_embed.py - training - Epoch: [2][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.2486 (613.8573)	
2019-01-08 16:57:53,186 - 10 - training_embed.py - training - Epoch: [2][1400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.8200 (613.8381)	
2019-01-08 16:57:53,420 - 10 - training_embed.py - training - Epoch: [2][1500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 601.0904 (613.8745)	
2019-01-08 16:57:53,640 - 10 - training_embed.py - training - Epoch: [2][1600/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 613.8948 (613.8754)	
2019-01-08 16:57:53,871 - 10 - training_embed.py - training - Epoch: [2][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.8771 (613.8766)	
2019-01-08 16:57:54,073 - 10 - training_embed.py - training - Epoch: [2][1800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.0215 (613.8636)	
2019-01-08 16:57:54,288 - 10 - training_embed.py - training - Epoch: [2][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.1821 (613.8314)	
2019-01-08 16:57:54,507 - 10 - training_embed.py - training - Epoch: [2][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.1545 (613.7895)	
2019-01-08 16:57:54,722 - 10 - training_embed.py - training - Epoch: [2][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.3035 (613.8037)	
2019-01-08 16:57:54,936 - 10 - training_embed.py - training - Epoch: [2][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.5674 (613.7714)	
2019-01-08 16:57:55,160 - 10 - training_embed.py - training - Epoch: [2][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.0303 (613.7456)	
2019-01-08 16:57:55,364 - 10 - training_embed.py - training - Epoch: [2][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.5462 (613.7541)	
2019-01-08 16:57:55,612 - 10 - training_embed.py - training - Epoch: [2][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.9099 (613.7692)	
2019-01-08 16:57:55,816 - 10 - training_embed.py - training - Epoch: [2][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.1197 (613.7836)	
2019-01-08 16:57:56,021 - 10 - training_embed.py - training - Epoch: [2][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.0148 (613.7783)	
2019-01-08 16:57:56,226 - 10 - training_embed.py - training - Epoch: [2][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.3749 (613.7723)	
2019-01-08 16:57:56,448 - 10 - training_embed.py - training - Epoch: [2][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 618.5287 (613.7588)	
2019-01-08 16:57:56,668 - 10 - training_embed.py - training - Epoch: [2][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.8235 (613.7513)	
2019-01-08 16:57:56,867 - 10 - training_embed.py - training - Epoch: [2][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 621.9589 (613.7438)	
2019-01-08 16:57:57,083 - 10 - training_embed.py - training - Epoch: [2][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.2772 (613.7308)	
2019-01-08 16:57:57,289 - 10 - training_embed.py - training - Epoch: [2][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 622.6522 (613.7400)	
2019-01-08 16:57:57,497 - 10 - training_embed.py - training - Epoch: [2][3400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.7097 (613.7239)	
2019-01-08 16:57:57,720 - 10 - training_embed.py - training - Epoch: [2][3500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 611.9667 (613.7139)	
2019-01-08 16:57:57,944 - 10 - training_embed.py - training - Epoch: [2][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.4402 (613.6952)	
2019-01-08 16:57:58,070 - 10 - training_embed.py - training - loss: 613.649397
2019-01-08 16:57:58,071 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 16:57:58,556 - 10 - training_embed.py - training - Epoch: [3][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.6371 (613.3199)	
2019-01-08 16:57:58,780 - 10 - training_embed.py - training - Epoch: [3][200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 619.3294 (613.3088)	
2019-01-08 16:57:59,003 - 10 - training_embed.py - training - Epoch: [3][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.1355 (613.4311)	
2019-01-08 16:57:59,231 - 10 - training_embed.py - training - Epoch: [3][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.5302 (613.4214)	
2019-01-08 16:57:59,449 - 10 - training_embed.py - training - Epoch: [3][500/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 610.7278 (613.2688)	
2019-01-08 16:57:59,660 - 10 - training_embed.py - training - Epoch: [3][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2637 (613.1595)	
2019-01-08 16:57:59,869 - 10 - training_embed.py - training - Epoch: [3][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2377 (613.1819)	
2019-01-08 16:58:00,094 - 10 - training_embed.py - training - Epoch: [3][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.3519 (613.2187)	
2019-01-08 16:58:00,299 - 10 - training_embed.py - training - Epoch: [3][900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 604.9362 (613.2167)	
2019-01-08 16:58:00,501 - 10 - training_embed.py - training - Epoch: [3][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.2795 (613.2034)	
2019-01-08 16:58:00,715 - 10 - training_embed.py - training - Epoch: [3][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 621.6323 (613.1843)	
2019-01-08 16:58:00,931 - 10 - training_embed.py - training - Epoch: [3][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.2319 (613.1434)	
2019-01-08 16:58:01,141 - 10 - training_embed.py - training - Epoch: [3][1300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 617.4567 (613.1751)	
2019-01-08 16:58:01,359 - 10 - training_embed.py - training - Epoch: [3][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.1218 (613.1370)	
2019-01-08 16:58:01,579 - 10 - training_embed.py - training - Epoch: [3][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.9072 (613.0978)	
2019-01-08 16:58:01,788 - 10 - training_embed.py - training - Epoch: [3][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.6569 (613.0955)	
2019-01-08 16:58:02,001 - 10 - training_embed.py - training - Epoch: [3][1700/3661]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 611.5208 (613.0900)	
2019-01-08 16:58:02,222 - 10 - training_embed.py - training - Epoch: [3][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.0541 (613.0925)	
2019-01-08 16:58:02,426 - 10 - training_embed.py - training - Epoch: [3][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2297 (613.0337)	
2019-01-08 16:58:02,633 - 10 - training_embed.py - training - Epoch: [3][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.2137 (613.0330)	
2019-01-08 16:58:02,852 - 10 - training_embed.py - training - Epoch: [3][2100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 617.7873 (613.0247)	
2019-01-08 16:58:03,080 - 10 - training_embed.py - training - Epoch: [3][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.9196 (613.0154)	
2019-01-08 16:58:03,299 - 10 - training_embed.py - training - Epoch: [3][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.1600 (612.9959)	
2019-01-08 16:58:03,497 - 10 - training_embed.py - training - Epoch: [3][2400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 618.9709 (612.9796)	
2019-01-08 16:58:03,719 - 10 - training_embed.py - training - Epoch: [3][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.5991 (612.9671)	
2019-01-08 16:58:03,939 - 10 - training_embed.py - training - Epoch: [3][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.1868 (612.9559)	
2019-01-08 16:58:04,138 - 10 - training_embed.py - training - Epoch: [3][2700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.4949 (612.9357)	
2019-01-08 16:58:04,358 - 10 - training_embed.py - training - Epoch: [3][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.2193 (612.9130)	
2019-01-08 16:58:04,568 - 10 - training_embed.py - training - Epoch: [3][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.5522 (612.8901)	
2019-01-08 16:58:04,782 - 10 - training_embed.py - training - Epoch: [3][3000/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 604.9285 (612.8630)	
2019-01-08 16:58:04,981 - 10 - training_embed.py - training - Epoch: [3][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3369 (612.8409)	
2019-01-08 16:58:05,196 - 10 - training_embed.py - training - Epoch: [3][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.2924 (612.8184)	
2019-01-08 16:58:05,410 - 10 - training_embed.py - training - Epoch: [3][3300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.2499 (612.8062)	
2019-01-08 16:58:05,609 - 10 - training_embed.py - training - Epoch: [3][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.8903 (612.7922)	
2019-01-08 16:58:05,820 - 10 - training_embed.py - training - Epoch: [3][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.7077 (612.8205)	
2019-01-08 16:58:06,033 - 10 - training_embed.py - training - Epoch: [3][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.0886 (612.8326)	
2019-01-08 16:58:06,161 - 10 - training_embed.py - training - loss: 612.799869
2019-01-08 16:58:06,162 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 16:58:06,567 - 10 - training_embed.py - training - Epoch: [4][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.2898 (612.1128)	
2019-01-08 16:58:06,771 - 10 - training_embed.py - training - Epoch: [4][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.2320 (612.2670)	
2019-01-08 16:58:06,981 - 10 - training_embed.py - training - Epoch: [4][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.9322 (612.4089)	
2019-01-08 16:58:07,198 - 10 - training_embed.py - training - Epoch: [4][400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.5474 (612.6174)	
2019-01-08 16:58:07,416 - 10 - training_embed.py - training - Epoch: [4][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.0740 (612.4695)	
2019-01-08 16:58:07,635 - 10 - training_embed.py - training - Epoch: [4][600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.1622 (612.5470)	
2019-01-08 16:58:07,851 - 10 - training_embed.py - training - Epoch: [4][700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.6983 (612.4728)	
2019-01-08 16:58:08,053 - 10 - training_embed.py - training - Epoch: [4][800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 605.8533 (612.4615)	
2019-01-08 16:58:08,257 - 10 - training_embed.py - training - Epoch: [4][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.8296 (612.4049)	
2019-01-08 16:58:08,479 - 10 - training_embed.py - training - Epoch: [4][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.9600 (612.4179)	
2019-01-08 16:58:08,690 - 10 - training_embed.py - training - Epoch: [4][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.3009 (612.3951)	
2019-01-08 16:58:08,902 - 10 - training_embed.py - training - Epoch: [4][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.6340 (612.3460)	
2019-01-08 16:58:09,119 - 10 - training_embed.py - training - Epoch: [4][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3732 (612.2843)	
2019-01-08 16:58:09,341 - 10 - training_embed.py - training - Epoch: [4][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.1871 (612.3115)	
2019-01-08 16:58:09,565 - 10 - training_embed.py - training - Epoch: [4][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.7935 (612.2579)	
2019-01-08 16:58:09,769 - 10 - training_embed.py - training - Epoch: [4][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.4943 (612.2376)	
2019-01-08 16:58:09,983 - 10 - training_embed.py - training - Epoch: [4][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.5540 (612.2307)	
2019-01-08 16:58:10,207 - 10 - training_embed.py - training - Epoch: [4][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.7855 (612.2113)	
2019-01-08 16:58:10,415 - 10 - training_embed.py - training - Epoch: [4][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.6597 (612.2101)	
2019-01-08 16:58:10,628 - 10 - training_embed.py - training - Epoch: [4][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.5038 (612.2070)	
2019-01-08 16:58:10,850 - 10 - training_embed.py - training - Epoch: [4][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.3630 (612.2015)	
2019-01-08 16:58:11,064 - 10 - training_embed.py - training - Epoch: [4][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.8344 (612.1712)	
2019-01-08 16:58:11,271 - 10 - training_embed.py - training - Epoch: [4][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.4532 (612.1230)	
2019-01-08 16:58:11,493 - 10 - training_embed.py - training - Epoch: [4][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.1852 (612.1023)	
2019-01-08 16:58:11,706 - 10 - training_embed.py - training - Epoch: [4][2500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.5444 (612.1173)	
2019-01-08 16:58:11,921 - 10 - training_embed.py - training - Epoch: [4][2600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.0200 (612.0781)	
2019-01-08 16:58:12,131 - 10 - training_embed.py - training - Epoch: [4][2700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.6108 (612.0463)	
2019-01-08 16:58:12,353 - 10 - training_embed.py - training - Epoch: [4][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9279 (612.0583)	
2019-01-08 16:58:12,575 - 10 - training_embed.py - training - Epoch: [4][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.4292 (612.0462)	
2019-01-08 16:58:12,790 - 10 - training_embed.py - training - Epoch: [4][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.7739 (612.0423)	
2019-01-08 16:58:13,006 - 10 - training_embed.py - training - Epoch: [4][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.7236 (612.0426)	
2019-01-08 16:58:13,232 - 10 - training_embed.py - training - Epoch: [4][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.7452 (612.0414)	
2019-01-08 16:58:13,456 - 10 - training_embed.py - training - Epoch: [4][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.4405 (612.0152)	
2019-01-08 16:58:13,667 - 10 - training_embed.py - training - Epoch: [4][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.1187 (611.9964)	
2019-01-08 16:58:13,897 - 10 - training_embed.py - training - Epoch: [4][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.7812 (612.0164)	
2019-01-08 16:58:14,127 - 10 - training_embed.py - training - Epoch: [4][3600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.0399 (612.0039)	
2019-01-08 16:58:14,267 - 10 - training_embed.py - training - loss: 611.948817
2019-01-08 16:58:14,267 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 16:58:14,690 - 10 - training_embed.py - training - Epoch: [5][100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.7949 (611.7512)	
2019-01-08 16:58:14,916 - 10 - training_embed.py - training - Epoch: [5][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.5755 (611.9312)	
2019-01-08 16:58:15,126 - 10 - training_embed.py - training - Epoch: [5][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.1061 (611.7929)	
2019-01-08 16:58:15,334 - 10 - training_embed.py - training - Epoch: [5][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.7676 (611.8045)	
2019-01-08 16:58:15,538 - 10 - training_embed.py - training - Epoch: [5][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3636 (611.6304)	
2019-01-08 16:58:15,759 - 10 - training_embed.py - training - Epoch: [5][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.6075 (611.4932)	
2019-01-08 16:58:15,964 - 10 - training_embed.py - training - Epoch: [5][700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.3489 (611.4669)	
2019-01-08 16:58:16,188 - 10 - training_embed.py - training - Epoch: [5][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.6895 (611.4118)	
2019-01-08 16:58:16,398 - 10 - training_embed.py - training - Epoch: [5][900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 601.6702 (611.4018)	
2019-01-08 16:58:16,603 - 10 - training_embed.py - training - Epoch: [5][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.4645 (611.3392)	
2019-01-08 16:58:16,822 - 10 - training_embed.py - training - Epoch: [5][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.7419 (611.3288)	
2019-01-08 16:58:17,028 - 10 - training_embed.py - training - Epoch: [5][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.5969 (611.3142)	
2019-01-08 16:58:17,229 - 10 - training_embed.py - training - Epoch: [5][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.5070 (611.3450)	
2019-01-08 16:58:17,441 - 10 - training_embed.py - training - Epoch: [5][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.6177 (611.3394)	
2019-01-08 16:58:17,666 - 10 - training_embed.py - training - Epoch: [5][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.0959 (611.3404)	
2019-01-08 16:58:17,884 - 10 - training_embed.py - training - Epoch: [5][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.4782 (611.3318)	
2019-01-08 16:58:18,106 - 10 - training_embed.py - training - Epoch: [5][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.2115 (611.2935)	
2019-01-08 16:58:18,329 - 10 - training_embed.py - training - Epoch: [5][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.0995 (611.3038)	
2019-01-08 16:58:18,537 - 10 - training_embed.py - training - Epoch: [5][1900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 618.1514 (611.2629)	
2019-01-08 16:58:18,738 - 10 - training_embed.py - training - Epoch: [5][2000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.5486 (611.2566)	
2019-01-08 16:58:18,968 - 10 - training_embed.py - training - Epoch: [5][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.7943 (611.2387)	
2019-01-08 16:58:19,179 - 10 - training_embed.py - training - Epoch: [5][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.9357 (611.2516)	
2019-01-08 16:58:19,396 - 10 - training_embed.py - training - Epoch: [5][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.6512 (611.2392)	
2019-01-08 16:58:19,600 - 10 - training_embed.py - training - Epoch: [5][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.7243 (611.2381)	
2019-01-08 16:58:19,808 - 10 - training_embed.py - training - Epoch: [5][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.8364 (611.2374)	
2019-01-08 16:58:20,011 - 10 - training_embed.py - training - Epoch: [5][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.0412 (611.2231)	
2019-01-08 16:58:20,213 - 10 - training_embed.py - training - Epoch: [5][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.7900 (611.2285)	
2019-01-08 16:58:20,420 - 10 - training_embed.py - training - Epoch: [5][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 619.8256 (611.2076)	
2019-01-08 16:58:20,625 - 10 - training_embed.py - training - Epoch: [5][2900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 611.9158 (611.2059)	
2019-01-08 16:58:20,845 - 10 - training_embed.py - training - Epoch: [5][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.1429 (611.2089)	
2019-01-08 16:58:21,048 - 10 - training_embed.py - training - Epoch: [5][3100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.3380 (611.1789)	
2019-01-08 16:58:21,270 - 10 - training_embed.py - training - Epoch: [5][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0529 (611.1631)	
2019-01-08 16:58:21,482 - 10 - training_embed.py - training - Epoch: [5][3300/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 613.7373 (611.1518)	
2019-01-08 16:58:21,697 - 10 - training_embed.py - training - Epoch: [5][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.2244 (611.1443)	
2019-01-08 16:58:21,936 - 10 - training_embed.py - training - Epoch: [5][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.1937 (611.1508)	
2019-01-08 16:58:22,157 - 10 - training_embed.py - training - Epoch: [5][3600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.9372 (611.1295)	
2019-01-08 16:58:22,280 - 10 - training_embed.py - training - loss: 611.095495
2019-01-08 16:58:22,281 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 16:58:22,704 - 10 - training_embed.py - training - Epoch: [6][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.6138 (610.4203)	
2019-01-08 16:58:22,921 - 10 - training_embed.py - training - Epoch: [6][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.5262 (610.5803)	
2019-01-08 16:58:23,132 - 10 - training_embed.py - training - Epoch: [6][300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 610.6938 (610.4954)	
2019-01-08 16:58:23,337 - 10 - training_embed.py - training - Epoch: [6][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.6378 (610.5038)	
2019-01-08 16:58:23,568 - 10 - training_embed.py - training - Epoch: [6][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.0621 (610.6161)	
2019-01-08 16:58:23,791 - 10 - training_embed.py - training - Epoch: [6][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.8236 (610.5612)	
2019-01-08 16:58:24,001 - 10 - training_embed.py - training - Epoch: [6][700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.5958 (610.6915)	
2019-01-08 16:58:24,226 - 10 - training_embed.py - training - Epoch: [6][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.4494 (610.7187)	
2019-01-08 16:58:24,433 - 10 - training_embed.py - training - Epoch: [6][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.7795 (610.6898)	
2019-01-08 16:58:24,657 - 10 - training_embed.py - training - Epoch: [6][1000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 610.9388 (610.6250)	
2019-01-08 16:58:24,873 - 10 - training_embed.py - training - Epoch: [6][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.1755 (610.6569)	
2019-01-08 16:58:25,091 - 10 - training_embed.py - training - Epoch: [6][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.0784 (610.6273)	
2019-01-08 16:58:25,294 - 10 - training_embed.py - training - Epoch: [6][1300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.0599 (610.6124)	
2019-01-08 16:58:25,497 - 10 - training_embed.py - training - Epoch: [6][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.9999 (610.6433)	
2019-01-08 16:58:25,728 - 10 - training_embed.py - training - Epoch: [6][1500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 603.7405 (610.6320)	
2019-01-08 16:58:25,933 - 10 - training_embed.py - training - Epoch: [6][1600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.0372 (610.5682)	
2019-01-08 16:58:26,144 - 10 - training_embed.py - training - Epoch: [6][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.6952 (610.5763)	
2019-01-08 16:58:26,356 - 10 - training_embed.py - training - Epoch: [6][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8932 (610.5482)	
2019-01-08 16:58:26,575 - 10 - training_embed.py - training - Epoch: [6][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.3591 (610.5277)	
2019-01-08 16:58:26,793 - 10 - training_embed.py - training - Epoch: [6][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.9319 (610.4914)	
2019-01-08 16:58:27,018 - 10 - training_embed.py - training - Epoch: [6][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.1513 (610.4545)	
2019-01-08 16:58:27,240 - 10 - training_embed.py - training - Epoch: [6][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.5317 (610.4324)	
2019-01-08 16:58:27,463 - 10 - training_embed.py - training - Epoch: [6][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.5793 (610.4256)	
2019-01-08 16:58:27,683 - 10 - training_embed.py - training - Epoch: [6][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.4644 (610.4282)	
2019-01-08 16:58:27,902 - 10 - training_embed.py - training - Epoch: [6][2500/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 607.1738 (610.4177)	
2019-01-08 16:58:28,120 - 10 - training_embed.py - training - Epoch: [6][2600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 611.0191 (610.4180)	
2019-01-08 16:58:28,323 - 10 - training_embed.py - training - Epoch: [6][2700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.9271 (610.4110)	
2019-01-08 16:58:28,553 - 10 - training_embed.py - training - Epoch: [6][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.8652 (610.3870)	
2019-01-08 16:58:28,774 - 10 - training_embed.py - training - Epoch: [6][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.0523 (610.3668)	
2019-01-08 16:58:28,981 - 10 - training_embed.py - training - Epoch: [6][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.8227 (610.3421)	
2019-01-08 16:58:29,201 - 10 - training_embed.py - training - Epoch: [6][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.8832 (610.3268)	
2019-01-08 16:58:29,402 - 10 - training_embed.py - training - Epoch: [6][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.6710 (610.3178)	
2019-01-08 16:58:29,623 - 10 - training_embed.py - training - Epoch: [6][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.8812 (610.3318)	
2019-01-08 16:58:29,821 - 10 - training_embed.py - training - Epoch: [6][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.2764 (610.3178)	
2019-01-08 16:58:30,022 - 10 - training_embed.py - training - Epoch: [6][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.1266 (610.3057)	
2019-01-08 16:58:30,241 - 10 - training_embed.py - training - Epoch: [6][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.6768 (610.2784)	
2019-01-08 16:58:30,371 - 10 - training_embed.py - training - loss: 610.240516
2019-01-08 16:58:30,372 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 16:58:30,850 - 10 - training_embed.py - training - Epoch: [7][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.5396 (609.9911)	
2019-01-08 16:58:31,068 - 10 - training_embed.py - training - Epoch: [7][200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.0503 (609.7756)	
2019-01-08 16:58:31,276 - 10 - training_embed.py - training - Epoch: [7][300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 610.5032 (609.9929)	
2019-01-08 16:58:31,494 - 10 - training_embed.py - training - Epoch: [7][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.4976 (609.9787)	
2019-01-08 16:58:31,699 - 10 - training_embed.py - training - Epoch: [7][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3539 (609.9178)	
2019-01-08 16:58:31,908 - 10 - training_embed.py - training - Epoch: [7][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.7725 (609.7890)	
2019-01-08 16:58:32,119 - 10 - training_embed.py - training - Epoch: [7][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.5692 (609.7394)	
2019-01-08 16:58:32,331 - 10 - training_embed.py - training - Epoch: [7][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.2684 (609.7583)	
2019-01-08 16:58:32,558 - 10 - training_embed.py - training - Epoch: [7][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9078 (609.6730)	
2019-01-08 16:58:32,767 - 10 - training_embed.py - training - Epoch: [7][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6602 (609.6276)	
2019-01-08 16:58:32,981 - 10 - training_embed.py - training - Epoch: [7][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.3256 (609.6668)	
2019-01-08 16:58:33,204 - 10 - training_embed.py - training - Epoch: [7][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.3979 (609.5802)	
2019-01-08 16:58:33,415 - 10 - training_embed.py - training - Epoch: [7][1300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.3467 (609.6526)	
2019-01-08 16:58:33,623 - 10 - training_embed.py - training - Epoch: [7][1400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.2515 (609.6114)	
2019-01-08 16:58:33,830 - 10 - training_embed.py - training - Epoch: [7][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.4318 (609.5793)	
2019-01-08 16:58:34,049 - 10 - training_embed.py - training - Epoch: [7][1600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 611.8496 (609.5712)	
2019-01-08 16:58:34,264 - 10 - training_embed.py - training - Epoch: [7][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.9358 (609.6226)	
2019-01-08 16:58:34,477 - 10 - training_embed.py - training - Epoch: [7][1800/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 609.3990 (609.6172)	
2019-01-08 16:58:34,685 - 10 - training_embed.py - training - Epoch: [7][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.8752 (609.5787)	
2019-01-08 16:58:34,885 - 10 - training_embed.py - training - Epoch: [7][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.5148 (609.5610)	
2019-01-08 16:58:35,087 - 10 - training_embed.py - training - Epoch: [7][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.5403 (609.5666)	
2019-01-08 16:58:35,299 - 10 - training_embed.py - training - Epoch: [7][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.9820 (609.5801)	
2019-01-08 16:58:35,504 - 10 - training_embed.py - training - Epoch: [7][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.2401 (609.5792)	
2019-01-08 16:58:35,724 - 10 - training_embed.py - training - Epoch: [7][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.0756 (609.5849)	
2019-01-08 16:58:35,927 - 10 - training_embed.py - training - Epoch: [7][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.0977 (609.5720)	
2019-01-08 16:58:36,146 - 10 - training_embed.py - training - Epoch: [7][2600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.5646 (609.5739)	
2019-01-08 16:58:36,363 - 10 - training_embed.py - training - Epoch: [7][2700/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 611.8965 (609.5470)	
2019-01-08 16:58:36,587 - 10 - training_embed.py - training - Epoch: [7][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.5850 (609.5470)	
2019-01-08 16:58:36,794 - 10 - training_embed.py - training - Epoch: [7][2900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.3075 (609.5273)	
2019-01-08 16:58:37,010 - 10 - training_embed.py - training - Epoch: [7][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.6429 (609.5457)	
2019-01-08 16:58:37,210 - 10 - training_embed.py - training - Epoch: [7][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.5764 (609.5060)	
2019-01-08 16:58:37,423 - 10 - training_embed.py - training - Epoch: [7][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.8024 (609.4680)	
2019-01-08 16:58:37,624 - 10 - training_embed.py - training - Epoch: [7][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.0098 (609.4499)	
2019-01-08 16:58:37,833 - 10 - training_embed.py - training - Epoch: [7][3400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.7031 (609.4585)	
2019-01-08 16:58:38,056 - 10 - training_embed.py - training - Epoch: [7][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.7410 (609.4225)	
2019-01-08 16:58:38,274 - 10 - training_embed.py - training - Epoch: [7][3600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 601.2822 (609.4292)	
2019-01-08 16:58:38,405 - 10 - training_embed.py - training - loss: 609.381334
2019-01-08 16:58:38,406 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 16:58:38,883 - 10 - training_embed.py - training - Epoch: [8][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.0738 (609.3945)	
2019-01-08 16:58:39,088 - 10 - training_embed.py - training - Epoch: [8][200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.7777 (609.1183)	
2019-01-08 16:58:39,302 - 10 - training_embed.py - training - Epoch: [8][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.3873 (608.9597)	
2019-01-08 16:58:39,519 - 10 - training_embed.py - training - Epoch: [8][400/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 610.9888 (609.1003)	
2019-01-08 16:58:39,724 - 10 - training_embed.py - training - Epoch: [8][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.8615 (608.9216)	
2019-01-08 16:58:39,935 - 10 - training_embed.py - training - Epoch: [8][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5468 (608.9220)	
2019-01-08 16:58:40,156 - 10 - training_embed.py - training - Epoch: [8][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.4668 (608.9136)	
2019-01-08 16:58:40,363 - 10 - training_embed.py - training - Epoch: [8][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.1002 (608.9229)	
2019-01-08 16:58:40,571 - 10 - training_embed.py - training - Epoch: [8][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.9788 (608.9253)	
2019-01-08 16:58:40,789 - 10 - training_embed.py - training - Epoch: [8][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6699 (608.9199)	
2019-01-08 16:58:41,001 - 10 - training_embed.py - training - Epoch: [8][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.4105 (608.8886)	
2019-01-08 16:58:41,223 - 10 - training_embed.py - training - Epoch: [8][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.4877 (608.9251)	
2019-01-08 16:58:41,443 - 10 - training_embed.py - training - Epoch: [8][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.5870 (608.8459)	
2019-01-08 16:58:41,651 - 10 - training_embed.py - training - Epoch: [8][1400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.8764 (608.8335)	
2019-01-08 16:58:41,865 - 10 - training_embed.py - training - Epoch: [8][1500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.5551 (608.7847)	
2019-01-08 16:58:42,086 - 10 - training_embed.py - training - Epoch: [8][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.5718 (608.7970)	
2019-01-08 16:58:42,312 - 10 - training_embed.py - training - Epoch: [8][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.0923 (608.7845)	
2019-01-08 16:58:42,527 - 10 - training_embed.py - training - Epoch: [8][1800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.0406 (608.7886)	
2019-01-08 16:58:42,731 - 10 - training_embed.py - training - Epoch: [8][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.2772 (608.7756)	
2019-01-08 16:58:42,945 - 10 - training_embed.py - training - Epoch: [8][2000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 606.7491 (608.8027)	
2019-01-08 16:58:43,148 - 10 - training_embed.py - training - Epoch: [8][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.6164 (608.8060)	
2019-01-08 16:58:43,349 - 10 - training_embed.py - training - Epoch: [8][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.5784 (608.8016)	
2019-01-08 16:58:43,558 - 10 - training_embed.py - training - Epoch: [8][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.6096 (608.7676)	
2019-01-08 16:58:43,765 - 10 - training_embed.py - training - Epoch: [8][2400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.3274 (608.7602)	
2019-01-08 16:58:43,975 - 10 - training_embed.py - training - Epoch: [8][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.0870 (608.7485)	
2019-01-08 16:58:44,210 - 10 - training_embed.py - training - Epoch: [8][2600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 601.3201 (608.7222)	
2019-01-08 16:58:44,439 - 10 - training_embed.py - training - Epoch: [8][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.0096 (608.6797)	
2019-01-08 16:58:44,649 - 10 - training_embed.py - training - Epoch: [8][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.1636 (608.6628)	
2019-01-08 16:58:44,879 - 10 - training_embed.py - training - Epoch: [8][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.3058 (608.6458)	
2019-01-08 16:58:45,098 - 10 - training_embed.py - training - Epoch: [8][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2902 (608.6221)	
2019-01-08 16:58:45,318 - 10 - training_embed.py - training - Epoch: [8][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.6439 (608.5996)	
2019-01-08 16:58:45,535 - 10 - training_embed.py - training - Epoch: [8][3200/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 605.6794 (608.5879)	
2019-01-08 16:58:45,761 - 10 - training_embed.py - training - Epoch: [8][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.8814 (608.6069)	
2019-01-08 16:58:46,003 - 10 - training_embed.py - training - Epoch: [8][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.3958 (608.5929)	
2019-01-08 16:58:46,230 - 10 - training_embed.py - training - Epoch: [8][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.4528 (608.5755)	
2019-01-08 16:58:46,453 - 10 - training_embed.py - training - Epoch: [8][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3224 (608.5758)	
2019-01-08 16:58:46,590 - 10 - training_embed.py - training - loss: 608.518816
2019-01-08 16:58:46,591 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 16:58:47,130 - 10 - training_embed.py - training - Epoch: [9][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.4062 (607.7017)	
2019-01-08 16:58:47,360 - 10 - training_embed.py - training - Epoch: [9][200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 602.9924 (608.1741)	
2019-01-08 16:58:47,575 - 10 - training_embed.py - training - Epoch: [9][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.4947 (608.6176)	
2019-01-08 16:58:47,787 - 10 - training_embed.py - training - Epoch: [9][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.8951 (608.3783)	
2019-01-08 16:58:48,037 - 10 - training_embed.py - training - Epoch: [9][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.4104 (608.3883)	
2019-01-08 16:58:48,254 - 10 - training_embed.py - training - Epoch: [9][600/3661]	Time 0.004 (0.002)	Data 0.000 (0.001)	Loss 609.0224 (608.3237)	
2019-01-08 16:58:48,471 - 10 - training_embed.py - training - Epoch: [9][700/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 605.6328 (608.3183)	
2019-01-08 16:58:48,711 - 10 - training_embed.py - training - Epoch: [9][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.2034 (608.2837)	
2019-01-08 16:58:48,932 - 10 - training_embed.py - training - Epoch: [9][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.1313 (608.1872)	
2019-01-08 16:58:49,157 - 10 - training_embed.py - training - Epoch: [9][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.6763 (608.1592)	
2019-01-08 16:58:49,384 - 10 - training_embed.py - training - Epoch: [9][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.5029 (608.1150)	
2019-01-08 16:58:49,637 - 10 - training_embed.py - training - Epoch: [9][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5673 (608.0817)	
2019-01-08 16:58:49,850 - 10 - training_embed.py - training - Epoch: [9][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.0873 (608.0444)	
2019-01-08 16:58:50,057 - 10 - training_embed.py - training - Epoch: [9][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.5934 (608.0320)	
2019-01-08 16:58:50,277 - 10 - training_embed.py - training - Epoch: [9][1500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.1137 (607.9700)	
2019-01-08 16:58:50,489 - 10 - training_embed.py - training - Epoch: [9][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.6359 (607.9757)	
2019-01-08 16:58:50,732 - 10 - training_embed.py - training - Epoch: [9][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.7160 (607.9345)	
2019-01-08 16:58:50,950 - 10 - training_embed.py - training - Epoch: [9][1800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.4663 (607.9536)	
2019-01-08 16:58:51,173 - 10 - training_embed.py - training - Epoch: [9][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.6185 (607.9243)	
2019-01-08 16:58:51,383 - 10 - training_embed.py - training - Epoch: [9][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.8685 (607.8659)	
2019-01-08 16:58:51,603 - 10 - training_embed.py - training - Epoch: [9][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.8771 (607.8524)	
2019-01-08 16:58:51,838 - 10 - training_embed.py - training - Epoch: [9][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.2870 (607.8832)	
2019-01-08 16:58:52,056 - 10 - training_embed.py - training - Epoch: [9][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.9854 (607.8664)	
2019-01-08 16:58:52,257 - 10 - training_embed.py - training - Epoch: [9][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.7552 (607.8651)	
2019-01-08 16:58:52,480 - 10 - training_embed.py - training - Epoch: [9][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.9814 (607.8484)	
2019-01-08 16:58:52,714 - 10 - training_embed.py - training - Epoch: [9][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.1094 (607.8490)	
2019-01-08 16:58:52,915 - 10 - training_embed.py - training - Epoch: [9][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.5641 (607.8249)	
2019-01-08 16:58:53,147 - 10 - training_embed.py - training - Epoch: [9][2800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 611.7562 (607.8257)	
2019-01-08 16:58:53,365 - 10 - training_embed.py - training - Epoch: [9][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.6478 (607.8081)	
2019-01-08 16:58:53,574 - 10 - training_embed.py - training - Epoch: [9][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.7654 (607.8024)	
2019-01-08 16:58:53,805 - 10 - training_embed.py - training - Epoch: [9][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.0649 (607.7948)	
2019-01-08 16:58:54,017 - 10 - training_embed.py - training - Epoch: [9][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.7755 (607.7712)	
2019-01-08 16:58:54,225 - 10 - training_embed.py - training - Epoch: [9][3300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.6469 (607.7582)	
2019-01-08 16:58:54,430 - 10 - training_embed.py - training - Epoch: [9][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.2405 (607.7269)	
2019-01-08 16:58:54,648 - 10 - training_embed.py - training - Epoch: [9][3500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.6333 (607.7157)	
2019-01-08 16:58:54,871 - 10 - training_embed.py - training - Epoch: [9][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3744 (607.6973)	
2019-01-08 16:58:54,995 - 10 - training_embed.py - training - loss: 607.650898
2019-01-08 16:58:54,995 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 16:58:55,422 - 10 - training_embed.py - training - Epoch: [10][100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 603.7893 (607.7986)	
2019-01-08 16:58:55,626 - 10 - training_embed.py - training - Epoch: [10][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.8228 (607.8127)	
2019-01-08 16:58:55,831 - 10 - training_embed.py - training - Epoch: [10][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.8460 (607.6467)	
2019-01-08 16:58:56,034 - 10 - training_embed.py - training - Epoch: [10][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.7521 (607.6316)	
2019-01-08 16:58:56,240 - 10 - training_embed.py - training - Epoch: [10][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.3129 (607.4909)	
2019-01-08 16:58:56,482 - 10 - training_embed.py - training - Epoch: [10][600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.0748 (607.3491)	
2019-01-08 16:58:56,706 - 10 - training_embed.py - training - Epoch: [10][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.4304 (607.2393)	
2019-01-08 16:58:56,921 - 10 - training_embed.py - training - Epoch: [10][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.4456 (607.2544)	
2019-01-08 16:58:57,144 - 10 - training_embed.py - training - Epoch: [10][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.9431 (607.2054)	
2019-01-08 16:58:57,355 - 10 - training_embed.py - training - Epoch: [10][1000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 600.0908 (607.1143)	
2019-01-08 16:58:57,581 - 10 - training_embed.py - training - Epoch: [10][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.2896 (607.1038)	
2019-01-08 16:58:57,803 - 10 - training_embed.py - training - Epoch: [10][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.7076 (607.0727)	
2019-01-08 16:58:58,008 - 10 - training_embed.py - training - Epoch: [10][1300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 610.1872 (607.0228)	
2019-01-08 16:58:58,268 - 10 - training_embed.py - training - Epoch: [10][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.0151 (607.0248)	
2019-01-08 16:58:58,491 - 10 - training_embed.py - training - Epoch: [10][1500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.2037 (607.0912)	
2019-01-08 16:58:58,745 - 10 - training_embed.py - training - Epoch: [10][1600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.3213 (607.0398)	
2019-01-08 16:58:58,955 - 10 - training_embed.py - training - Epoch: [10][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.6238 (607.0150)	
2019-01-08 16:58:59,201 - 10 - training_embed.py - training - Epoch: [10][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.9543 (607.0333)	
2019-01-08 16:58:59,410 - 10 - training_embed.py - training - Epoch: [10][1900/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 605.6849 (607.0654)	
2019-01-08 16:58:59,621 - 10 - training_embed.py - training - Epoch: [10][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.6613 (607.0356)	
2019-01-08 16:58:59,843 - 10 - training_embed.py - training - Epoch: [10][2100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 600.9749 (607.0058)	
2019-01-08 16:59:00,053 - 10 - training_embed.py - training - Epoch: [10][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.2186 (606.9822)	
2019-01-08 16:59:00,274 - 10 - training_embed.py - training - Epoch: [10][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.4897 (606.9711)	
2019-01-08 16:59:00,481 - 10 - training_embed.py - training - Epoch: [10][2400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 610.0654 (606.9674)	
2019-01-08 16:59:00,748 - 10 - training_embed.py - training - Epoch: [10][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.9315 (606.9599)	
2019-01-08 16:59:00,959 - 10 - training_embed.py - training - Epoch: [10][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.6844 (606.9264)	
2019-01-08 16:59:01,213 - 10 - training_embed.py - training - Epoch: [10][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.2371 (606.9300)	
2019-01-08 16:59:01,458 - 10 - training_embed.py - training - Epoch: [10][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3820 (606.9164)	
2019-01-08 16:59:01,679 - 10 - training_embed.py - training - Epoch: [10][2900/3661]	Time 0.005 (0.002)	Data 0.002 (0.001)	Loss 607.7119 (606.9091)	
2019-01-08 16:59:01,894 - 10 - training_embed.py - training - Epoch: [10][3000/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 611.5838 (606.9108)	
2019-01-08 16:59:02,150 - 10 - training_embed.py - training - Epoch: [10][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.7982 (606.8888)	
2019-01-08 16:59:02,370 - 10 - training_embed.py - training - Epoch: [10][3200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.0908 (606.8909)	
2019-01-08 16:59:02,628 - 10 - training_embed.py - training - Epoch: [10][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8135 (606.8782)	
2019-01-08 16:59:02,867 - 10 - training_embed.py - training - Epoch: [10][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3540 (606.8689)	
2019-01-08 16:59:03,099 - 10 - training_embed.py - training - Epoch: [10][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.0939 (606.8594)	
2019-01-08 16:59:03,345 - 10 - training_embed.py - training - Epoch: [10][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.4489 (606.8316)	
2019-01-08 16:59:03,474 - 10 - training_embed.py - training - loss: 606.778915
2019-01-08 16:59:03,475 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 16:59:03,943 - 10 - training_embed.py - training - Epoch: [11][100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.0291 (606.7256)	
2019-01-08 16:59:04,159 - 10 - training_embed.py - training - Epoch: [11][200/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 607.8937 (606.4327)	
2019-01-08 16:59:04,379 - 10 - training_embed.py - training - Epoch: [11][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.8282 (606.6356)	
2019-01-08 16:59:04,604 - 10 - training_embed.py - training - Epoch: [11][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.1892 (606.6343)	
2019-01-08 16:59:04,809 - 10 - training_embed.py - training - Epoch: [11][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.3314 (606.6361)	
2019-01-08 16:59:05,015 - 10 - training_embed.py - training - Epoch: [11][600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.2684 (606.4938)	
2019-01-08 16:59:05,229 - 10 - training_embed.py - training - Epoch: [11][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.4784 (606.4776)	
2019-01-08 16:59:05,438 - 10 - training_embed.py - training - Epoch: [11][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.2944 (606.4583)	
2019-01-08 16:59:05,640 - 10 - training_embed.py - training - Epoch: [11][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.2912 (606.4596)	
2019-01-08 16:59:05,851 - 10 - training_embed.py - training - Epoch: [11][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.2512 (606.3632)	
2019-01-08 16:59:06,053 - 10 - training_embed.py - training - Epoch: [11][1100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 606.0031 (606.2905)	
2019-01-08 16:59:06,280 - 10 - training_embed.py - training - Epoch: [11][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.2048 (606.2787)	
2019-01-08 16:59:06,505 - 10 - training_embed.py - training - Epoch: [11][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.6860 (606.2755)	
2019-01-08 16:59:06,741 - 10 - training_embed.py - training - Epoch: [11][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.9521 (606.2225)	
2019-01-08 16:59:06,964 - 10 - training_embed.py - training - Epoch: [11][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.2651 (606.2057)	
2019-01-08 16:59:07,178 - 10 - training_embed.py - training - Epoch: [11][1600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.9586 (606.2402)	
2019-01-08 16:59:07,398 - 10 - training_embed.py - training - Epoch: [11][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.4125 (606.2136)	
2019-01-08 16:59:07,604 - 10 - training_embed.py - training - Epoch: [11][1800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.3683 (606.1798)	
2019-01-08 16:59:07,810 - 10 - training_embed.py - training - Epoch: [11][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.3876 (606.1725)	
2019-01-08 16:59:08,015 - 10 - training_embed.py - training - Epoch: [11][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.3980 (606.1366)	
2019-01-08 16:59:08,230 - 10 - training_embed.py - training - Epoch: [11][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.3428 (606.1113)	
2019-01-08 16:59:08,433 - 10 - training_embed.py - training - Epoch: [11][2200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.7615 (606.0856)	
2019-01-08 16:59:08,638 - 10 - training_embed.py - training - Epoch: [11][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.8379 (606.0875)	
2019-01-08 16:59:08,859 - 10 - training_embed.py - training - Epoch: [11][2400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.6050 (606.0934)	
2019-01-08 16:59:09,069 - 10 - training_embed.py - training - Epoch: [11][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.3831 (606.0906)	
2019-01-08 16:59:09,280 - 10 - training_embed.py - training - Epoch: [11][2600/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 603.7991 (606.1005)	
2019-01-08 16:59:09,483 - 10 - training_embed.py - training - Epoch: [11][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.4553 (606.0819)	
2019-01-08 16:59:09,693 - 10 - training_embed.py - training - Epoch: [11][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.1048 (606.0613)	
2019-01-08 16:59:09,911 - 10 - training_embed.py - training - Epoch: [11][2900/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 607.1779 (606.0266)	
2019-01-08 16:59:10,113 - 10 - training_embed.py - training - Epoch: [11][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.2585 (606.0086)	
2019-01-08 16:59:10,327 - 10 - training_embed.py - training - Epoch: [11][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.0330 (605.9986)	
2019-01-08 16:59:10,544 - 10 - training_embed.py - training - Epoch: [11][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.3019 (605.9887)	
2019-01-08 16:59:10,765 - 10 - training_embed.py - training - Epoch: [11][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3616 (605.9908)	
2019-01-08 16:59:10,968 - 10 - training_embed.py - training - Epoch: [11][3400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.8198 (605.9752)	
2019-01-08 16:59:11,184 - 10 - training_embed.py - training - Epoch: [11][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.6586 (605.9498)	
2019-01-08 16:59:11,391 - 10 - training_embed.py - training - Epoch: [11][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.7313 (605.9544)	
2019-01-08 16:59:11,522 - 10 - training_embed.py - training - loss: 605.899681
2019-01-08 16:59:11,651 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 16:59:13,588 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1937.515 ms ~ 0.032 min ~ 1.938 sec
2019-01-08 16:59:15,646 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 3995.062 ms ~ 0.067 min ~ 3.995 sec
2019-01-08 16:59:15,646 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 16:59:15,646 - 10 - corpus.py - subactivity_sampler - 0 / 157
2019-01-08 16:59:15,647 - 10 - corpus.py - subactivity_sampler - [102679.  34908.  72220.  87415.  55011.  68221.  70275.  95238.  55085.
  78979.  25527. 127025.  64542.]
2019-01-08 17:04:21,920 - 10 - corpus.py - subactivity_sampler - 20 / 157
2019-01-08 17:04:21,920 - 10 - corpus.py - subactivity_sampler - [103389.  34292.  71643.  88308.  54431.  68405.  69488.  97271.  53444.
  79099.  23614. 130099.  63642.]
2019-01-08 17:08:54,825 - 10 - corpus.py - subactivity_sampler - 40 / 157
2019-01-08 17:08:54,826 - 10 - corpus.py - subactivity_sampler - [104724.  32988.  70458.  89926.  53469.  68708.  68304.  99366.  52205.
  79572.  22044. 132573.  62788.]
2019-01-08 17:13:56,730 - 10 - corpus.py - subactivity_sampler - 60 / 157
2019-01-08 17:13:56,731 - 10 - corpus.py - subactivity_sampler - [107312.  31155.  69486.  91053.  52788.  68557.  67763. 100262.  51149.
  80019.  20676. 134366.  62539.]
2019-01-08 17:18:46,399 - 10 - corpus.py - subactivity_sampler - 80 / 157
2019-01-08 17:18:46,399 - 10 - corpus.py - subactivity_sampler - [108212.  30098.  69371.  91548.  52607.  68208.  67595. 102012.  48585.
  80454.  19663. 137225.  61547.]
2019-01-08 17:24:06,408 - 10 - corpus.py - subactivity_sampler - 100 / 157
2019-01-08 17:24:06,408 - 10 - corpus.py - subactivity_sampler - [108506.  29462.  68922.  93013.  51500.  68330.  67097. 103448.  46315.
  81896.  18233. 139752.  60651.]
2019-01-08 17:27:40,298 - 10 - corpus.py - subactivity_sampler - 120 / 157
2019-01-08 17:27:40,299 - 10 - corpus.py - subactivity_sampler - [110998.  28176.  67935.  93398.  50618.  68047.  66974. 104601.  45425.
  82097.  17596. 141019.  60241.]
2019-01-08 17:33:34,513 - 10 - corpus.py - subactivity_sampler - 140 / 157
2019-01-08 17:33:34,513 - 10 - corpus.py - subactivity_sampler - [112464.  26602.  67238.  96047.  48381.  67654.  67302. 105011.  43956.
  83702.  15982. 142716.  60070.]
2019-01-08 17:38:19,088 - 10 - corpus.py - subactivity_sampler - [112732.  26065.  66529.  96999.  47948.  67892.  66970. 106408.  42773.
  83360.  14976. 144642.  59831.]
2019-01-08 17:38:19,089 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 2343442.759 ms ~ 39.057 min ~ 2343.443 sec
2019-01-08 17:38:19,089 - 10 - corpus.py - ordering_sampler - .
2019-01-08 17:38:29,807 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 17:38:29,807 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 19.  57.   0.  20. 141.   3.  41.  51.   0.   5.  18.   0.]
2019-01-08 17:38:29,807 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 17:38:29,851 - 10 - corpus.py - rho_sampling - ['50.4741', '21.6570', '50.9055', '183.0026', '27.2055', '33.4170', '34.1670', '35.8623', '11.6051', '664.5828', '94.0098', '87.3986']
2019-01-08 17:38:29,851 - 10 - pipeline.py - baseline - Iteration 3
2019-01-08 17:38:30,255 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 17:38:30,274 - 10 - accuracy_class.py - mof - # gt_labels: 14   # pr_labels: 13
2019-01-08 17:38:30,274 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 11', '1: 2', '2: 26', '3: 27', '4: 10', '5: 17', '6: 28', '7: 14', '8: 31', '9: 16', '10: 4', '11: 29', '12: 30']
2019-01-08 17:38:30,321 - 10 - accuracy_class.py - mof_val - frames true: 314592	frames overall : 937125
2019-01-08 17:38:30,322 - 10 - corpus.py - accuracy_corpus - Action: pancake
2019-01-08 17:38:30,322 - 10 - corpus.py - accuracy_corpus - MoF val: 0.33569907963185275
2019-01-08 17:38:30,322 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.33569907963185275
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35402
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 2: 0.282882  10332 / 36524
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1774
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 10: 0.079819  880 / 11025
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 11: 0.836173  38382 / 45902
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 14: 0.102241  1807 / 17674
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 207
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 17: 0.325046  12668 / 38973
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 26: 0.118045  6380 / 54047
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 27: 0.466959  78614 / 168353
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 28: 0.302892  18975 / 62646
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 29: 0.287525  123791 / 430540
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 30: 0.797471  22763 / 28544
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - label 31: 0.000000  0 / 5514
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - mof_classes - average class mof: 0.257075
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35402
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - iou_classes - label 2: 0.197715  10332 / 52257
2019-01-08 17:38:30,322 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 16750
2019-01-08 17:38:30,323 - 10 - accuracy_class.py - iou_classes - label 10: 0.015148  880 / 58093
2019-01-08 17:38:30,323 - 10 - accuracy_class.py - iou_classes - label 11: 0.319180  38382 / 120252
2019-01-08 17:38:30,323 - 10 - accuracy_class.py - iou_classes - label 14: 0.014778  1807 / 122275
2019-01-08 17:38:30,323 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 83567
2019-01-08 17:38:30,323 - 10 - accuracy_class.py - iou_classes - label 17: 0.134484  12668 / 94197
2019-01-08 17:38:30,323 - 10 - accuracy_class.py - iou_classes - label 26: 0.055869  6380 / 114196
2019-01-08 17:38:30,323 - 10 - accuracy_class.py - iou_classes - label 27: 0.420986  78614 / 186738
2019-01-08 17:38:30,323 - 10 - accuracy_class.py - iou_classes - label 28: 0.171501  18975 / 110641
2019-01-08 17:38:30,323 - 10 - accuracy_class.py - iou_classes - label 29: 0.274243  123791 / 451391
2019-01-08 17:38:30,323 - 10 - accuracy_class.py - iou_classes - label 30: 0.346933  22763 / 65612
2019-01-08 17:38:30,323 - 10 - accuracy_class.py - iou_classes - label 31: 0.000000  0 / 48287
2019-01-08 17:38:30,323 - 10 - accuracy_class.py - iou_classes - average IoU: 0.150064
2019-01-08 17:38:30,323 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.139346
2019-01-08 17:38:45,144 - 10 - f1_score.py - f1 - f1 score: 0.304025
2019-01-08 17:38:45,175 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 15323.544 ms ~ 0.255 min ~ 15.324 sec
2019-01-08 17:38:45,175 - 10 - corpus.py - embedding_training - .
2019-01-08 17:38:45,175 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 17:38:45,175 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 17:38:45,175 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 17:38:50,737 - 10 - training_embed.py - training - create model
2019-01-08 17:38:50,750 - 10 - training_embed.py - training - epochs: 12
2019-01-08 17:38:50,751 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 17:38:51,147 - 10 - training_embed.py - training - Epoch: [0][100/3661]	Time 0.002 (0.004)	Data 0.001 (0.003)	Loss 614.4244 (616.2648)	
2019-01-08 17:38:51,360 - 10 - training_embed.py - training - Epoch: [0][200/3661]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 613.3785 (616.0300)	
2019-01-08 17:38:51,571 - 10 - training_embed.py - training - Epoch: [0][300/3661]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 618.3665 (616.0851)	
2019-01-08 17:38:51,787 - 10 - training_embed.py - training - Epoch: [0][400/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 616.0682 (616.1407)	
2019-01-08 17:38:51,997 - 10 - training_embed.py - training - Epoch: [0][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.9850 (616.1091)	
2019-01-08 17:38:52,201 - 10 - training_embed.py - training - Epoch: [0][600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 616.0214 (616.2313)	
2019-01-08 17:38:52,410 - 10 - training_embed.py - training - Epoch: [0][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.8867 (616.3063)	
2019-01-08 17:38:52,619 - 10 - training_embed.py - training - Epoch: [0][800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 616.3155 (616.2685)	
2019-01-08 17:38:52,838 - 10 - training_embed.py - training - Epoch: [0][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8262 (616.2446)	
2019-01-08 17:38:53,043 - 10 - training_embed.py - training - Epoch: [0][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.4787 (616.2287)	
2019-01-08 17:38:53,275 - 10 - training_embed.py - training - Epoch: [0][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.8426 (616.1715)	
2019-01-08 17:38:53,496 - 10 - training_embed.py - training - Epoch: [0][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 621.1140 (616.1854)	
2019-01-08 17:38:53,703 - 10 - training_embed.py - training - Epoch: [0][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.3330 (616.2564)	
2019-01-08 17:38:53,937 - 10 - training_embed.py - training - Epoch: [0][1400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 612.0692 (616.2429)	
2019-01-08 17:38:54,156 - 10 - training_embed.py - training - Epoch: [0][1500/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 614.8574 (616.1999)	
2019-01-08 17:38:54,389 - 10 - training_embed.py - training - Epoch: [0][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.9520 (616.1611)	
2019-01-08 17:38:54,592 - 10 - training_embed.py - training - Epoch: [0][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.3862 (616.1686)	
2019-01-08 17:38:54,828 - 10 - training_embed.py - training - Epoch: [0][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.2029 (616.1100)	
2019-01-08 17:38:55,037 - 10 - training_embed.py - training - Epoch: [0][1900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.0378 (616.0844)	
2019-01-08 17:38:55,251 - 10 - training_embed.py - training - Epoch: [0][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 629.2900 (616.0863)	
2019-01-08 17:38:55,485 - 10 - training_embed.py - training - Epoch: [0][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.9557 (616.0733)	
2019-01-08 17:38:55,703 - 10 - training_embed.py - training - Epoch: [0][2200/3661]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 610.4440 (616.0462)	
2019-01-08 17:38:55,922 - 10 - training_embed.py - training - Epoch: [0][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.6634 (616.0483)	
2019-01-08 17:38:56,130 - 10 - training_embed.py - training - Epoch: [0][2400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 617.0706 (616.0366)	
2019-01-08 17:38:56,353 - 10 - training_embed.py - training - Epoch: [0][2500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 618.8781 (616.0234)	
2019-01-08 17:38:56,560 - 10 - training_embed.py - training - Epoch: [0][2600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.9334 (616.0168)	
2019-01-08 17:38:56,780 - 10 - training_embed.py - training - Epoch: [0][2700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 626.9562 (616.0208)	
2019-01-08 17:38:56,997 - 10 - training_embed.py - training - Epoch: [0][2800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.4499 (616.0147)	
2019-01-08 17:38:57,213 - 10 - training_embed.py - training - Epoch: [0][2900/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 614.3451 (615.9820)	
2019-01-08 17:38:57,446 - 10 - training_embed.py - training - Epoch: [0][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.8470 (615.9712)	
2019-01-08 17:38:57,659 - 10 - training_embed.py - training - Epoch: [0][3100/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 619.7715 (615.9466)	
2019-01-08 17:38:57,872 - 10 - training_embed.py - training - Epoch: [0][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.0135 (615.9621)	
2019-01-08 17:38:58,091 - 10 - training_embed.py - training - Epoch: [0][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 621.1933 (615.9496)	
2019-01-08 17:38:58,301 - 10 - training_embed.py - training - Epoch: [0][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.3336 (615.9364)	
2019-01-08 17:38:58,532 - 10 - training_embed.py - training - Epoch: [0][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 617.6653 (615.9325)	
2019-01-08 17:38:58,746 - 10 - training_embed.py - training - Epoch: [0][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.5109 (615.9227)	
2019-01-08 17:38:58,885 - 10 - training_embed.py - training - loss: 615.866282
2019-01-08 17:38:58,885 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 17:38:59,347 - 10 - training_embed.py - training - Epoch: [1][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.5987 (615.6302)	
2019-01-08 17:38:59,555 - 10 - training_embed.py - training - Epoch: [1][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.4667 (615.6261)	
2019-01-08 17:38:59,786 - 10 - training_embed.py - training - Epoch: [1][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.7972 (615.6420)	
2019-01-08 17:38:59,995 - 10 - training_embed.py - training - Epoch: [1][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.9048 (615.3542)	
2019-01-08 17:39:00,223 - 10 - training_embed.py - training - Epoch: [1][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.6404 (615.3298)	
2019-01-08 17:39:00,459 - 10 - training_embed.py - training - Epoch: [1][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8962 (615.2839)	
2019-01-08 17:39:00,667 - 10 - training_embed.py - training - Epoch: [1][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.4164 (615.2444)	
2019-01-08 17:39:00,893 - 10 - training_embed.py - training - Epoch: [1][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2438 (615.2156)	
2019-01-08 17:39:01,113 - 10 - training_embed.py - training - Epoch: [1][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.0438 (615.1031)	
2019-01-08 17:39:01,339 - 10 - training_embed.py - training - Epoch: [1][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.9662 (615.1380)	
2019-01-08 17:39:01,545 - 10 - training_embed.py - training - Epoch: [1][1100/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 616.1385 (615.1324)	
2019-01-08 17:39:01,756 - 10 - training_embed.py - training - Epoch: [1][1200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 615.2345 (615.0438)	
2019-01-08 17:39:01,966 - 10 - training_embed.py - training - Epoch: [1][1300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 617.5866 (615.0309)	
2019-01-08 17:39:02,172 - 10 - training_embed.py - training - Epoch: [1][1400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 616.8588 (615.0481)	
2019-01-08 17:39:02,393 - 10 - training_embed.py - training - Epoch: [1][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.6665 (615.0440)	
2019-01-08 17:39:02,602 - 10 - training_embed.py - training - Epoch: [1][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.3876 (615.0408)	
2019-01-08 17:39:02,820 - 10 - training_embed.py - training - Epoch: [1][1700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.8448 (615.0181)	
2019-01-08 17:39:03,043 - 10 - training_embed.py - training - Epoch: [1][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.8644 (615.0041)	
2019-01-08 17:39:03,272 - 10 - training_embed.py - training - Epoch: [1][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8345 (614.9974)	
2019-01-08 17:39:03,484 - 10 - training_embed.py - training - Epoch: [1][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.4775 (614.9817)	
2019-01-08 17:39:03,706 - 10 - training_embed.py - training - Epoch: [1][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3890 (615.0070)	
2019-01-08 17:39:03,936 - 10 - training_embed.py - training - Epoch: [1][2200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.0054 (614.9493)	
2019-01-08 17:39:04,161 - 10 - training_embed.py - training - Epoch: [1][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.6085 (614.9123)	
2019-01-08 17:39:04,377 - 10 - training_embed.py - training - Epoch: [1][2400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.2232 (614.8824)	
2019-01-08 17:39:04,587 - 10 - training_embed.py - training - Epoch: [1][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.9814 (614.8788)	
2019-01-08 17:39:04,811 - 10 - training_embed.py - training - Epoch: [1][2600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 617.2852 (614.8580)	
2019-01-08 17:39:05,055 - 10 - training_embed.py - training - Epoch: [1][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 619.6038 (614.8311)	
2019-01-08 17:39:05,268 - 10 - training_embed.py - training - Epoch: [1][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.6257 (614.8117)	
2019-01-08 17:39:05,495 - 10 - training_embed.py - training - Epoch: [1][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.8132 (614.7987)	
2019-01-08 17:39:05,708 - 10 - training_embed.py - training - Epoch: [1][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.4375 (614.7738)	
2019-01-08 17:39:05,918 - 10 - training_embed.py - training - Epoch: [1][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.8720 (614.7375)	
2019-01-08 17:39:06,136 - 10 - training_embed.py - training - Epoch: [1][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.8730 (614.7268)	
2019-01-08 17:39:06,354 - 10 - training_embed.py - training - Epoch: [1][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0267 (614.7121)	
2019-01-08 17:39:06,564 - 10 - training_embed.py - training - Epoch: [1][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2204 (614.7005)	
2019-01-08 17:39:06,773 - 10 - training_embed.py - training - Epoch: [1][3500/3661]	Time 0.003 (0.002)	Data 0.000 (0.001)	Loss 613.3045 (614.7045)	
2019-01-08 17:39:07,001 - 10 - training_embed.py - training - Epoch: [1][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 623.0575 (614.6878)	
2019-01-08 17:39:07,133 - 10 - training_embed.py - training - loss: 614.653370
2019-01-08 17:39:07,133 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 17:39:07,544 - 10 - training_embed.py - training - Epoch: [2][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3803 (613.7830)	
2019-01-08 17:39:07,747 - 10 - training_embed.py - training - Epoch: [2][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.3156 (613.9884)	
2019-01-08 17:39:07,959 - 10 - training_embed.py - training - Epoch: [2][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.4670 (613.8741)	
2019-01-08 17:39:08,165 - 10 - training_embed.py - training - Epoch: [2][400/3661]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 616.5031 (613.8689)	
2019-01-08 17:39:08,384 - 10 - training_embed.py - training - Epoch: [2][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.2349 (613.7066)	
2019-01-08 17:39:08,586 - 10 - training_embed.py - training - Epoch: [2][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.8119 (613.7072)	
2019-01-08 17:39:08,802 - 10 - training_embed.py - training - Epoch: [2][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.5143 (613.7201)	
2019-01-08 17:39:09,033 - 10 - training_embed.py - training - Epoch: [2][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.9291 (613.7284)	
2019-01-08 17:39:09,245 - 10 - training_embed.py - training - Epoch: [2][900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 611.6145 (613.7429)	
2019-01-08 17:39:09,472 - 10 - training_embed.py - training - Epoch: [2][1000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 616.2191 (613.6869)	
2019-01-08 17:39:09,689 - 10 - training_embed.py - training - Epoch: [2][1100/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 619.0516 (613.6678)	
2019-01-08 17:39:09,912 - 10 - training_embed.py - training - Epoch: [2][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.8095 (613.7166)	
2019-01-08 17:39:10,143 - 10 - training_embed.py - training - Epoch: [2][1300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.6061 (613.7107)	
2019-01-08 17:39:10,357 - 10 - training_embed.py - training - Epoch: [2][1400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.7487 (613.6952)	
2019-01-08 17:39:10,563 - 10 - training_embed.py - training - Epoch: [2][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.2448 (613.7224)	
2019-01-08 17:39:10,778 - 10 - training_embed.py - training - Epoch: [2][1600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.2984 (613.7170)	
2019-01-08 17:39:10,996 - 10 - training_embed.py - training - Epoch: [2][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.2794 (613.7244)	
2019-01-08 17:39:11,202 - 10 - training_embed.py - training - Epoch: [2][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0490 (613.7049)	
2019-01-08 17:39:11,409 - 10 - training_embed.py - training - Epoch: [2][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.0013 (613.6737)	
2019-01-08 17:39:11,619 - 10 - training_embed.py - training - Epoch: [2][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 616.4335 (613.6427)	
2019-01-08 17:39:11,843 - 10 - training_embed.py - training - Epoch: [2][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.3370 (613.6578)	
2019-01-08 17:39:12,046 - 10 - training_embed.py - training - Epoch: [2][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.0078 (613.6166)	
2019-01-08 17:39:12,254 - 10 - training_embed.py - training - Epoch: [2][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.9113 (613.5966)	
2019-01-08 17:39:12,484 - 10 - training_embed.py - training - Epoch: [2][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.3054 (613.5917)	
2019-01-08 17:39:12,714 - 10 - training_embed.py - training - Epoch: [2][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.7928 (613.5879)	
2019-01-08 17:39:12,943 - 10 - training_embed.py - training - Epoch: [2][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.5134 (613.5911)	
2019-01-08 17:39:13,171 - 10 - training_embed.py - training - Epoch: [2][2700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 608.5102 (613.5911)	
2019-01-08 17:39:13,384 - 10 - training_embed.py - training - Epoch: [2][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.6845 (613.5906)	
2019-01-08 17:39:13,602 - 10 - training_embed.py - training - Epoch: [2][2900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 615.5310 (613.5729)	
2019-01-08 17:39:13,822 - 10 - training_embed.py - training - Epoch: [2][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.2697 (613.5601)	
2019-01-08 17:39:14,048 - 10 - training_embed.py - training - Epoch: [2][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.2718 (613.5455)	
2019-01-08 17:39:14,264 - 10 - training_embed.py - training - Epoch: [2][3200/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 606.4366 (613.5334)	
2019-01-08 17:39:14,477 - 10 - training_embed.py - training - Epoch: [2][3300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 624.4685 (613.5282)	
2019-01-08 17:39:14,686 - 10 - training_embed.py - training - Epoch: [2][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.4747 (613.5115)	
2019-01-08 17:39:14,900 - 10 - training_embed.py - training - Epoch: [2][3500/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 611.8557 (613.5051)	
2019-01-08 17:39:15,107 - 10 - training_embed.py - training - Epoch: [2][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.0823 (613.4861)	
2019-01-08 17:39:15,249 - 10 - training_embed.py - training - loss: 613.441309
2019-01-08 17:39:15,249 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 17:39:15,655 - 10 - training_embed.py - training - Epoch: [3][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3048 (612.8076)	
2019-01-08 17:39:15,866 - 10 - training_embed.py - training - Epoch: [3][200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 620.1957 (612.8978)	
2019-01-08 17:39:16,076 - 10 - training_embed.py - training - Epoch: [3][300/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 610.9257 (612.8359)	
2019-01-08 17:39:16,282 - 10 - training_embed.py - training - Epoch: [3][400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.0618 (612.8360)	
2019-01-08 17:39:16,490 - 10 - training_embed.py - training - Epoch: [3][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.8235 (612.7411)	
2019-01-08 17:39:16,713 - 10 - training_embed.py - training - Epoch: [3][600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.1530 (612.6590)	
2019-01-08 17:39:16,944 - 10 - training_embed.py - training - Epoch: [3][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.8867 (612.6866)	
2019-01-08 17:39:17,155 - 10 - training_embed.py - training - Epoch: [3][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.4548 (612.7229)	
2019-01-08 17:39:17,375 - 10 - training_embed.py - training - Epoch: [3][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.8282 (612.7566)	
2019-01-08 17:39:17,585 - 10 - training_embed.py - training - Epoch: [3][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.6996 (612.7095)	
2019-01-08 17:39:17,806 - 10 - training_embed.py - training - Epoch: [3][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 623.0110 (612.6741)	
2019-01-08 17:39:18,021 - 10 - training_embed.py - training - Epoch: [3][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.5740 (612.6723)	
2019-01-08 17:39:18,233 - 10 - training_embed.py - training - Epoch: [3][1300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.6194 (612.6983)	
2019-01-08 17:39:18,458 - 10 - training_embed.py - training - Epoch: [3][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3602 (612.6696)	
2019-01-08 17:39:18,660 - 10 - training_embed.py - training - Epoch: [3][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.6997 (612.6271)	
2019-01-08 17:39:18,878 - 10 - training_embed.py - training - Epoch: [3][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.1534 (612.6155)	
2019-01-08 17:39:19,087 - 10 - training_embed.py - training - Epoch: [3][1700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.9617 (612.6080)	
2019-01-08 17:39:19,349 - 10 - training_embed.py - training - Epoch: [3][1800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 608.0571 (612.5945)	
2019-01-08 17:39:19,565 - 10 - training_embed.py - training - Epoch: [3][1900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.4153 (612.5541)	
2019-01-08 17:39:19,788 - 10 - training_embed.py - training - Epoch: [3][2000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 616.2894 (612.5589)	
2019-01-08 17:39:20,005 - 10 - training_embed.py - training - Epoch: [3][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.9882 (612.5408)	
2019-01-08 17:39:20,230 - 10 - training_embed.py - training - Epoch: [3][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.3156 (612.5203)	
2019-01-08 17:39:20,436 - 10 - training_embed.py - training - Epoch: [3][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.0810 (612.4963)	
2019-01-08 17:39:20,649 - 10 - training_embed.py - training - Epoch: [3][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.0450 (612.4721)	
2019-01-08 17:39:20,877 - 10 - training_embed.py - training - Epoch: [3][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.2654 (612.4682)	
2019-01-08 17:39:21,086 - 10 - training_embed.py - training - Epoch: [3][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.0673 (612.4521)	
2019-01-08 17:39:21,292 - 10 - training_embed.py - training - Epoch: [3][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.9316 (612.4316)	
2019-01-08 17:39:21,508 - 10 - training_embed.py - training - Epoch: [3][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.9288 (612.3952)	
2019-01-08 17:39:21,724 - 10 - training_embed.py - training - Epoch: [3][2900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.3756 (612.3671)	
2019-01-08 17:39:21,941 - 10 - training_embed.py - training - Epoch: [3][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.9120 (612.3519)	
2019-01-08 17:39:22,153 - 10 - training_embed.py - training - Epoch: [3][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.4446 (612.3158)	
2019-01-08 17:39:22,370 - 10 - training_embed.py - training - Epoch: [3][3200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.2821 (612.2855)	
2019-01-08 17:39:22,589 - 10 - training_embed.py - training - Epoch: [3][3300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.6308 (612.2673)	
2019-01-08 17:39:22,790 - 10 - training_embed.py - training - Epoch: [3][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.2858 (612.2479)	
2019-01-08 17:39:23,004 - 10 - training_embed.py - training - Epoch: [3][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2678 (612.2713)	
2019-01-08 17:39:23,213 - 10 - training_embed.py - training - Epoch: [3][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2090 (612.2715)	
2019-01-08 17:39:23,347 - 10 - training_embed.py - training - loss: 612.229120
2019-01-08 17:39:23,347 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 17:39:23,764 - 10 - training_embed.py - training - Epoch: [4][100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 616.7537 (611.1547)	
2019-01-08 17:39:23,982 - 10 - training_embed.py - training - Epoch: [4][200/3661]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 610.3765 (611.2291)	
2019-01-08 17:39:24,184 - 10 - training_embed.py - training - Epoch: [4][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.1743 (611.4067)	
2019-01-08 17:39:24,396 - 10 - training_embed.py - training - Epoch: [4][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.0250 (611.6939)	
2019-01-08 17:39:24,603 - 10 - training_embed.py - training - Epoch: [4][500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 603.4209 (611.5774)	
2019-01-08 17:39:24,812 - 10 - training_embed.py - training - Epoch: [4][600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 611.0878 (611.6485)	
2019-01-08 17:39:25,042 - 10 - training_embed.py - training - Epoch: [4][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.5261 (611.5523)	
2019-01-08 17:39:25,269 - 10 - training_embed.py - training - Epoch: [4][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.9539 (611.5640)	
2019-01-08 17:39:25,493 - 10 - training_embed.py - training - Epoch: [4][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.8076 (611.4983)	
2019-01-08 17:39:25,702 - 10 - training_embed.py - training - Epoch: [4][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.8870 (611.4987)	
2019-01-08 17:39:25,919 - 10 - training_embed.py - training - Epoch: [4][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.4708 (611.4908)	
2019-01-08 17:39:26,128 - 10 - training_embed.py - training - Epoch: [4][1200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 611.6711 (611.4793)	
2019-01-08 17:39:26,334 - 10 - training_embed.py - training - Epoch: [4][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.7955 (611.4096)	
2019-01-08 17:39:26,549 - 10 - training_embed.py - training - Epoch: [4][1400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.8962 (611.4455)	
2019-01-08 17:39:26,767 - 10 - training_embed.py - training - Epoch: [4][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.0529 (611.4076)	
2019-01-08 17:39:26,995 - 10 - training_embed.py - training - Epoch: [4][1600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 618.5630 (611.3975)	
2019-01-08 17:39:27,200 - 10 - training_embed.py - training - Epoch: [4][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.4909 (611.4008)	
2019-01-08 17:39:27,419 - 10 - training_embed.py - training - Epoch: [4][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.2258 (611.3499)	
2019-01-08 17:39:27,621 - 10 - training_embed.py - training - Epoch: [4][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.9892 (611.3373)	
2019-01-08 17:39:27,839 - 10 - training_embed.py - training - Epoch: [4][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.9341 (611.3380)	
2019-01-08 17:39:28,066 - 10 - training_embed.py - training - Epoch: [4][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2762 (611.3139)	
2019-01-08 17:39:28,291 - 10 - training_embed.py - training - Epoch: [4][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.7631 (611.2848)	
2019-01-08 17:39:28,505 - 10 - training_embed.py - training - Epoch: [4][2300/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 612.7952 (611.2350)	
2019-01-08 17:39:28,721 - 10 - training_embed.py - training - Epoch: [4][2400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.2488 (611.2142)	
2019-01-08 17:39:28,936 - 10 - training_embed.py - training - Epoch: [4][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.1305 (611.2238)	
2019-01-08 17:39:29,147 - 10 - training_embed.py - training - Epoch: [4][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.1638 (611.1872)	
2019-01-08 17:39:29,362 - 10 - training_embed.py - training - Epoch: [4][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.7864 (611.1536)	
2019-01-08 17:39:29,574 - 10 - training_embed.py - training - Epoch: [4][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.7271 (611.1613)	
2019-01-08 17:39:29,790 - 10 - training_embed.py - training - Epoch: [4][2900/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 608.2380 (611.1405)	
2019-01-08 17:39:30,003 - 10 - training_embed.py - training - Epoch: [4][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.8071 (611.1373)	
2019-01-08 17:39:30,214 - 10 - training_embed.py - training - Epoch: [4][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.7693 (611.1395)	
2019-01-08 17:39:30,424 - 10 - training_embed.py - training - Epoch: [4][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.6132 (611.1306)	
2019-01-08 17:39:30,630 - 10 - training_embed.py - training - Epoch: [4][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.6859 (611.1070)	
2019-01-08 17:39:30,851 - 10 - training_embed.py - training - Epoch: [4][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.5136 (611.0828)	
2019-01-08 17:39:31,061 - 10 - training_embed.py - training - Epoch: [4][3500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.0069 (611.0922)	
2019-01-08 17:39:31,290 - 10 - training_embed.py - training - Epoch: [4][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.7422 (611.0787)	
2019-01-08 17:39:31,423 - 10 - training_embed.py - training - loss: 611.016054
2019-01-08 17:39:31,423 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 17:39:31,877 - 10 - training_embed.py - training - Epoch: [5][100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 608.7498 (610.9522)	
2019-01-08 17:39:32,097 - 10 - training_embed.py - training - Epoch: [5][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.9992 (611.0063)	
2019-01-08 17:39:32,324 - 10 - training_embed.py - training - Epoch: [5][300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.5672 (610.8669)	
2019-01-08 17:39:32,532 - 10 - training_embed.py - training - Epoch: [5][400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.4991 (610.7484)	
2019-01-08 17:39:32,756 - 10 - training_embed.py - training - Epoch: [5][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.1299 (610.6217)	
2019-01-08 17:39:32,962 - 10 - training_embed.py - training - Epoch: [5][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.8228 (610.4887)	
2019-01-08 17:39:33,176 - 10 - training_embed.py - training - Epoch: [5][700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.6835 (610.4342)	
2019-01-08 17:39:33,391 - 10 - training_embed.py - training - Epoch: [5][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.6340 (610.3589)	
2019-01-08 17:39:33,604 - 10 - training_embed.py - training - Epoch: [5][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.2115 (610.3343)	
2019-01-08 17:39:33,835 - 10 - training_embed.py - training - Epoch: [5][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.7195 (610.2686)	
2019-01-08 17:39:34,040 - 10 - training_embed.py - training - Epoch: [5][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.9350 (610.2250)	
2019-01-08 17:39:34,265 - 10 - training_embed.py - training - Epoch: [5][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.6131 (610.2242)	
2019-01-08 17:39:34,494 - 10 - training_embed.py - training - Epoch: [5][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.5140 (610.2125)	
2019-01-08 17:39:34,698 - 10 - training_embed.py - training - Epoch: [5][1400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 600.3561 (610.2265)	
2019-01-08 17:39:34,914 - 10 - training_embed.py - training - Epoch: [5][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.1686 (610.2254)	
2019-01-08 17:39:35,144 - 10 - training_embed.py - training - Epoch: [5][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.3447 (610.2250)	
2019-01-08 17:39:35,350 - 10 - training_embed.py - training - Epoch: [5][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.8630 (610.1806)	
2019-01-08 17:39:35,609 - 10 - training_embed.py - training - Epoch: [5][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.6318 (610.1650)	
2019-01-08 17:39:35,860 - 10 - training_embed.py - training - Epoch: [5][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.2978 (610.1329)	
2019-01-08 17:39:36,090 - 10 - training_embed.py - training - Epoch: [5][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.0984 (610.1141)	
2019-01-08 17:39:36,306 - 10 - training_embed.py - training - Epoch: [5][2100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.0422 (610.0977)	
2019-01-08 17:39:36,534 - 10 - training_embed.py - training - Epoch: [5][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.1545 (610.0981)	
2019-01-08 17:39:36,744 - 10 - training_embed.py - training - Epoch: [5][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.4686 (610.0745)	
2019-01-08 17:39:36,969 - 10 - training_embed.py - training - Epoch: [5][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.1672 (610.0693)	
2019-01-08 17:39:37,195 - 10 - training_embed.py - training - Epoch: [5][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.6153 (610.0609)	
2019-01-08 17:39:37,435 - 10 - training_embed.py - training - Epoch: [5][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9426 (610.0250)	
2019-01-08 17:39:37,652 - 10 - training_embed.py - training - Epoch: [5][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.9728 (610.0061)	
2019-01-08 17:39:37,885 - 10 - training_embed.py - training - Epoch: [5][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.1619 (609.9760)	
2019-01-08 17:39:38,130 - 10 - training_embed.py - training - Epoch: [5][2900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.4028 (609.9658)	
2019-01-08 17:39:38,383 - 10 - training_embed.py - training - Epoch: [5][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.3178 (609.9684)	
2019-01-08 17:39:38,679 - 10 - training_embed.py - training - Epoch: [5][3100/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 610.2348 (609.9320)	
2019-01-08 17:39:38,926 - 10 - training_embed.py - training - Epoch: [5][3200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.2528 (609.9018)	
2019-01-08 17:39:39,199 - 10 - training_embed.py - training - Epoch: [5][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.0646 (609.8807)	
2019-01-08 17:39:39,562 - 10 - training_embed.py - training - Epoch: [5][3400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.3881 (609.8689)	
2019-01-08 17:39:39,938 - 10 - training_embed.py - training - Epoch: [5][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.6895 (609.8687)	
2019-01-08 17:39:40,268 - 10 - training_embed.py - training - Epoch: [5][3600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 606.7747 (609.8400)	
2019-01-08 17:39:40,416 - 10 - training_embed.py - training - loss: 609.799528
2019-01-08 17:39:40,416 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 17:39:40,957 - 10 - training_embed.py - training - Epoch: [6][100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.0319 (608.7080)	
2019-01-08 17:39:41,200 - 10 - training_embed.py - training - Epoch: [6][200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 618.8997 (609.0189)	
2019-01-08 17:39:41,443 - 10 - training_embed.py - training - Epoch: [6][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.1014 (609.0409)	
2019-01-08 17:39:41,669 - 10 - training_embed.py - training - Epoch: [6][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.3273 (609.1293)	
2019-01-08 17:39:41,893 - 10 - training_embed.py - training - Epoch: [6][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.2849 (609.1135)	
2019-01-08 17:39:42,132 - 10 - training_embed.py - training - Epoch: [6][600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.4852 (609.0947)	
2019-01-08 17:39:42,421 - 10 - training_embed.py - training - Epoch: [6][700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.2035 (609.2044)	
2019-01-08 17:39:42,681 - 10 - training_embed.py - training - Epoch: [6][800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.5925 (609.2238)	
2019-01-08 17:39:42,955 - 10 - training_embed.py - training - Epoch: [6][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.9485 (609.1923)	
2019-01-08 17:39:43,237 - 10 - training_embed.py - training - Epoch: [6][1000/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 610.2828 (609.1293)	
2019-01-08 17:39:43,501 - 10 - training_embed.py - training - Epoch: [6][1100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 604.7103 (609.1250)	
2019-01-08 17:39:43,770 - 10 - training_embed.py - training - Epoch: [6][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.8907 (609.1004)	
2019-01-08 17:39:44,034 - 10 - training_embed.py - training - Epoch: [6][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.2975 (609.0587)	
2019-01-08 17:39:44,263 - 10 - training_embed.py - training - Epoch: [6][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9258 (609.0698)	
2019-01-08 17:39:44,504 - 10 - training_embed.py - training - Epoch: [6][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.9134 (609.0510)	
2019-01-08 17:39:44,722 - 10 - training_embed.py - training - Epoch: [6][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.9095 (608.9898)	
2019-01-08 17:39:44,980 - 10 - training_embed.py - training - Epoch: [6][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.2656 (608.9807)	
2019-01-08 17:39:45,227 - 10 - training_embed.py - training - Epoch: [6][1800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.6426 (608.9687)	
2019-01-08 17:39:45,470 - 10 - training_embed.py - training - Epoch: [6][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.7153 (608.9357)	
2019-01-08 17:39:45,722 - 10 - training_embed.py - training - Epoch: [6][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.0287 (608.8828)	
2019-01-08 17:39:45,966 - 10 - training_embed.py - training - Epoch: [6][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.1900 (608.8374)	
2019-01-08 17:39:46,226 - 10 - training_embed.py - training - Epoch: [6][2200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.2780 (608.8111)	
2019-01-08 17:39:46,501 - 10 - training_embed.py - training - Epoch: [6][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.0088 (608.8043)	
2019-01-08 17:39:46,780 - 10 - training_embed.py - training - Epoch: [6][2400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 606.9227 (608.8028)	
2019-01-08 17:39:47,055 - 10 - training_embed.py - training - Epoch: [6][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.7534 (608.7870)	
2019-01-08 17:39:47,374 - 10 - training_embed.py - training - Epoch: [6][2600/3661]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 609.3218 (608.7911)	
2019-01-08 17:39:47,624 - 10 - training_embed.py - training - Epoch: [6][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.7269 (608.7916)	
2019-01-08 17:39:47,886 - 10 - training_embed.py - training - Epoch: [6][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.9677 (608.7733)	
2019-01-08 17:39:48,138 - 10 - training_embed.py - training - Epoch: [6][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.4273 (608.7476)	
2019-01-08 17:39:48,376 - 10 - training_embed.py - training - Epoch: [6][3000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.9288 (608.7237)	
2019-01-08 17:39:48,623 - 10 - training_embed.py - training - Epoch: [6][3100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 616.9914 (608.6993)	
2019-01-08 17:39:48,888 - 10 - training_embed.py - training - Epoch: [6][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.8356 (608.6816)	
2019-01-08 17:39:49,146 - 10 - training_embed.py - training - Epoch: [6][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.4033 (608.6753)	
2019-01-08 17:39:49,415 - 10 - training_embed.py - training - Epoch: [6][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.3915 (608.6617)	
2019-01-08 17:39:49,656 - 10 - training_embed.py - training - Epoch: [6][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.9136 (608.6444)	
2019-01-08 17:39:49,953 - 10 - training_embed.py - training - Epoch: [6][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.6923 (608.6178)	
2019-01-08 17:39:50,114 - 10 - training_embed.py - training - loss: 608.579727
2019-01-08 17:39:50,114 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 17:39:50,672 - 10 - training_embed.py - training - Epoch: [7][100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.2083 (607.9600)	
2019-01-08 17:39:50,915 - 10 - training_embed.py - training - Epoch: [7][200/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 608.0413 (607.9607)	
2019-01-08 17:39:51,168 - 10 - training_embed.py - training - Epoch: [7][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.6857 (608.0532)	
2019-01-08 17:39:51,407 - 10 - training_embed.py - training - Epoch: [7][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.7329 (608.0415)	
2019-01-08 17:39:51,655 - 10 - training_embed.py - training - Epoch: [7][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.1611 (608.0164)	
2019-01-08 17:39:51,947 - 10 - training_embed.py - training - Epoch: [7][600/3661]	Time 0.006 (0.002)	Data 0.002 (0.001)	Loss 609.8404 (607.8591)	
2019-01-08 17:39:52,262 - 10 - training_embed.py - training - Epoch: [7][700/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 604.3143 (607.7843)	
2019-01-08 17:39:52,569 - 10 - training_embed.py - training - Epoch: [7][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.3096 (607.7486)	
2019-01-08 17:39:52,861 - 10 - training_embed.py - training - Epoch: [7][900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.8478 (607.6773)	
2019-01-08 17:39:53,113 - 10 - training_embed.py - training - Epoch: [7][1000/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 609.4755 (607.6676)	
2019-01-08 17:39:53,369 - 10 - training_embed.py - training - Epoch: [7][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3425 (607.6723)	
2019-01-08 17:39:53,635 - 10 - training_embed.py - training - Epoch: [7][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.4673 (607.5781)	
2019-01-08 17:39:53,910 - 10 - training_embed.py - training - Epoch: [7][1300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.4655 (607.6789)	
2019-01-08 17:39:54,186 - 10 - training_embed.py - training - Epoch: [7][1400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.0062 (607.6576)	
2019-01-08 17:39:54,428 - 10 - training_embed.py - training - Epoch: [7][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.1199 (607.6412)	
2019-01-08 17:39:54,666 - 10 - training_embed.py - training - Epoch: [7][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.5529 (607.6535)	
2019-01-08 17:39:54,898 - 10 - training_embed.py - training - Epoch: [7][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.4088 (607.6964)	
2019-01-08 17:39:55,136 - 10 - training_embed.py - training - Epoch: [7][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.5842 (607.6783)	
2019-01-08 17:39:55,362 - 10 - training_embed.py - training - Epoch: [7][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.8376 (607.6600)	
2019-01-08 17:39:55,620 - 10 - training_embed.py - training - Epoch: [7][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.8240 (607.6502)	
2019-01-08 17:39:55,860 - 10 - training_embed.py - training - Epoch: [7][2100/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 604.2784 (607.6393)	
2019-01-08 17:39:56,113 - 10 - training_embed.py - training - Epoch: [7][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.8631 (607.6515)	
2019-01-08 17:39:56,373 - 10 - training_embed.py - training - Epoch: [7][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.1364 (607.6197)	
2019-01-08 17:39:56,611 - 10 - training_embed.py - training - Epoch: [7][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.7883 (607.6287)	
2019-01-08 17:39:56,852 - 10 - training_embed.py - training - Epoch: [7][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.9897 (607.5991)	
2019-01-08 17:39:57,097 - 10 - training_embed.py - training - Epoch: [7][2600/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 607.3260 (607.5883)	
2019-01-08 17:39:57,368 - 10 - training_embed.py - training - Epoch: [7][2700/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 607.3516 (607.5488)	
2019-01-08 17:39:57,600 - 10 - training_embed.py - training - Epoch: [7][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.5383 (607.5431)	
2019-01-08 17:39:57,834 - 10 - training_embed.py - training - Epoch: [7][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.5814 (607.5299)	
2019-01-08 17:39:58,118 - 10 - training_embed.py - training - Epoch: [7][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.5624 (607.5401)	
2019-01-08 17:39:58,376 - 10 - training_embed.py - training - Epoch: [7][3100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 606.0181 (607.5118)	
2019-01-08 17:39:58,609 - 10 - training_embed.py - training - Epoch: [7][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.5941 (607.4754)	
2019-01-08 17:39:58,844 - 10 - training_embed.py - training - Epoch: [7][3300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.6531 (607.4476)	
2019-01-08 17:39:59,055 - 10 - training_embed.py - training - Epoch: [7][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.0411 (607.4431)	
2019-01-08 17:39:59,276 - 10 - training_embed.py - training - Epoch: [7][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.8509 (607.3988)	
2019-01-08 17:39:59,476 - 10 - training_embed.py - training - Epoch: [7][3600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 599.6829 (607.3975)	
2019-01-08 17:39:59,608 - 10 - training_embed.py - training - loss: 607.354086
2019-01-08 17:39:59,608 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 17:40:00,069 - 10 - training_embed.py - training - Epoch: [8][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.7989 (607.1139)	
2019-01-08 17:40:00,288 - 10 - training_embed.py - training - Epoch: [8][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.7766 (606.9661)	
2019-01-08 17:40:00,497 - 10 - training_embed.py - training - Epoch: [8][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.9826 (606.7472)	
2019-01-08 17:40:00,707 - 10 - training_embed.py - training - Epoch: [8][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2256 (606.9213)	
2019-01-08 17:40:00,942 - 10 - training_embed.py - training - Epoch: [8][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.8395 (606.7151)	
2019-01-08 17:40:01,189 - 10 - training_embed.py - training - Epoch: [8][600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.6445 (606.6583)	
2019-01-08 17:40:01,422 - 10 - training_embed.py - training - Epoch: [8][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.4872 (606.6286)	
2019-01-08 17:40:01,630 - 10 - training_embed.py - training - Epoch: [8][800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 602.8737 (606.6180)	
2019-01-08 17:40:01,852 - 10 - training_embed.py - training - Epoch: [8][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.8940 (606.6576)	
2019-01-08 17:40:02,081 - 10 - training_embed.py - training - Epoch: [8][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.3036 (606.6271)	
2019-01-08 17:40:02,316 - 10 - training_embed.py - training - Epoch: [8][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.1481 (606.5887)	
2019-01-08 17:40:02,533 - 10 - training_embed.py - training - Epoch: [8][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.3958 (606.6065)	
2019-01-08 17:40:02,758 - 10 - training_embed.py - training - Epoch: [8][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.6479 (606.5278)	
2019-01-08 17:40:02,974 - 10 - training_embed.py - training - Epoch: [8][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5060 (606.5137)	
2019-01-08 17:40:03,181 - 10 - training_embed.py - training - Epoch: [8][1500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.8932 (606.4623)	
2019-01-08 17:40:03,411 - 10 - training_embed.py - training - Epoch: [8][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6995 (606.4560)	
2019-01-08 17:40:03,616 - 10 - training_embed.py - training - Epoch: [8][1700/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 602.8456 (606.4290)	
2019-01-08 17:40:03,828 - 10 - training_embed.py - training - Epoch: [8][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.6547 (606.4312)	
2019-01-08 17:40:04,034 - 10 - training_embed.py - training - Epoch: [8][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6826 (606.4131)	
2019-01-08 17:40:04,257 - 10 - training_embed.py - training - Epoch: [8][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.0611 (606.4486)	
2019-01-08 17:40:04,491 - 10 - training_embed.py - training - Epoch: [8][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.4677 (606.4462)	
2019-01-08 17:40:04,725 - 10 - training_embed.py - training - Epoch: [8][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.1387 (606.4365)	
2019-01-08 17:40:04,965 - 10 - training_embed.py - training - Epoch: [8][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.6317 (606.3813)	
2019-01-08 17:40:05,190 - 10 - training_embed.py - training - Epoch: [8][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.6024 (606.3687)	
2019-01-08 17:40:05,411 - 10 - training_embed.py - training - Epoch: [8][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.8548 (606.3427)	
2019-01-08 17:40:05,622 - 10 - training_embed.py - training - Epoch: [8][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.4026 (606.3146)	
2019-01-08 17:40:05,846 - 10 - training_embed.py - training - Epoch: [8][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.1009 (606.2819)	
2019-01-08 17:40:06,074 - 10 - training_embed.py - training - Epoch: [8][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.3093 (606.2608)	
2019-01-08 17:40:06,283 - 10 - training_embed.py - training - Epoch: [8][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.7335 (606.2543)	
2019-01-08 17:40:06,494 - 10 - training_embed.py - training - Epoch: [8][3000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.4612 (606.2304)	
2019-01-08 17:40:06,697 - 10 - training_embed.py - training - Epoch: [8][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.6148 (606.2173)	
2019-01-08 17:40:06,949 - 10 - training_embed.py - training - Epoch: [8][3200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 600.9963 (606.2105)	
2019-01-08 17:40:07,253 - 10 - training_embed.py - training - Epoch: [8][3300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 613.8110 (606.2173)	
2019-01-08 17:40:07,526 - 10 - training_embed.py - training - Epoch: [8][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.7111 (606.2064)	
2019-01-08 17:40:07,766 - 10 - training_embed.py - training - Epoch: [8][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.2708 (606.1920)	
2019-01-08 17:40:08,006 - 10 - training_embed.py - training - Epoch: [8][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6309 (606.1775)	
2019-01-08 17:40:08,189 - 10 - training_embed.py - training - loss: 606.122364
2019-01-08 17:40:08,190 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 17:40:08,856 - 10 - training_embed.py - training - Epoch: [9][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.2479 (605.1151)	
2019-01-08 17:40:09,143 - 10 - training_embed.py - training - Epoch: [9][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.1954 (605.6224)	
2019-01-08 17:40:09,413 - 10 - training_embed.py - training - Epoch: [9][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.3803 (606.1340)	
2019-01-08 17:40:09,683 - 10 - training_embed.py - training - Epoch: [9][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.9464 (605.8186)	
2019-01-08 17:40:09,913 - 10 - training_embed.py - training - Epoch: [9][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.9545 (605.7096)	
2019-01-08 17:40:10,143 - 10 - training_embed.py - training - Epoch: [9][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.2584 (605.6987)	
2019-01-08 17:40:10,372 - 10 - training_embed.py - training - Epoch: [9][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.1873 (605.6855)	
2019-01-08 17:40:10,594 - 10 - training_embed.py - training - Epoch: [9][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.8691 (605.6351)	
2019-01-08 17:40:10,810 - 10 - training_embed.py - training - Epoch: [9][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.6050 (605.5776)	
2019-01-08 17:40:11,055 - 10 - training_embed.py - training - Epoch: [9][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.1540 (605.5164)	
2019-01-08 17:40:11,331 - 10 - training_embed.py - training - Epoch: [9][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.1552 (605.4765)	
2019-01-08 17:40:11,587 - 10 - training_embed.py - training - Epoch: [9][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.1832 (605.4215)	
2019-01-08 17:40:11,823 - 10 - training_embed.py - training - Epoch: [9][1300/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 605.8804 (605.3804)	
2019-01-08 17:40:12,057 - 10 - training_embed.py - training - Epoch: [9][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.3397 (605.3862)	
2019-01-08 17:40:12,280 - 10 - training_embed.py - training - Epoch: [9][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.8666 (605.3383)	
2019-01-08 17:40:12,505 - 10 - training_embed.py - training - Epoch: [9][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.6296 (605.3052)	
2019-01-08 17:40:12,747 - 10 - training_embed.py - training - Epoch: [9][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.7686 (605.2392)	
2019-01-08 17:40:12,977 - 10 - training_embed.py - training - Epoch: [9][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.4154 (605.2529)	
2019-01-08 17:40:13,200 - 10 - training_embed.py - training - Epoch: [9][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.2592 (605.2183)	
2019-01-08 17:40:13,428 - 10 - training_embed.py - training - Epoch: [9][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 597.3889 (605.1349)	
2019-01-08 17:40:13,650 - 10 - training_embed.py - training - Epoch: [9][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.8522 (605.1338)	
2019-01-08 17:40:13,877 - 10 - training_embed.py - training - Epoch: [9][2200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.3235 (605.1464)	
2019-01-08 17:40:14,092 - 10 - training_embed.py - training - Epoch: [9][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.7150 (605.1062)	
2019-01-08 17:40:14,312 - 10 - training_embed.py - training - Epoch: [9][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.7563 (605.0993)	
2019-01-08 17:40:14,531 - 10 - training_embed.py - training - Epoch: [9][2500/3661]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 604.2753 (605.0717)	
2019-01-08 17:40:14,740 - 10 - training_embed.py - training - Epoch: [9][2600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 611.7922 (605.0806)	
2019-01-08 17:40:14,959 - 10 - training_embed.py - training - Epoch: [9][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 592.9489 (605.0493)	
2019-01-08 17:40:15,224 - 10 - training_embed.py - training - Epoch: [9][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.7791 (605.0493)	
2019-01-08 17:40:15,456 - 10 - training_embed.py - training - Epoch: [9][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.9955 (605.0238)	
2019-01-08 17:40:15,688 - 10 - training_embed.py - training - Epoch: [9][3000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 606.1613 (605.0236)	
2019-01-08 17:40:15,902 - 10 - training_embed.py - training - Epoch: [9][3100/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 604.1304 (605.0058)	
2019-01-08 17:40:16,115 - 10 - training_embed.py - training - Epoch: [9][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.8885 (604.9898)	
2019-01-08 17:40:16,332 - 10 - training_embed.py - training - Epoch: [9][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.3035 (604.9728)	
2019-01-08 17:40:16,552 - 10 - training_embed.py - training - Epoch: [9][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.4349 (604.9528)	
2019-01-08 17:40:16,768 - 10 - training_embed.py - training - Epoch: [9][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.4761 (604.9438)	
2019-01-08 17:40:16,986 - 10 - training_embed.py - training - Epoch: [9][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.4578 (604.9299)	
2019-01-08 17:40:17,122 - 10 - training_embed.py - training - loss: 604.882457
2019-01-08 17:40:17,122 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 17:40:17,533 - 10 - training_embed.py - training - Epoch: [10][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.6254 (604.6062)	
2019-01-08 17:40:17,741 - 10 - training_embed.py - training - Epoch: [10][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.2896 (604.5698)	
2019-01-08 17:40:17,956 - 10 - training_embed.py - training - Epoch: [10][300/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 605.7949 (604.3899)	
2019-01-08 17:40:18,182 - 10 - training_embed.py - training - Epoch: [10][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.2106 (604.4861)	
2019-01-08 17:40:18,400 - 10 - training_embed.py - training - Epoch: [10][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.1165 (604.3469)	
2019-01-08 17:40:18,614 - 10 - training_embed.py - training - Epoch: [10][600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.8081 (604.2233)	
2019-01-08 17:40:18,826 - 10 - training_embed.py - training - Epoch: [10][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.0735 (604.0573)	
2019-01-08 17:40:19,048 - 10 - training_embed.py - training - Epoch: [10][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.4860 (604.0456)	
2019-01-08 17:40:19,282 - 10 - training_embed.py - training - Epoch: [10][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.3555 (604.0074)	
2019-01-08 17:40:19,491 - 10 - training_embed.py - training - Epoch: [10][1000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 600.6021 (603.9293)	
2019-01-08 17:40:19,701 - 10 - training_embed.py - training - Epoch: [10][1100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 601.8254 (603.9166)	
2019-01-08 17:40:19,931 - 10 - training_embed.py - training - Epoch: [10][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.4946 (603.9297)	
2019-01-08 17:40:20,141 - 10 - training_embed.py - training - Epoch: [10][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3752 (603.8842)	
2019-01-08 17:40:20,352 - 10 - training_embed.py - training - Epoch: [10][1400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 598.6766 (603.9035)	
2019-01-08 17:40:20,580 - 10 - training_embed.py - training - Epoch: [10][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.4952 (603.9568)	
2019-01-08 17:40:20,788 - 10 - training_embed.py - training - Epoch: [10][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.5013 (603.8934)	
2019-01-08 17:40:20,995 - 10 - training_embed.py - training - Epoch: [10][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.9579 (603.8747)	
2019-01-08 17:40:21,215 - 10 - training_embed.py - training - Epoch: [10][1800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.4109 (603.8892)	
2019-01-08 17:40:21,432 - 10 - training_embed.py - training - Epoch: [10][1900/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 595.9378 (603.9273)	
2019-01-08 17:40:21,645 - 10 - training_embed.py - training - Epoch: [10][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.1169 (603.9035)	
2019-01-08 17:40:21,879 - 10 - training_embed.py - training - Epoch: [10][2100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.1333 (603.8730)	
2019-01-08 17:40:22,103 - 10 - training_embed.py - training - Epoch: [10][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.6910 (603.8436)	
2019-01-08 17:40:22,325 - 10 - training_embed.py - training - Epoch: [10][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.2420 (603.8464)	
2019-01-08 17:40:22,537 - 10 - training_embed.py - training - Epoch: [10][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.3124 (603.8473)	
2019-01-08 17:40:22,750 - 10 - training_embed.py - training - Epoch: [10][2500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 604.6755 (603.8222)	
2019-01-08 17:40:22,976 - 10 - training_embed.py - training - Epoch: [10][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.5018 (603.7917)	
2019-01-08 17:40:23,199 - 10 - training_embed.py - training - Epoch: [10][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.6087 (603.7871)	
2019-01-08 17:40:23,424 - 10 - training_embed.py - training - Epoch: [10][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.7263 (603.7807)	
2019-01-08 17:40:23,655 - 10 - training_embed.py - training - Epoch: [10][2900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.7755 (603.7719)	
2019-01-08 17:40:23,868 - 10 - training_embed.py - training - Epoch: [10][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.0849 (603.7536)	
2019-01-08 17:40:24,086 - 10 - training_embed.py - training - Epoch: [10][3100/3661]	Time 0.007 (0.002)	Data 0.003 (0.001)	Loss 600.6116 (603.7351)	
2019-01-08 17:40:24,293 - 10 - training_embed.py - training - Epoch: [10][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.2689 (603.7347)	
2019-01-08 17:40:24,523 - 10 - training_embed.py - training - Epoch: [10][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.0305 (603.7129)	
2019-01-08 17:40:24,740 - 10 - training_embed.py - training - Epoch: [10][3400/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 605.7845 (603.7139)	
2019-01-08 17:40:24,952 - 10 - training_embed.py - training - Epoch: [10][3500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 601.7866 (603.7014)	
2019-01-08 17:40:25,158 - 10 - training_embed.py - training - Epoch: [10][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.4661 (603.6724)	
2019-01-08 17:40:25,299 - 10 - training_embed.py - training - loss: 603.634039
2019-01-08 17:40:25,299 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 17:40:25,751 - 10 - training_embed.py - training - Epoch: [11][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.7066 (602.8778)	
2019-01-08 17:40:25,959 - 10 - training_embed.py - training - Epoch: [11][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.5583 (602.6748)	
2019-01-08 17:40:26,176 - 10 - training_embed.py - training - Epoch: [11][300/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 606.8336 (603.0194)	
2019-01-08 17:40:26,385 - 10 - training_embed.py - training - Epoch: [11][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.0228 (603.1002)	
2019-01-08 17:40:26,610 - 10 - training_embed.py - training - Epoch: [11][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.5276 (603.1046)	
2019-01-08 17:40:26,823 - 10 - training_embed.py - training - Epoch: [11][600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 596.6791 (602.9904)	
2019-01-08 17:40:27,028 - 10 - training_embed.py - training - Epoch: [11][700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 602.3899 (602.9733)	
2019-01-08 17:40:27,249 - 10 - training_embed.py - training - Epoch: [11][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.5409 (602.9261)	
2019-01-08 17:40:27,463 - 10 - training_embed.py - training - Epoch: [11][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.6370 (602.9200)	
2019-01-08 17:40:27,692 - 10 - training_embed.py - training - Epoch: [11][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.9587 (602.8400)	
2019-01-08 17:40:27,918 - 10 - training_embed.py - training - Epoch: [11][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.4319 (602.7668)	
2019-01-08 17:40:28,143 - 10 - training_embed.py - training - Epoch: [11][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.6007 (602.7542)	
2019-01-08 17:40:28,367 - 10 - training_embed.py - training - Epoch: [11][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.2152 (602.7610)	
2019-01-08 17:40:28,569 - 10 - training_embed.py - training - Epoch: [11][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.5316 (602.6971)	
2019-01-08 17:40:28,793 - 10 - training_embed.py - training - Epoch: [11][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.9598 (602.7034)	
2019-01-08 17:40:29,015 - 10 - training_embed.py - training - Epoch: [11][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.4500 (602.7316)	
2019-01-08 17:40:29,231 - 10 - training_embed.py - training - Epoch: [11][1700/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 600.4960 (602.7028)	
2019-01-08 17:40:29,455 - 10 - training_embed.py - training - Epoch: [11][1800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 597.4534 (602.6846)	
2019-01-08 17:40:29,663 - 10 - training_embed.py - training - Epoch: [11][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.2819 (602.6815)	
2019-01-08 17:40:29,881 - 10 - training_embed.py - training - Epoch: [11][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.5055 (602.6551)	
2019-01-08 17:40:30,087 - 10 - training_embed.py - training - Epoch: [11][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.3399 (602.6375)	
2019-01-08 17:40:30,316 - 10 - training_embed.py - training - Epoch: [11][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.1448 (602.6033)	
2019-01-08 17:40:30,522 - 10 - training_embed.py - training - Epoch: [11][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.5665 (602.6036)	
2019-01-08 17:40:30,733 - 10 - training_embed.py - training - Epoch: [11][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.7631 (602.5972)	
2019-01-08 17:40:30,937 - 10 - training_embed.py - training - Epoch: [11][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.0962 (602.6001)	
2019-01-08 17:40:31,150 - 10 - training_embed.py - training - Epoch: [11][2600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 601.6873 (602.6057)	
2019-01-08 17:40:31,369 - 10 - training_embed.py - training - Epoch: [11][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.8600 (602.6007)	
2019-01-08 17:40:31,576 - 10 - training_embed.py - training - Epoch: [11][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.0525 (602.5742)	
2019-01-08 17:40:31,801 - 10 - training_embed.py - training - Epoch: [11][2900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 602.0380 (602.5413)	
2019-01-08 17:40:32,025 - 10 - training_embed.py - training - Epoch: [11][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.1972 (602.5088)	
2019-01-08 17:40:32,231 - 10 - training_embed.py - training - Epoch: [11][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 594.4000 (602.4962)	
2019-01-08 17:40:32,460 - 10 - training_embed.py - training - Epoch: [11][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.2095 (602.4815)	
2019-01-08 17:40:32,678 - 10 - training_embed.py - training - Epoch: [11][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.9549 (602.4747)	
2019-01-08 17:40:32,900 - 10 - training_embed.py - training - Epoch: [11][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.7994 (602.4568)	
2019-01-08 17:40:33,112 - 10 - training_embed.py - training - Epoch: [11][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.3885 (602.4287)	
2019-01-08 17:40:33,334 - 10 - training_embed.py - training - Epoch: [11][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.5634 (602.4302)	
2019-01-08 17:40:33,470 - 10 - training_embed.py - training - loss: 602.373973
2019-01-08 17:40:33,594 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 17:40:35,481 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1886.303 ms ~ 0.031 min ~ 1.886 sec
2019-01-08 17:40:37,468 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 3873.745 ms ~ 0.065 min ~ 3.874 sec
2019-01-08 17:40:37,468 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 17:40:37,469 - 10 - corpus.py - subactivity_sampler - 0 / 157
2019-01-08 17:40:37,469 - 10 - corpus.py - subactivity_sampler - [112732.  26065.  66529.  96999.  47948.  67892.  66970. 106408.  42773.
  83360.  14976. 144642.  59831.]
2019-01-08 17:45:33,629 - 10 - corpus.py - subactivity_sampler - 20 / 157
2019-01-08 17:45:33,630 - 10 - corpus.py - subactivity_sampler - [113320.  25284.  65704.  98006.  47762.  68073.  66714. 106849.  41557.
  84451.  14326. 146030.  59049.]
2019-01-08 17:49:59,666 - 10 - corpus.py - subactivity_sampler - 40 / 157
2019-01-08 17:49:59,666 - 10 - corpus.py - subactivity_sampler - [113581.  25006.  64470.  99439.  46534.  67987.  65984. 109822.  39837.
  84949.  13064. 148037.  58415.]
2019-01-08 17:55:13,203 - 10 - corpus.py - subactivity_sampler - 60 / 157
2019-01-08 17:55:13,204 - 10 - corpus.py - subactivity_sampler - [114275.  24281.  63947.  99893.  46264.  67779.  65352. 111610.  38388.
  85694.  12734. 148705.  58203.]
2019-01-08 18:00:05,732 - 10 - corpus.py - subactivity_sampler - 80 / 157
2019-01-08 18:00:05,733 - 10 - corpus.py - subactivity_sampler - [114303.  24165.  63454. 100690.  46151.  67457.  64571. 112871.  36901.
  86375.  12357. 150134.  57696.]
2019-01-08 18:05:42,037 - 10 - corpus.py - subactivity_sampler - 100 / 157
2019-01-08 18:05:42,037 - 10 - corpus.py - subactivity_sampler - [114551.  23930.  62941. 101220.  45926.  67097.  63946. 114875.  34693.
  87510.  11657. 151828.  56951.]
2019-01-08 18:09:27,217 - 10 - corpus.py - subactivity_sampler - 120 / 157
2019-01-08 18:09:27,217 - 10 - corpus.py - subactivity_sampler - [114744.  23922.  62661. 101895.  44814.  67314.  63715. 115314.  34008.
  88333.  11318. 152879.  56208.]
2019-01-08 18:15:44,970 - 10 - corpus.py - subactivity_sampler - 140 / 157
2019-01-08 18:15:44,970 - 10 - corpus.py - subactivity_sampler - [114871.  23637.  62038. 103181.  43966.  67133.  63825. 115710.  32514.
  89550.  10871. 154512.  55317.]
2019-01-08 18:20:37,197 - 10 - corpus.py - subactivity_sampler - [115245.  23373.  61682. 103342.  43886.  66866.  63623. 116778.  31412.
  89781.  10647. 155627.  54863.]
2019-01-08 18:20:37,197 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 2399729.111 ms ~ 39.995 min ~ 2399.729 sec
2019-01-08 18:20:37,197 - 10 - corpus.py - ordering_sampler - .
2019-01-08 18:20:48,137 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 18:20:48,137 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 20.  47.  15.  14. 120.  15.  34.  59.  38.   1.   9.   2.]
2019-01-08 18:20:48,138 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 18:20:48,163 - 10 - corpus.py - rho_sampling - ['50.3595', '6.2006', '920.4190', '70.6176', '0.2108', '10.1325', '34.5679', '40.3259', '3.0843', '29.3858', '35.2015', '2.4771']
2019-01-08 18:20:48,163 - 10 - pipeline.py - baseline - Iteration 4
2019-01-08 18:20:48,585 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 18:20:48,604 - 10 - accuracy_class.py - mof - # gt_labels: 14   # pr_labels: 13
2019-01-08 18:20:48,605 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 26', '1: 2', '2: 11', '3: 27', '4: 10', '5: 17', '6: 28', '7: 14', '8: 31', '9: 16', '10: 4', '11: 29', '12: 30']
2019-01-08 18:20:48,652 - 10 - accuracy_class.py - mof_val - frames true: 328245	frames overall : 937125
2019-01-08 18:20:48,653 - 10 - corpus.py - accuracy_corpus - Action: pancake
2019-01-08 18:20:48,653 - 10 - corpus.py - accuracy_corpus - MoF val: 0.35026810724289714
2019-01-08 18:20:48,653 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.34794291049753234
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35402
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 2: 0.263087  9609 / 36524
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1774
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 10: 0.057324  632 / 11025
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 11: 0.090628  4160 / 45902
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 14: 0.102241  1807 / 17674
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 207
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 17: 0.331229  12909 / 38973
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 26: 0.769312  41579 / 54047
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 27: 0.501530  84434 / 168353
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 28: 0.308208  19308 / 62646
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 29: 0.305839  131676 / 430540
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 30: 0.775329  22131 / 28544
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - label 31: 0.000000  0 / 5514
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - mof_classes - average class mof: 0.250338
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35402
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - iou_classes - label 2: 0.191079  9609 / 50288
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 12421
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - iou_classes - label 10: 0.011644  632 / 54279
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - iou_classes - label 11: 0.040223  4160 / 103424
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - iou_classes - label 14: 0.013623  1807 / 132645
2019-01-08 18:20:48,653 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 89988
2019-01-08 18:20:48,654 - 10 - accuracy_class.py - iou_classes - label 17: 0.138911  12909 / 92930
2019-01-08 18:20:48,654 - 10 - accuracy_class.py - iou_classes - label 26: 0.325566  41579 / 127713
2019-01-08 18:20:48,654 - 10 - accuracy_class.py - iou_classes - label 27: 0.450889  84434 / 187261
2019-01-08 18:20:48,654 - 10 - accuracy_class.py - iou_classes - label 28: 0.180514  19308 / 106961
2019-01-08 18:20:48,654 - 10 - accuracy_class.py - iou_classes - label 29: 0.289722  131676 / 454491
2019-01-08 18:20:48,654 - 10 - accuracy_class.py - iou_classes - label 30: 0.361169  22131 / 61276
2019-01-08 18:20:48,654 - 10 - accuracy_class.py - iou_classes - label 31: 0.000000  0 / 36926
2019-01-08 18:20:48,654 - 10 - accuracy_class.py - iou_classes - average IoU: 0.154103
2019-01-08 18:20:48,654 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.143096
2019-01-08 18:21:04,503 - 10 - f1_score.py - f1 - f1 score: 0.288523
2019-01-08 18:21:04,533 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 16370.692 ms ~ 0.273 min ~ 16.371 sec
2019-01-08 18:21:04,534 - 10 - corpus.py - embedding_training - .
2019-01-08 18:21:04,534 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 18:21:04,534 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 18:21:04,534 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 18:21:10,270 - 10 - training_embed.py - training - create model
2019-01-08 18:21:10,271 - 10 - training_embed.py - training - epochs: 12
2019-01-08 18:21:10,271 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 18:21:10,677 - 10 - training_embed.py - training - Epoch: [0][100/3661]	Time 0.003 (0.004)	Data 0.001 (0.003)	Loss 617.2391 (616.7195)	
2019-01-08 18:21:10,885 - 10 - training_embed.py - training - Epoch: [0][200/3661]	Time 0.001 (0.003)	Data 0.000 (0.002)	Loss 613.5550 (616.5393)	
2019-01-08 18:21:11,114 - 10 - training_embed.py - training - Epoch: [0][300/3661]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 616.8322 (616.7195)	
2019-01-08 18:21:11,323 - 10 - training_embed.py - training - Epoch: [0][400/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 619.6660 (616.7294)	
2019-01-08 18:21:11,594 - 10 - training_embed.py - training - Epoch: [0][500/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 612.8017 (616.7331)	
2019-01-08 18:21:11,848 - 10 - training_embed.py - training - Epoch: [0][600/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 617.4665 (616.9003)	
2019-01-08 18:21:12,085 - 10 - training_embed.py - training - Epoch: [0][700/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 616.6221 (616.9184)	
2019-01-08 18:21:12,334 - 10 - training_embed.py - training - Epoch: [0][800/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 616.1326 (616.8733)	
2019-01-08 18:21:12,560 - 10 - training_embed.py - training - Epoch: [0][900/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 614.1554 (616.8670)	
2019-01-08 18:21:12,798 - 10 - training_embed.py - training - Epoch: [0][1000/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 613.2721 (616.8639)	
2019-01-08 18:21:13,012 - 10 - training_embed.py - training - Epoch: [0][1100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.2200 (616.7910)	
2019-01-08 18:21:13,241 - 10 - training_embed.py - training - Epoch: [0][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 621.8870 (616.7716)	
2019-01-08 18:21:13,509 - 10 - training_embed.py - training - Epoch: [0][1300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 617.4791 (616.7962)	
2019-01-08 18:21:13,801 - 10 - training_embed.py - training - Epoch: [0][1400/3661]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 615.8554 (616.7893)	
2019-01-08 18:21:14,135 - 10 - training_embed.py - training - Epoch: [0][1500/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 614.5812 (616.7493)	
2019-01-08 18:21:14,367 - 10 - training_embed.py - training - Epoch: [0][1600/3661]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 615.4677 (616.7059)	
2019-01-08 18:21:14,769 - 10 - training_embed.py - training - Epoch: [0][1700/3661]	Time 0.008 (0.003)	Data 0.006 (0.001)	Loss 612.3256 (616.7041)	
2019-01-08 18:21:15,130 - 10 - training_embed.py - training - Epoch: [0][1800/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 615.1685 (616.6360)	
2019-01-08 18:21:15,374 - 10 - training_embed.py - training - Epoch: [0][1900/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 609.2441 (616.6004)	
2019-01-08 18:21:15,600 - 10 - training_embed.py - training - Epoch: [0][2000/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 630.1237 (616.6026)	
2019-01-08 18:21:15,817 - 10 - training_embed.py - training - Epoch: [0][2100/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 607.7406 (616.5837)	
2019-01-08 18:21:16,031 - 10 - training_embed.py - training - Epoch: [0][2200/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 608.7368 (616.5542)	
2019-01-08 18:21:16,263 - 10 - training_embed.py - training - Epoch: [0][2300/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 616.1932 (616.5666)	
2019-01-08 18:21:16,506 - 10 - training_embed.py - training - Epoch: [0][2400/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 618.5173 (616.5435)	
2019-01-08 18:21:16,778 - 10 - training_embed.py - training - Epoch: [0][2500/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 619.8664 (616.5313)	
2019-01-08 18:21:17,022 - 10 - training_embed.py - training - Epoch: [0][2600/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 619.3030 (616.5273)	
2019-01-08 18:21:17,242 - 10 - training_embed.py - training - Epoch: [0][2700/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 623.9063 (616.5111)	
2019-01-08 18:21:17,470 - 10 - training_embed.py - training - Epoch: [0][2800/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 612.9094 (616.4946)	
2019-01-08 18:21:17,691 - 10 - training_embed.py - training - Epoch: [0][2900/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 613.3564 (616.4596)	
2019-01-08 18:21:17,911 - 10 - training_embed.py - training - Epoch: [0][3000/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 615.4603 (616.4530)	
2019-01-08 18:21:18,134 - 10 - training_embed.py - training - Epoch: [0][3100/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 623.2119 (616.4216)	
2019-01-08 18:21:18,361 - 10 - training_embed.py - training - Epoch: [0][3200/3661]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 614.2813 (616.4221)	
2019-01-08 18:21:18,607 - 10 - training_embed.py - training - Epoch: [0][3300/3661]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 620.5682 (616.3983)	
2019-01-08 18:21:18,873 - 10 - training_embed.py - training - Epoch: [0][3400/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 619.8181 (616.3830)	
2019-01-08 18:21:19,110 - 10 - training_embed.py - training - Epoch: [0][3500/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 619.4401 (616.3779)	
2019-01-08 18:21:19,343 - 10 - training_embed.py - training - Epoch: [0][3600/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 615.1295 (616.3624)	
2019-01-08 18:21:19,480 - 10 - training_embed.py - training - loss: 616.296321
2019-01-08 18:21:19,481 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 18:21:19,954 - 10 - training_embed.py - training - Epoch: [1][100/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 612.1726 (615.8964)	
2019-01-08 18:21:20,188 - 10 - training_embed.py - training - Epoch: [1][200/3661]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 608.0186 (615.7775)	
2019-01-08 18:21:20,401 - 10 - training_embed.py - training - Epoch: [1][300/3661]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 616.0153 (616.0105)	
2019-01-08 18:21:20,636 - 10 - training_embed.py - training - Epoch: [1][400/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 618.2120 (615.6515)	
2019-01-08 18:21:20,866 - 10 - training_embed.py - training - Epoch: [1][500/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 618.4226 (615.6003)	
2019-01-08 18:21:21,124 - 10 - training_embed.py - training - Epoch: [1][600/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 615.7762 (615.5482)	
2019-01-08 18:21:21,371 - 10 - training_embed.py - training - Epoch: [1][700/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 615.2562 (615.4895)	
2019-01-08 18:21:21,601 - 10 - training_embed.py - training - Epoch: [1][800/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 613.9630 (615.4584)	
2019-01-08 18:21:21,833 - 10 - training_embed.py - training - Epoch: [1][900/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 619.1470 (615.3236)	
2019-01-08 18:21:22,051 - 10 - training_embed.py - training - Epoch: [1][1000/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 616.3484 (615.3585)	
2019-01-08 18:21:22,275 - 10 - training_embed.py - training - Epoch: [1][1100/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 614.3565 (615.3557)	
2019-01-08 18:21:22,501 - 10 - training_embed.py - training - Epoch: [1][1200/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 613.3607 (615.2756)	
2019-01-08 18:21:22,874 - 10 - training_embed.py - training - Epoch: [1][1300/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 617.2821 (615.2644)	
2019-01-08 18:21:23,255 - 10 - training_embed.py - training - Epoch: [1][1400/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 618.6414 (615.2545)	
2019-01-08 18:21:23,513 - 10 - training_embed.py - training - Epoch: [1][1500/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 619.0336 (615.2310)	
2019-01-08 18:21:23,752 - 10 - training_embed.py - training - Epoch: [1][1600/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 607.6153 (615.2275)	
2019-01-08 18:21:23,972 - 10 - training_embed.py - training - Epoch: [1][1700/3661]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 616.8820 (615.2187)	
2019-01-08 18:21:24,193 - 10 - training_embed.py - training - Epoch: [1][1800/3661]	Time 0.004 (0.003)	Data 0.003 (0.001)	Loss 613.6374 (615.2047)	
2019-01-08 18:21:24,427 - 10 - training_embed.py - training - Epoch: [1][1900/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 613.1309 (615.1799)	
2019-01-08 18:21:24,643 - 10 - training_embed.py - training - Epoch: [1][2000/3661]	Time 0.006 (0.003)	Data 0.004 (0.001)	Loss 617.3095 (615.1643)	
2019-01-08 18:21:24,880 - 10 - training_embed.py - training - Epoch: [1][2100/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 611.9835 (615.1739)	
2019-01-08 18:21:25,105 - 10 - training_embed.py - training - Epoch: [1][2200/3661]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 607.7275 (615.1335)	
2019-01-08 18:21:25,332 - 10 - training_embed.py - training - Epoch: [1][2300/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 614.3906 (615.0953)	
2019-01-08 18:21:25,545 - 10 - training_embed.py - training - Epoch: [1][2400/3661]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 608.3322 (615.0750)	
2019-01-08 18:21:25,778 - 10 - training_embed.py - training - Epoch: [1][2500/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 618.2077 (615.0606)	
2019-01-08 18:21:26,020 - 10 - training_embed.py - training - Epoch: [1][2600/3661]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 615.7886 (615.0269)	
2019-01-08 18:21:26,257 - 10 - training_embed.py - training - Epoch: [1][2700/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 619.3656 (615.0066)	
2019-01-08 18:21:26,477 - 10 - training_embed.py - training - Epoch: [1][2800/3661]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 617.5820 (614.9829)	
2019-01-08 18:21:26,714 - 10 - training_embed.py - training - Epoch: [1][2900/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 615.3542 (614.9816)	
2019-01-08 18:21:26,947 - 10 - training_embed.py - training - Epoch: [1][3000/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 612.8244 (614.9460)	
2019-01-08 18:21:27,174 - 10 - training_embed.py - training - Epoch: [1][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6044 (614.9173)	
2019-01-08 18:21:27,404 - 10 - training_embed.py - training - Epoch: [1][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.6843 (614.9127)	
2019-01-08 18:21:27,627 - 10 - training_embed.py - training - Epoch: [1][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3316 (614.8841)	
2019-01-08 18:21:27,866 - 10 - training_embed.py - training - Epoch: [1][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.2938 (614.8659)	
2019-01-08 18:21:28,102 - 10 - training_embed.py - training - Epoch: [1][3500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.7416 (614.8697)	
2019-01-08 18:21:28,326 - 10 - training_embed.py - training - Epoch: [1][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 623.2548 (614.8404)	
2019-01-08 18:21:28,457 - 10 - training_embed.py - training - loss: 614.804651
2019-01-08 18:21:28,458 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 18:21:28,933 - 10 - training_embed.py - training - Epoch: [2][100/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 610.7210 (613.7780)	
2019-01-08 18:21:29,139 - 10 - training_embed.py - training - Epoch: [2][200/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 615.4297 (614.1347)	
2019-01-08 18:21:29,346 - 10 - training_embed.py - training - Epoch: [2][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.7805 (613.9895)	
2019-01-08 18:21:29,572 - 10 - training_embed.py - training - Epoch: [2][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.0972 (613.8633)	
2019-01-08 18:21:29,784 - 10 - training_embed.py - training - Epoch: [2][500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.3484 (613.7243)	
2019-01-08 18:21:29,989 - 10 - training_embed.py - training - Epoch: [2][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.0474 (613.7336)	
2019-01-08 18:21:30,223 - 10 - training_embed.py - training - Epoch: [2][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.4483 (613.7210)	
2019-01-08 18:21:30,428 - 10 - training_embed.py - training - Epoch: [2][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3887 (613.7172)	
2019-01-08 18:21:30,649 - 10 - training_embed.py - training - Epoch: [2][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.1680 (613.7379)	
2019-01-08 18:21:30,872 - 10 - training_embed.py - training - Epoch: [2][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.1815 (613.6755)	
2019-01-08 18:21:31,085 - 10 - training_embed.py - training - Epoch: [2][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.5532 (613.6511)	
2019-01-08 18:21:31,298 - 10 - training_embed.py - training - Epoch: [2][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.9062 (613.6841)	
2019-01-08 18:21:31,517 - 10 - training_embed.py - training - Epoch: [2][1300/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 611.3503 (613.6841)	
2019-01-08 18:21:31,748 - 10 - training_embed.py - training - Epoch: [2][1400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.0609 (613.6801)	
2019-01-08 18:21:31,970 - 10 - training_embed.py - training - Epoch: [2][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.2061 (613.7073)	
2019-01-08 18:21:32,198 - 10 - training_embed.py - training - Epoch: [2][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.9669 (613.6761)	
2019-01-08 18:21:32,401 - 10 - training_embed.py - training - Epoch: [2][1700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.8204 (613.6919)	
2019-01-08 18:21:32,631 - 10 - training_embed.py - training - Epoch: [2][1800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.2833 (613.6520)	
2019-01-08 18:21:32,846 - 10 - training_embed.py - training - Epoch: [2][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.6827 (613.6253)	
2019-01-08 18:21:33,054 - 10 - training_embed.py - training - Epoch: [2][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.6646 (613.5824)	
2019-01-08 18:21:33,281 - 10 - training_embed.py - training - Epoch: [2][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.0447 (613.5800)	
2019-01-08 18:21:33,496 - 10 - training_embed.py - training - Epoch: [2][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.1768 (613.5257)	
2019-01-08 18:21:33,718 - 10 - training_embed.py - training - Epoch: [2][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.9259 (613.5132)	
2019-01-08 18:21:33,920 - 10 - training_embed.py - training - Epoch: [2][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.6912 (613.5007)	
2019-01-08 18:21:34,150 - 10 - training_embed.py - training - Epoch: [2][2500/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 605.8686 (613.4945)	
2019-01-08 18:21:34,356 - 10 - training_embed.py - training - Epoch: [2][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.3804 (613.4930)	
2019-01-08 18:21:34,581 - 10 - training_embed.py - training - Epoch: [2][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.8964 (613.4937)	
2019-01-08 18:21:34,809 - 10 - training_embed.py - training - Epoch: [2][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.4155 (613.4770)	
2019-01-08 18:21:35,051 - 10 - training_embed.py - training - Epoch: [2][2900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 613.6574 (613.4706)	
2019-01-08 18:21:35,282 - 10 - training_embed.py - training - Epoch: [2][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.7043 (613.4568)	
2019-01-08 18:21:35,510 - 10 - training_embed.py - training - Epoch: [2][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 616.1013 (613.4444)	
2019-01-08 18:21:35,733 - 10 - training_embed.py - training - Epoch: [2][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6669 (613.4250)	
2019-01-08 18:21:35,950 - 10 - training_embed.py - training - Epoch: [2][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 625.3009 (613.4211)	
2019-01-08 18:21:36,172 - 10 - training_embed.py - training - Epoch: [2][3400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.4553 (613.3969)	
2019-01-08 18:21:36,392 - 10 - training_embed.py - training - Epoch: [2][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.9793 (613.3956)	
2019-01-08 18:21:36,641 - 10 - training_embed.py - training - Epoch: [2][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.6703 (613.3685)	
2019-01-08 18:21:36,783 - 10 - training_embed.py - training - loss: 613.315042
2019-01-08 18:21:36,784 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 18:21:37,260 - 10 - training_embed.py - training - Epoch: [3][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.0622 (612.5445)	
2019-01-08 18:21:37,471 - 10 - training_embed.py - training - Epoch: [3][200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 621.9988 (612.6686)	
2019-01-08 18:21:37,696 - 10 - training_embed.py - training - Epoch: [3][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.0030 (612.5074)	
2019-01-08 18:21:37,925 - 10 - training_embed.py - training - Epoch: [3][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.7601 (612.4898)	
2019-01-08 18:21:38,148 - 10 - training_embed.py - training - Epoch: [3][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.0013 (612.4326)	
2019-01-08 18:21:38,376 - 10 - training_embed.py - training - Epoch: [3][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.8688 (612.3822)	
2019-01-08 18:21:38,605 - 10 - training_embed.py - training - Epoch: [3][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3266 (612.3851)	
2019-01-08 18:21:38,816 - 10 - training_embed.py - training - Epoch: [3][800/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 617.2136 (612.4201)	
2019-01-08 18:21:39,041 - 10 - training_embed.py - training - Epoch: [3][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.8478 (612.4405)	
2019-01-08 18:21:39,265 - 10 - training_embed.py - training - Epoch: [3][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.9448 (612.3963)	
2019-01-08 18:21:39,501 - 10 - training_embed.py - training - Epoch: [3][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 621.1181 (612.3468)	
2019-01-08 18:21:39,710 - 10 - training_embed.py - training - Epoch: [3][1200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.1370 (612.3572)	
2019-01-08 18:21:39,925 - 10 - training_embed.py - training - Epoch: [3][1300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.4589 (612.3722)	
2019-01-08 18:21:40,145 - 10 - training_embed.py - training - Epoch: [3][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.3022 (612.3452)	
2019-01-08 18:21:40,378 - 10 - training_embed.py - training - Epoch: [3][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.9258 (612.2976)	
2019-01-08 18:21:40,600 - 10 - training_embed.py - training - Epoch: [3][1600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 607.9659 (612.2856)	
2019-01-08 18:21:40,817 - 10 - training_embed.py - training - Epoch: [3][1700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.9698 (612.2837)	
2019-01-08 18:21:41,028 - 10 - training_embed.py - training - Epoch: [3][1800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.7432 (612.2920)	
2019-01-08 18:21:41,247 - 10 - training_embed.py - training - Epoch: [3][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.6301 (612.2399)	
2019-01-08 18:21:41,474 - 10 - training_embed.py - training - Epoch: [3][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.4352 (612.2276)	
2019-01-08 18:21:41,686 - 10 - training_embed.py - training - Epoch: [3][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 619.3394 (612.2080)	
2019-01-08 18:21:41,906 - 10 - training_embed.py - training - Epoch: [3][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8919 (612.1816)	
2019-01-08 18:21:42,133 - 10 - training_embed.py - training - Epoch: [3][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.4120 (612.1656)	
2019-01-08 18:21:42,364 - 10 - training_embed.py - training - Epoch: [3][2400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.9454 (612.1277)	
2019-01-08 18:21:42,567 - 10 - training_embed.py - training - Epoch: [3][2500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.0552 (612.1154)	
2019-01-08 18:21:42,781 - 10 - training_embed.py - training - Epoch: [3][2600/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 607.1281 (612.0728)	
2019-01-08 18:21:42,990 - 10 - training_embed.py - training - Epoch: [3][2700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.3299 (612.0645)	
2019-01-08 18:21:43,216 - 10 - training_embed.py - training - Epoch: [3][2800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 613.6952 (612.0206)	
2019-01-08 18:21:43,427 - 10 - training_embed.py - training - Epoch: [3][2900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.5572 (611.9871)	
2019-01-08 18:21:43,664 - 10 - training_embed.py - training - Epoch: [3][3000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.5012 (611.9629)	
2019-01-08 18:21:43,880 - 10 - training_embed.py - training - Epoch: [3][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.4419 (611.9254)	
2019-01-08 18:21:44,105 - 10 - training_embed.py - training - Epoch: [3][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.4423 (611.8831)	
2019-01-08 18:21:44,345 - 10 - training_embed.py - training - Epoch: [3][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6813 (611.8624)	
2019-01-08 18:21:44,572 - 10 - training_embed.py - training - Epoch: [3][3400/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 608.4063 (611.8484)	
2019-01-08 18:21:44,786 - 10 - training_embed.py - training - Epoch: [3][3500/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 610.2764 (611.8734)	
2019-01-08 18:21:45,005 - 10 - training_embed.py - training - Epoch: [3][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.3531 (611.8679)	
2019-01-08 18:21:45,140 - 10 - training_embed.py - training - loss: 611.825533
2019-01-08 18:21:45,141 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 18:21:45,611 - 10 - training_embed.py - training - Epoch: [4][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.6872 (610.5379)	
2019-01-08 18:21:45,839 - 10 - training_embed.py - training - Epoch: [4][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.0650 (610.5847)	
2019-01-08 18:21:46,073 - 10 - training_embed.py - training - Epoch: [4][300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 615.4650 (610.7863)	
2019-01-08 18:21:46,296 - 10 - training_embed.py - training - Epoch: [4][400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 606.9546 (611.0777)	
2019-01-08 18:21:46,509 - 10 - training_embed.py - training - Epoch: [4][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.0034 (610.9467)	
2019-01-08 18:21:46,738 - 10 - training_embed.py - training - Epoch: [4][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2468 (611.0684)	
2019-01-08 18:21:46,960 - 10 - training_embed.py - training - Epoch: [4][700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.1983 (610.9798)	
2019-01-08 18:21:47,198 - 10 - training_embed.py - training - Epoch: [4][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3690 (611.0003)	
2019-01-08 18:21:47,422 - 10 - training_embed.py - training - Epoch: [4][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.1672 (610.9362)	
2019-01-08 18:21:47,647 - 10 - training_embed.py - training - Epoch: [4][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.6064 (610.9161)	
2019-01-08 18:21:47,867 - 10 - training_embed.py - training - Epoch: [4][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.0914 (610.8941)	
2019-01-08 18:21:48,091 - 10 - training_embed.py - training - Epoch: [4][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.8488 (610.8752)	
2019-01-08 18:21:48,319 - 10 - training_embed.py - training - Epoch: [4][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.2075 (610.8194)	
2019-01-08 18:21:48,559 - 10 - training_embed.py - training - Epoch: [4][1400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.9946 (610.8440)	
2019-01-08 18:21:48,774 - 10 - training_embed.py - training - Epoch: [4][1500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 617.5739 (610.8103)	
2019-01-08 18:21:49,003 - 10 - training_embed.py - training - Epoch: [4][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 619.1867 (610.7883)	
2019-01-08 18:21:49,241 - 10 - training_embed.py - training - Epoch: [4][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.4365 (610.7814)	
2019-01-08 18:21:49,456 - 10 - training_embed.py - training - Epoch: [4][1800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.2203 (610.7421)	
2019-01-08 18:21:49,673 - 10 - training_embed.py - training - Epoch: [4][1900/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 611.4896 (610.7151)	
2019-01-08 18:21:49,893 - 10 - training_embed.py - training - Epoch: [4][2000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.2071 (610.7106)	
2019-01-08 18:21:50,124 - 10 - training_embed.py - training - Epoch: [4][2100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.9996 (610.6682)	
2019-01-08 18:21:50,346 - 10 - training_embed.py - training - Epoch: [4][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.8488 (610.6442)	
2019-01-08 18:21:50,560 - 10 - training_embed.py - training - Epoch: [4][2300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 611.1721 (610.6048)	
2019-01-08 18:21:50,771 - 10 - training_embed.py - training - Epoch: [4][2400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.7435 (610.5741)	
2019-01-08 18:21:50,989 - 10 - training_embed.py - training - Epoch: [4][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.0959 (610.5834)	
2019-01-08 18:21:51,216 - 10 - training_embed.py - training - Epoch: [4][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.0233 (610.5332)	
2019-01-08 18:21:51,432 - 10 - training_embed.py - training - Epoch: [4][2700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.7964 (610.5019)	
2019-01-08 18:21:51,679 - 10 - training_embed.py - training - Epoch: [4][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.6613 (610.5125)	
2019-01-08 18:21:51,895 - 10 - training_embed.py - training - Epoch: [4][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.8544 (610.4875)	
2019-01-08 18:21:52,130 - 10 - training_embed.py - training - Epoch: [4][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.0309 (610.4778)	
2019-01-08 18:21:52,342 - 10 - training_embed.py - training - Epoch: [4][3100/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 599.6384 (610.4802)	
2019-01-08 18:21:52,568 - 10 - training_embed.py - training - Epoch: [4][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 617.4246 (610.4708)	
2019-01-08 18:21:52,797 - 10 - training_embed.py - training - Epoch: [4][3300/3661]	Time 0.005 (0.002)	Data 0.001 (0.001)	Loss 610.4875 (610.4455)	
2019-01-08 18:21:53,026 - 10 - training_embed.py - training - Epoch: [4][3400/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 608.6982 (610.4293)	
2019-01-08 18:21:53,247 - 10 - training_embed.py - training - Epoch: [4][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.8660 (610.4242)	
2019-01-08 18:21:53,478 - 10 - training_embed.py - training - Epoch: [4][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.4683 (610.3993)	
2019-01-08 18:21:53,625 - 10 - training_embed.py - training - loss: 610.334720
2019-01-08 18:21:53,625 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 18:21:54,103 - 10 - training_embed.py - training - Epoch: [5][100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.3474 (610.1381)	
2019-01-08 18:21:54,305 - 10 - training_embed.py - training - Epoch: [5][200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.3867 (610.2832)	
2019-01-08 18:21:54,534 - 10 - training_embed.py - training - Epoch: [5][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.9031 (610.1751)	
2019-01-08 18:21:54,759 - 10 - training_embed.py - training - Epoch: [5][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.5026 (610.0026)	
2019-01-08 18:21:54,982 - 10 - training_embed.py - training - Epoch: [5][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.0722 (609.8643)	
2019-01-08 18:21:55,196 - 10 - training_embed.py - training - Epoch: [5][600/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 606.9750 (609.7218)	
2019-01-08 18:21:55,405 - 10 - training_embed.py - training - Epoch: [5][700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 616.5997 (609.6672)	
2019-01-08 18:21:55,630 - 10 - training_embed.py - training - Epoch: [5][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.9255 (609.5948)	
2019-01-08 18:21:55,845 - 10 - training_embed.py - training - Epoch: [5][900/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 598.8646 (609.5120)	
2019-01-08 18:21:56,067 - 10 - training_embed.py - training - Epoch: [5][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.3544 (609.4458)	
2019-01-08 18:21:56,295 - 10 - training_embed.py - training - Epoch: [5][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.8491 (609.3845)	
2019-01-08 18:21:56,506 - 10 - training_embed.py - training - Epoch: [5][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.4791 (609.3743)	
2019-01-08 18:21:56,740 - 10 - training_embed.py - training - Epoch: [5][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.0718 (609.3659)	
2019-01-08 18:21:56,950 - 10 - training_embed.py - training - Epoch: [5][1400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 596.7671 (609.3475)	
2019-01-08 18:21:57,174 - 10 - training_embed.py - training - Epoch: [5][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.9161 (609.3602)	
2019-01-08 18:21:57,394 - 10 - training_embed.py - training - Epoch: [5][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5091 (609.3332)	
2019-01-08 18:21:57,609 - 10 - training_embed.py - training - Epoch: [5][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.1271 (609.2954)	
2019-01-08 18:21:57,831 - 10 - training_embed.py - training - Epoch: [5][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.8625 (609.2692)	
2019-01-08 18:21:58,046 - 10 - training_embed.py - training - Epoch: [5][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.6722 (609.2411)	
2019-01-08 18:21:58,287 - 10 - training_embed.py - training - Epoch: [5][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.2438 (609.2169)	
2019-01-08 18:21:58,510 - 10 - training_embed.py - training - Epoch: [5][2100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 607.1719 (609.1920)	
2019-01-08 18:21:58,732 - 10 - training_embed.py - training - Epoch: [5][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.8420 (609.1963)	
2019-01-08 18:21:58,965 - 10 - training_embed.py - training - Epoch: [5][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.7719 (609.1793)	
2019-01-08 18:21:59,194 - 10 - training_embed.py - training - Epoch: [5][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.4358 (609.1726)	
2019-01-08 18:21:59,407 - 10 - training_embed.py - training - Epoch: [5][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.0342 (609.1509)	
2019-01-08 18:21:59,638 - 10 - training_embed.py - training - Epoch: [5][2600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.8311 (609.1139)	
2019-01-08 18:21:59,853 - 10 - training_embed.py - training - Epoch: [5][2700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.1050 (609.0919)	
2019-01-08 18:22:00,058 - 10 - training_embed.py - training - Epoch: [5][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.7902 (609.0533)	
2019-01-08 18:22:00,276 - 10 - training_embed.py - training - Epoch: [5][2900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 608.6870 (609.0453)	
2019-01-08 18:22:00,483 - 10 - training_embed.py - training - Epoch: [5][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.8384 (609.0410)	
2019-01-08 18:22:00,708 - 10 - training_embed.py - training - Epoch: [5][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.7573 (609.0019)	
2019-01-08 18:22:00,917 - 10 - training_embed.py - training - Epoch: [5][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.7493 (608.9749)	
2019-01-08 18:22:01,145 - 10 - training_embed.py - training - Epoch: [5][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.4611 (608.9430)	
2019-01-08 18:22:01,363 - 10 - training_embed.py - training - Epoch: [5][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3167 (608.9304)	
2019-01-08 18:22:01,571 - 10 - training_embed.py - training - Epoch: [5][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.6635 (608.9184)	
2019-01-08 18:22:01,806 - 10 - training_embed.py - training - Epoch: [5][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.0469 (608.8810)	
2019-01-08 18:22:01,958 - 10 - training_embed.py - training - loss: 608.839979
2019-01-08 18:22:01,958 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 18:22:02,432 - 10 - training_embed.py - training - Epoch: [6][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.3932 (607.8204)	
2019-01-08 18:22:02,649 - 10 - training_embed.py - training - Epoch: [6][200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 618.0462 (608.0240)	
2019-01-08 18:22:02,870 - 10 - training_embed.py - training - Epoch: [6][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2598 (608.0777)	
2019-01-08 18:22:03,087 - 10 - training_embed.py - training - Epoch: [6][400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.7393 (608.1402)	
2019-01-08 18:22:03,316 - 10 - training_embed.py - training - Epoch: [6][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.5778 (608.1426)	
2019-01-08 18:22:03,552 - 10 - training_embed.py - training - Epoch: [6][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.5403 (608.1174)	
2019-01-08 18:22:03,858 - 10 - training_embed.py - training - Epoch: [6][700/3661]	Time 0.007 (0.002)	Data 0.003 (0.001)	Loss 611.0915 (608.1800)	
2019-01-08 18:22:04,287 - 10 - training_embed.py - training - Epoch: [6][800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.7360 (608.1876)	
2019-01-08 18:22:04,570 - 10 - training_embed.py - training - Epoch: [6][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.6620 (608.1421)	
2019-01-08 18:22:04,806 - 10 - training_embed.py - training - Epoch: [6][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3682 (608.0419)	
2019-01-08 18:22:05,087 - 10 - training_embed.py - training - Epoch: [6][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.1016 (608.0426)	
2019-01-08 18:22:05,354 - 10 - training_embed.py - training - Epoch: [6][1200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.1501 (607.9994)	
2019-01-08 18:22:05,594 - 10 - training_embed.py - training - Epoch: [6][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.2861 (607.9508)	
2019-01-08 18:22:05,932 - 10 - training_embed.py - training - Epoch: [6][1400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.7545 (607.9503)	
2019-01-08 18:22:06,234 - 10 - training_embed.py - training - Epoch: [6][1500/3661]	Time 0.006 (0.002)	Data 0.002 (0.001)	Loss 600.1104 (607.9312)	
2019-01-08 18:22:06,482 - 10 - training_embed.py - training - Epoch: [6][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.7404 (607.8775)	
2019-01-08 18:22:06,740 - 10 - training_embed.py - training - Epoch: [6][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.0815 (607.8297)	
2019-01-08 18:22:06,976 - 10 - training_embed.py - training - Epoch: [6][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.8026 (607.8014)	
2019-01-08 18:22:07,212 - 10 - training_embed.py - training - Epoch: [6][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.3493 (607.7903)	
2019-01-08 18:22:07,438 - 10 - training_embed.py - training - Epoch: [6][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.7768 (607.7530)	
2019-01-08 18:22:07,675 - 10 - training_embed.py - training - Epoch: [6][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.8410 (607.6909)	
2019-01-08 18:22:07,927 - 10 - training_embed.py - training - Epoch: [6][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.5166 (607.6591)	
2019-01-08 18:22:08,181 - 10 - training_embed.py - training - Epoch: [6][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.4338 (607.6379)	
2019-01-08 18:22:08,419 - 10 - training_embed.py - training - Epoch: [6][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3322 (607.6321)	
2019-01-08 18:22:08,677 - 10 - training_embed.py - training - Epoch: [6][2500/3661]	Time 0.007 (0.002)	Data 0.003 (0.001)	Loss 605.7104 (607.6180)	
2019-01-08 18:22:09,053 - 10 - training_embed.py - training - Epoch: [6][2600/3661]	Time 0.007 (0.002)	Data 0.001 (0.001)	Loss 606.3708 (607.6248)	
2019-01-08 18:22:09,306 - 10 - training_embed.py - training - Epoch: [6][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.4452 (607.6172)	
2019-01-08 18:22:09,715 - 10 - training_embed.py - training - Epoch: [6][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.7789 (607.5877)	
2019-01-08 18:22:10,064 - 10 - training_embed.py - training - Epoch: [6][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3956 (607.5515)	
2019-01-08 18:22:10,313 - 10 - training_embed.py - training - Epoch: [6][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.5770 (607.5262)	
2019-01-08 18:22:10,626 - 10 - training_embed.py - training - Epoch: [6][3100/3661]	Time 0.011 (0.002)	Data 0.001 (0.001)	Loss 613.2387 (607.4968)	
2019-01-08 18:22:11,037 - 10 - training_embed.py - training - Epoch: [6][3200/3661]	Time 0.009 (0.002)	Data 0.001 (0.001)	Loss 608.3697 (607.4796)	
2019-01-08 18:22:11,345 - 10 - training_embed.py - training - Epoch: [6][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5654 (607.4644)	
2019-01-08 18:22:11,576 - 10 - training_embed.py - training - Epoch: [6][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.7098 (607.4420)	
2019-01-08 18:22:11,784 - 10 - training_embed.py - training - Epoch: [6][3500/3661]	Time 0.003 (0.002)	Data 0.000 (0.001)	Loss 604.3017 (607.4160)	
2019-01-08 18:22:12,007 - 10 - training_embed.py - training - Epoch: [6][3600/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 612.6840 (607.3898)	
2019-01-08 18:22:12,158 - 10 - training_embed.py - training - loss: 607.339151
2019-01-08 18:22:12,159 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 18:22:12,643 - 10 - training_embed.py - training - Epoch: [7][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.8139 (606.6062)	
2019-01-08 18:22:12,877 - 10 - training_embed.py - training - Epoch: [7][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.8162 (606.7585)	
2019-01-08 18:22:13,101 - 10 - training_embed.py - training - Epoch: [7][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.3335 (606.8090)	
2019-01-08 18:22:13,326 - 10 - training_embed.py - training - Epoch: [7][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.0362 (606.7194)	
2019-01-08 18:22:13,559 - 10 - training_embed.py - training - Epoch: [7][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.6497 (606.7158)	
2019-01-08 18:22:13,787 - 10 - training_embed.py - training - Epoch: [7][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.1208 (606.5615)	
2019-01-08 18:22:14,002 - 10 - training_embed.py - training - Epoch: [7][700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.8132 (606.4890)	
2019-01-08 18:22:14,232 - 10 - training_embed.py - training - Epoch: [7][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.4163 (606.4314)	
2019-01-08 18:22:14,453 - 10 - training_embed.py - training - Epoch: [7][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.9963 (606.3487)	
2019-01-08 18:22:14,668 - 10 - training_embed.py - training - Epoch: [7][1000/3661]	Time 0.006 (0.002)	Data 0.005 (0.001)	Loss 605.6851 (606.3060)	
2019-01-08 18:22:14,873 - 10 - training_embed.py - training - Epoch: [7][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6494 (606.2874)	
2019-01-08 18:22:15,102 - 10 - training_embed.py - training - Epoch: [7][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.4679 (606.1763)	
2019-01-08 18:22:15,332 - 10 - training_embed.py - training - Epoch: [7][1300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 605.6039 (606.2764)	
2019-01-08 18:22:15,543 - 10 - training_embed.py - training - Epoch: [7][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.7225 (606.2471)	
2019-01-08 18:22:15,770 - 10 - training_embed.py - training - Epoch: [7][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.1884 (606.2092)	
2019-01-08 18:22:15,976 - 10 - training_embed.py - training - Epoch: [7][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.1058 (606.2119)	
2019-01-08 18:22:16,199 - 10 - training_embed.py - training - Epoch: [7][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.4455 (606.2289)	
2019-01-08 18:22:16,423 - 10 - training_embed.py - training - Epoch: [7][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.6777 (606.2010)	
2019-01-08 18:22:16,646 - 10 - training_embed.py - training - Epoch: [7][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.1537 (606.1861)	
2019-01-08 18:22:16,878 - 10 - training_embed.py - training - Epoch: [7][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.3793 (606.1891)	
2019-01-08 18:22:17,105 - 10 - training_embed.py - training - Epoch: [7][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.5607 (606.1883)	
2019-01-08 18:22:17,337 - 10 - training_embed.py - training - Epoch: [7][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.7758 (606.1930)	
2019-01-08 18:22:17,565 - 10 - training_embed.py - training - Epoch: [7][2300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.2488 (606.1701)	
2019-01-08 18:22:17,792 - 10 - training_embed.py - training - Epoch: [7][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.1689 (606.1616)	
2019-01-08 18:22:18,020 - 10 - training_embed.py - training - Epoch: [7][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.4498 (606.1344)	
2019-01-08 18:22:18,245 - 10 - training_embed.py - training - Epoch: [7][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.5013 (606.1088)	
2019-01-08 18:22:18,472 - 10 - training_embed.py - training - Epoch: [7][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.0869 (606.0641)	
2019-01-08 18:22:18,705 - 10 - training_embed.py - training - Epoch: [7][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.9476 (606.0503)	
2019-01-08 18:22:18,925 - 10 - training_embed.py - training - Epoch: [7][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.5461 (606.0421)	
2019-01-08 18:22:19,154 - 10 - training_embed.py - training - Epoch: [7][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.8673 (606.0569)	
2019-01-08 18:22:19,386 - 10 - training_embed.py - training - Epoch: [7][3100/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 602.1403 (606.0154)	
2019-01-08 18:22:19,638 - 10 - training_embed.py - training - Epoch: [7][3200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.2021 (605.9811)	
2019-01-08 18:22:19,884 - 10 - training_embed.py - training - Epoch: [7][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.9574 (605.9518)	
2019-01-08 18:22:20,116 - 10 - training_embed.py - training - Epoch: [7][3400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.9418 (605.9392)	
2019-01-08 18:22:20,403 - 10 - training_embed.py - training - Epoch: [7][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.0161 (605.8850)	
2019-01-08 18:22:20,645 - 10 - training_embed.py - training - Epoch: [7][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.2516 (605.8705)	
2019-01-08 18:22:20,881 - 10 - training_embed.py - training - loss: 605.829943
2019-01-08 18:22:20,881 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 18:22:21,512 - 10 - training_embed.py - training - Epoch: [8][100/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 609.8517 (605.0226)	
2019-01-08 18:22:21,752 - 10 - training_embed.py - training - Epoch: [8][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.6870 (604.9833)	
2019-01-08 18:22:21,966 - 10 - training_embed.py - training - Epoch: [8][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.1395 (604.8800)	
2019-01-08 18:22:22,189 - 10 - training_embed.py - training - Epoch: [8][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2314 (605.1040)	
2019-01-08 18:22:22,418 - 10 - training_embed.py - training - Epoch: [8][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.0707 (604.9236)	
2019-01-08 18:22:22,640 - 10 - training_embed.py - training - Epoch: [8][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.9470 (604.8307)	
2019-01-08 18:22:22,857 - 10 - training_embed.py - training - Epoch: [8][700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.3120 (604.8430)	
2019-01-08 18:22:23,083 - 10 - training_embed.py - training - Epoch: [8][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.7177 (604.8175)	
2019-01-08 18:22:23,324 - 10 - training_embed.py - training - Epoch: [8][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.1834 (604.8568)	
2019-01-08 18:22:23,545 - 10 - training_embed.py - training - Epoch: [8][1000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 603.2797 (604.8518)	
2019-01-08 18:22:23,760 - 10 - training_embed.py - training - Epoch: [8][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.9064 (604.8391)	
2019-01-08 18:22:23,979 - 10 - training_embed.py - training - Epoch: [8][1200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 596.9970 (604.8834)	
2019-01-08 18:22:24,187 - 10 - training_embed.py - training - Epoch: [8][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.1344 (604.8129)	
2019-01-08 18:22:24,411 - 10 - training_embed.py - training - Epoch: [8][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.3667 (604.7916)	
2019-01-08 18:22:24,640 - 10 - training_embed.py - training - Epoch: [8][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.1373 (604.7341)	
2019-01-08 18:22:24,860 - 10 - training_embed.py - training - Epoch: [8][1600/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 600.7017 (604.7172)	
2019-01-08 18:22:25,094 - 10 - training_embed.py - training - Epoch: [8][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.7266 (604.6994)	
2019-01-08 18:22:25,324 - 10 - training_embed.py - training - Epoch: [8][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.4586 (604.6969)	
2019-01-08 18:22:25,549 - 10 - training_embed.py - training - Epoch: [8][1900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 601.9136 (604.6636)	
2019-01-08 18:22:25,785 - 10 - training_embed.py - training - Epoch: [8][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.2151 (604.6857)	
2019-01-08 18:22:26,002 - 10 - training_embed.py - training - Epoch: [8][2100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 604.2993 (604.6732)	
2019-01-08 18:22:26,228 - 10 - training_embed.py - training - Epoch: [8][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.7820 (604.6579)	
2019-01-08 18:22:26,444 - 10 - training_embed.py - training - Epoch: [8][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.3310 (604.6235)	
2019-01-08 18:22:26,666 - 10 - training_embed.py - training - Epoch: [8][2400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.8612 (604.6125)	
2019-01-08 18:22:26,896 - 10 - training_embed.py - training - Epoch: [8][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.7423 (604.5969)	
2019-01-08 18:22:27,115 - 10 - training_embed.py - training - Epoch: [8][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.1595 (604.5653)	
2019-01-08 18:22:27,340 - 10 - training_embed.py - training - Epoch: [8][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.0003 (604.5290)	
2019-01-08 18:22:27,561 - 10 - training_embed.py - training - Epoch: [8][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.4970 (604.5091)	
2019-01-08 18:22:27,800 - 10 - training_embed.py - training - Epoch: [8][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 598.8570 (604.4893)	
2019-01-08 18:22:28,029 - 10 - training_embed.py - training - Epoch: [8][3000/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 601.7535 (604.4634)	
2019-01-08 18:22:28,254 - 10 - training_embed.py - training - Epoch: [8][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.5801 (604.4438)	
2019-01-08 18:22:28,481 - 10 - training_embed.py - training - Epoch: [8][3200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 601.0878 (604.4337)	
2019-01-08 18:22:28,693 - 10 - training_embed.py - training - Epoch: [8][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2158 (604.4268)	
2019-01-08 18:22:28,917 - 10 - training_embed.py - training - Epoch: [8][3400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 598.4244 (604.4124)	
2019-01-08 18:22:29,137 - 10 - training_embed.py - training - Epoch: [8][3500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.2175 (604.3911)	
2019-01-08 18:22:29,352 - 10 - training_embed.py - training - Epoch: [8][3600/3661]	Time 0.006 (0.002)	Data 0.003 (0.001)	Loss 601.9059 (604.3674)	
2019-01-08 18:22:29,485 - 10 - training_embed.py - training - loss: 604.311047
2019-01-08 18:22:29,485 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 18:22:29,950 - 10 - training_embed.py - training - Epoch: [9][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.6196 (603.3431)	
2019-01-08 18:22:30,175 - 10 - training_embed.py - training - Epoch: [9][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.9648 (603.7386)	
2019-01-08 18:22:30,390 - 10 - training_embed.py - training - Epoch: [9][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.0667 (604.2580)	
2019-01-08 18:22:30,609 - 10 - training_embed.py - training - Epoch: [9][400/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 602.3307 (603.9669)	
2019-01-08 18:22:30,833 - 10 - training_embed.py - training - Epoch: [9][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.6285 (603.9345)	
2019-01-08 18:22:31,057 - 10 - training_embed.py - training - Epoch: [9][600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.1152 (603.9729)	
2019-01-08 18:22:31,281 - 10 - training_embed.py - training - Epoch: [9][700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.8785 (603.9493)	
2019-01-08 18:22:31,498 - 10 - training_embed.py - training - Epoch: [9][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.9025 (603.9248)	
2019-01-08 18:22:31,719 - 10 - training_embed.py - training - Epoch: [9][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.2246 (603.8352)	
2019-01-08 18:22:31,919 - 10 - training_embed.py - training - Epoch: [9][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.4551 (603.7335)	
2019-01-08 18:22:32,136 - 10 - training_embed.py - training - Epoch: [9][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.3347 (603.6601)	
2019-01-08 18:22:32,354 - 10 - training_embed.py - training - Epoch: [9][1200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 602.5934 (603.5634)	
2019-01-08 18:22:32,577 - 10 - training_embed.py - training - Epoch: [9][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.2990 (603.4997)	
2019-01-08 18:22:32,795 - 10 - training_embed.py - training - Epoch: [9][1400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.4570 (603.4602)	
2019-01-08 18:22:33,017 - 10 - training_embed.py - training - Epoch: [9][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.4495 (603.4034)	
2019-01-08 18:22:33,233 - 10 - training_embed.py - training - Epoch: [9][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 597.7853 (603.3665)	
2019-01-08 18:22:33,467 - 10 - training_embed.py - training - Epoch: [9][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.4325 (603.2992)	
2019-01-08 18:22:33,695 - 10 - training_embed.py - training - Epoch: [9][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3879 (603.2915)	
2019-01-08 18:22:33,927 - 10 - training_embed.py - training - Epoch: [9][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.8474 (603.2396)	
2019-01-08 18:22:34,163 - 10 - training_embed.py - training - Epoch: [9][2000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 595.9912 (603.1526)	
2019-01-08 18:22:34,376 - 10 - training_embed.py - training - Epoch: [9][2100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 601.8587 (603.1368)	
2019-01-08 18:22:34,599 - 10 - training_embed.py - training - Epoch: [9][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3843 (603.1347)	
2019-01-08 18:22:34,813 - 10 - training_embed.py - training - Epoch: [9][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 593.3653 (603.0940)	
2019-01-08 18:22:35,035 - 10 - training_embed.py - training - Epoch: [9][2400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.1838 (603.0814)	
2019-01-08 18:22:35,301 - 10 - training_embed.py - training - Epoch: [9][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.7636 (603.0390)	
2019-01-08 18:22:35,547 - 10 - training_embed.py - training - Epoch: [9][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.2477 (603.0441)	
2019-01-08 18:22:35,788 - 10 - training_embed.py - training - Epoch: [9][2700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 592.3773 (603.0024)	
2019-01-08 18:22:36,049 - 10 - training_embed.py - training - Epoch: [9][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.1998 (602.9997)	
2019-01-08 18:22:36,320 - 10 - training_embed.py - training - Epoch: [9][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 594.7436 (602.9624)	
2019-01-08 18:22:36,564 - 10 - training_embed.py - training - Epoch: [9][3000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 602.1115 (602.9510)	
2019-01-08 18:22:36,794 - 10 - training_embed.py - training - Epoch: [9][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.9232 (602.9265)	
2019-01-08 18:22:37,014 - 10 - training_embed.py - training - Epoch: [9][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.0637 (602.9019)	
2019-01-08 18:22:37,237 - 10 - training_embed.py - training - Epoch: [9][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.2660 (602.8798)	
2019-01-08 18:22:37,448 - 10 - training_embed.py - training - Epoch: [9][3400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 600.6460 (602.8583)	
2019-01-08 18:22:37,671 - 10 - training_embed.py - training - Epoch: [9][3500/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 611.8237 (602.8451)	
2019-01-08 18:22:37,884 - 10 - training_embed.py - training - Epoch: [9][3600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.8527 (602.8299)	
2019-01-08 18:22:38,017 - 10 - training_embed.py - training - loss: 602.780289
2019-01-08 18:22:38,017 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 18:22:38,483 - 10 - training_embed.py - training - Epoch: [10][100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.7672 (602.5414)	
2019-01-08 18:22:38,715 - 10 - training_embed.py - training - Epoch: [10][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.7368 (602.3705)	
2019-01-08 18:22:38,923 - 10 - training_embed.py - training - Epoch: [10][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.4140 (602.0733)	
2019-01-08 18:22:39,132 - 10 - training_embed.py - training - Epoch: [10][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.1700 (602.0941)	
2019-01-08 18:22:39,370 - 10 - training_embed.py - training - Epoch: [10][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.6909 (602.0268)	
2019-01-08 18:22:39,581 - 10 - training_embed.py - training - Epoch: [10][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.3600 (601.8567)	
2019-01-08 18:22:39,818 - 10 - training_embed.py - training - Epoch: [10][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.6183 (601.7234)	
2019-01-08 18:22:40,063 - 10 - training_embed.py - training - Epoch: [10][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.3562 (601.7686)	
2019-01-08 18:22:40,289 - 10 - training_embed.py - training - Epoch: [10][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 600.5769 (601.7356)	
2019-01-08 18:22:40,512 - 10 - training_embed.py - training - Epoch: [10][1000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 597.3047 (601.6535)	
2019-01-08 18:22:40,750 - 10 - training_embed.py - training - Epoch: [10][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.2881 (601.6642)	
2019-01-08 18:22:40,982 - 10 - training_embed.py - training - Epoch: [10][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.7397 (601.6619)	
2019-01-08 18:22:41,214 - 10 - training_embed.py - training - Epoch: [10][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.7977 (601.6019)	
2019-01-08 18:22:41,433 - 10 - training_embed.py - training - Epoch: [10][1400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 596.4221 (601.6108)	
2019-01-08 18:22:41,658 - 10 - training_embed.py - training - Epoch: [10][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.2157 (601.6444)	
2019-01-08 18:22:41,872 - 10 - training_embed.py - training - Epoch: [10][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.1114 (601.5693)	
2019-01-08 18:22:42,083 - 10 - training_embed.py - training - Epoch: [10][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 599.7502 (601.5258)	
2019-01-08 18:22:42,293 - 10 - training_embed.py - training - Epoch: [10][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.7894 (601.5384)	
2019-01-08 18:22:42,505 - 10 - training_embed.py - training - Epoch: [10][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 596.9767 (601.5697)	
2019-01-08 18:22:42,724 - 10 - training_embed.py - training - Epoch: [10][2000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 599.2889 (601.5499)	
2019-01-08 18:22:42,943 - 10 - training_embed.py - training - Epoch: [10][2100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.8477 (601.5151)	
2019-01-08 18:22:43,168 - 10 - training_embed.py - training - Epoch: [10][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.2504 (601.4682)	
2019-01-08 18:22:43,392 - 10 - training_embed.py - training - Epoch: [10][2300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 602.3718 (601.4524)	
2019-01-08 18:22:43,632 - 10 - training_embed.py - training - Epoch: [10][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.0279 (601.4525)	
2019-01-08 18:22:43,859 - 10 - training_embed.py - training - Epoch: [10][2500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 603.1968 (601.4259)	
2019-01-08 18:22:44,079 - 10 - training_embed.py - training - Epoch: [10][2600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 599.9778 (601.3965)	
2019-01-08 18:22:44,293 - 10 - training_embed.py - training - Epoch: [10][2700/3661]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 599.5616 (601.3919)	
2019-01-08 18:22:44,511 - 10 - training_embed.py - training - Epoch: [10][2800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 600.7947 (601.3861)	
2019-01-08 18:22:44,739 - 10 - training_embed.py - training - Epoch: [10][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.7833 (601.3772)	
2019-01-08 18:22:44,944 - 10 - training_embed.py - training - Epoch: [10][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.2278 (601.3467)	
2019-01-08 18:22:45,161 - 10 - training_embed.py - training - Epoch: [10][3100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 595.2872 (601.3347)	
2019-01-08 18:22:45,379 - 10 - training_embed.py - training - Epoch: [10][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.0891 (601.3336)	
2019-01-08 18:22:45,612 - 10 - training_embed.py - training - Epoch: [10][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.7285 (601.3077)	
2019-01-08 18:22:45,824 - 10 - training_embed.py - training - Epoch: [10][3400/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 601.3560 (601.3185)	
2019-01-08 18:22:46,046 - 10 - training_embed.py - training - Epoch: [10][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.9766 (601.3149)	
2019-01-08 18:22:46,254 - 10 - training_embed.py - training - Epoch: [10][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.6802 (601.2802)	
2019-01-08 18:22:46,383 - 10 - training_embed.py - training - loss: 601.235442
2019-01-08 18:22:46,383 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 18:22:46,856 - 10 - training_embed.py - training - Epoch: [11][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.3461 (600.7894)	
2019-01-08 18:22:47,065 - 10 - training_embed.py - training - Epoch: [11][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.5350 (600.4748)	
2019-01-08 18:22:47,288 - 10 - training_embed.py - training - Epoch: [11][300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 604.4666 (600.6380)	
2019-01-08 18:22:47,506 - 10 - training_embed.py - training - Epoch: [11][400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 602.9551 (600.6612)	
2019-01-08 18:22:47,725 - 10 - training_embed.py - training - Epoch: [11][500/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 602.5362 (600.5976)	
2019-01-08 18:22:47,959 - 10 - training_embed.py - training - Epoch: [11][600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 593.7704 (600.4913)	
2019-01-08 18:22:48,206 - 10 - training_embed.py - training - Epoch: [11][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.9856 (600.4557)	
2019-01-08 18:22:48,417 - 10 - training_embed.py - training - Epoch: [11][800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 594.9872 (600.3977)	
2019-01-08 18:22:48,640 - 10 - training_embed.py - training - Epoch: [11][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.2802 (600.3650)	
2019-01-08 18:22:48,860 - 10 - training_embed.py - training - Epoch: [11][1000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.7984 (600.2978)	
2019-01-08 18:22:49,083 - 10 - training_embed.py - training - Epoch: [11][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 595.7244 (600.2174)	
2019-01-08 18:22:49,316 - 10 - training_embed.py - training - Epoch: [11][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.9924 (600.1993)	
2019-01-08 18:22:49,531 - 10 - training_embed.py - training - Epoch: [11][1300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 600.6304 (600.1665)	
2019-01-08 18:22:49,769 - 10 - training_embed.py - training - Epoch: [11][1400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 602.6671 (600.1068)	
2019-01-08 18:22:50,005 - 10 - training_embed.py - training - Epoch: [11][1500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.2050 (600.1141)	
2019-01-08 18:22:50,237 - 10 - training_embed.py - training - Epoch: [11][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.1899 (600.1274)	
2019-01-08 18:22:50,615 - 10 - training_embed.py - training - Epoch: [11][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 595.4945 (600.0896)	
2019-01-08 18:22:50,825 - 10 - training_embed.py - training - Epoch: [11][1800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 597.0648 (600.0803)	
2019-01-08 18:22:51,045 - 10 - training_embed.py - training - Epoch: [11][1900/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 596.4589 (600.0690)	
2019-01-08 18:22:51,270 - 10 - training_embed.py - training - Epoch: [11][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.5432 (600.0346)	
2019-01-08 18:22:51,498 - 10 - training_embed.py - training - Epoch: [11][2100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.4715 (600.0102)	
2019-01-08 18:22:51,719 - 10 - training_embed.py - training - Epoch: [11][2200/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 597.9302 (599.9821)	
2019-01-08 18:22:51,951 - 10 - training_embed.py - training - Epoch: [11][2300/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 596.9384 (599.9638)	
2019-01-08 18:22:52,178 - 10 - training_embed.py - training - Epoch: [11][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.2530 (599.9474)	
2019-01-08 18:22:52,394 - 10 - training_embed.py - training - Epoch: [11][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.4645 (599.9576)	
2019-01-08 18:22:52,615 - 10 - training_embed.py - training - Epoch: [11][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.1447 (599.9514)	
2019-01-08 18:22:52,829 - 10 - training_embed.py - training - Epoch: [11][2700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 603.7982 (599.9306)	
2019-01-08 18:22:53,043 - 10 - training_embed.py - training - Epoch: [11][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.2515 (599.9033)	
2019-01-08 18:22:53,279 - 10 - training_embed.py - training - Epoch: [11][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.4780 (599.8682)	
2019-01-08 18:22:53,509 - 10 - training_embed.py - training - Epoch: [11][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.9890 (599.8385)	
2019-01-08 18:22:53,721 - 10 - training_embed.py - training - Epoch: [11][3100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 592.5209 (599.8207)	
2019-01-08 18:22:53,932 - 10 - training_embed.py - training - Epoch: [11][3200/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 597.5655 (599.8013)	
2019-01-08 18:22:54,160 - 10 - training_embed.py - training - Epoch: [11][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.8568 (599.7816)	
2019-01-08 18:22:54,382 - 10 - training_embed.py - training - Epoch: [11][3400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 600.8257 (599.7566)	
2019-01-08 18:22:54,616 - 10 - training_embed.py - training - Epoch: [11][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.9347 (599.7283)	
2019-01-08 18:22:54,838 - 10 - training_embed.py - training - Epoch: [11][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.6727 (599.7272)	
2019-01-08 18:22:54,965 - 10 - training_embed.py - training - loss: 599.673876
2019-01-08 18:22:55,116 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 18:22:57,046 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1929.584 ms ~ 0.032 min ~ 1.930 sec
2019-01-08 18:22:59,047 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 3930.690 ms ~ 0.066 min ~ 3.931 sec
2019-01-08 18:22:59,047 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 18:22:59,047 - 10 - corpus.py - subactivity_sampler - 0 / 157
2019-01-08 18:22:59,048 - 10 - corpus.py - subactivity_sampler - [115245.  23373.  61682. 103342.  43886.  66866.  63623. 116778.  31412.
  89781.  10647. 155627.  54863.]
2019-01-08 18:28:00,678 - 10 - corpus.py - subactivity_sampler - 20 / 157
2019-01-08 18:28:00,678 - 10 - corpus.py - subactivity_sampler - [115689.  22782.  61125. 104428.  43407.  67047.  63187. 117749.  29443.
  90660.  10329. 156696.  54583.]
2019-01-08 18:32:27,618 - 10 - corpus.py - subactivity_sampler - 40 / 157
2019-01-08 18:32:27,618 - 10 - corpus.py - subactivity_sampler - [115733.  22755.  60491. 105040.  43294.  66854.  62858. 118492.  27981.
  91688.   9952. 157449.  54538.]
2019-01-08 18:37:40,222 - 10 - corpus.py - subactivity_sampler - 60 / 157
2019-01-08 18:37:40,222 - 10 - corpus.py - subactivity_sampler - [116659.  22355.  59543. 105435.  43183.  66843.  62666. 119104.  26837.
  92215.   9628. 158261.  54396.]
2019-01-08 18:42:32,122 - 10 - corpus.py - subactivity_sampler - 80 / 157
2019-01-08 18:42:32,122 - 10 - corpus.py - subactivity_sampler - [116806.  22276.  59363. 106258.  42421.  67004.  62439. 119698.  25478.
  93240.   9016. 159321.  53805.]
2019-01-08 18:47:57,450 - 10 - corpus.py - subactivity_sampler - 100 / 157
2019-01-08 18:47:57,450 - 10 - corpus.py - subactivity_sampler - [116816.  22137.  58632. 107474.  42120.  67026.  61247. 120660.  24555.
  94099.   8721. 159960.  53678.]
2019-01-08 18:51:30,685 - 10 - corpus.py - subactivity_sampler - 120 / 157
2019-01-08 18:51:30,685 - 10 - corpus.py - subactivity_sampler - [117048.  22128.  58399. 107509.  42019.  67172.  61021. 121041.  23452.
  95012.   8504. 160365.  53455.]
2019-01-08 18:57:18,102 - 10 - corpus.py - subactivity_sampler - 140 / 157
2019-01-08 18:57:18,103 - 10 - corpus.py - subactivity_sampler - [117267.  21961.  57907. 108467.  41452.  66888.  60499. 122164.  22292.
  95651.   8172. 161152.  53253.]
2019-01-08 19:01:57,409 - 10 - corpus.py - subactivity_sampler - [117401.  21808.  57954. 108424.  41481.  66903.  60351. 122499.  21856.
  95793.   8052. 161759.  52844.]
2019-01-08 19:01:57,409 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 2338361.847 ms ~ 38.973 min ~ 2338.362 sec
2019-01-08 19:01:57,409 - 10 - corpus.py - ordering_sampler - .
2019-01-08 19:02:07,822 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 19:02:07,822 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 20.  56.   0.  21. 140.  37.  35.  68.  63.  11.   8.  15.]
2019-01-08 19:02:07,822 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 19:02:07,855 - 10 - corpus.py - rho_sampling - ['50.2015', '19.8600', '52.0977', '183.5699', '5.4870', '70.9350', '544.3249', '5.8620', '3.6848', '1.4937', '339.2938', '845.4643']
2019-01-08 19:02:07,855 - 10 - pipeline.py - baseline - Iteration 5
2019-01-08 19:02:08,223 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 19:02:08,242 - 10 - accuracy_class.py - mof - # gt_labels: 14   # pr_labels: 13
2019-01-08 19:02:08,243 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 26', '1: 2', '2: 11', '3: 27', '4: 10', '5: 17', '6: 28', '7: 14', '8: 31', '9: 16', '10: 4', '11: 29', '12: 30']
2019-01-08 19:02:08,293 - 10 - accuracy_class.py - mof_val - frames true: 335267	frames overall : 937125
2019-01-08 19:02:08,294 - 10 - corpus.py - accuracy_corpus - Action: pancake
2019-01-08 19:02:08,294 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3577612378284647
2019-01-08 19:02:08,294 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3577612378284647
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35402
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 2: 0.250630  9154 / 36524
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1774
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 10: 0.059592  657 / 11025
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 11: 0.097164  4460 / 45902
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 14: 0.112255  1984 / 17674
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 207
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 17: 0.352654  13744 / 38973
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 26: 0.781024  42212 / 54047
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 27: 0.523822  88187 / 168353
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 28: 0.305031  19109 / 62646
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 29: 0.312872  134704 / 430540
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 30: 0.737668  21056 / 28544
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - label 31: 0.000000  0 / 5514
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - mof_classes - average class mof: 0.252337
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35402
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - iou_classes - label 2: 0.186140  9154 / 49178
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 9826
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - iou_classes - label 10: 0.012671  657 / 51849
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - iou_classes - label 11: 0.044871  4460 / 99396
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - iou_classes - label 14: 0.014357  1984 / 138189
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 96000
2019-01-08 19:02:08,294 - 10 - accuracy_class.py - iou_classes - label 17: 0.149177  13744 / 92132
2019-01-08 19:02:08,295 - 10 - accuracy_class.py - iou_classes - label 26: 0.326627  42212 / 129236
2019-01-08 19:02:08,295 - 10 - accuracy_class.py - iou_classes - label 27: 0.467612  88187 / 188590
2019-01-08 19:02:08,295 - 10 - accuracy_class.py - iou_classes - label 28: 0.183938  19109 / 103888
2019-01-08 19:02:08,295 - 10 - accuracy_class.py - iou_classes - label 29: 0.294374  134704 / 457595
2019-01-08 19:02:08,295 - 10 - accuracy_class.py - iou_classes - label 30: 0.349002  21056 / 60332
2019-01-08 19:02:08,295 - 10 - accuracy_class.py - iou_classes - label 31: 0.000000  0 / 27370
2019-01-08 19:02:08,295 - 10 - accuracy_class.py - iou_classes - average IoU: 0.156059
2019-01-08 19:02:08,295 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.144912
2019-01-08 19:02:23,281 - 10 - f1_score.py - f1 - f1 score: 0.289912
2019-01-08 19:02:23,318 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 15462.516 ms ~ 0.258 min ~ 15.463 sec
2019-01-08 19:02:23,318 - 10 - corpus.py - embedding_training - .
2019-01-08 19:02:23,318 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 19:02:23,318 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 19:02:23,318 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 19:02:29,074 - 10 - training_embed.py - training - create model
2019-01-08 19:02:29,075 - 10 - training_embed.py - training - epochs: 12
2019-01-08 19:02:29,075 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 19:02:29,513 - 10 - training_embed.py - training - Epoch: [0][100/3661]	Time 0.002 (0.004)	Data 0.001 (0.003)	Loss 615.2990 (617.1122)	
2019-01-08 19:02:29,717 - 10 - training_embed.py - training - Epoch: [0][200/3661]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 611.5288 (616.9738)	
2019-01-08 19:02:29,932 - 10 - training_embed.py - training - Epoch: [0][300/3661]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 615.3334 (617.2682)	
2019-01-08 19:02:30,145 - 10 - training_embed.py - training - Epoch: [0][400/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 619.3746 (617.2283)	
2019-01-08 19:02:30,357 - 10 - training_embed.py - training - Epoch: [0][500/3661]	Time 0.003 (0.003)	Data 0.001 (0.001)	Loss 611.5198 (617.2487)	
2019-01-08 19:02:30,590 - 10 - training_embed.py - training - Epoch: [0][600/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 616.4770 (617.3487)	
2019-01-08 19:02:30,810 - 10 - training_embed.py - training - Epoch: [0][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.1897 (617.3471)	
2019-01-08 19:02:31,020 - 10 - training_embed.py - training - Epoch: [0][800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 618.1708 (617.2662)	
2019-01-08 19:02:31,250 - 10 - training_embed.py - training - Epoch: [0][900/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 614.6204 (617.2421)	
2019-01-08 19:02:31,486 - 10 - training_embed.py - training - Epoch: [0][1000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.5871 (617.1984)	
2019-01-08 19:02:31,684 - 10 - training_embed.py - training - Epoch: [0][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.8879 (617.1169)	
2019-01-08 19:02:31,928 - 10 - training_embed.py - training - Epoch: [0][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 623.2820 (617.0918)	
2019-01-08 19:02:32,150 - 10 - training_embed.py - training - Epoch: [0][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.8248 (617.1218)	
2019-01-08 19:02:32,364 - 10 - training_embed.py - training - Epoch: [0][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9764 (617.1129)	
2019-01-08 19:02:32,597 - 10 - training_embed.py - training - Epoch: [0][1500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 615.2962 (617.0808)	
2019-01-08 19:02:32,804 - 10 - training_embed.py - training - Epoch: [0][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.1901 (617.0151)	
2019-01-08 19:02:33,046 - 10 - training_embed.py - training - Epoch: [0][1700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 612.3658 (617.0232)	
2019-01-08 19:02:33,266 - 10 - training_embed.py - training - Epoch: [0][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.3360 (616.9543)	
2019-01-08 19:02:33,474 - 10 - training_embed.py - training - Epoch: [0][1900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 610.6812 (616.9203)	
2019-01-08 19:02:33,691 - 10 - training_embed.py - training - Epoch: [0][2000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 628.6828 (616.9191)	
2019-01-08 19:02:33,904 - 10 - training_embed.py - training - Epoch: [0][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.0407 (616.9122)	
2019-01-08 19:02:34,116 - 10 - training_embed.py - training - Epoch: [0][2200/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 608.7073 (616.8772)	
2019-01-08 19:02:34,341 - 10 - training_embed.py - training - Epoch: [0][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.1506 (616.8821)	
2019-01-08 19:02:34,545 - 10 - training_embed.py - training - Epoch: [0][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.0674 (616.8623)	
2019-01-08 19:02:34,752 - 10 - training_embed.py - training - Epoch: [0][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 622.7858 (616.8430)	
2019-01-08 19:02:34,969 - 10 - training_embed.py - training - Epoch: [0][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.6016 (616.8138)	
2019-01-08 19:02:35,197 - 10 - training_embed.py - training - Epoch: [0][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 622.1743 (616.7990)	
2019-01-08 19:02:35,435 - 10 - training_embed.py - training - Epoch: [0][2800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.7377 (616.7806)	
2019-01-08 19:02:35,634 - 10 - training_embed.py - training - Epoch: [0][2900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 615.6003 (616.7475)	
2019-01-08 19:02:35,842 - 10 - training_embed.py - training - Epoch: [0][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.8694 (616.7309)	
2019-01-08 19:02:36,065 - 10 - training_embed.py - training - Epoch: [0][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 624.5290 (616.6954)	
2019-01-08 19:02:36,281 - 10 - training_embed.py - training - Epoch: [0][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.9778 (616.6862)	
2019-01-08 19:02:36,484 - 10 - training_embed.py - training - Epoch: [0][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.7112 (616.6534)	
2019-01-08 19:02:36,716 - 10 - training_embed.py - training - Epoch: [0][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.8138 (616.6314)	
2019-01-08 19:02:36,924 - 10 - training_embed.py - training - Epoch: [0][3500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 618.5467 (616.6245)	
2019-01-08 19:02:37,126 - 10 - training_embed.py - training - Epoch: [0][3600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.5862 (616.6041)	
2019-01-08 19:02:37,253 - 10 - training_embed.py - training - loss: 616.540052
2019-01-08 19:02:37,257 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 19:02:37,743 - 10 - training_embed.py - training - Epoch: [1][100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 611.2493 (615.8879)	
2019-01-08 19:02:37,948 - 10 - training_embed.py - training - Epoch: [1][200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.6714 (615.7363)	
2019-01-08 19:02:38,149 - 10 - training_embed.py - training - Epoch: [1][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.6744 (615.9800)	
2019-01-08 19:02:38,367 - 10 - training_embed.py - training - Epoch: [1][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.9051 (615.6271)	
2019-01-08 19:02:38,595 - 10 - training_embed.py - training - Epoch: [1][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 617.8218 (615.6657)	
2019-01-08 19:02:38,819 - 10 - training_embed.py - training - Epoch: [1][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.3807 (615.5617)	
2019-01-08 19:02:39,037 - 10 - training_embed.py - training - Epoch: [1][700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 616.7987 (615.5456)	
2019-01-08 19:02:39,248 - 10 - training_embed.py - training - Epoch: [1][800/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 616.0719 (615.5041)	
2019-01-08 19:02:39,448 - 10 - training_embed.py - training - Epoch: [1][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.2414 (615.3354)	
2019-01-08 19:02:39,656 - 10 - training_embed.py - training - Epoch: [1][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.7946 (615.4012)	
2019-01-08 19:02:39,878 - 10 - training_embed.py - training - Epoch: [1][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.9939 (615.4074)	
2019-01-08 19:02:40,086 - 10 - training_embed.py - training - Epoch: [1][1200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 617.5061 (615.3201)	
2019-01-08 19:02:40,308 - 10 - training_embed.py - training - Epoch: [1][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.2604 (615.3072)	
2019-01-08 19:02:40,520 - 10 - training_embed.py - training - Epoch: [1][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.9205 (615.3025)	
2019-01-08 19:02:40,737 - 10 - training_embed.py - training - Epoch: [1][1500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 620.3865 (615.2971)	
2019-01-08 19:02:40,955 - 10 - training_embed.py - training - Epoch: [1][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.7990 (615.3079)	
2019-01-08 19:02:41,175 - 10 - training_embed.py - training - Epoch: [1][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.7796 (615.2684)	
2019-01-08 19:02:41,407 - 10 - training_embed.py - training - Epoch: [1][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.2449 (615.2716)	
2019-01-08 19:02:41,604 - 10 - training_embed.py - training - Epoch: [1][1900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.9925 (615.2573)	
2019-01-08 19:02:41,820 - 10 - training_embed.py - training - Epoch: [1][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 618.5894 (615.2498)	
2019-01-08 19:02:42,043 - 10 - training_embed.py - training - Epoch: [1][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.6976 (615.2514)	
2019-01-08 19:02:42,277 - 10 - training_embed.py - training - Epoch: [1][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.6406 (615.2022)	
2019-01-08 19:02:42,491 - 10 - training_embed.py - training - Epoch: [1][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.1873 (615.1773)	
2019-01-08 19:02:42,719 - 10 - training_embed.py - training - Epoch: [1][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.7448 (615.1485)	
2019-01-08 19:02:42,925 - 10 - training_embed.py - training - Epoch: [1][2500/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 614.8244 (615.1344)	
2019-01-08 19:02:43,142 - 10 - training_embed.py - training - Epoch: [1][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.2923 (615.0996)	
2019-01-08 19:02:43,376 - 10 - training_embed.py - training - Epoch: [1][2700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 618.5613 (615.0763)	
2019-01-08 19:02:43,587 - 10 - training_embed.py - training - Epoch: [1][2800/3661]	Time 0.007 (0.002)	Data 0.006 (0.001)	Loss 614.4495 (615.0503)	
2019-01-08 19:02:43,810 - 10 - training_embed.py - training - Epoch: [1][2900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 612.7825 (615.0422)	
2019-01-08 19:02:44,013 - 10 - training_embed.py - training - Epoch: [1][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.6233 (614.9993)	
2019-01-08 19:02:44,229 - 10 - training_embed.py - training - Epoch: [1][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.5323 (614.9674)	
2019-01-08 19:02:44,445 - 10 - training_embed.py - training - Epoch: [1][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.1663 (614.9531)	
2019-01-08 19:02:44,658 - 10 - training_embed.py - training - Epoch: [1][3300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 611.2140 (614.9293)	
2019-01-08 19:02:44,873 - 10 - training_embed.py - training - Epoch: [1][3400/3661]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 613.9483 (614.8985)	
2019-01-08 19:02:45,112 - 10 - training_embed.py - training - Epoch: [1][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.6290 (614.9114)	
2019-01-08 19:02:45,330 - 10 - training_embed.py - training - Epoch: [1][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 620.4795 (614.8820)	
2019-01-08 19:02:45,464 - 10 - training_embed.py - training - loss: 614.836405
2019-01-08 19:02:45,464 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 19:02:45,944 - 10 - training_embed.py - training - Epoch: [2][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.0851 (613.7921)	
2019-01-08 19:02:46,171 - 10 - training_embed.py - training - Epoch: [2][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.1567 (614.1085)	
2019-01-08 19:02:46,394 - 10 - training_embed.py - training - Epoch: [2][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.3525 (613.9470)	
2019-01-08 19:02:46,616 - 10 - training_embed.py - training - Epoch: [2][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.6569 (613.8115)	
2019-01-08 19:02:46,821 - 10 - training_embed.py - training - Epoch: [2][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.2620 (613.6470)	
2019-01-08 19:02:47,052 - 10 - training_embed.py - training - Epoch: [2][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.6871 (613.6285)	
2019-01-08 19:02:47,268 - 10 - training_embed.py - training - Epoch: [2][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.4673 (613.6443)	
2019-01-08 19:02:47,503 - 10 - training_embed.py - training - Epoch: [2][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.3019 (613.6478)	
2019-01-08 19:02:47,730 - 10 - training_embed.py - training - Epoch: [2][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5439 (613.6481)	
2019-01-08 19:02:47,939 - 10 - training_embed.py - training - Epoch: [2][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 621.7131 (613.5746)	
2019-01-08 19:02:48,159 - 10 - training_embed.py - training - Epoch: [2][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.0126 (613.5714)	
2019-01-08 19:02:48,363 - 10 - training_embed.py - training - Epoch: [2][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8837 (613.5995)	
2019-01-08 19:02:48,604 - 10 - training_embed.py - training - Epoch: [2][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.2372 (613.5808)	
2019-01-08 19:02:48,821 - 10 - training_embed.py - training - Epoch: [2][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3839 (613.5848)	
2019-01-08 19:02:49,044 - 10 - training_embed.py - training - Epoch: [2][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.0539 (613.6003)	
2019-01-08 19:02:49,252 - 10 - training_embed.py - training - Epoch: [2][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.3765 (613.5793)	
2019-01-08 19:02:49,473 - 10 - training_embed.py - training - Epoch: [2][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.0001 (613.5758)	
2019-01-08 19:02:49,678 - 10 - training_embed.py - training - Epoch: [2][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3940 (613.5414)	
2019-01-08 19:02:49,909 - 10 - training_embed.py - training - Epoch: [2][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.1201 (613.5155)	
2019-01-08 19:02:50,122 - 10 - training_embed.py - training - Epoch: [2][2000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.8697 (613.4526)	
2019-01-08 19:02:50,331 - 10 - training_embed.py - training - Epoch: [2][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.6299 (613.4519)	
2019-01-08 19:02:50,565 - 10 - training_embed.py - training - Epoch: [2][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.0019 (613.3924)	
2019-01-08 19:02:50,791 - 10 - training_embed.py - training - Epoch: [2][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.3984 (613.3790)	
2019-01-08 19:02:51,011 - 10 - training_embed.py - training - Epoch: [2][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.8483 (613.3559)	
2019-01-08 19:02:51,214 - 10 - training_embed.py - training - Epoch: [2][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.3349 (613.3398)	
2019-01-08 19:02:51,437 - 10 - training_embed.py - training - Epoch: [2][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.5439 (613.3291)	
2019-01-08 19:02:51,642 - 10 - training_embed.py - training - Epoch: [2][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.4566 (613.3217)	
2019-01-08 19:02:51,870 - 10 - training_embed.py - training - Epoch: [2][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.5963 (613.3110)	
2019-01-08 19:02:52,075 - 10 - training_embed.py - training - Epoch: [2][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.1608 (613.3022)	
2019-01-08 19:02:52,314 - 10 - training_embed.py - training - Epoch: [2][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.8617 (613.2796)	
2019-01-08 19:02:52,536 - 10 - training_embed.py - training - Epoch: [2][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.7671 (613.2665)	
2019-01-08 19:02:52,756 - 10 - training_embed.py - training - Epoch: [2][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.1828 (613.2456)	
2019-01-08 19:02:52,985 - 10 - training_embed.py - training - Epoch: [2][3300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 623.4867 (613.2495)	
2019-01-08 19:02:53,205 - 10 - training_embed.py - training - Epoch: [2][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.5169 (613.2194)	
2019-01-08 19:02:53,439 - 10 - training_embed.py - training - Epoch: [2][3500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.2617 (613.2066)	
2019-01-08 19:02:53,668 - 10 - training_embed.py - training - Epoch: [2][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.5636 (613.1918)	
2019-01-08 19:02:53,798 - 10 - training_embed.py - training - loss: 613.135029
2019-01-08 19:02:53,798 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 19:02:54,285 - 10 - training_embed.py - training - Epoch: [3][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.4846 (612.3904)	
2019-01-08 19:02:54,503 - 10 - training_embed.py - training - Epoch: [3][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 621.4340 (612.4738)	
2019-01-08 19:02:54,720 - 10 - training_embed.py - training - Epoch: [3][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.0029 (612.3757)	
2019-01-08 19:02:54,926 - 10 - training_embed.py - training - Epoch: [3][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.2109 (612.3550)	
2019-01-08 19:02:55,128 - 10 - training_embed.py - training - Epoch: [3][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.5381 (612.3389)	
2019-01-08 19:02:55,354 - 10 - training_embed.py - training - Epoch: [3][600/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 611.9799 (612.2414)	
2019-01-08 19:02:55,571 - 10 - training_embed.py - training - Epoch: [3][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.0396 (612.2596)	
2019-01-08 19:02:55,794 - 10 - training_embed.py - training - Epoch: [3][800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.4678 (612.2611)	
2019-01-08 19:02:56,027 - 10 - training_embed.py - training - Epoch: [3][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5219 (612.2889)	
2019-01-08 19:02:56,238 - 10 - training_embed.py - training - Epoch: [3][1000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 617.8165 (612.2189)	
2019-01-08 19:02:56,448 - 10 - training_embed.py - training - Epoch: [3][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.6243 (612.1689)	
2019-01-08 19:02:56,662 - 10 - training_embed.py - training - Epoch: [3][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.5407 (612.1626)	
2019-01-08 19:02:56,870 - 10 - training_embed.py - training - Epoch: [3][1300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.8915 (612.1962)	
2019-01-08 19:02:57,086 - 10 - training_embed.py - training - Epoch: [3][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.9555 (612.1670)	
2019-01-08 19:02:57,307 - 10 - training_embed.py - training - Epoch: [3][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.4683 (612.1106)	
2019-01-08 19:02:57,511 - 10 - training_embed.py - training - Epoch: [3][1600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.6235 (612.0895)	
2019-01-08 19:02:57,712 - 10 - training_embed.py - training - Epoch: [3][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.0618 (612.0361)	
2019-01-08 19:02:57,942 - 10 - training_embed.py - training - Epoch: [3][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.4418 (612.0149)	
2019-01-08 19:02:58,151 - 10 - training_embed.py - training - Epoch: [3][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.8625 (611.9393)	
2019-01-08 19:02:58,373 - 10 - training_embed.py - training - Epoch: [3][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.7890 (611.9081)	
2019-01-08 19:02:58,591 - 10 - training_embed.py - training - Epoch: [3][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.6694 (611.8999)	
2019-01-08 19:02:58,824 - 10 - training_embed.py - training - Epoch: [3][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.3754 (611.8684)	
2019-01-08 19:02:59,051 - 10 - training_embed.py - training - Epoch: [3][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.1760 (611.8351)	
2019-01-08 19:02:59,263 - 10 - training_embed.py - training - Epoch: [3][2400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.1934 (611.8121)	
2019-01-08 19:02:59,489 - 10 - training_embed.py - training - Epoch: [3][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.3579 (611.7902)	
2019-01-08 19:02:59,701 - 10 - training_embed.py - training - Epoch: [3][2600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.2135 (611.7441)	
2019-01-08 19:02:59,914 - 10 - training_embed.py - training - Epoch: [3][2700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 603.3878 (611.7259)	
2019-01-08 19:03:00,147 - 10 - training_embed.py - training - Epoch: [3][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.6532 (611.6840)	
2019-01-08 19:03:00,383 - 10 - training_embed.py - training - Epoch: [3][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.1411 (611.6501)	
2019-01-08 19:03:00,596 - 10 - training_embed.py - training - Epoch: [3][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.5417 (611.6180)	
2019-01-08 19:03:00,809 - 10 - training_embed.py - training - Epoch: [3][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3839 (611.5807)	
2019-01-08 19:03:01,030 - 10 - training_embed.py - training - Epoch: [3][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.8190 (611.5375)	
2019-01-08 19:03:01,234 - 10 - training_embed.py - training - Epoch: [3][3300/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 609.0804 (611.5076)	
2019-01-08 19:03:01,469 - 10 - training_embed.py - training - Epoch: [3][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3811 (611.4781)	
2019-01-08 19:03:01,692 - 10 - training_embed.py - training - Epoch: [3][3500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.3939 (611.4959)	
2019-01-08 19:03:01,940 - 10 - training_embed.py - training - Epoch: [3][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.4882 (611.4781)	
2019-01-08 19:03:02,069 - 10 - training_embed.py - training - loss: 611.433806
2019-01-08 19:03:02,069 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 19:03:02,565 - 10 - training_embed.py - training - Epoch: [4][100/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 615.0087 (610.3051)	
2019-01-08 19:03:02,788 - 10 - training_embed.py - training - Epoch: [4][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3187 (610.2301)	
2019-01-08 19:03:03,017 - 10 - training_embed.py - training - Epoch: [4][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.4440 (610.3324)	
2019-01-08 19:03:03,251 - 10 - training_embed.py - training - Epoch: [4][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2642 (610.6860)	
2019-01-08 19:03:03,464 - 10 - training_embed.py - training - Epoch: [4][500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 604.9637 (610.5118)	
2019-01-08 19:03:03,669 - 10 - training_embed.py - training - Epoch: [4][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.1813 (610.5555)	
2019-01-08 19:03:03,896 - 10 - training_embed.py - training - Epoch: [4][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.4310 (610.4531)	
2019-01-08 19:03:04,112 - 10 - training_embed.py - training - Epoch: [4][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.1196 (610.4462)	
2019-01-08 19:03:04,345 - 10 - training_embed.py - training - Epoch: [4][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.3036 (610.3974)	
2019-01-08 19:03:04,560 - 10 - training_embed.py - training - Epoch: [4][1000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 601.4416 (610.3738)	
2019-01-08 19:03:04,766 - 10 - training_embed.py - training - Epoch: [4][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.4008 (610.3578)	
2019-01-08 19:03:04,982 - 10 - training_embed.py - training - Epoch: [4][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.2582 (610.3226)	
2019-01-08 19:03:05,187 - 10 - training_embed.py - training - Epoch: [4][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.2804 (610.2594)	
2019-01-08 19:03:05,428 - 10 - training_embed.py - training - Epoch: [4][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.3830 (610.2903)	
2019-01-08 19:03:05,635 - 10 - training_embed.py - training - Epoch: [4][1500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 615.3115 (610.2496)	
2019-01-08 19:03:05,846 - 10 - training_embed.py - training - Epoch: [4][1600/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 617.6292 (610.2263)	
2019-01-08 19:03:06,056 - 10 - training_embed.py - training - Epoch: [4][1700/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 612.9311 (610.2130)	
2019-01-08 19:03:06,281 - 10 - training_embed.py - training - Epoch: [4][1800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.5985 (610.1827)	
2019-01-08 19:03:06,504 - 10 - training_embed.py - training - Epoch: [4][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.6136 (610.1580)	
2019-01-08 19:03:06,744 - 10 - training_embed.py - training - Epoch: [4][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.5822 (610.1665)	
2019-01-08 19:03:06,975 - 10 - training_embed.py - training - Epoch: [4][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.5259 (610.1165)	
2019-01-08 19:03:07,172 - 10 - training_embed.py - training - Epoch: [4][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6947 (610.0786)	
2019-01-08 19:03:07,385 - 10 - training_embed.py - training - Epoch: [4][2300/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 607.4477 (610.0345)	
2019-01-08 19:03:07,623 - 10 - training_embed.py - training - Epoch: [4][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3206 (610.0076)	
2019-01-08 19:03:07,849 - 10 - training_embed.py - training - Epoch: [4][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.6774 (610.0173)	
2019-01-08 19:03:08,055 - 10 - training_embed.py - training - Epoch: [4][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.2288 (609.9675)	
2019-01-08 19:03:08,259 - 10 - training_embed.py - training - Epoch: [4][2700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.1151 (609.9401)	
2019-01-08 19:03:08,477 - 10 - training_embed.py - training - Epoch: [4][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.4831 (609.9521)	
2019-01-08 19:03:08,712 - 10 - training_embed.py - training - Epoch: [4][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8210 (609.9397)	
2019-01-08 19:03:08,932 - 10 - training_embed.py - training - Epoch: [4][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.9172 (609.9266)	
2019-01-08 19:03:09,144 - 10 - training_embed.py - training - Epoch: [4][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.3290 (609.9282)	
2019-01-08 19:03:09,342 - 10 - training_embed.py - training - Epoch: [4][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6557 (609.9058)	
2019-01-08 19:03:09,543 - 10 - training_embed.py - training - Epoch: [4][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5996 (609.8641)	
2019-01-08 19:03:09,760 - 10 - training_embed.py - training - Epoch: [4][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.4128 (609.8414)	
2019-01-08 19:03:09,987 - 10 - training_embed.py - training - Epoch: [4][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.8595 (609.8246)	
2019-01-08 19:03:10,218 - 10 - training_embed.py - training - Epoch: [4][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.4019 (609.8025)	
2019-01-08 19:03:10,349 - 10 - training_embed.py - training - loss: 609.730811
2019-01-08 19:03:10,349 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 19:03:10,830 - 10 - training_embed.py - training - Epoch: [5][100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.0410 (609.6571)	
2019-01-08 19:03:11,050 - 10 - training_embed.py - training - Epoch: [5][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5953 (609.6190)	
2019-01-08 19:03:11,257 - 10 - training_embed.py - training - Epoch: [5][300/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 615.8886 (609.4910)	
2019-01-08 19:03:11,477 - 10 - training_embed.py - training - Epoch: [5][400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 600.9758 (609.2898)	
2019-01-08 19:03:11,691 - 10 - training_embed.py - training - Epoch: [5][500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 615.0167 (609.1852)	
2019-01-08 19:03:11,892 - 10 - training_embed.py - training - Epoch: [5][600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 604.7336 (609.0396)	
2019-01-08 19:03:12,109 - 10 - training_embed.py - training - Epoch: [5][700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 616.1054 (609.0024)	
2019-01-08 19:03:12,332 - 10 - training_embed.py - training - Epoch: [5][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.0744 (608.9081)	
2019-01-08 19:03:12,535 - 10 - training_embed.py - training - Epoch: [5][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.5117 (608.8477)	
2019-01-08 19:03:12,756 - 10 - training_embed.py - training - Epoch: [5][1000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 600.6212 (608.7898)	
2019-01-08 19:03:12,966 - 10 - training_embed.py - training - Epoch: [5][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.9944 (608.7295)	
2019-01-08 19:03:13,162 - 10 - training_embed.py - training - Epoch: [5][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.0602 (608.6853)	
2019-01-08 19:03:13,376 - 10 - training_embed.py - training - Epoch: [5][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.9377 (608.6807)	
2019-01-08 19:03:13,607 - 10 - training_embed.py - training - Epoch: [5][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.9214 (608.6884)	
2019-01-08 19:03:13,843 - 10 - training_embed.py - training - Epoch: [5][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.1566 (608.6611)	
2019-01-08 19:03:14,058 - 10 - training_embed.py - training - Epoch: [5][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.6168 (608.6310)	
2019-01-08 19:03:14,264 - 10 - training_embed.py - training - Epoch: [5][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.3206 (608.5837)	
2019-01-08 19:03:14,485 - 10 - training_embed.py - training - Epoch: [5][1800/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 610.0775 (608.5398)	
2019-01-08 19:03:14,719 - 10 - training_embed.py - training - Epoch: [5][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3537 (608.5097)	
2019-01-08 19:03:14,922 - 10 - training_embed.py - training - Epoch: [5][2000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 605.0568 (608.4741)	
2019-01-08 19:03:15,147 - 10 - training_embed.py - training - Epoch: [5][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.1033 (608.4595)	
2019-01-08 19:03:15,350 - 10 - training_embed.py - training - Epoch: [5][2200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 605.6769 (608.4428)	
2019-01-08 19:03:15,555 - 10 - training_embed.py - training - Epoch: [5][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3773 (608.4142)	
2019-01-08 19:03:15,776 - 10 - training_embed.py - training - Epoch: [5][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.2209 (608.3993)	
2019-01-08 19:03:16,013 - 10 - training_embed.py - training - Epoch: [5][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.8079 (608.3731)	
2019-01-08 19:03:16,246 - 10 - training_embed.py - training - Epoch: [5][2600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.1967 (608.3260)	
2019-01-08 19:03:16,462 - 10 - training_embed.py - training - Epoch: [5][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.8050 (608.3022)	
2019-01-08 19:03:16,662 - 10 - training_embed.py - training - Epoch: [5][2800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.8902 (608.2637)	
2019-01-08 19:03:16,868 - 10 - training_embed.py - training - Epoch: [5][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.0906 (608.2422)	
2019-01-08 19:03:17,086 - 10 - training_embed.py - training - Epoch: [5][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.7392 (608.2294)	
2019-01-08 19:03:17,317 - 10 - training_embed.py - training - Epoch: [5][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.4579 (608.1858)	
2019-01-08 19:03:17,547 - 10 - training_embed.py - training - Epoch: [5][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.6087 (608.1612)	
2019-01-08 19:03:17,749 - 10 - training_embed.py - training - Epoch: [5][3300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 611.0308 (608.1269)	
2019-01-08 19:03:17,982 - 10 - training_embed.py - training - Epoch: [5][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3278 (608.1126)	
2019-01-08 19:03:18,186 - 10 - training_embed.py - training - Epoch: [5][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.9017 (608.1006)	
2019-01-08 19:03:18,419 - 10 - training_embed.py - training - Epoch: [5][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.6165 (608.0646)	
2019-01-08 19:03:18,546 - 10 - training_embed.py - training - loss: 608.021687
2019-01-08 19:03:18,546 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 19:03:19,028 - 10 - training_embed.py - training - Epoch: [6][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.5882 (606.6740)	
2019-01-08 19:03:19,228 - 10 - training_embed.py - training - Epoch: [6][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.1635 (606.9114)	
2019-01-08 19:03:19,453 - 10 - training_embed.py - training - Epoch: [6][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.8104 (606.9832)	
2019-01-08 19:03:19,681 - 10 - training_embed.py - training - Epoch: [6][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.7081 (607.1170)	
2019-01-08 19:03:19,910 - 10 - training_embed.py - training - Epoch: [6][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.3461 (607.0708)	
2019-01-08 19:03:20,117 - 10 - training_embed.py - training - Epoch: [6][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.5350 (607.0591)	
2019-01-08 19:03:20,332 - 10 - training_embed.py - training - Epoch: [6][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.5966 (607.1231)	
2019-01-08 19:03:20,552 - 10 - training_embed.py - training - Epoch: [6][800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.3474 (607.1195)	
2019-01-08 19:03:20,778 - 10 - training_embed.py - training - Epoch: [6][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.7021 (607.0990)	
2019-01-08 19:03:20,985 - 10 - training_embed.py - training - Epoch: [6][1000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.8282 (607.0408)	
2019-01-08 19:03:21,209 - 10 - training_embed.py - training - Epoch: [6][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.7627 (607.0146)	
2019-01-08 19:03:21,420 - 10 - training_embed.py - training - Epoch: [6][1200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.3604 (606.9497)	
2019-01-08 19:03:21,621 - 10 - training_embed.py - training - Epoch: [6][1300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 600.1920 (606.8959)	
2019-01-08 19:03:21,839 - 10 - training_embed.py - training - Epoch: [6][1400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.9714 (606.9141)	
2019-01-08 19:03:22,058 - 10 - training_embed.py - training - Epoch: [6][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.0310 (606.9073)	
2019-01-08 19:03:22,275 - 10 - training_embed.py - training - Epoch: [6][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.4378 (606.8526)	
2019-01-08 19:03:22,497 - 10 - training_embed.py - training - Epoch: [6][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.5059 (606.8090)	
2019-01-08 19:03:22,709 - 10 - training_embed.py - training - Epoch: [6][1800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.6924 (606.7864)	
2019-01-08 19:03:22,926 - 10 - training_embed.py - training - Epoch: [6][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.0157 (606.7856)	
2019-01-08 19:03:23,150 - 10 - training_embed.py - training - Epoch: [6][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.7343 (606.7300)	
2019-01-08 19:03:23,359 - 10 - training_embed.py - training - Epoch: [6][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.1468 (606.6742)	
2019-01-08 19:03:23,585 - 10 - training_embed.py - training - Epoch: [6][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.8141 (606.6346)	
2019-01-08 19:03:23,795 - 10 - training_embed.py - training - Epoch: [6][2300/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 601.7488 (606.6165)	
2019-01-08 19:03:24,015 - 10 - training_embed.py - training - Epoch: [6][2400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 602.3179 (606.6071)	
2019-01-08 19:03:24,228 - 10 - training_embed.py - training - Epoch: [6][2500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 603.2836 (606.5889)	
2019-01-08 19:03:24,445 - 10 - training_embed.py - training - Epoch: [6][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.0634 (606.5864)	
2019-01-08 19:03:24,671 - 10 - training_embed.py - training - Epoch: [6][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.5452 (606.5866)	
2019-01-08 19:03:24,872 - 10 - training_embed.py - training - Epoch: [6][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.5051 (606.5633)	
2019-01-08 19:03:25,101 - 10 - training_embed.py - training - Epoch: [6][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.6215 (606.5364)	
2019-01-08 19:03:25,326 - 10 - training_embed.py - training - Epoch: [6][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.8322 (606.4975)	
2019-01-08 19:03:25,556 - 10 - training_embed.py - training - Epoch: [6][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.3523 (606.4594)	
2019-01-08 19:03:25,761 - 10 - training_embed.py - training - Epoch: [6][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.7338 (606.4387)	
2019-01-08 19:03:25,996 - 10 - training_embed.py - training - Epoch: [6][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.8204 (606.4290)	
2019-01-08 19:03:26,207 - 10 - training_embed.py - training - Epoch: [6][3400/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 601.0233 (606.4031)	
2019-01-08 19:03:26,432 - 10 - training_embed.py - training - Epoch: [6][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.9144 (606.3688)	
2019-01-08 19:03:26,664 - 10 - training_embed.py - training - Epoch: [6][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.9120 (606.3549)	
2019-01-08 19:03:26,799 - 10 - training_embed.py - training - loss: 606.305164
2019-01-08 19:03:26,800 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 19:03:27,279 - 10 - training_embed.py - training - Epoch: [7][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.3019 (605.3427)	
2019-01-08 19:03:27,494 - 10 - training_embed.py - training - Epoch: [7][200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 603.5766 (605.6005)	
2019-01-08 19:03:27,712 - 10 - training_embed.py - training - Epoch: [7][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.4597 (605.6083)	
2019-01-08 19:03:27,938 - 10 - training_embed.py - training - Epoch: [7][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.8039 (605.5533)	
2019-01-08 19:03:28,147 - 10 - training_embed.py - training - Epoch: [7][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.0206 (605.4761)	
2019-01-08 19:03:28,372 - 10 - training_embed.py - training - Epoch: [7][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.7620 (605.3898)	
2019-01-08 19:03:28,575 - 10 - training_embed.py - training - Epoch: [7][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6664 (605.3533)	
2019-01-08 19:03:28,785 - 10 - training_embed.py - training - Epoch: [7][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.3640 (605.2816)	
2019-01-08 19:03:29,017 - 10 - training_embed.py - training - Epoch: [7][900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.0289 (605.1819)	
2019-01-08 19:03:29,246 - 10 - training_embed.py - training - Epoch: [7][1000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.0825 (605.1195)	
2019-01-08 19:03:29,463 - 10 - training_embed.py - training - Epoch: [7][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.7571 (605.0958)	
2019-01-08 19:03:29,687 - 10 - training_embed.py - training - Epoch: [7][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.0217 (605.0141)	
2019-01-08 19:03:29,906 - 10 - training_embed.py - training - Epoch: [7][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.6557 (605.0975)	
2019-01-08 19:03:30,106 - 10 - training_embed.py - training - Epoch: [7][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.0174 (605.0763)	
2019-01-08 19:03:30,341 - 10 - training_embed.py - training - Epoch: [7][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.1948 (605.0501)	
2019-01-08 19:03:30,548 - 10 - training_embed.py - training - Epoch: [7][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.6000 (605.0285)	
2019-01-08 19:03:30,762 - 10 - training_embed.py - training - Epoch: [7][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.5092 (605.0458)	
2019-01-08 19:03:30,975 - 10 - training_embed.py - training - Epoch: [7][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.5613 (605.0059)	
2019-01-08 19:03:31,186 - 10 - training_embed.py - training - Epoch: [7][1900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 604.5693 (604.9981)	
2019-01-08 19:03:31,422 - 10 - training_embed.py - training - Epoch: [7][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.2573 (604.9965)	
2019-01-08 19:03:31,631 - 10 - training_embed.py - training - Epoch: [7][2100/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 603.5084 (604.9801)	
2019-01-08 19:03:31,844 - 10 - training_embed.py - training - Epoch: [7][2200/3661]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 603.2151 (604.9784)	
2019-01-08 19:03:32,056 - 10 - training_embed.py - training - Epoch: [7][2300/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 602.1662 (604.9284)	
2019-01-08 19:03:32,255 - 10 - training_embed.py - training - Epoch: [7][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.8942 (604.9298)	
2019-01-08 19:03:32,471 - 10 - training_embed.py - training - Epoch: [7][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.5526 (604.9043)	
2019-01-08 19:03:32,702 - 10 - training_embed.py - training - Epoch: [7][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.4744 (604.8713)	
2019-01-08 19:03:32,904 - 10 - training_embed.py - training - Epoch: [7][2700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 604.9228 (604.8350)	
2019-01-08 19:03:33,139 - 10 - training_embed.py - training - Epoch: [7][2800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.2515 (604.8281)	
2019-01-08 19:03:33,359 - 10 - training_embed.py - training - Epoch: [7][2900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 604.0382 (604.8204)	
2019-01-08 19:03:33,568 - 10 - training_embed.py - training - Epoch: [7][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.4462 (604.8300)	
2019-01-08 19:03:33,784 - 10 - training_embed.py - training - Epoch: [7][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.0081 (604.7844)	
2019-01-08 19:03:34,026 - 10 - training_embed.py - training - Epoch: [7][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.3477 (604.7491)	
2019-01-08 19:03:34,254 - 10 - training_embed.py - training - Epoch: [7][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.4612 (604.7199)	
2019-01-08 19:03:34,467 - 10 - training_embed.py - training - Epoch: [7][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.6224 (604.6937)	
2019-01-08 19:03:34,680 - 10 - training_embed.py - training - Epoch: [7][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.3756 (604.6467)	
2019-01-08 19:03:34,879 - 10 - training_embed.py - training - Epoch: [7][3600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 594.1795 (604.6194)	
2019-01-08 19:03:35,009 - 10 - training_embed.py - training - loss: 604.577553
2019-01-08 19:03:35,010 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 19:03:35,519 - 10 - training_embed.py - training - Epoch: [8][100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.8762 (603.7581)	
2019-01-08 19:03:35,738 - 10 - training_embed.py - training - Epoch: [8][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.3697 (603.6249)	
2019-01-08 19:03:35,941 - 10 - training_embed.py - training - Epoch: [8][300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 605.7684 (603.4267)	
2019-01-08 19:03:36,159 - 10 - training_embed.py - training - Epoch: [8][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.6063 (603.6820)	
2019-01-08 19:03:36,385 - 10 - training_embed.py - training - Epoch: [8][500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 610.7476 (603.5638)	
2019-01-08 19:03:36,594 - 10 - training_embed.py - training - Epoch: [8][600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.5179 (603.4769)	
2019-01-08 19:03:36,832 - 10 - training_embed.py - training - Epoch: [8][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6648 (603.4676)	
2019-01-08 19:03:37,044 - 10 - training_embed.py - training - Epoch: [8][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.0367 (603.4617)	
2019-01-08 19:03:37,262 - 10 - training_embed.py - training - Epoch: [8][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.1247 (603.5048)	
2019-01-08 19:03:37,489 - 10 - training_embed.py - training - Epoch: [8][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.6965 (603.4740)	
2019-01-08 19:03:37,702 - 10 - training_embed.py - training - Epoch: [8][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.8943 (603.4744)	
2019-01-08 19:03:37,927 - 10 - training_embed.py - training - Epoch: [8][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.9667 (603.4927)	
2019-01-08 19:03:38,125 - 10 - training_embed.py - training - Epoch: [8][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.5233 (603.4329)	
2019-01-08 19:03:38,336 - 10 - training_embed.py - training - Epoch: [8][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.3989 (603.4227)	
2019-01-08 19:03:38,537 - 10 - training_embed.py - training - Epoch: [8][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.8528 (603.3609)	
2019-01-08 19:03:38,771 - 10 - training_embed.py - training - Epoch: [8][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.9195 (603.3433)	
2019-01-08 19:03:38,990 - 10 - training_embed.py - training - Epoch: [8][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.0379 (603.3298)	
2019-01-08 19:03:39,221 - 10 - training_embed.py - training - Epoch: [8][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.7280 (603.3242)	
2019-01-08 19:03:39,434 - 10 - training_embed.py - training - Epoch: [8][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.0974 (603.3058)	
2019-01-08 19:03:39,636 - 10 - training_embed.py - training - Epoch: [8][2000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 603.1420 (603.3164)	
2019-01-08 19:03:39,855 - 10 - training_embed.py - training - Epoch: [8][2100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 602.0667 (603.2976)	
2019-01-08 19:03:40,062 - 10 - training_embed.py - training - Epoch: [8][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 607.1703 (603.2720)	
2019-01-08 19:03:40,295 - 10 - training_embed.py - training - Epoch: [8][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.9875 (603.2224)	
2019-01-08 19:03:40,501 - 10 - training_embed.py - training - Epoch: [8][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.7336 (603.2011)	
2019-01-08 19:03:40,708 - 10 - training_embed.py - training - Epoch: [8][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.6865 (603.1793)	
2019-01-08 19:03:40,931 - 10 - training_embed.py - training - Epoch: [8][2600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 598.2298 (603.1319)	
2019-01-08 19:03:41,174 - 10 - training_embed.py - training - Epoch: [8][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.9669 (603.0937)	
2019-01-08 19:03:41,378 - 10 - training_embed.py - training - Epoch: [8][2800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 606.6860 (603.0575)	
2019-01-08 19:03:41,600 - 10 - training_embed.py - training - Epoch: [8][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 595.9048 (603.0369)	
2019-01-08 19:03:41,816 - 10 - training_embed.py - training - Epoch: [8][3000/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 600.1107 (603.0161)	
2019-01-08 19:03:42,019 - 10 - training_embed.py - training - Epoch: [8][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.7215 (602.9892)	
2019-01-08 19:03:42,228 - 10 - training_embed.py - training - Epoch: [8][3200/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 600.3235 (602.9804)	
2019-01-08 19:03:42,455 - 10 - training_embed.py - training - Epoch: [8][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.7114 (602.9686)	
2019-01-08 19:03:42,662 - 10 - training_embed.py - training - Epoch: [8][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.9788 (602.9461)	
2019-01-08 19:03:42,874 - 10 - training_embed.py - training - Epoch: [8][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.4489 (602.9139)	
2019-01-08 19:03:43,085 - 10 - training_embed.py - training - Epoch: [8][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.1121 (602.8891)	
2019-01-08 19:03:43,213 - 10 - training_embed.py - training - loss: 602.836584
2019-01-08 19:03:43,213 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 19:03:43,700 - 10 - training_embed.py - training - Epoch: [9][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.6996 (601.5347)	
2019-01-08 19:03:43,939 - 10 - training_embed.py - training - Epoch: [9][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.6518 (602.0118)	
2019-01-08 19:03:44,145 - 10 - training_embed.py - training - Epoch: [9][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.0210 (602.5549)	
2019-01-08 19:03:44,360 - 10 - training_embed.py - training - Epoch: [9][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.9310 (602.2987)	
2019-01-08 19:03:44,563 - 10 - training_embed.py - training - Epoch: [9][500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 601.3510 (602.3043)	
2019-01-08 19:03:44,787 - 10 - training_embed.py - training - Epoch: [9][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.0939 (602.3413)	
2019-01-08 19:03:45,009 - 10 - training_embed.py - training - Epoch: [9][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.5312 (602.2582)	
2019-01-08 19:03:45,234 - 10 - training_embed.py - training - Epoch: [9][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.8797 (602.2258)	
2019-01-08 19:03:45,448 - 10 - training_embed.py - training - Epoch: [9][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.5805 (602.1744)	
2019-01-08 19:03:45,668 - 10 - training_embed.py - training - Epoch: [9][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.8433 (602.0816)	
2019-01-08 19:03:45,891 - 10 - training_embed.py - training - Epoch: [9][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.2395 (602.0243)	
2019-01-08 19:03:46,092 - 10 - training_embed.py - training - Epoch: [9][1200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 602.7419 (601.9566)	
2019-01-08 19:03:46,326 - 10 - training_embed.py - training - Epoch: [9][1300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 598.1229 (601.8976)	
2019-01-08 19:03:46,538 - 10 - training_embed.py - training - Epoch: [9][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.6708 (601.8606)	
2019-01-08 19:03:46,743 - 10 - training_embed.py - training - Epoch: [9][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.0369 (601.8000)	
2019-01-08 19:03:46,952 - 10 - training_embed.py - training - Epoch: [9][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 595.9505 (601.7724)	
2019-01-08 19:03:47,175 - 10 - training_embed.py - training - Epoch: [9][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.6953 (601.7137)	
2019-01-08 19:03:47,385 - 10 - training_embed.py - training - Epoch: [9][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.7466 (601.7045)	
2019-01-08 19:03:47,631 - 10 - training_embed.py - training - Epoch: [9][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.5410 (601.6480)	
2019-01-08 19:03:47,842 - 10 - training_embed.py - training - Epoch: [9][2000/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 593.7490 (601.5432)	
2019-01-08 19:03:48,064 - 10 - training_embed.py - training - Epoch: [9][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6140 (601.5396)	
2019-01-08 19:03:48,275 - 10 - training_embed.py - training - Epoch: [9][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.1179 (601.5124)	
2019-01-08 19:03:48,501 - 10 - training_embed.py - training - Epoch: [9][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 592.0754 (601.4647)	
2019-01-08 19:03:48,716 - 10 - training_embed.py - training - Epoch: [9][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.9809 (601.4413)	
2019-01-08 19:03:48,922 - 10 - training_embed.py - training - Epoch: [9][2500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 597.4807 (601.3859)	
2019-01-08 19:03:49,130 - 10 - training_embed.py - training - Epoch: [9][2600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.3781 (601.3889)	
2019-01-08 19:03:49,345 - 10 - training_embed.py - training - Epoch: [9][2700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 593.7698 (601.3458)	
2019-01-08 19:03:49,569 - 10 - training_embed.py - training - Epoch: [9][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.5643 (601.3419)	
2019-01-08 19:03:49,770 - 10 - training_embed.py - training - Epoch: [9][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 594.6002 (601.3023)	
2019-01-08 19:03:49,995 - 10 - training_embed.py - training - Epoch: [9][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.0772 (601.2824)	
2019-01-08 19:03:50,212 - 10 - training_embed.py - training - Epoch: [9][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 598.8155 (601.2523)	
2019-01-08 19:03:50,432 - 10 - training_embed.py - training - Epoch: [9][3200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 596.9091 (601.2182)	
2019-01-08 19:03:50,634 - 10 - training_embed.py - training - Epoch: [9][3300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 596.6043 (601.1947)	
2019-01-08 19:03:50,869 - 10 - training_embed.py - training - Epoch: [9][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.6219 (601.1736)	
2019-01-08 19:03:51,104 - 10 - training_embed.py - training - Epoch: [9][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.1011 (601.1469)	
2019-01-08 19:03:51,312 - 10 - training_embed.py - training - Epoch: [9][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.2897 (601.1305)	
2019-01-08 19:03:51,441 - 10 - training_embed.py - training - loss: 601.078730
2019-01-08 19:03:51,441 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 19:03:51,930 - 10 - training_embed.py - training - Epoch: [10][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.4368 (600.4576)	
2019-01-08 19:03:52,130 - 10 - training_embed.py - training - Epoch: [10][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.1409 (600.4782)	
2019-01-08 19:03:52,368 - 10 - training_embed.py - training - Epoch: [10][300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 596.4793 (600.3137)	
2019-01-08 19:03:52,575 - 10 - training_embed.py - training - Epoch: [10][400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.1743 (600.3465)	
2019-01-08 19:03:52,789 - 10 - training_embed.py - training - Epoch: [10][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.5393 (600.2200)	
2019-01-08 19:03:53,001 - 10 - training_embed.py - training - Epoch: [10][600/3661]	Time 0.006 (0.002)	Data 0.004 (0.001)	Loss 599.2868 (600.0769)	
2019-01-08 19:03:53,228 - 10 - training_embed.py - training - Epoch: [10][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.8215 (599.9261)	
2019-01-08 19:03:53,450 - 10 - training_embed.py - training - Epoch: [10][800/3661]	Time 0.008 (0.002)	Data 0.005 (0.001)	Loss 598.7687 (599.9301)	
2019-01-08 19:03:53,671 - 10 - training_embed.py - training - Epoch: [10][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.4525 (599.8840)	
2019-01-08 19:03:53,888 - 10 - training_embed.py - training - Epoch: [10][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.9272 (599.8101)	
2019-01-08 19:03:54,100 - 10 - training_embed.py - training - Epoch: [10][1100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 597.8841 (599.7912)	
2019-01-08 19:03:54,337 - 10 - training_embed.py - training - Epoch: [10][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.5668 (599.7746)	
2019-01-08 19:03:54,539 - 10 - training_embed.py - training - Epoch: [10][1300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 603.0922 (599.7205)	
2019-01-08 19:03:54,761 - 10 - training_embed.py - training - Epoch: [10][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.4357 (599.7403)	
2019-01-08 19:03:54,977 - 10 - training_embed.py - training - Epoch: [10][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.5553 (599.7706)	
2019-01-08 19:03:55,181 - 10 - training_embed.py - training - Epoch: [10][1600/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 597.3274 (599.7013)	
2019-01-08 19:03:55,415 - 10 - training_embed.py - training - Epoch: [10][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.8875 (599.6761)	
2019-01-08 19:03:55,653 - 10 - training_embed.py - training - Epoch: [10][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.2159 (599.6738)	
2019-01-08 19:03:55,877 - 10 - training_embed.py - training - Epoch: [10][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 592.5096 (599.6983)	
2019-01-08 19:03:56,097 - 10 - training_embed.py - training - Epoch: [10][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.7352 (599.6773)	
2019-01-08 19:03:56,307 - 10 - training_embed.py - training - Epoch: [10][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.2179 (599.6438)	
2019-01-08 19:03:56,529 - 10 - training_embed.py - training - Epoch: [10][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 596.9459 (599.6014)	
2019-01-08 19:03:56,773 - 10 - training_embed.py - training - Epoch: [10][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.5754 (599.5935)	
2019-01-08 19:03:56,992 - 10 - training_embed.py - training - Epoch: [10][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.4210 (599.5788)	
2019-01-08 19:03:57,231 - 10 - training_embed.py - training - Epoch: [10][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.1381 (599.5439)	
2019-01-08 19:03:57,436 - 10 - training_embed.py - training - Epoch: [10][2600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 594.8079 (599.5079)	
2019-01-08 19:03:57,648 - 10 - training_embed.py - training - Epoch: [10][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.1790 (599.5014)	
2019-01-08 19:03:57,853 - 10 - training_embed.py - training - Epoch: [10][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 600.5419 (599.4997)	
2019-01-08 19:03:58,078 - 10 - training_embed.py - training - Epoch: [10][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.1344 (599.4918)	
2019-01-08 19:03:58,316 - 10 - training_embed.py - training - Epoch: [10][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.3482 (599.4572)	
2019-01-08 19:03:58,539 - 10 - training_embed.py - training - Epoch: [10][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.6494 (599.4354)	
2019-01-08 19:03:58,767 - 10 - training_embed.py - training - Epoch: [10][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.4047 (599.4289)	
2019-01-08 19:03:58,979 - 10 - training_embed.py - training - Epoch: [10][3300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 599.6765 (599.4014)	
2019-01-08 19:03:59,207 - 10 - training_embed.py - training - Epoch: [10][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.5173 (599.3975)	
2019-01-08 19:03:59,421 - 10 - training_embed.py - training - Epoch: [10][3500/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 596.8007 (599.3795)	
2019-01-08 19:03:59,659 - 10 - training_embed.py - training - Epoch: [10][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 592.2982 (599.3488)	
2019-01-08 19:03:59,793 - 10 - training_embed.py - training - loss: 599.302127
2019-01-08 19:03:59,794 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 19:04:00,303 - 10 - training_embed.py - training - Epoch: [11][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.3769 (598.6198)	
2019-01-08 19:04:00,514 - 10 - training_embed.py - training - Epoch: [11][200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 598.2873 (598.4781)	
2019-01-08 19:04:00,723 - 10 - training_embed.py - training - Epoch: [11][300/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 602.5997 (598.7895)	
2019-01-08 19:04:00,942 - 10 - training_embed.py - training - Epoch: [11][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.7176 (598.7909)	
2019-01-08 19:04:01,158 - 10 - training_embed.py - training - Epoch: [11][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 597.1812 (598.6958)	
2019-01-08 19:04:01,383 - 10 - training_embed.py - training - Epoch: [11][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.2051 (598.5493)	
2019-01-08 19:04:01,602 - 10 - training_embed.py - training - Epoch: [11][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.7762 (598.4425)	
2019-01-08 19:04:01,805 - 10 - training_embed.py - training - Epoch: [11][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.9490 (598.3515)	
2019-01-08 19:04:02,029 - 10 - training_embed.py - training - Epoch: [11][900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 592.9787 (598.3001)	
2019-01-08 19:04:02,258 - 10 - training_embed.py - training - Epoch: [11][1000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 597.6785 (598.2002)	
2019-01-08 19:04:02,479 - 10 - training_embed.py - training - Epoch: [11][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.2241 (598.1453)	
2019-01-08 19:04:02,700 - 10 - training_embed.py - training - Epoch: [11][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 589.1925 (598.1130)	
2019-01-08 19:04:02,902 - 10 - training_embed.py - training - Epoch: [11][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.9058 (598.0703)	
2019-01-08 19:04:03,118 - 10 - training_embed.py - training - Epoch: [11][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.7275 (597.9985)	
2019-01-08 19:04:03,340 - 10 - training_embed.py - training - Epoch: [11][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.2535 (598.0003)	
2019-01-08 19:04:03,543 - 10 - training_embed.py - training - Epoch: [11][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.0795 (598.0303)	
2019-01-08 19:04:03,743 - 10 - training_embed.py - training - Epoch: [11][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.3303 (597.9892)	
2019-01-08 19:04:03,978 - 10 - training_embed.py - training - Epoch: [11][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.6503 (597.9829)	
2019-01-08 19:04:04,196 - 10 - training_embed.py - training - Epoch: [11][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 589.6840 (597.9692)	
2019-01-08 19:04:04,425 - 10 - training_embed.py - training - Epoch: [11][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 598.2235 (597.9302)	
2019-01-08 19:04:04,647 - 10 - training_embed.py - training - Epoch: [11][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.6389 (597.9087)	
2019-01-08 19:04:04,862 - 10 - training_embed.py - training - Epoch: [11][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.0817 (597.8887)	
2019-01-08 19:04:05,080 - 10 - training_embed.py - training - Epoch: [11][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 592.9001 (597.8664)	
2019-01-08 19:04:05,292 - 10 - training_embed.py - training - Epoch: [11][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.4069 (597.8394)	
2019-01-08 19:04:05,521 - 10 - training_embed.py - training - Epoch: [11][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.8481 (597.8433)	
2019-01-08 19:04:05,722 - 10 - training_embed.py - training - Epoch: [11][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.2693 (597.8416)	
2019-01-08 19:04:05,932 - 10 - training_embed.py - training - Epoch: [11][2700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 601.0314 (597.8158)	
2019-01-08 19:04:06,131 - 10 - training_embed.py - training - Epoch: [11][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.2518 (597.7789)	
2019-01-08 19:04:06,354 - 10 - training_embed.py - training - Epoch: [11][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.6612 (597.7457)	
2019-01-08 19:04:06,560 - 10 - training_embed.py - training - Epoch: [11][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 600.2105 (597.7108)	
2019-01-08 19:04:06,781 - 10 - training_embed.py - training - Epoch: [11][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 593.1653 (597.6889)	
2019-01-08 19:04:07,012 - 10 - training_embed.py - training - Epoch: [11][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 594.1051 (597.6709)	
2019-01-08 19:04:07,224 - 10 - training_embed.py - training - Epoch: [11][3300/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 605.1766 (597.6414)	
2019-01-08 19:04:07,445 - 10 - training_embed.py - training - Epoch: [11][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.0880 (597.6108)	
2019-01-08 19:04:07,666 - 10 - training_embed.py - training - Epoch: [11][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.2478 (597.5757)	
2019-01-08 19:04:07,891 - 10 - training_embed.py - training - Epoch: [11][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 594.3358 (597.5609)	
2019-01-08 19:04:08,037 - 10 - training_embed.py - training - loss: 597.503773
2019-01-08 19:04:08,157 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 19:04:10,094 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1936.071 ms ~ 0.032 min ~ 1.936 sec
2019-01-08 19:04:12,094 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 3936.882 ms ~ 0.066 min ~ 3.937 sec
2019-01-08 19:04:12,095 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 19:04:12,095 - 10 - corpus.py - subactivity_sampler - 0 / 157
2019-01-08 19:04:12,095 - 10 - corpus.py - subactivity_sampler - [117401.  21808.  57954. 108424.  41481.  66903.  60351. 122499.  21856.
  95793.   8052. 161759.  52844.]
2019-01-08 19:09:02,442 - 10 - corpus.py - subactivity_sampler - 20 / 157
2019-01-08 19:09:02,442 - 10 - corpus.py - subactivity_sampler - [117441.  21535.  57303. 109320.  41195.  67005.  60287. 122785.  20947.
  95912.   7893. 163174.  52328.]
2019-01-08 19:13:22,412 - 10 - corpus.py - subactivity_sampler - 40 / 157
2019-01-08 19:13:22,412 - 10 - corpus.py - subactivity_sampler - [118013.  21087.  56721. 109626.  41077.  67061.  60471. 123473.  19891.
  95952.   7812. 163627.  52314.]
2019-01-08 19:18:33,029 - 10 - corpus.py - subactivity_sampler - 60 / 157
2019-01-08 19:18:33,029 - 10 - corpus.py - subactivity_sampler - [118316.  20989.  55896. 110196.  40639.  67370.  59927. 124646.  18837.
  96137.   7651. 164404.  52117.]
2019-01-08 19:23:24,157 - 10 - corpus.py - subactivity_sampler - 80 / 157
2019-01-08 19:23:24,158 - 10 - corpus.py - subactivity_sampler - [118327.  20788.  55680. 110651.  40244.  67489.  59864. 124922.  18324.
  95849.   7217. 165640.  52130.]
2019-01-08 19:28:48,661 - 10 - corpus.py - subactivity_sampler - 100 / 157
2019-01-08 19:28:48,661 - 10 - corpus.py - subactivity_sampler - [118581.  20732.  54900. 111476.  39866.  67518.  59601. 125269.  17990.
  96308.   7115. 165967.  51802.]
2019-01-08 19:32:25,524 - 10 - corpus.py - subactivity_sampler - 120 / 157
2019-01-08 19:32:25,524 - 10 - corpus.py - subactivity_sampler - [119431.  20513.  54273. 111498.  39926.  67612.  59384. 125474.  17598.
  96503.   7055. 166261.  51597.]
2019-01-08 19:38:22,590 - 10 - corpus.py - subactivity_sampler - 140 / 157
2019-01-08 19:38:22,590 - 10 - corpus.py - subactivity_sampler - [119388.  20316.  54200. 111922.  39719.  67792.  59081. 126262.  16524.
  96816.   6998. 166519.  51588.]
2019-01-08 19:43:09,610 - 10 - corpus.py - subactivity_sampler - [119499.  20338.  53765. 112220.  39699.  67786.  58965. 126387.  16169.
  96893.   6965. 166854.  51585.]
2019-01-08 19:43:09,610 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 2337515.735 ms ~ 38.959 min ~ 2337.516 sec
2019-01-08 19:43:09,610 - 10 - corpus.py - ordering_sampler - .
2019-01-08 19:43:20,090 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 19:43:20,090 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 19.  46.  20.  21. 136.  28.  10. 101.  58.  31.   2.   1.]
2019-01-08 19:43:20,090 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 19:43:20,130 - 10 - corpus.py - rho_sampling - ['52.1169', '5.4360', '915.0545', '63.3426', '2.1258', '10.0870', '57.2711', '4.5588', '566.1860', '11.3335', '42.0198', '596.4173']
2019-01-08 19:43:20,130 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 19:43:20,491 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 19:43:20,509 - 10 - accuracy_class.py - mof - # gt_labels: 14   # pr_labels: 13
2019-01-08 19:43:20,510 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 26', '1: 2', '2: 11', '3: 27', '4: 10', '5: 17', '6: 28', '7: 14', '8: 31', '9: 16', '10: 4', '11: 29', '12: 30']
2019-01-08 19:43:20,556 - 10 - accuracy_class.py - mof_val - frames true: 346279	frames overall : 937125
2019-01-08 19:43:20,556 - 10 - corpus.py - accuracy_corpus - Action: pancake
2019-01-08 19:43:20,556 - 10 - corpus.py - accuracy_corpus - MoF val: 0.36951207149526477
2019-01-08 19:43:20,556 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.36951207149526477
2019-01-08 19:43:20,556 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35402
2019-01-08 19:43:20,556 - 10 - accuracy_class.py - mof_classes - label 2: 0.242827  8869 / 36524
2019-01-08 19:43:20,556 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1774
2019-01-08 19:43:20,556 - 10 - accuracy_class.py - mof_classes - label 10: 0.059955  661 / 11025
2019-01-08 19:43:20,556 - 10 - accuracy_class.py - mof_classes - label 11: 0.086358  3964 / 45902
2019-01-08 19:43:20,556 - 10 - accuracy_class.py - mof_classes - label 14: 0.139640  2468 / 17674
2019-01-08 19:43:20,556 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 207
2019-01-08 19:43:20,556 - 10 - accuracy_class.py - mof_classes - label 17: 0.356734  13903 / 38973
2019-01-08 19:43:20,556 - 10 - accuracy_class.py - mof_classes - label 26: 0.788758  42630 / 54047
2019-01-08 19:43:20,556 - 10 - accuracy_class.py - mof_classes - label 27: 0.544831  91724 / 168353
2019-01-08 19:43:20,556 - 10 - accuracy_class.py - mof_classes - label 28: 0.277751  17400 / 62646
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - mof_classes - label 29: 0.331830  142866 / 430540
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - mof_classes - label 30: 0.763523  21794 / 28544
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - mof_classes - label 31: 0.000000  0 / 5514
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - mof_classes - average class mof: 0.256586
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35402
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 2: 0.184798  8869 / 47993
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 8739
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 10: 0.013203  661 / 50063
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 11: 0.041420  3964 / 95703
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 14: 0.017430  2468 / 141593
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 97100
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 17: 0.149726  13903 / 92856
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 26: 0.325629  42630 / 130916
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 27: 0.485700  91724 / 188849
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 28: 0.166969  17400 / 104211
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 29: 0.314317  142866 / 454528
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 30: 0.373601  21794 / 58335
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - label 31: 0.000000  0 / 21683
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - average IoU: 0.159446
2019-01-08 19:43:20,557 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.148057
2019-01-08 19:43:35,510 - 10 - f1_score.py - f1 - f1 score: 0.294746
2019-01-08 19:43:35,540 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 15409.967 ms ~ 0.257 min ~ 15.410 sec
2019-01-08 19:43:35,540 - 10 - corpus.py - embedding_training - .
2019-01-08 19:43:35,540 - 10 - dataset_torch.py - load_data - create DataLoader
2019-01-08 19:43:35,540 - 10 - dataset_torch.py - __init__ - FeatureDataset
2019-01-08 19:43:35,540 - 10 - dataset_torch.py - _with_predictions - __init__
2019-01-08 19:43:41,026 - 10 - training_embed.py - training - create model
2019-01-08 19:43:41,029 - 10 - training_embed.py - training - epochs: 12
2019-01-08 19:43:41,029 - 10 - training_embed.py - training - Epoch # 0
2019-01-08 19:43:41,471 - 10 - training_embed.py - training - Epoch: [0][100/3661]	Time 0.002 (0.004)	Data 0.001 (0.003)	Loss 613.8749 (617.2538)	
2019-01-08 19:43:41,690 - 10 - training_embed.py - training - Epoch: [0][200/3661]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 611.8350 (617.1104)	
2019-01-08 19:43:41,921 - 10 - training_embed.py - training - Epoch: [0][300/3661]	Time 0.002 (0.003)	Data 0.001 (0.002)	Loss 616.2720 (617.4117)	
2019-01-08 19:43:42,135 - 10 - training_embed.py - training - Epoch: [0][400/3661]	Time 0.003 (0.003)	Data 0.001 (0.002)	Loss 621.0977 (617.4176)	
2019-01-08 19:43:42,340 - 10 - training_embed.py - training - Epoch: [0][500/3661]	Time 0.001 (0.003)	Data 0.000 (0.001)	Loss 615.1974 (617.4055)	
2019-01-08 19:43:42,562 - 10 - training_embed.py - training - Epoch: [0][600/3661]	Time 0.002 (0.003)	Data 0.001 (0.001)	Loss 617.4690 (617.5063)	
2019-01-08 19:43:42,790 - 10 - training_embed.py - training - Epoch: [0][700/3661]	Time 0.003 (0.003)	Data 0.002 (0.001)	Loss 613.8897 (617.5001)	
2019-01-08 19:43:43,006 - 10 - training_embed.py - training - Epoch: [0][800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.1526 (617.4139)	
2019-01-08 19:43:43,237 - 10 - training_embed.py - training - Epoch: [0][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2817 (617.3985)	
2019-01-08 19:43:43,455 - 10 - training_embed.py - training - Epoch: [0][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.3013 (617.3476)	
2019-01-08 19:43:43,668 - 10 - training_embed.py - training - Epoch: [0][1100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.7259 (617.2625)	
2019-01-08 19:43:43,892 - 10 - training_embed.py - training - Epoch: [0][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 621.0578 (617.2551)	
2019-01-08 19:43:44,127 - 10 - training_embed.py - training - Epoch: [0][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.4359 (617.2722)	
2019-01-08 19:43:44,368 - 10 - training_embed.py - training - Epoch: [0][1400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 613.7193 (617.2334)	
2019-01-08 19:43:44,583 - 10 - training_embed.py - training - Epoch: [0][1500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.3417 (617.1779)	
2019-01-08 19:43:44,798 - 10 - training_embed.py - training - Epoch: [0][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.6115 (617.1202)	
2019-01-08 19:43:45,021 - 10 - training_embed.py - training - Epoch: [0][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2394 (617.1312)	
2019-01-08 19:43:45,249 - 10 - training_embed.py - training - Epoch: [0][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.5424 (617.0562)	
2019-01-08 19:43:45,462 - 10 - training_embed.py - training - Epoch: [0][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.6299 (617.0371)	
2019-01-08 19:43:45,693 - 10 - training_embed.py - training - Epoch: [0][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 628.8546 (617.0419)	
2019-01-08 19:43:45,912 - 10 - training_embed.py - training - Epoch: [0][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.7482 (617.0174)	
2019-01-08 19:43:46,124 - 10 - training_embed.py - training - Epoch: [0][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.9307 (616.9815)	
2019-01-08 19:43:46,341 - 10 - training_embed.py - training - Epoch: [0][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.4574 (616.9663)	
2019-01-08 19:43:46,544 - 10 - training_embed.py - training - Epoch: [0][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.4469 (616.9432)	
2019-01-08 19:43:46,783 - 10 - training_embed.py - training - Epoch: [0][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.5311 (616.9242)	
2019-01-08 19:43:46,999 - 10 - training_embed.py - training - Epoch: [0][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 618.5323 (616.8931)	
2019-01-08 19:43:47,209 - 10 - training_embed.py - training - Epoch: [0][2700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 621.5635 (616.8717)	
2019-01-08 19:43:47,418 - 10 - training_embed.py - training - Epoch: [0][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.5599 (616.8510)	
2019-01-08 19:43:47,633 - 10 - training_embed.py - training - Epoch: [0][2900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 617.3939 (616.8211)	
2019-01-08 19:43:47,849 - 10 - training_embed.py - training - Epoch: [0][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.5211 (616.7999)	
2019-01-08 19:43:48,072 - 10 - training_embed.py - training - Epoch: [0][3100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 624.2057 (616.7625)	
2019-01-08 19:43:48,292 - 10 - training_embed.py - training - Epoch: [0][3200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.1414 (616.7449)	
2019-01-08 19:43:48,512 - 10 - training_embed.py - training - Epoch: [0][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.2888 (616.7095)	
2019-01-08 19:43:48,728 - 10 - training_embed.py - training - Epoch: [0][3400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 621.9723 (616.6807)	
2019-01-08 19:43:48,964 - 10 - training_embed.py - training - Epoch: [0][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 621.2112 (616.6735)	
2019-01-08 19:43:49,214 - 10 - training_embed.py - training - Epoch: [0][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.6057 (616.6629)	
2019-01-08 19:43:49,347 - 10 - training_embed.py - training - loss: 616.593840
2019-01-08 19:43:49,348 - 10 - training_embed.py - training - Epoch # 1
2019-01-08 19:43:49,835 - 10 - training_embed.py - training - Epoch: [1][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3926 (616.0412)	
2019-01-08 19:43:50,064 - 10 - training_embed.py - training - Epoch: [1][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.2399 (615.7904)	
2019-01-08 19:43:50,275 - 10 - training_embed.py - training - Epoch: [1][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.4966 (615.8866)	
2019-01-08 19:43:50,502 - 10 - training_embed.py - training - Epoch: [1][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.8087 (615.5710)	
2019-01-08 19:43:50,711 - 10 - training_embed.py - training - Epoch: [1][500/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 618.8175 (615.6178)	
2019-01-08 19:43:50,919 - 10 - training_embed.py - training - Epoch: [1][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.6219 (615.5287)	
2019-01-08 19:43:51,143 - 10 - training_embed.py - training - Epoch: [1][700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.1718 (615.5094)	
2019-01-08 19:43:51,360 - 10 - training_embed.py - training - Epoch: [1][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.9969 (615.4815)	
2019-01-08 19:43:51,583 - 10 - training_embed.py - training - Epoch: [1][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 619.6192 (615.3275)	
2019-01-08 19:43:51,792 - 10 - training_embed.py - training - Epoch: [1][1000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 615.6290 (615.3843)	
2019-01-08 19:43:51,993 - 10 - training_embed.py - training - Epoch: [1][1100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.5411 (615.3634)	
2019-01-08 19:43:52,204 - 10 - training_embed.py - training - Epoch: [1][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.4064 (615.2747)	
2019-01-08 19:43:52,449 - 10 - training_embed.py - training - Epoch: [1][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 620.0494 (615.2721)	
2019-01-08 19:43:52,673 - 10 - training_embed.py - training - Epoch: [1][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.5953 (615.2672)	
2019-01-08 19:43:52,914 - 10 - training_embed.py - training - Epoch: [1][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 619.9852 (615.2545)	
2019-01-08 19:43:53,132 - 10 - training_embed.py - training - Epoch: [1][1600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.7552 (615.2318)	
2019-01-08 19:43:53,360 - 10 - training_embed.py - training - Epoch: [1][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.6055 (615.1963)	
2019-01-08 19:43:53,590 - 10 - training_embed.py - training - Epoch: [1][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.1744 (615.1890)	
2019-01-08 19:43:53,797 - 10 - training_embed.py - training - Epoch: [1][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.4741 (615.1675)	
2019-01-08 19:43:54,036 - 10 - training_embed.py - training - Epoch: [1][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 618.5843 (615.1553)	
2019-01-08 19:43:54,253 - 10 - training_embed.py - training - Epoch: [1][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.1536 (615.1426)	
2019-01-08 19:43:54,468 - 10 - training_embed.py - training - Epoch: [1][2200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.5283 (615.0978)	
2019-01-08 19:43:54,685 - 10 - training_embed.py - training - Epoch: [1][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.2516 (615.0733)	
2019-01-08 19:43:54,909 - 10 - training_embed.py - training - Epoch: [1][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.5363 (615.0523)	
2019-01-08 19:43:55,112 - 10 - training_embed.py - training - Epoch: [1][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.0571 (615.0273)	
2019-01-08 19:43:55,343 - 10 - training_embed.py - training - Epoch: [1][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.6790 (614.9931)	
2019-01-08 19:43:55,556 - 10 - training_embed.py - training - Epoch: [1][2700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 618.6699 (614.9690)	
2019-01-08 19:43:55,775 - 10 - training_embed.py - training - Epoch: [1][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.6968 (614.9372)	
2019-01-08 19:43:55,996 - 10 - training_embed.py - training - Epoch: [1][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.7316 (614.9308)	
2019-01-08 19:43:56,207 - 10 - training_embed.py - training - Epoch: [1][3000/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 609.8922 (614.8920)	
2019-01-08 19:43:56,435 - 10 - training_embed.py - training - Epoch: [1][3100/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 614.6242 (614.8604)	
2019-01-08 19:43:56,657 - 10 - training_embed.py - training - Epoch: [1][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.6490 (614.8428)	
2019-01-08 19:43:56,864 - 10 - training_embed.py - training - Epoch: [1][3300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.1099 (614.8169)	
2019-01-08 19:43:57,066 - 10 - training_embed.py - training - Epoch: [1][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3619 (614.7862)	
2019-01-08 19:43:57,305 - 10 - training_embed.py - training - Epoch: [1][3500/3661]	Time 0.006 (0.002)	Data 0.001 (0.001)	Loss 612.8981 (614.8000)	
2019-01-08 19:43:57,508 - 10 - training_embed.py - training - Epoch: [1][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.4053 (614.7670)	
2019-01-08 19:43:57,661 - 10 - training_embed.py - training - loss: 614.727181
2019-01-08 19:43:57,661 - 10 - training_embed.py - training - Epoch # 2
2019-01-08 19:43:58,142 - 10 - training_embed.py - training - Epoch: [2][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.9484 (613.4565)	
2019-01-08 19:43:58,374 - 10 - training_embed.py - training - Epoch: [2][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.5539 (613.8618)	
2019-01-08 19:43:58,593 - 10 - training_embed.py - training - Epoch: [2][300/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 612.4203 (613.8108)	
2019-01-08 19:43:58,840 - 10 - training_embed.py - training - Epoch: [2][400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 615.3839 (613.7619)	
2019-01-08 19:43:59,062 - 10 - training_embed.py - training - Epoch: [2][500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.4657 (613.6031)	
2019-01-08 19:43:59,286 - 10 - training_embed.py - training - Epoch: [2][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.4252 (613.6151)	
2019-01-08 19:43:59,518 - 10 - training_embed.py - training - Epoch: [2][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.0085 (613.6109)	
2019-01-08 19:43:59,740 - 10 - training_embed.py - training - Epoch: [2][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.0898 (613.5900)	
2019-01-08 19:43:59,954 - 10 - training_embed.py - training - Epoch: [2][900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 612.4199 (613.5583)	
2019-01-08 19:44:00,191 - 10 - training_embed.py - training - Epoch: [2][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 619.2775 (613.4940)	
2019-01-08 19:44:00,398 - 10 - training_embed.py - training - Epoch: [2][1100/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 621.4191 (613.4372)	
2019-01-08 19:44:00,608 - 10 - training_embed.py - training - Epoch: [2][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.1417 (613.4591)	
2019-01-08 19:44:00,836 - 10 - training_embed.py - training - Epoch: [2][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3328 (613.4275)	
2019-01-08 19:44:01,055 - 10 - training_embed.py - training - Epoch: [2][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3912 (613.4255)	
2019-01-08 19:44:01,275 - 10 - training_embed.py - training - Epoch: [2][1500/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 606.9802 (613.4446)	
2019-01-08 19:44:01,491 - 10 - training_embed.py - training - Epoch: [2][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8470 (613.4259)	
2019-01-08 19:44:01,704 - 10 - training_embed.py - training - Epoch: [2][1700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.2725 (613.4238)	
2019-01-08 19:44:01,922 - 10 - training_embed.py - training - Epoch: [2][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.3070 (613.3714)	
2019-01-08 19:44:02,156 - 10 - training_embed.py - training - Epoch: [2][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 612.6351 (613.3415)	
2019-01-08 19:44:02,394 - 10 - training_embed.py - training - Epoch: [2][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 613.9764 (613.2843)	
2019-01-08 19:44:02,614 - 10 - training_embed.py - training - Epoch: [2][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.7155 (613.2605)	
2019-01-08 19:44:02,821 - 10 - training_embed.py - training - Epoch: [2][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.7527 (613.2011)	
2019-01-08 19:44:03,026 - 10 - training_embed.py - training - Epoch: [2][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.8862 (613.1795)	
2019-01-08 19:44:03,254 - 10 - training_embed.py - training - Epoch: [2][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.8378 (613.1445)	
2019-01-08 19:44:03,466 - 10 - training_embed.py - training - Epoch: [2][2500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.3041 (613.1332)	
2019-01-08 19:44:03,722 - 10 - training_embed.py - training - Epoch: [2][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.9456 (613.1155)	
2019-01-08 19:44:03,936 - 10 - training_embed.py - training - Epoch: [2][2700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 611.0937 (613.1053)	
2019-01-08 19:44:04,157 - 10 - training_embed.py - training - Epoch: [2][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.5115 (613.1034)	
2019-01-08 19:44:04,404 - 10 - training_embed.py - training - Epoch: [2][2900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.1325 (613.0870)	
2019-01-08 19:44:04,629 - 10 - training_embed.py - training - Epoch: [2][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.3935 (613.0585)	
2019-01-08 19:44:04,848 - 10 - training_embed.py - training - Epoch: [2][3100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 614.4092 (613.0369)	
2019-01-08 19:44:05,085 - 10 - training_embed.py - training - Epoch: [2][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.3257 (613.0105)	
2019-01-08 19:44:05,309 - 10 - training_embed.py - training - Epoch: [2][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 623.0156 (613.0022)	
2019-01-08 19:44:05,519 - 10 - training_embed.py - training - Epoch: [2][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.5641 (612.9632)	
2019-01-08 19:44:05,761 - 10 - training_embed.py - training - Epoch: [2][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.4662 (612.9433)	
2019-01-08 19:44:06,009 - 10 - training_embed.py - training - Epoch: [2][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.2979 (612.9210)	
2019-01-08 19:44:06,136 - 10 - training_embed.py - training - loss: 612.862751
2019-01-08 19:44:06,136 - 10 - training_embed.py - training - Epoch # 3
2019-01-08 19:44:06,620 - 10 - training_embed.py - training - Epoch: [3][100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.2253 (612.1218)	
2019-01-08 19:44:06,840 - 10 - training_embed.py - training - Epoch: [3][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 619.3021 (612.0768)	
2019-01-08 19:44:07,065 - 10 - training_embed.py - training - Epoch: [3][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.4799 (611.9565)	
2019-01-08 19:44:07,304 - 10 - training_embed.py - training - Epoch: [3][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.8077 (611.8740)	
2019-01-08 19:44:07,508 - 10 - training_embed.py - training - Epoch: [3][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.9266 (611.8671)	
2019-01-08 19:44:07,718 - 10 - training_embed.py - training - Epoch: [3][600/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 612.9293 (611.8202)	
2019-01-08 19:44:07,931 - 10 - training_embed.py - training - Epoch: [3][700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 604.7751 (611.8250)	
2019-01-08 19:44:08,170 - 10 - training_embed.py - training - Epoch: [3][800/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 613.1145 (611.8371)	
2019-01-08 19:44:08,406 - 10 - training_embed.py - training - Epoch: [3][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.3741 (611.8335)	
2019-01-08 19:44:08,620 - 10 - training_embed.py - training - Epoch: [3][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.4464 (611.7683)	
2019-01-08 19:44:08,836 - 10 - training_embed.py - training - Epoch: [3][1100/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 617.5140 (611.7464)	
2019-01-08 19:44:09,043 - 10 - training_embed.py - training - Epoch: [3][1200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 616.0114 (611.7382)	
2019-01-08 19:44:09,274 - 10 - training_embed.py - training - Epoch: [3][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.2311 (611.7694)	
2019-01-08 19:44:09,502 - 10 - training_embed.py - training - Epoch: [3][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.4280 (611.7206)	
2019-01-08 19:44:09,748 - 10 - training_embed.py - training - Epoch: [3][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.7372 (611.6835)	
2019-01-08 19:44:09,963 - 10 - training_embed.py - training - Epoch: [3][1600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.9568 (611.6477)	
2019-01-08 19:44:10,192 - 10 - training_embed.py - training - Epoch: [3][1700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 608.7105 (611.6127)	
2019-01-08 19:44:10,417 - 10 - training_embed.py - training - Epoch: [3][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.7220 (611.5888)	
2019-01-08 19:44:10,632 - 10 - training_embed.py - training - Epoch: [3][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 610.4604 (611.5240)	
2019-01-08 19:44:10,878 - 10 - training_embed.py - training - Epoch: [3][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.9130 (611.4974)	
2019-01-08 19:44:11,084 - 10 - training_embed.py - training - Epoch: [3][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.3895 (611.4875)	
2019-01-08 19:44:11,317 - 10 - training_embed.py - training - Epoch: [3][2200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 614.5698 (611.4616)	
2019-01-08 19:44:11,532 - 10 - training_embed.py - training - Epoch: [3][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.5317 (611.4305)	
2019-01-08 19:44:11,755 - 10 - training_embed.py - training - Epoch: [3][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.9684 (611.3927)	
2019-01-08 19:44:11,991 - 10 - training_embed.py - training - Epoch: [3][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.3331 (611.3801)	
2019-01-08 19:44:12,205 - 10 - training_embed.py - training - Epoch: [3][2600/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 601.7239 (611.3204)	
2019-01-08 19:44:12,406 - 10 - training_embed.py - training - Epoch: [3][2700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 603.5202 (611.3057)	
2019-01-08 19:44:12,610 - 10 - training_embed.py - training - Epoch: [3][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.2348 (611.2561)	
2019-01-08 19:44:12,846 - 10 - training_embed.py - training - Epoch: [3][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.1377 (611.2199)	
2019-01-08 19:44:13,082 - 10 - training_embed.py - training - Epoch: [3][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6138 (611.1875)	
2019-01-08 19:44:13,306 - 10 - training_embed.py - training - Epoch: [3][3100/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 609.5154 (611.1409)	
2019-01-08 19:44:13,513 - 10 - training_embed.py - training - Epoch: [3][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.4639 (611.1009)	
2019-01-08 19:44:13,716 - 10 - training_embed.py - training - Epoch: [3][3300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 611.7970 (611.0681)	
2019-01-08 19:44:13,927 - 10 - training_embed.py - training - Epoch: [3][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.2986 (611.0437)	
2019-01-08 19:44:14,156 - 10 - training_embed.py - training - Epoch: [3][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.0860 (611.0583)	
2019-01-08 19:44:14,393 - 10 - training_embed.py - training - Epoch: [3][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.0162 (611.0455)	
2019-01-08 19:44:14,523 - 10 - training_embed.py - training - loss: 610.999192
2019-01-08 19:44:14,523 - 10 - training_embed.py - training - Epoch # 4
2019-01-08 19:44:14,997 - 10 - training_embed.py - training - Epoch: [4][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.4986 (609.8864)	
2019-01-08 19:44:15,225 - 10 - training_embed.py - training - Epoch: [4][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.0915 (609.6058)	
2019-01-08 19:44:15,444 - 10 - training_embed.py - training - Epoch: [4][300/3661]	Time 0.007 (0.002)	Data 0.003 (0.001)	Loss 609.2719 (609.6763)	
2019-01-08 19:44:15,673 - 10 - training_embed.py - training - Epoch: [4][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.9187 (610.0439)	
2019-01-08 19:44:15,885 - 10 - training_embed.py - training - Epoch: [4][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.7194 (609.9072)	
2019-01-08 19:44:16,097 - 10 - training_embed.py - training - Epoch: [4][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.3707 (609.9536)	
2019-01-08 19:44:16,322 - 10 - training_embed.py - training - Epoch: [4][700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.8957 (609.8454)	
2019-01-08 19:44:16,542 - 10 - training_embed.py - training - Epoch: [4][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.0709 (609.8339)	
2019-01-08 19:44:16,754 - 10 - training_embed.py - training - Epoch: [4][900/3661]	Time 0.004 (0.002)	Data 0.001 (0.001)	Loss 605.4427 (609.7810)	
2019-01-08 19:44:16,992 - 10 - training_embed.py - training - Epoch: [4][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.8132 (609.7779)	
2019-01-08 19:44:17,194 - 10 - training_embed.py - training - Epoch: [4][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.6732 (609.7687)	
2019-01-08 19:44:17,413 - 10 - training_embed.py - training - Epoch: [4][1200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 606.5186 (609.7516)	
2019-01-08 19:44:17,639 - 10 - training_embed.py - training - Epoch: [4][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.5133 (609.6904)	
2019-01-08 19:44:17,848 - 10 - training_embed.py - training - Epoch: [4][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.4719 (609.6998)	
2019-01-08 19:44:18,074 - 10 - training_embed.py - training - Epoch: [4][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.0994 (609.6656)	
2019-01-08 19:44:18,295 - 10 - training_embed.py - training - Epoch: [4][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.2940 (609.6462)	
2019-01-08 19:44:18,512 - 10 - training_embed.py - training - Epoch: [4][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.4169 (609.6254)	
2019-01-08 19:44:18,733 - 10 - training_embed.py - training - Epoch: [4][1800/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 602.6235 (609.5985)	
2019-01-08 19:44:18,974 - 10 - training_embed.py - training - Epoch: [4][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.7968 (609.5626)	
2019-01-08 19:44:19,208 - 10 - training_embed.py - training - Epoch: [4][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 611.9344 (609.5724)	
2019-01-08 19:44:19,440 - 10 - training_embed.py - training - Epoch: [4][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.2762 (609.5418)	
2019-01-08 19:44:19,659 - 10 - training_embed.py - training - Epoch: [4][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.4147 (609.4992)	
2019-01-08 19:44:19,871 - 10 - training_embed.py - training - Epoch: [4][2300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.1357 (609.4669)	
2019-01-08 19:44:20,118 - 10 - training_embed.py - training - Epoch: [4][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.8572 (609.4372)	
2019-01-08 19:44:20,331 - 10 - training_embed.py - training - Epoch: [4][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.9869 (609.4506)	
2019-01-08 19:44:20,556 - 10 - training_embed.py - training - Epoch: [4][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.0502 (609.3978)	
2019-01-08 19:44:20,771 - 10 - training_embed.py - training - Epoch: [4][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.1848 (609.3536)	
2019-01-08 19:44:20,980 - 10 - training_embed.py - training - Epoch: [4][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.1855 (609.3589)	
2019-01-08 19:44:21,188 - 10 - training_embed.py - training - Epoch: [4][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.7495 (609.3535)	
2019-01-08 19:44:21,409 - 10 - training_embed.py - training - Epoch: [4][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.8948 (609.3335)	
2019-01-08 19:44:21,633 - 10 - training_embed.py - training - Epoch: [4][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.6232 (609.3360)	
2019-01-08 19:44:21,850 - 10 - training_embed.py - training - Epoch: [4][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.8872 (609.3098)	
2019-01-08 19:44:22,068 - 10 - training_embed.py - training - Epoch: [4][3300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 610.5918 (609.2733)	
2019-01-08 19:44:22,286 - 10 - training_embed.py - training - Epoch: [4][3400/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 607.2780 (609.2482)	
2019-01-08 19:44:22,516 - 10 - training_embed.py - training - Epoch: [4][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.3831 (609.2327)	
2019-01-08 19:44:22,724 - 10 - training_embed.py - training - Epoch: [4][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.1277 (609.2035)	
2019-01-08 19:44:22,874 - 10 - training_embed.py - training - loss: 609.132042
2019-01-08 19:44:22,875 - 10 - training_embed.py - training - Epoch # 5
2019-01-08 19:44:23,343 - 10 - training_embed.py - training - Epoch: [5][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.8355 (608.6980)	
2019-01-08 19:44:23,552 - 10 - training_embed.py - training - Epoch: [5][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.3549 (608.8295)	
2019-01-08 19:44:23,786 - 10 - training_embed.py - training - Epoch: [5][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.5676 (608.7288)	
2019-01-08 19:44:24,014 - 10 - training_embed.py - training - Epoch: [5][400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 597.5873 (608.5346)	
2019-01-08 19:44:24,235 - 10 - training_embed.py - training - Epoch: [5][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.9128 (608.4748)	
2019-01-08 19:44:24,440 - 10 - training_embed.py - training - Epoch: [5][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.4096 (608.3664)	
2019-01-08 19:44:24,645 - 10 - training_embed.py - training - Epoch: [5][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 617.8602 (608.3475)	
2019-01-08 19:44:24,867 - 10 - training_embed.py - training - Epoch: [5][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.1884 (608.2597)	
2019-01-08 19:44:25,098 - 10 - training_embed.py - training - Epoch: [5][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.8416 (608.1643)	
2019-01-08 19:44:25,329 - 10 - training_embed.py - training - Epoch: [5][1000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.1779 (608.0738)	
2019-01-08 19:44:25,547 - 10 - training_embed.py - training - Epoch: [5][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5369 (608.0089)	
2019-01-08 19:44:25,753 - 10 - training_embed.py - training - Epoch: [5][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.8783 (607.9822)	
2019-01-08 19:44:25,960 - 10 - training_embed.py - training - Epoch: [5][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 614.7738 (607.9697)	
2019-01-08 19:44:26,187 - 10 - training_embed.py - training - Epoch: [5][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.8837 (607.9632)	
2019-01-08 19:44:26,412 - 10 - training_embed.py - training - Epoch: [5][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.5146 (607.9394)	
2019-01-08 19:44:26,623 - 10 - training_embed.py - training - Epoch: [5][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.7840 (607.9187)	
2019-01-08 19:44:26,826 - 10 - training_embed.py - training - Epoch: [5][1700/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 608.8816 (607.8584)	
2019-01-08 19:44:27,040 - 10 - training_embed.py - training - Epoch: [5][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 610.7862 (607.8106)	
2019-01-08 19:44:27,261 - 10 - training_embed.py - training - Epoch: [5][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 611.2858 (607.7734)	
2019-01-08 19:44:27,470 - 10 - training_embed.py - training - Epoch: [5][2000/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 605.0737 (607.7428)	
2019-01-08 19:44:27,708 - 10 - training_embed.py - training - Epoch: [5][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.2607 (607.7124)	
2019-01-08 19:44:27,940 - 10 - training_embed.py - training - Epoch: [5][2200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 605.2654 (607.6973)	
2019-01-08 19:44:28,168 - 10 - training_embed.py - training - Epoch: [5][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.6424 (607.6610)	
2019-01-08 19:44:28,390 - 10 - training_embed.py - training - Epoch: [5][2400/3661]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 602.8794 (607.6543)	
2019-01-08 19:44:28,617 - 10 - training_embed.py - training - Epoch: [5][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.0616 (607.6265)	
2019-01-08 19:44:28,857 - 10 - training_embed.py - training - Epoch: [5][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.4462 (607.5832)	
2019-01-08 19:44:29,076 - 10 - training_embed.py - training - Epoch: [5][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.5910 (607.5596)	
2019-01-08 19:44:29,280 - 10 - training_embed.py - training - Epoch: [5][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.8018 (607.5207)	
2019-01-08 19:44:29,504 - 10 - training_embed.py - training - Epoch: [5][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.9647 (607.4907)	
2019-01-08 19:44:29,728 - 10 - training_embed.py - training - Epoch: [5][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.1660 (607.4775)	
2019-01-08 19:44:29,961 - 10 - training_embed.py - training - Epoch: [5][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.2013 (607.4397)	
2019-01-08 19:44:30,219 - 10 - training_embed.py - training - Epoch: [5][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.6796 (607.4111)	
2019-01-08 19:44:30,426 - 10 - training_embed.py - training - Epoch: [5][3300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 607.0307 (607.3649)	
2019-01-08 19:44:30,644 - 10 - training_embed.py - training - Epoch: [5][3400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 605.5461 (607.3509)	
2019-01-08 19:44:30,885 - 10 - training_embed.py - training - Epoch: [5][3500/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 608.6715 (607.3363)	
2019-01-08 19:44:31,100 - 10 - training_embed.py - training - Epoch: [5][3600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 597.9217 (607.3015)	
2019-01-08 19:44:31,248 - 10 - training_embed.py - training - loss: 607.257594
2019-01-08 19:44:31,248 - 10 - training_embed.py - training - Epoch # 6
2019-01-08 19:44:31,708 - 10 - training_embed.py - training - Epoch: [6][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 615.4965 (605.9942)	
2019-01-08 19:44:31,925 - 10 - training_embed.py - training - Epoch: [6][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 616.0634 (606.0997)	
2019-01-08 19:44:32,153 - 10 - training_embed.py - training - Epoch: [6][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.9808 (606.1130)	
2019-01-08 19:44:32,378 - 10 - training_embed.py - training - Epoch: [6][400/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 606.2604 (606.2903)	
2019-01-08 19:44:32,620 - 10 - training_embed.py - training - Epoch: [6][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.2693 (606.2367)	
2019-01-08 19:44:32,848 - 10 - training_embed.py - training - Epoch: [6][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.0972 (606.1747)	
2019-01-08 19:44:33,071 - 10 - training_embed.py - training - Epoch: [6][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 609.5610 (606.2646)	
2019-01-08 19:44:33,312 - 10 - training_embed.py - training - Epoch: [6][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.9708 (606.2202)	
2019-01-08 19:44:33,527 - 10 - training_embed.py - training - Epoch: [6][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.0909 (606.1905)	
2019-01-08 19:44:33,743 - 10 - training_embed.py - training - Epoch: [6][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.2051 (606.1207)	
2019-01-08 19:44:33,963 - 10 - training_embed.py - training - Epoch: [6][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.7235 (606.0764)	
2019-01-08 19:44:34,193 - 10 - training_embed.py - training - Epoch: [6][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 605.6799 (606.0171)	
2019-01-08 19:44:34,436 - 10 - training_embed.py - training - Epoch: [6][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.6977 (605.9776)	
2019-01-08 19:44:34,666 - 10 - training_embed.py - training - Epoch: [6][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.6093 (606.0001)	
2019-01-08 19:44:34,901 - 10 - training_embed.py - training - Epoch: [6][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.7513 (606.0086)	
2019-01-08 19:44:35,119 - 10 - training_embed.py - training - Epoch: [6][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.8478 (605.9416)	
2019-01-08 19:44:35,341 - 10 - training_embed.py - training - Epoch: [6][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.1495 (605.8904)	
2019-01-08 19:44:35,543 - 10 - training_embed.py - training - Epoch: [6][1800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 607.7921 (605.8739)	
2019-01-08 19:44:35,762 - 10 - training_embed.py - training - Epoch: [6][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.2448 (605.8660)	
2019-01-08 19:44:35,965 - 10 - training_embed.py - training - Epoch: [6][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.9907 (605.8089)	
2019-01-08 19:44:36,198 - 10 - training_embed.py - training - Epoch: [6][2100/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.6664 (605.7683)	
2019-01-08 19:44:36,433 - 10 - training_embed.py - training - Epoch: [6][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.8760 (605.7333)	
2019-01-08 19:44:36,655 - 10 - training_embed.py - training - Epoch: [6][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.8658 (605.7302)	
2019-01-08 19:44:36,888 - 10 - training_embed.py - training - Epoch: [6][2400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.6926 (605.7139)	
2019-01-08 19:44:37,110 - 10 - training_embed.py - training - Epoch: [6][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.4529 (605.6910)	
2019-01-08 19:44:37,341 - 10 - training_embed.py - training - Epoch: [6][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.8517 (605.6883)	
2019-01-08 19:44:37,547 - 10 - training_embed.py - training - Epoch: [6][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.9471 (605.6850)	
2019-01-08 19:44:37,758 - 10 - training_embed.py - training - Epoch: [6][2800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 612.2766 (605.6599)	
2019-01-08 19:44:37,968 - 10 - training_embed.py - training - Epoch: [6][2900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 609.5938 (605.6363)	
2019-01-08 19:44:38,191 - 10 - training_embed.py - training - Epoch: [6][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.0990 (605.6058)	
2019-01-08 19:44:38,411 - 10 - training_embed.py - training - Epoch: [6][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 612.0618 (605.5632)	
2019-01-08 19:44:38,629 - 10 - training_embed.py - training - Epoch: [6][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.5182 (605.5438)	
2019-01-08 19:44:38,841 - 10 - training_embed.py - training - Epoch: [6][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.5581 (605.5276)	
2019-01-08 19:44:39,058 - 10 - training_embed.py - training - Epoch: [6][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.6653 (605.4969)	
2019-01-08 19:44:39,288 - 10 - training_embed.py - training - Epoch: [6][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.1048 (605.4642)	
2019-01-08 19:44:39,494 - 10 - training_embed.py - training - Epoch: [6][3600/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 609.4810 (605.4343)	
2019-01-08 19:44:39,635 - 10 - training_embed.py - training - loss: 605.373316
2019-01-08 19:44:39,636 - 10 - training_embed.py - training - Epoch # 7
2019-01-08 19:44:40,124 - 10 - training_embed.py - training - Epoch: [7][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.6381 (604.3912)	
2019-01-08 19:44:40,327 - 10 - training_embed.py - training - Epoch: [7][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.9677 (604.4956)	
2019-01-08 19:44:40,554 - 10 - training_embed.py - training - Epoch: [7][300/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.1723 (604.5888)	
2019-01-08 19:44:40,770 - 10 - training_embed.py - training - Epoch: [7][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.7571 (604.4775)	
2019-01-08 19:44:41,007 - 10 - training_embed.py - training - Epoch: [7][500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 603.7805 (604.4261)	
2019-01-08 19:44:41,245 - 10 - training_embed.py - training - Epoch: [7][600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.6682 (604.3446)	
2019-01-08 19:44:41,461 - 10 - training_embed.py - training - Epoch: [7][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.5754 (604.2732)	
2019-01-08 19:44:41,686 - 10 - training_embed.py - training - Epoch: [7][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.9675 (604.2002)	
2019-01-08 19:44:41,898 - 10 - training_embed.py - training - Epoch: [7][900/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 604.5292 (604.1676)	
2019-01-08 19:44:42,118 - 10 - training_embed.py - training - Epoch: [7][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.4009 (604.1137)	
2019-01-08 19:44:42,325 - 10 - training_embed.py - training - Epoch: [7][1100/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 601.4341 (604.0981)	
2019-01-08 19:44:42,532 - 10 - training_embed.py - training - Epoch: [7][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.2382 (603.9849)	
2019-01-08 19:44:42,760 - 10 - training_embed.py - training - Epoch: [7][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.0554 (604.0802)	
2019-01-08 19:44:42,993 - 10 - training_embed.py - training - Epoch: [7][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.5568 (604.0543)	
2019-01-08 19:44:43,216 - 10 - training_embed.py - training - Epoch: [7][1500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.9582 (604.0063)	
2019-01-08 19:44:43,439 - 10 - training_embed.py - training - Epoch: [7][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.8071 (603.9594)	
2019-01-08 19:44:43,660 - 10 - training_embed.py - training - Epoch: [7][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 613.3242 (603.9757)	
2019-01-08 19:44:43,896 - 10 - training_embed.py - training - Epoch: [7][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.5775 (603.9339)	
2019-01-08 19:44:44,121 - 10 - training_embed.py - training - Epoch: [7][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.5182 (603.9181)	
2019-01-08 19:44:44,326 - 10 - training_embed.py - training - Epoch: [7][2000/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 599.5347 (603.9106)	
2019-01-08 19:44:44,573 - 10 - training_embed.py - training - Epoch: [7][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.1002 (603.8962)	
2019-01-08 19:44:44,778 - 10 - training_embed.py - training - Epoch: [7][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.1265 (603.8912)	
2019-01-08 19:44:44,999 - 10 - training_embed.py - training - Epoch: [7][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.7349 (603.8397)	
2019-01-08 19:44:45,215 - 10 - training_embed.py - training - Epoch: [7][2400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 598.7461 (603.8320)	
2019-01-08 19:44:45,438 - 10 - training_embed.py - training - Epoch: [7][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.7995 (603.8000)	
2019-01-08 19:44:45,662 - 10 - training_embed.py - training - Epoch: [7][2600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.5015 (603.7679)	
2019-01-08 19:44:45,871 - 10 - training_embed.py - training - Epoch: [7][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.8165 (603.7329)	
2019-01-08 19:44:46,082 - 10 - training_embed.py - training - Epoch: [7][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.8065 (603.7236)	
2019-01-08 19:44:46,289 - 10 - training_embed.py - training - Epoch: [7][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.0441 (603.7136)	
2019-01-08 19:44:46,518 - 10 - training_embed.py - training - Epoch: [7][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.5137 (603.7226)	
2019-01-08 19:44:46,739 - 10 - training_embed.py - training - Epoch: [7][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.3422 (603.6777)	
2019-01-08 19:44:46,969 - 10 - training_embed.py - training - Epoch: [7][3200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 609.1677 (603.6390)	
2019-01-08 19:44:47,170 - 10 - training_embed.py - training - Epoch: [7][3300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 597.7516 (603.6184)	
2019-01-08 19:44:47,382 - 10 - training_embed.py - training - Epoch: [7][3400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 600.0234 (603.5899)	
2019-01-08 19:44:47,598 - 10 - training_embed.py - training - Epoch: [7][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 604.8803 (603.5327)	
2019-01-08 19:44:47,854 - 10 - training_embed.py - training - Epoch: [7][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 594.2013 (603.5141)	
2019-01-08 19:44:47,989 - 10 - training_embed.py - training - loss: 603.475107
2019-01-08 19:44:47,989 - 10 - training_embed.py - training - Epoch # 8
2019-01-08 19:44:48,488 - 10 - training_embed.py - training - Epoch: [8][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6515 (602.4472)	
2019-01-08 19:44:48,704 - 10 - training_embed.py - training - Epoch: [8][200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 600.0851 (602.3645)	
2019-01-08 19:44:48,937 - 10 - training_embed.py - training - Epoch: [8][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.8567 (602.2938)	
2019-01-08 19:44:49,162 - 10 - training_embed.py - training - Epoch: [8][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.6757 (602.4529)	
2019-01-08 19:44:49,401 - 10 - training_embed.py - training - Epoch: [8][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 608.8212 (602.3383)	
2019-01-08 19:44:49,605 - 10 - training_embed.py - training - Epoch: [8][600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.3111 (602.2791)	
2019-01-08 19:44:49,837 - 10 - training_embed.py - training - Epoch: [8][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.8339 (602.2490)	
2019-01-08 19:44:50,080 - 10 - training_embed.py - training - Epoch: [8][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.7029 (602.2194)	
2019-01-08 19:44:50,299 - 10 - training_embed.py - training - Epoch: [8][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.7482 (602.2615)	
2019-01-08 19:44:50,523 - 10 - training_embed.py - training - Epoch: [8][1000/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 599.9397 (602.2226)	
2019-01-08 19:44:50,741 - 10 - training_embed.py - training - Epoch: [8][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.0027 (602.2075)	
2019-01-08 19:44:50,960 - 10 - training_embed.py - training - Epoch: [8][1200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 593.6467 (602.2254)	
2019-01-08 19:44:51,172 - 10 - training_embed.py - training - Epoch: [8][1300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 596.1710 (602.1597)	
2019-01-08 19:44:51,406 - 10 - training_embed.py - training - Epoch: [8][1400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.4127 (602.1565)	
2019-01-08 19:44:51,620 - 10 - training_embed.py - training - Epoch: [8][1500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 598.4157 (602.0937)	
2019-01-08 19:44:51,853 - 10 - training_embed.py - training - Epoch: [8][1600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.9484 (602.0808)	
2019-01-08 19:44:52,079 - 10 - training_embed.py - training - Epoch: [8][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 604.3328 (602.0606)	
2019-01-08 19:44:52,298 - 10 - training_embed.py - training - Epoch: [8][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.9177 (602.0480)	
2019-01-08 19:44:52,526 - 10 - training_embed.py - training - Epoch: [8][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.5208 (602.0288)	
2019-01-08 19:44:52,745 - 10 - training_embed.py - training - Epoch: [8][2000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 600.8946 (602.0304)	
2019-01-08 19:44:52,986 - 10 - training_embed.py - training - Epoch: [8][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.1147 (602.0256)	
2019-01-08 19:44:53,198 - 10 - training_embed.py - training - Epoch: [8][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.1130 (602.0025)	
2019-01-08 19:44:53,407 - 10 - training_embed.py - training - Epoch: [8][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.5789 (601.9614)	
2019-01-08 19:44:53,623 - 10 - training_embed.py - training - Epoch: [8][2400/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 603.6531 (601.9460)	
2019-01-08 19:44:53,853 - 10 - training_embed.py - training - Epoch: [8][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.7072 (601.9180)	
2019-01-08 19:44:54,077 - 10 - training_embed.py - training - Epoch: [8][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.0130 (601.8724)	
2019-01-08 19:44:54,281 - 10 - training_embed.py - training - Epoch: [8][2700/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 599.1057 (601.8427)	
2019-01-08 19:44:54,493 - 10 - training_embed.py - training - Epoch: [8][2800/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 606.2024 (601.8114)	
2019-01-08 19:44:54,701 - 10 - training_embed.py - training - Epoch: [8][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.4034 (601.7859)	
2019-01-08 19:44:54,953 - 10 - training_embed.py - training - Epoch: [8][3000/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 600.1609 (601.7622)	
2019-01-08 19:44:55,176 - 10 - training_embed.py - training - Epoch: [8][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.9245 (601.7221)	
2019-01-08 19:44:55,405 - 10 - training_embed.py - training - Epoch: [8][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.3416 (601.7089)	
2019-01-08 19:44:55,614 - 10 - training_embed.py - training - Epoch: [8][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 607.6929 (601.7007)	
2019-01-08 19:44:55,836 - 10 - training_embed.py - training - Epoch: [8][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.7085 (601.6688)	
2019-01-08 19:44:56,059 - 10 - training_embed.py - training - Epoch: [8][3500/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 605.9457 (601.6398)	
2019-01-08 19:44:56,292 - 10 - training_embed.py - training - Epoch: [8][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.7373 (601.6088)	
2019-01-08 19:44:56,431 - 10 - training_embed.py - training - loss: 601.559879
2019-01-08 19:44:56,432 - 10 - training_embed.py - training - Epoch # 9
2019-01-08 19:44:56,899 - 10 - training_embed.py - training - Epoch: [9][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.7709 (600.4158)	
2019-01-08 19:44:57,125 - 10 - training_embed.py - training - Epoch: [9][200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.7137 (600.6272)	
2019-01-08 19:44:57,352 - 10 - training_embed.py - training - Epoch: [9][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.9493 (601.0357)	
2019-01-08 19:44:57,555 - 10 - training_embed.py - training - Epoch: [9][400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 600.6831 (600.8299)	
2019-01-08 19:44:57,791 - 10 - training_embed.py - training - Epoch: [9][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.0380 (600.8288)	
2019-01-08 19:44:58,001 - 10 - training_embed.py - training - Epoch: [9][600/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 602.4168 (600.9185)	
2019-01-08 19:44:58,220 - 10 - training_embed.py - training - Epoch: [9][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.2361 (600.8865)	
2019-01-08 19:44:58,458 - 10 - training_embed.py - training - Epoch: [9][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.9111 (600.7955)	
2019-01-08 19:44:58,666 - 10 - training_embed.py - training - Epoch: [9][900/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 599.7824 (600.7608)	
2019-01-08 19:44:58,885 - 10 - training_embed.py - training - Epoch: [9][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.6046 (600.6718)	
2019-01-08 19:44:59,092 - 10 - training_embed.py - training - Epoch: [9][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.8980 (600.6001)	
2019-01-08 19:44:59,312 - 10 - training_embed.py - training - Epoch: [9][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.0391 (600.5395)	
2019-01-08 19:44:59,532 - 10 - training_embed.py - training - Epoch: [9][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.7750 (600.4605)	
2019-01-08 19:44:59,766 - 10 - training_embed.py - training - Epoch: [9][1400/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 597.3397 (600.4328)	
2019-01-08 19:44:59,973 - 10 - training_embed.py - training - Epoch: [9][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.0060 (600.3652)	
2019-01-08 19:45:00,231 - 10 - training_embed.py - training - Epoch: [9][1600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 592.9280 (600.3297)	
2019-01-08 19:45:00,443 - 10 - training_embed.py - training - Epoch: [9][1700/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 600.1891 (600.2497)	
2019-01-08 19:45:00,655 - 10 - training_embed.py - training - Epoch: [9][1800/3661]	Time 0.005 (0.002)	Data 0.004 (0.001)	Loss 602.7531 (600.2248)	
2019-01-08 19:45:00,881 - 10 - training_embed.py - training - Epoch: [9][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.6775 (600.1597)	
2019-01-08 19:45:01,085 - 10 - training_embed.py - training - Epoch: [9][2000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 593.2204 (600.0833)	
2019-01-08 19:45:01,329 - 10 - training_embed.py - training - Epoch: [9][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.3098 (600.0804)	
2019-01-08 19:45:01,543 - 10 - training_embed.py - training - Epoch: [9][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.8100 (600.0627)	
2019-01-08 19:45:01,755 - 10 - training_embed.py - training - Epoch: [9][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 587.7225 (600.0180)	
2019-01-08 19:45:01,968 - 10 - training_embed.py - training - Epoch: [9][2400/3661]	Time 0.005 (0.002)	Data 0.003 (0.001)	Loss 600.8068 (600.0075)	
2019-01-08 19:45:02,205 - 10 - training_embed.py - training - Epoch: [9][2500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.2121 (599.9495)	
2019-01-08 19:45:02,419 - 10 - training_embed.py - training - Epoch: [9][2600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 608.7933 (599.9443)	
2019-01-08 19:45:02,643 - 10 - training_embed.py - training - Epoch: [9][2700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.6052 (599.8993)	
2019-01-08 19:45:02,846 - 10 - training_embed.py - training - Epoch: [9][2800/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 601.8464 (599.8979)	
2019-01-08 19:45:03,063 - 10 - training_embed.py - training - Epoch: [9][2900/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 593.2952 (599.8614)	
2019-01-08 19:45:03,281 - 10 - training_embed.py - training - Epoch: [9][3000/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 596.3405 (599.8313)	
2019-01-08 19:45:03,498 - 10 - training_embed.py - training - Epoch: [9][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.2567 (599.8063)	
2019-01-08 19:45:03,726 - 10 - training_embed.py - training - Epoch: [9][3200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.7451 (599.7699)	
2019-01-08 19:45:03,957 - 10 - training_embed.py - training - Epoch: [9][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.7313 (599.7454)	
2019-01-08 19:45:04,177 - 10 - training_embed.py - training - Epoch: [9][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.6188 (599.7235)	
2019-01-08 19:45:04,407 - 10 - training_embed.py - training - Epoch: [9][3500/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 606.2933 (599.7048)	
2019-01-08 19:45:04,640 - 10 - training_embed.py - training - Epoch: [9][3600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.6044 (599.6779)	
2019-01-08 19:45:04,777 - 10 - training_embed.py - training - loss: 599.623722
2019-01-08 19:45:04,777 - 10 - training_embed.py - training - Epoch # 10
2019-01-08 19:45:05,267 - 10 - training_embed.py - training - Epoch: [10][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.3173 (599.0310)	
2019-01-08 19:45:05,473 - 10 - training_embed.py - training - Epoch: [10][200/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 593.8196 (598.7970)	
2019-01-08 19:45:05,712 - 10 - training_embed.py - training - Epoch: [10][300/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 595.4468 (598.6787)	
2019-01-08 19:45:05,915 - 10 - training_embed.py - training - Epoch: [10][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 606.6839 (598.7077)	
2019-01-08 19:45:06,144 - 10 - training_embed.py - training - Epoch: [10][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.9157 (598.6059)	
2019-01-08 19:45:06,359 - 10 - training_embed.py - training - Epoch: [10][600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 597.2126 (598.4642)	
2019-01-08 19:45:06,562 - 10 - training_embed.py - training - Epoch: [10][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 591.2029 (598.3625)	
2019-01-08 19:45:06,764 - 10 - training_embed.py - training - Epoch: [10][800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.0020 (598.3623)	
2019-01-08 19:45:06,987 - 10 - training_embed.py - training - Epoch: [10][900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.5497 (598.3077)	
2019-01-08 19:45:07,191 - 10 - training_embed.py - training - Epoch: [10][1000/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 594.1753 (598.2039)	
2019-01-08 19:45:07,420 - 10 - training_embed.py - training - Epoch: [10][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.8802 (598.1906)	
2019-01-08 19:45:07,624 - 10 - training_embed.py - training - Epoch: [10][1200/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 603.7748 (598.1621)	
2019-01-08 19:45:07,847 - 10 - training_embed.py - training - Epoch: [10][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 600.9407 (598.1272)	
2019-01-08 19:45:08,064 - 10 - training_embed.py - training - Epoch: [10][1400/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 592.3063 (598.1276)	
2019-01-08 19:45:08,274 - 10 - training_embed.py - training - Epoch: [10][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.1290 (598.1477)	
2019-01-08 19:45:08,502 - 10 - training_embed.py - training - Epoch: [10][1600/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 596.1754 (598.0881)	
2019-01-08 19:45:08,714 - 10 - training_embed.py - training - Epoch: [10][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.8369 (598.0592)	
2019-01-08 19:45:08,927 - 10 - training_embed.py - training - Epoch: [10][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.2215 (598.0609)	
2019-01-08 19:45:09,138 - 10 - training_embed.py - training - Epoch: [10][1900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 594.1231 (598.0836)	
2019-01-08 19:45:09,357 - 10 - training_embed.py - training - Epoch: [10][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 592.1645 (598.0574)	
2019-01-08 19:45:09,563 - 10 - training_embed.py - training - Epoch: [10][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.3483 (598.0245)	
2019-01-08 19:45:09,796 - 10 - training_embed.py - training - Epoch: [10][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 592.5566 (597.9722)	
2019-01-08 19:45:10,020 - 10 - training_embed.py - training - Epoch: [10][2300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.4741 (597.9659)	
2019-01-08 19:45:10,229 - 10 - training_embed.py - training - Epoch: [10][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.4672 (597.9424)	
2019-01-08 19:45:10,449 - 10 - training_embed.py - training - Epoch: [10][2500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 602.6937 (597.9026)	
2019-01-08 19:45:10,669 - 10 - training_embed.py - training - Epoch: [10][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 591.9627 (597.8702)	
2019-01-08 19:45:10,900 - 10 - training_embed.py - training - Epoch: [10][2700/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 599.2430 (597.8580)	
2019-01-08 19:45:11,138 - 10 - training_embed.py - training - Epoch: [10][2800/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 601.1415 (597.8518)	
2019-01-08 19:45:11,354 - 10 - training_embed.py - training - Epoch: [10][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 601.5312 (597.8447)	
2019-01-08 19:45:11,580 - 10 - training_embed.py - training - Epoch: [10][3000/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 596.0799 (597.8106)	
2019-01-08 19:45:11,822 - 10 - training_embed.py - training - Epoch: [10][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.0677 (597.7860)	
2019-01-08 19:45:12,046 - 10 - training_embed.py - training - Epoch: [10][3200/3661]	Time 0.005 (0.002)	Data 0.002 (0.001)	Loss 600.0439 (597.7840)	
2019-01-08 19:45:12,271 - 10 - training_embed.py - training - Epoch: [10][3300/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 602.0311 (597.7533)	
2019-01-08 19:45:12,484 - 10 - training_embed.py - training - Epoch: [10][3400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.5901 (597.7506)	
2019-01-08 19:45:12,702 - 10 - training_embed.py - training - Epoch: [10][3500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 593.8272 (597.7338)	
2019-01-08 19:45:12,946 - 10 - training_embed.py - training - Epoch: [10][3600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 592.5471 (597.7015)	
2019-01-08 19:45:13,087 - 10 - training_embed.py - training - loss: 597.663758
2019-01-08 19:45:13,087 - 10 - training_embed.py - training - Epoch # 11
2019-01-08 19:45:13,574 - 10 - training_embed.py - training - Epoch: [11][100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.6992 (597.0319)	
2019-01-08 19:45:13,777 - 10 - training_embed.py - training - Epoch: [11][200/3661]	Time 0.002 (0.002)	Data 0.000 (0.001)	Loss 594.8524 (596.9028)	
2019-01-08 19:45:13,992 - 10 - training_embed.py - training - Epoch: [11][300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.5152 (597.0236)	
2019-01-08 19:45:14,214 - 10 - training_embed.py - training - Epoch: [11][400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.0809 (597.1453)	
2019-01-08 19:45:14,421 - 10 - training_embed.py - training - Epoch: [11][500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 597.8758 (597.0725)	
2019-01-08 19:45:14,648 - 10 - training_embed.py - training - Epoch: [11][600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 586.5082 (596.9110)	
2019-01-08 19:45:14,870 - 10 - training_embed.py - training - Epoch: [11][700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.2615 (596.7919)	
2019-01-08 19:45:15,077 - 10 - training_embed.py - training - Epoch: [11][800/3661]	Time 0.004 (0.002)	Data 0.003 (0.001)	Loss 589.4180 (596.6685)	
2019-01-08 19:45:15,307 - 10 - training_embed.py - training - Epoch: [11][900/3661]	Time 0.003 (0.002)	Data 0.001 (0.001)	Loss 595.8937 (596.6232)	
2019-01-08 19:45:15,521 - 10 - training_embed.py - training - Epoch: [11][1000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.2352 (596.5003)	
2019-01-08 19:45:15,748 - 10 - training_embed.py - training - Epoch: [11][1100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 591.6802 (596.4282)	
2019-01-08 19:45:15,979 - 10 - training_embed.py - training - Epoch: [11][1200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 589.6983 (596.3938)	
2019-01-08 19:45:16,180 - 10 - training_embed.py - training - Epoch: [11][1300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.2454 (596.3507)	
2019-01-08 19:45:16,390 - 10 - training_embed.py - training - Epoch: [11][1400/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 599.1107 (596.2804)	
2019-01-08 19:45:16,618 - 10 - training_embed.py - training - Epoch: [11][1500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 603.0513 (596.2511)	
2019-01-08 19:45:16,823 - 10 - training_embed.py - training - Epoch: [11][1600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 597.2514 (596.2552)	
2019-01-08 19:45:17,056 - 10 - training_embed.py - training - Epoch: [11][1700/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.0066 (596.2033)	
2019-01-08 19:45:17,280 - 10 - training_embed.py - training - Epoch: [11][1800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.6630 (596.1937)	
2019-01-08 19:45:17,499 - 10 - training_embed.py - training - Epoch: [11][1900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 591.0617 (596.1794)	
2019-01-08 19:45:17,720 - 10 - training_embed.py - training - Epoch: [11][2000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 594.0877 (596.1430)	
2019-01-08 19:45:17,947 - 10 - training_embed.py - training - Epoch: [11][2100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.6676 (596.1102)	
2019-01-08 19:45:18,163 - 10 - training_embed.py - training - Epoch: [11][2200/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 592.2130 (596.0835)	
2019-01-08 19:45:18,368 - 10 - training_embed.py - training - Epoch: [11][2300/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 591.0379 (596.0714)	
2019-01-08 19:45:18,581 - 10 - training_embed.py - training - Epoch: [11][2400/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.1415 (596.0363)	
2019-01-08 19:45:18,786 - 10 - training_embed.py - training - Epoch: [11][2500/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 599.2744 (596.0410)	
2019-01-08 19:45:19,039 - 10 - training_embed.py - training - Epoch: [11][2600/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 593.8094 (596.0385)	
2019-01-08 19:45:19,258 - 10 - training_embed.py - training - Epoch: [11][2700/3661]	Time 0.006 (0.002)	Data 0.003 (0.001)	Loss 599.3809 (596.0110)	
2019-01-08 19:45:19,483 - 10 - training_embed.py - training - Epoch: [11][2800/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 599.0290 (595.9819)	
2019-01-08 19:45:19,687 - 10 - training_embed.py - training - Epoch: [11][2900/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 598.8998 (595.9515)	
2019-01-08 19:45:19,897 - 10 - training_embed.py - training - Epoch: [11][3000/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 596.3777 (595.9178)	
2019-01-08 19:45:20,117 - 10 - training_embed.py - training - Epoch: [11][3100/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 588.4521 (595.9092)	
2019-01-08 19:45:20,325 - 10 - training_embed.py - training - Epoch: [11][3200/3661]	Time 0.001 (0.002)	Data 0.000 (0.001)	Loss 590.6680 (595.8834)	
2019-01-08 19:45:20,556 - 10 - training_embed.py - training - Epoch: [11][3300/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 602.4814 (595.8482)	
2019-01-08 19:45:20,765 - 10 - training_embed.py - training - Epoch: [11][3400/3661]	Time 0.004 (0.002)	Data 0.002 (0.001)	Loss 596.7302 (595.8131)	
2019-01-08 19:45:20,986 - 10 - training_embed.py - training - Epoch: [11][3500/3661]	Time 0.002 (0.002)	Data 0.001 (0.001)	Loss 595.1718 (595.7711)	
2019-01-08 19:45:21,205 - 10 - training_embed.py - training - Epoch: [11][3600/3661]	Time 0.003 (0.002)	Data 0.002 (0.001)	Loss 593.0402 (595.7425)	
2019-01-08 19:45:21,345 - 10 - training_embed.py - training - loss: 595.676594
2019-01-08 19:45:21,465 - 10 - corpus.py - one_gaussian_model - Fit Gaussian Mixture Model to the whole dataset at once
2019-01-08 19:45:23,359 - 10 - utils.py - wrap - <function Corpus._gaussians_fit at 0x7f392a299268> took 1893.608 ms ~ 0.032 min ~ 1.894 sec
2019-01-08 19:45:25,377 - 10 - utils.py - wrap - <function Corpus.one_gaussian_model at 0x7f392a299378> took 3912.110 ms ~ 0.065 min ~ 3.912 sec
2019-01-08 19:45:25,378 - 10 - corpus.py - subactivity_sampler - .
2019-01-08 19:45:25,378 - 10 - corpus.py - subactivity_sampler - 0 / 157
2019-01-08 19:45:25,378 - 10 - corpus.py - subactivity_sampler - [119499.  20338.  53765. 112220.  39699.  67786.  58965. 126387.  16169.
  96893.   6965. 166854.  51585.]
2019-01-08 19:50:19,942 - 10 - corpus.py - subactivity_sampler - 20 / 157
2019-01-08 19:50:19,942 - 10 - corpus.py - subactivity_sampler - [119969.  19747.  53247. 112858.  39340.  67980.  58735. 127371.  15515.
  96921.   6634. 167718.  51090.]
2019-01-08 19:54:44,010 - 10 - corpus.py - subactivity_sampler - 40 / 157
2019-01-08 19:54:44,010 - 10 - corpus.py - subactivity_sampler - [120154.  19574.  52831. 113026.  39433.  68131.  58302. 127733.  14845.
  96971.   6588. 168410.  51127.]
2019-01-08 19:59:52,504 - 10 - corpus.py - subactivity_sampler - 60 / 157
2019-01-08 19:59:52,504 - 10 - corpus.py - subactivity_sampler - [120247.  19597.  52494. 113314.  39361.  68798.  57195. 128350.  13936.
  97431.   6562. 168913.  50927.]
2019-01-08 20:04:41,158 - 10 - corpus.py - subactivity_sampler - 80 / 157
2019-01-08 20:04:41,158 - 10 - corpus.py - subactivity_sampler - [120564.  19570.  51468. 114219.  39230.  68832.  56499. 129112.  13507.
  97422.   6408. 169512.  50782.]
2019-01-08 20:10:02,099 - 10 - corpus.py - subactivity_sampler - 100 / 157
2019-01-08 20:10:02,099 - 10 - corpus.py - subactivity_sampler - [120931.  19045.  51392. 114388.  39250.  68626.  55990. 129693.  13383.
  97460.   6136. 170457.  50374.]
2019-01-08 20:13:38,482 - 10 - corpus.py - subactivity_sampler - 120 / 157
2019-01-08 20:13:38,483 - 10 - corpus.py - subactivity_sampler - [121306.  19010.  50970. 114457.  39099.  68762.  55440. 130029.  13159.
  97097.   6038. 171439.  50319.]
2019-01-08 20:19:33,966 - 10 - corpus.py - subactivity_sampler - 140 / 157
2019-01-08 20:19:33,966 - 10 - corpus.py - subactivity_sampler - [121467.  18938.  51123. 114533.  38856.  68751.  55153. 130430.  12706.
  97195.   5986. 171629.  50358.]
2019-01-08 20:24:20,929 - 10 - corpus.py - subactivity_sampler - [121470.  18651.  50652. 115165.  38823.  68480.  54828. 131259.  12462.
  97089.   5953. 171940.  50353.]
2019-01-08 20:24:20,929 - 10 - utils.py - wrap - <function Corpus.subactivity_sampler at 0x7f392a299730> took 2335551.433 ms ~ 38.926 min ~ 2335.551 sec
2019-01-08 20:24:20,929 - 10 - corpus.py - ordering_sampler - .
2019-01-08 20:24:31,495 - 10 - corpus.py - ordering_sampler - total background: 0
2019-01-08 20:24:31,495 - 10 - corpus.py - ordering_sampler - inv_count_vec: [ 17.  40.   0.  22. 134.  44.  15. 115.   2.  36.   3.   1.]
2019-01-08 20:24:31,496 - 10 - corpus.py - rho_sampling - rho sampling
2019-01-08 20:24:31,518 - 10 - corpus.py - rho_sampling - ['49.6207', '0.4617', '11.7299', '1.3090', '17.2768', '9.4823', '2.5544', '83.5274', '1.8739', '54.1328', '10.0276', '292.0573']
2019-01-08 20:24:31,518 - 10 - pipeline.py - baseline - Iteration 6
2019-01-08 20:24:31,901 - 10 - accuracy_class.py - _create_voting_table - No background!
2019-01-08 20:24:31,920 - 10 - accuracy_class.py - mof - # gt_labels: 14   # pr_labels: 13
2019-01-08 20:24:31,921 - 10 - accuracy_class.py - mof - Correspondences: segmentation to gt : ['-1: 0', '0: 26', '1: 2', '2: 11', '3: 27', '4: 10', '5: 17', '6: 28', '7: 14', '8: 31', '9: 16', '10: 4', '11: 29', '12: 30']
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_val - frames true: 352098	frames overall : 937125
2019-01-08 20:24:31,972 - 10 - corpus.py - accuracy_corpus - Action: pancake
2019-01-08 20:24:31,972 - 10 - corpus.py - accuracy_corpus - MoF val: 0.3757214885954382
2019-01-08 20:24:31,972 - 10 - corpus.py - accuracy_corpus - previous dic -> MoF val: 0.3757214885954382
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 0: 0.000000  0 / 35402
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 2: 0.227193  8298 / 36524
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 4: 0.000000  0 / 1774
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 10: 0.043991  485 / 11025
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 11: 0.086663  3978 / 45902
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 14: 0.160462  2836 / 17674
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 16: 0.000000  0 / 207
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 17: 0.351448  13697 / 38973
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 26: 0.794827  42958 / 54047
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 27: 0.557198  93806 / 168353
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 28: 0.280752  17588 / 62646
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 29: 0.340751  146707 / 430540
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 30: 0.761806  21745 / 28544
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - label 31: 0.000000  0 / 5514
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - mof_classes - average class mof: 0.257507
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - iou_classes - label 0: 0.000000  0 / 35402
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - iou_classes - label 2: 0.177016  8298 / 46877
2019-01-08 20:24:31,972 - 10 - accuracy_class.py - iou_classes - label 4: 0.000000  0 / 7727
2019-01-08 20:24:31,973 - 10 - accuracy_class.py - iou_classes - label 10: 0.009825  485 / 49363
2019-01-08 20:24:31,973 - 10 - accuracy_class.py - iou_classes - label 11: 0.042970  3978 / 92576
2019-01-08 20:24:31,973 - 10 - accuracy_class.py - iou_classes - label 14: 0.019412  2836 / 146097
2019-01-08 20:24:31,973 - 10 - accuracy_class.py - iou_classes - label 16: 0.000000  0 / 97296
2019-01-08 20:24:31,973 - 10 - accuracy_class.py - iou_classes - label 17: 0.146092  13697 / 93756
2019-01-08 20:24:31,973 - 10 - accuracy_class.py - iou_classes - label 26: 0.324067  42958 / 132559
2019-01-08 20:24:31,973 - 10 - accuracy_class.py - iou_classes - label 27: 0.494465  93806 / 189712
2019-01-08 20:24:31,973 - 10 - accuracy_class.py - iou_classes - label 28: 0.176081  17588 / 99886
2019-01-08 20:24:31,973 - 10 - accuracy_class.py - iou_classes - label 29: 0.321886  146707 / 455773
2019-01-08 20:24:31,973 - 10 - accuracy_class.py - iou_classes - label 30: 0.380477  21745 / 57152
2019-01-08 20:24:31,973 - 10 - accuracy_class.py - iou_classes - label 31: 0.000000  0 / 17976
2019-01-08 20:24:31,973 - 10 - accuracy_class.py - iou_classes - average IoU: 0.160945
2019-01-08 20:24:31,973 - 10 - accuracy_class.py - iou_classes - average IoU with bg: 0.149449
2019-01-08 20:24:46,974 - 10 - f1_score.py - f1 - f1 score: 0.296945
2019-01-08 20:24:47,003 - 10 - utils.py - wrap - <function Corpus.accuracy_corpus at 0x7f392a299ae8> took 15485.022 ms ~ 0.258 min ~ 15.485 sec
2019-01-08 20:24:47,035 - 10 - utils.py - wrap - <function baseline at 0x7f3991d24e18> took 17456010.349 ms ~ 290.934 min ~ 17456.010 sec
